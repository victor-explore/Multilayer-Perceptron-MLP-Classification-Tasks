{"cells":[{"cell_type":"markdown","metadata":{"id":"MeB6E_4ATJcd"},"source":["# **3 - Classification tasks**"]},{"cell_type":"markdown","metadata":{"id":"kEZXH-a4TVS-"},"source":["## **3.1.1 - MLP - Binary classification**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKV2MDvgXnBb"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6258,"status":"ok","timestamp":1711355003192,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"KIW52XkSXp3o","outputId":"1d96568f-4b40-4a5c-ae58-1ee379de8fe8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osGIQQLoX-J-"},"outputs":[],"source":["# Load the dataset\n","data_path = '/content/drive/MyDrive/PRNN Assignment 2/multi_class_classification_data_group_25_train.txt'\n","data = np.genfromtxt(data_path, delimiter='\\t', skip_header=1)\n","\n","# Separate features and labels\n","X = data[:, :-1]  # All rows, all columns except the last one\n","y = data[:, -1].astype(int)  # Last column, convert to integer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5o4iyUafYGY-"},"outputs":[],"source":["# Shuffle the dataset\n","np.random.seed(42)  # For reproducibility\n","indices = np.arange(X.shape[0])\n","np.random.shuffle(indices)\n","\n","X = X[indices]\n","y = y[indices]\n","\n","# Split the data\n","split_idx = int(0.8 * X.shape[0])  # 80% for training\n","\n","X_train, X_test = X[:split_idx], X[split_idx:]\n","y_train, y_test = y[:split_idx], y[split_idx:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GEHRhhYYIpe"},"outputs":[],"source":["def one_hot_encode(y):\n","    \"\"\"\n","    Convert an array of labels to one-hot encoded format.\n","    \"\"\"\n","    num_classes = np.unique(y).shape[0]\n","    one_hot = np.zeros((y.shape[0], num_classes))\n","    one_hot[np.arange(y.shape[0]), y] = 1\n","    return one_hot\n","\n","# Convert labels to one-hot encoding\n","y_train_one_hot = one_hot_encode(y_train)\n","y_test_one_hot = one_hot_encode(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLg6wuUVYNUE"},"outputs":[],"source":["def normalize_features(X):\n","    \"\"\"\n","    Normalize features to have mean=0 and std=1.\n","    \"\"\"\n","    mean = np.mean(X, axis=0)\n","    std = np.std(X, axis=0)\n","    return (X - mean) / std\n","\n","# Normalize training and testing sets\n","X_train_normalized = normalize_features(X_train)\n","X_test_normalized = normalize_features(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxT7UOpabs9H"},"outputs":[],"source":["class FullyConnectedNeuralNetwork:\n","    def __init__(self, input_size, output_size, hidden_layers, loss_function, learning_rate=0.01, l1_reg=0.0, l2_reg=0.0):\n","        \"\"\"\n","        Initializes the neural network with the given architecture, specified loss function, and learning rate.\n","\n","        Parameters:\n","        - input_size (int): The size of the input layer (number of input features).\n","        - output_size (int): The size of the output layer (number of output classes or values).\n","        - hidden_layers (list): List of integers specifying the number of nodes in each hidden layer.\n","        - loss_function (str): String indicating the type of loss function to use ('mse' for mean squared error, etc.).\n","        - learning_rate (float): The learning rate for the training process. Default is 0.01.\n","        \"\"\"\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_layers = hidden_layers\n","        self.loss_function = loss_function\n","        self.learning_rate = learning_rate  # Set the learning rate here\n","        self.l1_reg = l1_reg\n","        self.l2_reg = l2_reg\n","\n","        # Initialize network architecture\n","        self.weights = []  # List to store weights matrices\n","        self.biases = []   # List to store bias vectors\n","        self._initialize_weights_biases()\n","\n","    def _initialize_weights_biases(self):\n","        \"\"\"\n","        Initializes weights and biases for all layers in the network using small random values.\n","        \"\"\"\n","        # Create a list of all layer sizes (input layer + hidden layers + output layer)\n","        layer_sizes = [self.input_size] + self.hidden_layers + [self.output_size]\n","\n","        # Initialize weights and biases for each layer\n","        for i in range(len(layer_sizes) - 1):\n","            # Weight matrix shape: (current layer size, next layer size)\n","            weight_matrix = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n","            # Bias vector shape: (next layer size,)\n","            bias_vector = np.zeros((layer_sizes[i+1],))\n","\n","            self.weights.append(weight_matrix)\n","            self.biases.append(bias_vector)\n","\n","    def relu(self, Z):\n","        \"\"\"\n","        Applies the ReLU activation function element-wise to the input array.\n","        \"\"\"\n","        A = np.maximum(0, Z)\n","        # Debug: Print some statistics of the activations\n","        print(\"ReLU Activation - Max:\", np.max(A), \"Min:\", np.min(A))\n","        return A\n","\n","    def softmax(self, Z):\n","        \"\"\"\n","        Applies the softmax function to each row of the input array.\n","        \"\"\"\n","        shiftZ = Z - np.max(Z, axis=1, keepdims=True)  # Shift for numerical stability\n","        exps = np.exp(shiftZ)\n","        softmax_outputs = exps / np.sum(exps, axis=1, keepdims=True)\n","        # Debug: Print some statistics of the softmax outputs\n","        print(\"Softmax Output - Max:\", np.max(softmax_outputs), \"Min:\", np.min(softmax_outputs), \"Sum (first example):\", np.sum(softmax_outputs[0]))\n","        return softmax_outputs\n","\n","    def forward_pass(self, X):\n","        \"\"\"\n","        Performs the forward pass through the network and returns the output.\n","\n","        Parameters:\n","        - X (np.array): Input data array (batch size, input_size).\n","\n","        Returns:\n","        - np.array: The output of the network after passing through all layers.\n","        \"\"\"\n","        A = X  # Initialize A with the input; A will hold the output of the current layer\n","        activations = []  # List to store intermediate activations for use in backpropagation\n","\n","        # Iterate through all layers except the last one\n","        for i in range(len(self.weights) - 1):\n","            Z = np.dot(A, self.weights[i]) + self.biases[i]  # Compute the linear combination\n","            A = self.relu(Z)  # Apply ReLU activation function\n","            activations.append(A)  # Store the activation for later use\n","\n","        # Last layer (output layer)\n","        Z_final = np.dot(A, self.weights[-1]) + self.biases[-1]\n","        if self.loss_function == 'crossentropy':\n","            # Use softmax for classification tasks\n","            A_final = self.softmax(Z_final)\n","        else:\n","            # For regression tasks (or other tasks not using softmax), apply a linear activation\n","            A_final = Z_final  # Linear activation for the output layer\n","\n","        activations.append(A_final)  # Store final activation\n","\n","        return A_final, activations\n","\n","    def compute_loss(self, predicted, actual):\n","        \"\"\"\n","        Computes the loss using the specified loss function, adding L1 and L2 regularization penalties.\n","\n","        Parameters:\n","        - predicted (np.array): Predicted probabilities or values from the forward pass.\n","        - actual (np.array): Actual labels (for classification) or values (for regression).\n","\n","        Returns:\n","        - float: The computed loss, including regularization penalties.\n","        \"\"\"\n","        m = actual.shape[0]  # Number of examples\n","        # Base loss calculation\n","        if self.loss_function == 'crossentropy':\n","          log_probs = -np.log(predicted[range(m), actual.argmax(axis=1)])\n","          base_loss = np.sum(log_probs) / m\n","        elif self.loss_function == 'mse':\n","          base_loss = np.sum((predicted - actual) ** 2) / m\n","        else:\n","          raise ValueError(\"Unsupported loss function specified.\")\n","\n","        # L1 Regularization\n","        l1_loss = 0\n","        if self.l1_reg > 0:\n","          l1_loss = np.sum([np.sum(np.abs(w)) for w in self.weights])\n","\n","        # L2 Regularization\n","        l2_loss = 0\n","        if self.l2_reg > 0:\n","          l2_loss = np.sum([np.sum(w ** 2) for w in self.weights])\n","\n","        # Total loss with L1 and L2 regularization\n","        total_loss = base_loss + (self.l1_reg * l1_loss) + (self.l2_reg * l2_loss) / (2 * m)\n","\n","        return total_loss\n","\n","\n","    def backpropagate(self, input_data, actual, activations):\n","        \"\"\"\n","        Performs backpropagation and updates weights and biases based on the computed gradients,\n","        including L1 and L2 regularization adjustments.\n","\n","        Parameters:\n","        - input_data (np.array): Input data batch.\n","        - actual (np.array): Actual labels for classification or values for regression.\n","        - activations (list): List of activations from the forward pass.\n","        \"\"\"\n","        gradients = []  # To store gradients for weights and biases\n","        m = actual.shape[0]  # Number of examples\n","\n","        # Start with the gradient of the loss w.r.t. the final activation\n","        if self.loss_function == 'crossentropy':\n","            delta = activations[-1] - actual\n","        elif self.loss_function == 'mse':\n","            delta = (activations[-1] - actual) * 1\n","\n","        for i in reversed(range(len(self.weights))):\n","            A_prev = input_data if i == 0 else activations[i-1]\n","\n","            # Compute gradients with L2 regularization adjustment\n","            dW = np.dot(A_prev.T, delta) / m + (self.l2_reg * self.weights[i]) / m\n","            db = np.sum(delta, axis=0) / m\n","            # Debug: Print some statistics of the gradients\n","            print(f\"Layer {i} - Gradient Weights Max:\", np.max(dW), \"Min:\", np.min(dW))\n","\n","            # Prepare for next layer's backpropagation, if not at the input layer\n","            if i > 0:\n","                delta = np.dot(delta, self.weights[i].T) * (A_prev > 0)  # Assuming ReLU activation\n","\n","            gradients.append((dW, db))\n","\n","        # Update weights and biases, including L1 regularization handling\n","        for i in range(len(self.weights)):\n","            dW, db = gradients[-1-i]\n","\n","            # L1 regularization adjustment for weights update\n","            # This simplistic approach adjusts weights directly, simulating sub-gradient for L1\n","            if self.l1_reg > 0:\n","                dW += (self.l1_reg * np.sign(self.weights[i])) / m\n","\n","            self.weights[i] -= self.learning_rate * dW\n","            self.biases[i] -= self.learning_rate * db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nrQsD0UYtQs"},"outputs":[],"source":["\n","model = FullyConnectedNeuralNetwork(\n","    input_size=X_train_normalized.shape[1],\n","    output_size=y_train_one_hot.shape[1],\n","    hidden_layers=[32],\n","    loss_function='crossentropy',\n","    learning_rate=0.05\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZ1DwYLVYvWC"},"outputs":[],"source":["def train(model, X_train, y_train, X_test, y_test, epochs=10000001):\n","    training_losses = []\n","    test_accuracies = []\n","\n","    for epoch in range(epochs):\n","        # Forward pass\n","        predictions, activations = model.forward_pass(X_train)\n","\n","        # Compute loss\n","        loss = model.compute_loss(predictions, y_train)\n","        training_losses.append(loss)\n","\n","        # Backpropagation\n","        model.backpropagate(X_train, y_train, activations)\n","\n","        # Compute test accuracy\n","        test_predictions, _ = model.forward_pass(X_test)\n","        test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n","        test_accuracies.append(test_accuracy)\n","\n","        # Print progress\n","        if epoch % 10 == 0:\n","            print(f\"Epoch {epoch}, Loss: {loss}, Test Accuracy: {test_accuracy}\")\n","\n","    return training_losses, test_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31776,"status":"ok","timestamp":1711355076391,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"LY__PVmWYy4l","outputId":"4c1a0ffc-a107-4ae8-87d9-b6be3d008c77"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Layer 1 - Gradient Weights Max: 0.012335746350370127 Min: -0.012335746350370127\n","Layer 0 - Gradient Weights Max: 0.008295707042487142 Min: -0.006586711168525786\n","ReLU Activation - Max: 0.33394233986189137 Min: 0.0\n","Softmax Output - Max: 0.5283392279491785 Min: 0.47166077205082146 Sum (first example): 1.0\n","Epoch 180, Loss: 0.6869519771384078, Test Accuracy: 0.6975\n","ReLU Activation - Max: 0.3637168876308329 Min: 0.0\n","Softmax Output - Max: 0.5308292044354913 Min: 0.46917079556450864 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012446132835886288 Min: -0.012446132835886288\n","Layer 0 - Gradient Weights Max: 0.008397427102895866 Min: -0.006645734710809468\n","ReLU Activation - Max: 0.33695792825033255 Min: 0.0\n","Softmax Output - Max: 0.5287989044437681 Min: 0.47120109555623196 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 0.36701405960049227 Min: 0.0\n","Softmax Output - Max: 0.5313413888735269 Min: 0.46865861112647306 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012557294804002004 Min: -0.012557294804002002\n","Layer 0 - Gradient Weights Max: 0.00847792800141975 Min: -0.006709698254567465\n","ReLU Activation - Max: 0.3399999805795904 Min: 0.0\n","Softmax Output - Max: 0.5292663763136912 Min: 0.4707336236863088 Sum (first example): 1.0\n","ReLU Activation - Max: 0.3703403616687517 Min: 0.0\n","Softmax Output - Max: 0.5318624721857883 Min: 0.46813752781421164 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012669207156886892 Min: -0.012669207156886892\n","Layer 0 - Gradient Weights Max: 0.008526824030737979 Min: -0.006802376807456744\n","ReLU Activation - Max: 0.34306745624225143 Min: 0.0\n","Softmax Output - Max: 0.5297417260550983 Min: 0.4702582739449017 Sum (first example): 1.0\n","ReLU Activation - Max: 0.37369136640763584 Min: 0.0\n","Softmax Output - Max: 0.5323925636751886 Min: 0.46760743632481144 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.012781806767854127 Min: -0.012781806767854127\n","Layer 0 - Gradient Weights Max: 0.008615287028684608 Min: -0.006871316445461168\n","ReLU Activation - Max: 0.34616405215239904 Min: 0.0\n","Softmax Output - Max: 0.5302253181954618 Min: 0.4697746818045382 Sum (first example): 1.0\n","ReLU Activation - Max: 0.37707274259828716 Min: 0.0\n","Softmax Output - Max: 0.5329319429242442 Min: 0.4670680570757559 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0128950725301926 Min: -0.0128950725301926\n","Layer 0 - Gradient Weights Max: 0.008683859661830846 Min: -0.006941129891143857\n","ReLU Activation - Max: 0.3492868449754198 Min: 0.0\n","Softmax Output - Max: 0.5307172157425347 Min: 0.46928278425746534 Sum (first example): 1.0\n","ReLU Activation - Max: 0.38048758818988165 Min: 0.0\n","Softmax Output - Max: 0.5334807649621096 Min: 0.4665192350378904 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013009123182503798 Min: -0.013009123182503798\n","Layer 0 - Gradient Weights Max: 0.008754488887838866 Min: -0.006999025922950008\n","ReLU Activation - Max: 0.3524363881715414 Min: 0.0\n","Softmax Output - Max: 0.5312172397766683 Min: 0.46878276022333165 Sum (first example): 1.0\n","ReLU Activation - Max: 0.38393655276880084 Min: 0.0\n","Softmax Output - Max: 0.5340390006186518 Min: 0.4659609993813481 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013123971140591783 Min: -0.013123971140591783\n","Layer 0 - Gradient Weights Max: 0.008832927323022871 Min: -0.007075700260567773\n","ReLU Activation - Max: 0.35561640840102515 Min: 0.0\n","Softmax Output - Max: 0.5317258213583799 Min: 0.4682741786416201 Sum (first example): 1.0\n","ReLU Activation - Max: 0.38741539265952607 Min: 0.0\n","Softmax Output - Max: 0.5346069234956412 Min: 0.4653930765043588 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013239570979383091 Min: -0.01323957097938309\n","Layer 0 - Gradient Weights Max: 0.008905540256678609 Min: -0.007139458282473258\n","ReLU Activation - Max: 0.3588229207421686 Min: 0.0\n","Softmax Output - Max: 0.5322430630174272 Min: 0.46775693698257276 Sum (first example): 1.0\n","ReLU Activation - Max: 0.39092777829892145 Min: 0.0\n","Softmax Output - Max: 0.5351845143476197 Min: 0.4648154856523802 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013355911495914887 Min: -0.013355911495914887\n","Layer 0 - Gradient Weights Max: 0.008978307403364631 Min: -0.007184239538577732\n","ReLU Activation - Max: 0.3620561538781054 Min: 0.0\n","Softmax Output - Max: 0.5327690737659011 Min: 0.4672309262340989 Sum (first example): 1.0\n","ReLU Activation - Max: 0.3944724002451834 Min: 0.0\n","Softmax Output - Max: 0.5357720562780692 Min: 0.4642279437219307 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013473018478080842 Min: -0.013473018478080844\n","Layer 0 - Gradient Weights Max: 0.009059982734541918 Min: -0.007229483899397015\n","ReLU Activation - Max: 0.36531632894663835 Min: 0.0\n","Softmax Output - Max: 0.5333037966022998 Min: 0.46669620339770024 Sum (first example): 0.9999999999999999\n","Epoch 190, Loss: 0.6856856354974734, Test Accuracy: 0.7\n","ReLU Activation - Max: 0.3980460698534357 Min: 0.0\n","Softmax Output - Max: 0.5363694880576112 Min: 0.4636305119423888 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013590862307177513 Min: -0.013590862307177515\n","Layer 0 - Gradient Weights Max: 0.009145406842308948 Min: -0.007294897670673882\n","ReLU Activation - Max: 0.368608065651465 Min: 0.0\n","Softmax Output - Max: 0.5338476445367748 Min: 0.4661523554632252 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4016506571161554 Min: 0.0\n","Softmax Output - Max: 0.536977297957256 Min: 0.46302270204274404 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01370945003321085 Min: -0.013709450033210849\n","Layer 0 - Gradient Weights Max: 0.009241188720115734 Min: -0.007364869492623143\n","ReLU Activation - Max: 0.37193097849292306 Min: 0.0\n","Softmax Output - Max: 0.534400664096576 Min: 0.46559933590342384 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4052874072734313 Min: 0.0\n","Softmax Output - Max: 0.5375955089324541 Min: 0.4624044910675459 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013828743003685594 Min: -0.013828743003685594\n","Layer 0 - Gradient Weights Max: 0.009314078732212478 Min: -0.0074413917123738954\n","ReLU Activation - Max: 0.3752812552611602 Min: 0.0\n","Softmax Output - Max: 0.5349628177892752 Min: 0.4650371822107248 Sum (first example): 1.0\n","ReLU Activation - Max: 0.40895395971796966 Min: 0.0\n","Softmax Output - Max: 0.5382242966847993 Min: 0.4617757033152006 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013948746260490343 Min: -0.013948746260490343\n","Layer 0 - Gradient Weights Max: 0.009405105538818305 Min: -0.007502871150227625\n","ReLU Activation - Max: 0.37866257253154356 Min: 0.0\n","Softmax Output - Max: 0.5355344756908786 Min: 0.46446552430912136 Sum (first example): 1.0\n","ReLU Activation - Max: 0.41265500238211594 Min: 0.0\n","Softmax Output - Max: 0.5388638627451214 Min: 0.46113613725487856 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014069452115313748 Min: -0.014069452115313748\n","Layer 0 - Gradient Weights Max: 0.009502831125706396 Min: -0.007553716066436705\n","ReLU Activation - Max: 0.38207335204452725 Min: 0.0\n","Softmax Output - Max: 0.5361156864217034 Min: 0.4638843135782966 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4163883004878562 Min: 0.0\n","Softmax Output - Max: 0.5395140425779713 Min: 0.4604859574220287 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014190858734763433 Min: -0.014190858734763433\n","Layer 0 - Gradient Weights Max: 0.009583793911765821 Min: -0.0076275817064427555\n","ReLU Activation - Max: 0.3855150995904677 Min: 0.0\n","Softmax Output - Max: 0.5367063750522256 Min: 0.4632936249477743 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4201534303119552 Min: 0.0\n","Softmax Output - Max: 0.5401750132888874 Min: 0.4598249867111125 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014313029478842323 Min: -0.014313029478842323\n","Layer 0 - Gradient Weights Max: 0.009665946961393354 Min: -0.007696667298698295\n","ReLU Activation - Max: 0.3889893264890011 Min: 0.0\n","Softmax Output - Max: 0.5373071969104497 Min: 0.4626928030895503 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4239524839405634 Min: 0.0\n","Softmax Output - Max: 0.5408473438662161 Min: 0.45915265613378375 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014435896504898231 Min: -0.014435896504898231\n","Layer 0 - Gradient Weights Max: 0.009762840136702717 Min: -0.007732125104033761\n","ReLU Activation - Max: 0.3924924986646053 Min: 0.0\n","Softmax Output - Max: 0.5379180842777059 Min: 0.4620819157222942 Sum (first example): 1.0\n","ReLU Activation - Max: 0.42778579425607527 Min: 0.0\n","Softmax Output - Max: 0.5415310990480484 Min: 0.4584689009519516 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014559466365236164 Min: -0.014559466365236164\n","Layer 0 - Gradient Weights Max: 0.00986260435777318 Min: -0.007785538663964165\n","ReLU Activation - Max: 0.39602583804863933 Min: 0.0\n","Softmax Output - Max: 0.538539241649775 Min: 0.46146075835022504 Sum (first example): 1.0\n","ReLU Activation - Max: 0.43165549518924806 Min: 0.0\n","Softmax Output - Max: 0.5422266252551221 Min: 0.45777337474487795 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014683669681306896 Min: -0.014683669681306894\n","Layer 0 - Gradient Weights Max: 0.009952895439273288 Min: -0.00786600335958268\n","ReLU Activation - Max: 0.39959164005876724 Min: 0.0\n","Softmax Output - Max: 0.5391708700058319 Min: 0.4608291299941682 Sum (first example): 1.0\n","Epoch 200, Loss: 0.6841771527604397, Test Accuracy: 0.7010714285714286\n","ReLU Activation - Max: 0.4355624610101826 Min: 0.0\n","Softmax Output - Max: 0.5429340926605676 Min: 0.4570659073394323 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014808477646133005 Min: -0.014808477646133005\n","Layer 0 - Gradient Weights Max: 0.010042418343048884 Min: -0.007956741839300846\n","ReLU Activation - Max: 0.4031866534987111 Min: 0.0\n","Softmax Output - Max: 0.539813324905221 Min: 0.4601866750947789 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4395024529371857 Min: 0.0\n","Softmax Output - Max: 0.5436538068998077 Min: 0.4563461931001923 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014933935521810134 Min: -0.014933935521810136\n","Layer 0 - Gradient Weights Max: 0.01013311045718825 Min: -0.008020972385811643\n","ReLU Activation - Max: 0.4068126769925122 Min: 0.0\n","Softmax Output - Max: 0.5404663881823016 Min: 0.45953361181769836 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4434749527690001 Min: 0.0\n","Softmax Output - Max: 0.544385676681019 Min: 0.45561432331898105 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015060018794118292 Min: -0.015060018794118292\n","Layer 0 - Gradient Weights Max: 0.01023365223746103 Min: -0.008120785679031694\n","ReLU Activation - Max: 0.41047283251657635 Min: 0.0\n","Softmax Output - Max: 0.5411303144798519 Min: 0.4588696855201481 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4474842412709404 Min: 0.0\n","Softmax Output - Max: 0.5451297749245464 Min: 0.45487022507545366 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015186766541532103 Min: -0.015186766541532101\n","Layer 0 - Gradient Weights Max: 0.010321158144703024 Min: -0.008176142921944773\n","ReLU Activation - Max: 0.4141624795008881 Min: 0.0\n","Softmax Output - Max: 0.5418051427329269 Min: 0.4581948572670731 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4515281109350432 Min: 0.0\n","Softmax Output - Max: 0.5458864543013433 Min: 0.45411354569865664 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.015314186321426764 Min: -0.015314186321426764\n","Layer 0 - Gradient Weights Max: 0.010427526431093218 Min: -0.008242808423781955\n","ReLU Activation - Max: 0.41788168368182926 Min: 0.0\n","Softmax Output - Max: 0.5424911694962133 Min: 0.45750883050378666 Sum (first example): 1.0\n","ReLU Activation - Max: 0.45560606794659886 Min: 0.0\n","Softmax Output - Max: 0.546655700999514 Min: 0.4533442990004861 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015442216977068973 Min: -0.015442216977068969\n","Layer 0 - Gradient Weights Max: 0.010513882229935906 Min: -0.008312992479936972\n","ReLU Activation - Max: 0.4216343294851743 Min: 0.0\n","Softmax Output - Max: 0.5431885519418301 Min: 0.4568114480581698 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4597188655158176 Min: 0.0\n","Softmax Output - Max: 0.5474378510005283 Min: 0.4525621489994716 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015570820839931965 Min: -0.015570820839931965\n","Layer 0 - Gradient Weights Max: 0.010611389744529603 Min: -0.008399986968196506\n","ReLU Activation - Max: 0.42542174882827993 Min: 0.0\n","Softmax Output - Max: 0.5438973416646897 Min: 0.4561026583353102 Sum (first example): 1.0\n","ReLU Activation - Max: 0.46386843305411435 Min: 0.0\n","Softmax Output - Max: 0.5482329096154754 Min: 0.45176709038452456 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0157000143072898 Min: -0.0157000143072898\n","Layer 0 - Gradient Weights Max: 0.010698784681141481 Min: -0.008489762885122154\n","ReLU Activation - Max: 0.4292438099564695 Min: 0.0\n","Softmax Output - Max: 0.5446176501132525 Min: 0.45538234988674753 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4680513231889205 Min: 0.0\n","Softmax Output - Max: 0.5490411828002012 Min: 0.45095881719979886 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015829751658765445 Min: -0.015829751658765445\n","Layer 0 - Gradient Weights Max: 0.010813719244233298 Min: -0.008569624556960944\n","ReLU Activation - Max: 0.43310129258144386 Min: 0.0\n","Softmax Output - Max: 0.5453498031007346 Min: 0.4546501968992654 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4722689385935643 Min: 0.0\n","Softmax Output - Max: 0.5498628529594668 Min: 0.4501371470405332 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015960069255908208 Min: -0.015960069255908208\n","Layer 0 - Gradient Weights Max: 0.010905411539683794 Min: -0.008642195833268772\n","ReLU Activation - Max: 0.4369898596746039 Min: 0.0\n","Softmax Output - Max: 0.5460939038652374 Min: 0.4539060961347627 Sum (first example): 1.0\n","Epoch 210, Loss: 0.6823884249538432, Test Accuracy: 0.6989285714285715\n","ReLU Activation - Max: 0.47652150003682425 Min: 0.0\n","Softmax Output - Max: 0.5506981034992943 Min: 0.4493018965007057 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016091003403918815 Min: -0.016091003403918815\n","Layer 0 - Gradient Weights Max: 0.011021106829757368 Min: -0.008696445761915882\n","ReLU Activation - Max: 0.4409109788944018 Min: 0.0\n","Softmax Output - Max: 0.5468499920732992 Min: 0.4531500079267009 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4808128042566555 Min: 0.0\n","Softmax Output - Max: 0.551546972191688 Min: 0.44845302780831203 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.016222478102549863 Min: -0.016222478102549863\n","Layer 0 - Gradient Weights Max: 0.011116631607159925 Min: -0.00875244278776111\n","ReLU Activation - Max: 0.44486160954167214 Min: 0.0\n","Softmax Output - Max: 0.547618435093989 Min: 0.45238156490601106 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4851398642195386 Min: 0.0\n","Softmax Output - Max: 0.5524097630792001 Min: 0.44759023692079997 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016354489942630168 Min: -0.01635448994263017\n","Layer 0 - Gradient Weights Max: 0.011206153871800576 Min: -0.00883665372143021\n","ReLU Activation - Max: 0.44884370022494263 Min: 0.0\n","Softmax Output - Max: 0.5483991174948536 Min: 0.4516008825051464 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4895044344204872 Min: 0.0\n","Softmax Output - Max: 0.5532864770879957 Min: 0.44671352291200433 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01648700294620937 Min: -0.01648700294620937\n","Layer 0 - Gradient Weights Max: 0.011350399013599435 Min: -0.00889599275202668\n","ReLU Activation - Max: 0.45286196124568295 Min: 0.0\n","Softmax Output - Max: 0.5491922559973038 Min: 0.45080774400269613 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4939053692064888 Min: 0.0\n","Softmax Output - Max: 0.554177544257297 Min: 0.44582245574270296 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016619975883049032 Min: -0.01661997588304903\n","Layer 0 - Gradient Weights Max: 0.011452309917637122 Min: -0.008984502480512902\n","ReLU Activation - Max: 0.4569148171169882 Min: 0.0\n","Softmax Output - Max: 0.5499978141410226 Min: 0.45000218585897744 Sum (first example): 1.0\n","ReLU Activation - Max: 0.4983461503671728 Min: 0.0\n","Softmax Output - Max: 0.5550826550071905 Min: 0.44491734499280955 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016753490005798878 Min: -0.016753490005798878\n","Layer 0 - Gradient Weights Max: 0.011544576883825928 Min: -0.009066624938474687\n","ReLU Activation - Max: 0.460999639096745 Min: 0.0\n","Softmax Output - Max: 0.5508165300885056 Min: 0.44918346991149444 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5028233599866014 Min: 0.0\n","Softmax Output - Max: 0.5560024055501079 Min: 0.44399759444989206 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01688749134347129 Min: -0.01688749134347129\n","Layer 0 - Gradient Weights Max: 0.011639486888094202 Min: -0.009155298531621813\n","ReLU Activation - Max: 0.46511709311013927 Min: 0.0\n","Softmax Output - Max: 0.5516482664146859 Min: 0.448351733585314 Sum (first example): 1.0\n","ReLU Activation - Max: 0.507338395024243 Min: 0.0\n","Softmax Output - Max: 0.5569367055767076 Min: 0.4430632944232925 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01702199841914497 Min: -0.017021998419144972\n","Layer 0 - Gradient Weights Max: 0.011743741990541429 Min: -0.009234190268866694\n","ReLU Activation - Max: 0.46926967134309994 Min: 0.0\n","Softmax Output - Max: 0.5524927610951575 Min: 0.44750723890484245 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5118908645676662 Min: 0.0\n","Softmax Output - Max: 0.5578856726559167 Min: 0.4421143273440833 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017156960986700033 Min: -0.017156960986700033\n","Layer 0 - Gradient Weights Max: 0.01183800925778011 Min: -0.009311046006082397\n","ReLU Activation - Max: 0.4734559560999596 Min: 0.0\n","Softmax Output - Max: 0.5533508277561341 Min: 0.4466491722438659 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5164809553461129 Min: 0.0\n","Softmax Output - Max: 0.5588500851896392 Min: 0.4411499148103608 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017292355418942834 Min: -0.017292355418942834\n","Layer 0 - Gradient Weights Max: 0.011940097484567768 Min: -0.00939038227712565\n","ReLU Activation - Max: 0.47767436962836407 Min: 0.0\n","Softmax Output - Max: 0.5542224331213418 Min: 0.44577756687865816 Sum (first example): 1.0\n","Epoch 220, Loss: 0.6802832417675841, Test Accuracy: 0.6989285714285715\n","ReLU Activation - Max: 0.5211076876923814 Min: 0.0\n","Softmax Output - Max: 0.5598298812592925 Min: 0.4401701187407076 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017428167736359503 Min: -0.017428167736359503\n","Layer 0 - Gradient Weights Max: 0.012041955693392673 Min: -0.009456069273739738\n","ReLU Activation - Max: 0.4819252068765953 Min: 0.0\n","Softmax Output - Max: 0.5551077914368738 Min: 0.44489220856312617 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5257718175773365 Min: 0.0\n","Softmax Output - Max: 0.5608252538603227 Min: 0.4391747461396774 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.017564374286516835 Min: -0.017564374286516835\n","Layer 0 - Gradient Weights Max: 0.012166208424993085 Min: -0.009550447572061922\n","ReLU Activation - Max: 0.4862110919696906 Min: 0.0\n","Softmax Output - Max: 0.5560069392205682 Min: 0.4439930607794318 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5304751820429274 Min: 0.0\n","Softmax Output - Max: 0.5618363181487331 Min: 0.438163681851267 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.01770093855038499 Min: -0.017700938550384994\n","Layer 0 - Gradient Weights Max: 0.012254469875646528 Min: -0.009613363780273576\n","ReLU Activation - Max: 0.490528781411159 Min: 0.0\n","Softmax Output - Max: 0.556920462487593 Min: 0.44307953751240703 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5352140470866718 Min: 0.0\n","Softmax Output - Max: 0.5628634351547533 Min: 0.4371365648452467 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017837902702731764 Min: -0.01783790270273176\n","Layer 0 - Gradient Weights Max: 0.012353130515736598 Min: -0.009707617561662307\n","ReLU Activation - Max: 0.49488181067707027 Min: 0.0\n","Softmax Output - Max: 0.5578480552001607 Min: 0.44215194479983927 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5399890456912755 Min: 0.0\n","Softmax Output - Max: 0.5639066352265083 Min: 0.43609336477349164 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01797525361357433 Min: -0.01797525361357433\n","Layer 0 - Gradient Weights Max: 0.012449788285119529 Min: -0.009777408198716395\n","ReLU Activation - Max: 0.49926652732061294 Min: 0.0\n","Softmax Output - Max: 0.5587898979594071 Min: 0.44121010204059286 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5448009407712562 Min: 0.0\n","Softmax Output - Max: 0.5649660383990563 Min: 0.43503396160094365 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018112996286160126 Min: -0.018112996286160123\n","Layer 0 - Gradient Weights Max: 0.0125358896655482 Min: -0.0098373121451094\n","ReLU Activation - Max: 0.5036807837791364 Min: 0.0\n","Softmax Output - Max: 0.5597463549395 Min: 0.44025364506049997 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5496521089219725 Min: 0.0\n","Softmax Output - Max: 0.566041692672291 Min: 0.43395830732770907 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01825110554802491 Min: -0.01825110554802491\n","Layer 0 - Gradient Weights Max: 0.012623691327195891 Min: -0.009911492171660631\n","ReLU Activation - Max: 0.5081234124149974 Min: 0.0\n","Softmax Output - Max: 0.5607174068264446 Min: 0.4392825931735555 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5545425323716483 Min: 0.0\n","Softmax Output - Max: 0.5671338641482513 Min: 0.4328661358517486 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018389514261831292 Min: -0.018389514261831295\n","Layer 0 - Gradient Weights Max: 0.012735846727411284 Min: -0.009986140921697728\n","ReLU Activation - Max: 0.5126046238932495 Min: 0.0\n","Softmax Output - Max: 0.5617031464937724 Min: 0.4382968535062276 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5594706978221065 Min: 0.0\n","Softmax Output - Max: 0.5682429439018197 Min: 0.43175705609818027 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01852814172743897 Min: -0.01852814172743897\n","Layer 0 - Gradient Weights Max: 0.012863680415978178 Min: -0.010061854767500922\n","ReLU Activation - Max: 0.5171236011088066 Min: 0.0\n","Softmax Output - Max: 0.5627022109857778 Min: 0.4372977890142223 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5644448474542874 Min: 0.0\n","Softmax Output - Max: 0.5693689385668393 Min: 0.4306310614331606 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018667013014243013 Min: -0.018667013014243016\n","Layer 0 - Gradient Weights Max: 0.012949701662032596 Min: -0.010145544819408677\n","ReLU Activation - Max: 0.5216748145942389 Min: 0.0\n","Softmax Output - Max: 0.5637157157707794 Min: 0.4362842842292207 Sum (first example): 1.0\n","Epoch 230, Loss: 0.6778188789629624, Test Accuracy: 0.6982142857142857\n","ReLU Activation - Max: 0.5694570002013415 Min: 0.0\n","Softmax Output - Max: 0.5705118030156132 Min: 0.42948819698438684 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018806135854513406 Min: -0.018806135854513403\n","Layer 0 - Gradient Weights Max: 0.013052487534635065 Min: -0.01023217272103346\n","ReLU Activation - Max: 0.5262620943158951 Min: 0.0\n","Softmax Output - Max: 0.5647444403147165 Min: 0.43525555968528346 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5745082314081109 Min: 0.0\n","Softmax Output - Max: 0.5716722312046841 Min: 0.4283277687953159 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018945457265495735 Min: -0.018945457265495735\n","Layer 0 - Gradient Weights Max: 0.0131698985646055 Min: -0.010307560390854096\n","ReLU Activation - Max: 0.5308849388995303 Min: 0.0\n","Softmax Output - Max: 0.5657888172690609 Min: 0.434211182730939 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5795996783603956 Min: 0.0\n","Softmax Output - Max: 0.5728504967128254 Min: 0.42714950328717455 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019084935650583706 Min: -0.019084935650583706\n","Layer 0 - Gradient Weights Max: 0.013280355394056363 Min: -0.010387943455033138\n","ReLU Activation - Max: 0.5355430778792755 Min: 0.0\n","Softmax Output - Max: 0.5668483860529081 Min: 0.4331516139470919 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5847343697594604 Min: 0.0\n","Softmax Output - Max: 0.5740460682647232 Min: 0.42595393173527685 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01922449504886599 Min: -0.019224495048865985\n","Layer 0 - Gradient Weights Max: 0.013393192370073453 Min: -0.01049364527785081\n","ReLU Activation - Max: 0.5402413739665267 Min: 0.0\n","Softmax Output - Max: 0.5679233440465596 Min: 0.43207665595344025 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5899054735564218 Min: 0.0\n","Softmax Output - Max: 0.5752593664909896 Min: 0.4247406335090104 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.01936416457442759 Min: -0.01936416457442759\n","Layer 0 - Gradient Weights Max: 0.013493274099149494 Min: -0.01057596973100771\n","ReLU Activation - Max: 0.5449748431540093 Min: 0.0\n","Softmax Output - Max: 0.5690142108317023 Min: 0.4309857891682976 Sum (first example): 1.0\n","ReLU Activation - Max: 0.5951149202499522 Min: 0.0\n","Softmax Output - Max: 0.5764906286031775 Min: 0.4235093713968226 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019503947599678415 Min: -0.019503947599678415\n","Layer 0 - Gradient Weights Max: 0.013572740419962473 Min: -0.010718493340342902\n","ReLU Activation - Max: 0.5497466191081466 Min: 0.0\n","Softmax Output - Max: 0.570121421056094 Min: 0.42987857894390596 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6003601923210895 Min: 0.0\n","Softmax Output - Max: 0.577740290193128 Min: 0.42225970980687205 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019643851653398988 Min: -0.019643851653398985\n","Layer 0 - Gradient Weights Max: 0.013662518387378813 Min: -0.010794416713310159\n","ReLU Activation - Max: 0.5545498276812698 Min: 0.0\n","Softmax Output - Max: 0.5712447541171737 Min: 0.4287552458828262 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6056456315875457 Min: 0.0\n","Softmax Output - Max: 0.5790081598462047 Min: 0.42099184015379537 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01978380968949639 Min: -0.01978380968949639\n","Layer 0 - Gradient Weights Max: 0.013772083484745927 Min: -0.01088650035795083\n","ReLU Activation - Max: 0.5593898544514507 Min: 0.0\n","Softmax Output - Max: 0.5723843458227491 Min: 0.42761565417725084 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6109685765038984 Min: 0.0\n","Softmax Output - Max: 0.5802945705989263 Min: 0.41970542940107375 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019923784176504127 Min: -0.019923784176504124\n","Layer 0 - Gradient Weights Max: 0.013863345027103815 Min: -0.010967660181150628\n","ReLU Activation - Max: 0.564264710223586 Min: 0.0\n","Softmax Output - Max: 0.5735399357503063 Min: 0.4264600642496937 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6163318288271656 Min: 0.0\n","Softmax Output - Max: 0.5815994107711041 Min: 0.41840058922889584 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.020063728431671996 Min: -0.020063728431671996\n","Layer 0 - Gradient Weights Max: 0.01398502125821952 Min: -0.011057843011782814\n","ReLU Activation - Max: 0.5691755472618216 Min: 0.0\n","Softmax Output - Max: 0.5747116653797701 Min: 0.42528833462022986 Sum (first example): 1.0\n","Epoch 240, Loss: 0.6749609121058342, Test Accuracy: 0.6982142857142857\n","ReLU Activation - Max: 0.6217344297984626 Min: 0.0\n","Softmax Output - Max: 0.5829225946751785 Min: 0.41707740532482146 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02020361557416957 Min: -0.02020361557416957\n","Layer 0 - Gradient Weights Max: 0.014073414021430614 Min: -0.011125546618188337\n","ReLU Activation - Max: 0.5741173140563067 Min: 0.0\n","Softmax Output - Max: 0.5758998847415797 Min: 0.42410011525842023 Sum (first example): 1.0\n","ReLU Activation - Max: 0.627175995088274 Min: 0.0\n","Softmax Output - Max: 0.584264734561634 Min: 0.415735265438366 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.020343461453671125 Min: -0.02034346145367112\n","Layer 0 - Gradient Weights Max: 0.014169601867220976 Min: -0.01118358231615005\n","ReLU Activation - Max: 0.5790922820620958 Min: 0.0\n","Softmax Output - Max: 0.5771048941433249 Min: 0.4228951058566751 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6326577042416484 Min: 0.0\n","Softmax Output - Max: 0.5856258980100943 Min: 0.4143741019899056 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.020483177854297367 Min: -0.020483177854297367\n","Layer 0 - Gradient Weights Max: 0.014298148396953958 Min: -0.011302348204492068\n","ReLU Activation - Max: 0.5841064847044914 Min: 0.0\n","Softmax Output - Max: 0.5783269592628868 Min: 0.42167304073711326 Sum (first example): 1.0\n","ReLU Activation - Max: 0.638178643502726 Min: 0.0\n","Softmax Output - Max: 0.5870065781782591 Min: 0.4129934218217409 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02062273618384766 Min: -0.02062273618384766\n","Layer 0 - Gradient Weights Max: 0.014430548165632268 Min: -0.01136513994951783\n","ReLU Activation - Max: 0.5891537620791758 Min: 0.0\n","Softmax Output - Max: 0.5795658938337975 Min: 0.42043410616620247 Sum (first example): 1.0\n","ReLU Activation - Max: 0.643742131216513 Min: 0.0\n","Softmax Output - Max: 0.5884065282768761 Min: 0.41159347172312377 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02076208984885761 Min: -0.02076208984885761\n","Layer 0 - Gradient Weights Max: 0.014531434673618962 Min: -0.011486430356286877\n","ReLU Activation - Max: 0.5942382296476969 Min: 0.0\n","Softmax Output - Max: 0.580822186062278 Min: 0.41917781393772197 Sum (first example): 1.0\n","ReLU Activation - Max: 0.649341966749423 Min: 0.0\n","Softmax Output - Max: 0.5898260525754352 Min: 0.41017394742456476 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.020901237277126537 Min: -0.020901237277126537\n","Layer 0 - Gradient Weights Max: 0.014633563231281115 Min: -0.011576819049073508\n","ReLU Activation - Max: 0.5993542880825387 Min: 0.0\n","Softmax Output - Max: 0.5820954011963922 Min: 0.41790459880360786 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6549819275647122 Min: 0.0\n","Softmax Output - Max: 0.5912645661028968 Min: 0.40873543389710326 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021040152639360682 Min: -0.021040152639360682\n","Layer 0 - Gradient Weights Max: 0.014725791109029933 Min: -0.011661290220414013\n","ReLU Activation - Max: 0.6045029480252758 Min: 0.0\n","Softmax Output - Max: 0.5833856957549068 Min: 0.41661430424509316 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6606613963184215 Min: 0.0\n","Softmax Output - Max: 0.5927227342811483 Min: 0.40727726571885164 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02117880846535745 Min: -0.021178808465357447\n","Layer 0 - Gradient Weights Max: 0.014825472999342821 Min: -0.011745887820110695\n","ReLU Activation - Max: 0.6096865556121831 Min: 0.0\n","Softmax Output - Max: 0.5846933384471326 Min: 0.41530666155286744 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6663789243755147 Min: 0.0\n","Softmax Output - Max: 0.5942001605327926 Min: 0.4057998394672075 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021317169788837503 Min: -0.021317169788837503\n","Layer 0 - Gradient Weights Max: 0.01493465453782041 Min: -0.011834316075208058\n","ReLU Activation - Max: 0.6149061679832658 Min: 0.0\n","Softmax Output - Max: 0.5860186513215191 Min: 0.41398134867848096 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 0.6721306670219278 Min: 0.0\n","Softmax Output - Max: 0.5956974053307137 Min: 0.40430259466928625 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021455186922538498 Min: -0.021455186922538495\n","Layer 0 - Gradient Weights Max: 0.015023373239548487 Min: -0.011909671148106279\n","ReLU Activation - Max: 0.6201586384289977 Min: 0.0\n","Softmax Output - Max: 0.5873618506431998 Min: 0.41263814935680027 Sum (first example): 1.0\n","Epoch 250, Loss: 0.6716777678347876, Test Accuracy: 0.6982142857142857\n","ReLU Activation - Max: 0.6779164714975023 Min: 0.0\n","Softmax Output - Max: 0.5972144750642372 Min: 0.4027855249357628 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021592808145596793 Min: -0.021592808145596797\n","Layer 0 - Gradient Weights Max: 0.015109346646236903 Min: -0.011983173568288622\n","ReLU Activation - Max: 0.6254435605999954 Min: 0.0\n","Softmax Output - Max: 0.5887228708303248 Min: 0.4112771291696751 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6837413178646039 Min: 0.0\n","Softmax Output - Max: 0.5987513726661065 Min: 0.4012486273338936 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021730016873372362 Min: -0.021730016873372362\n","Layer 0 - Gradient Weights Max: 0.015223538146186084 Min: -0.012077650892319589\n","ReLU Activation - Max: 0.6307649168269331 Min: 0.0\n","Softmax Output - Max: 0.5901012181986435 Min: 0.4098987818013566 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6896048450707779 Min: 0.0\n","Softmax Output - Max: 0.6003079684036412 Min: 0.3996920315963588 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021866731580129556 Min: -0.021866731580129556\n","Layer 0 - Gradient Weights Max: 0.015327941356665504 Min: -0.01216789886460289\n","ReLU Activation - Max: 0.6361210263790533 Min: 0.0\n","Softmax Output - Max: 0.5914970170353921 Min: 0.40850298296460785 Sum (first example): 1.0\n","ReLU Activation - Max: 0.6955050309652184 Min: 0.0\n","Softmax Output - Max: 0.6018842631259363 Min: 0.39811573687406365 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022002902018624263 Min: -0.022002902018624267\n","Layer 0 - Gradient Weights Max: 0.015421292890551457 Min: -0.012229255609557784\n","ReLU Activation - Max: 0.6415093714923653 Min: 0.0\n","Softmax Output - Max: 0.5929105532869995 Min: 0.4070894467130004 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7014423273951771 Min: 0.0\n","Softmax Output - Max: 0.6034803897492407 Min: 0.39651961025075944 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022138522107898337 Min: -0.022138522107898337\n","Layer 0 - Gradient Weights Max: 0.015514985122067108 Min: -0.012313555120972115\n","ReLU Activation - Max: 0.6469310598815302 Min: 0.0\n","Softmax Output - Max: 0.5943424529694604 Min: 0.4056575470305397 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7074160437476373 Min: 0.0\n","Softmax Output - Max: 0.6050970321548053 Min: 0.39490296784519474 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02227351859106235 Min: -0.022273518591062347\n","Layer 0 - Gradient Weights Max: 0.01561031567340132 Min: -0.01239574366857037\n","ReLU Activation - Max: 0.6523894956988348 Min: 0.0\n","Softmax Output - Max: 0.5957918254099159 Min: 0.40420817459008407 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 0.7134299935822082 Min: 0.0\n","Softmax Output - Max: 0.6067332483522799 Min: 0.39326675164772007 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022407885760197453 Min: -0.022407885760197453\n","Layer 0 - Gradient Weights Max: 0.015700191517321195 Min: -0.01245035254104145\n","ReLU Activation - Max: 0.6578801342671485 Min: 0.0\n","Softmax Output - Max: 0.5972592495076258 Min: 0.40274075049237434 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7194820453365302 Min: 0.0\n","Softmax Output - Max: 0.6083894164560353 Min: 0.3916105835439647 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022541566690237675 Min: -0.02254156669023768\n","Layer 0 - Gradient Weights Max: 0.015778458776153057 Min: -0.012560390609686143\n","ReLU Activation - Max: 0.6634056775878302 Min: 0.0\n","Softmax Output - Max: 0.5987446577038487 Min: 0.4012553422961513 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7255655203391922 Min: 0.0\n","Softmax Output - Max: 0.6100657839895942 Min: 0.3899342160104058 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022674534880832247 Min: -0.022674534880832247\n","Layer 0 - Gradient Weights Max: 0.01585906207919655 Min: -0.012646570113957572\n","ReLU Activation - Max: 0.6689624239290348 Min: 0.0\n","Softmax Output - Max: 0.6002479305824685 Min: 0.3997520694175314 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7316879029155885 Min: 0.0\n","Softmax Output - Max: 0.6117621539258915 Min: 0.3882378460741085 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022806738581187494 Min: -0.022806738581187494\n","Layer 0 - Gradient Weights Max: 0.015955724515565282 Min: -0.012734115453782096\n","ReLU Activation - Max: 0.6745529152928869 Min: 0.0\n","Softmax Output - Max: 0.601769331014288 Min: 0.398230668985712 Sum (first example): 1.0\n","Epoch 260, Loss: 0.6679523390495171, Test Accuracy: 0.6967857142857142\n","ReLU Activation - Max: 0.7378463988835497 Min: 0.0\n","Softmax Output - Max: 0.6134813042376631 Min: 0.3865186957623369 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02293810234874567 Min: -0.02293810234874567\n","Layer 0 - Gradient Weights Max: 0.016048484333248507 Min: -0.012813273817800796\n","ReLU Activation - Max: 0.6801755878713245 Min: 0.0\n","Softmax Output - Max: 0.6033092089643204 Min: 0.39669079103567967 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7440396572731431 Min: 0.0\n","Softmax Output - Max: 0.6152207082993602 Min: 0.38477929170063985 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023068597979896202 Min: -0.023068597979896202\n","Layer 0 - Gradient Weights Max: 0.016142380779782955 Min: -0.012896244533925107\n","ReLU Activation - Max: 0.6858313370979406 Min: 0.0\n","Softmax Output - Max: 0.6048671777790093 Min: 0.3951328222209906 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7502686656615696 Min: 0.0\n","Softmax Output - Max: 0.6169799843349331 Min: 0.383020015665067 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023198193404212746 Min: -0.023198193404212746\n","Layer 0 - Gradient Weights Max: 0.016235667263148776 Min: -0.012978975267249006\n","ReLU Activation - Max: 0.6915199578177179 Min: 0.0\n","Softmax Output - Max: 0.6064429768582204 Min: 0.3935570231417797 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7565331782180567 Min: 0.0\n","Softmax Output - Max: 0.6187588825656096 Min: 0.3812411174343903 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023326835791380753 Min: -0.023326835791380746\n","Layer 0 - Gradient Weights Max: 0.01632637994709181 Min: -0.013032965188477859\n","ReLU Activation - Max: 0.6972378443113953 Min: 0.0\n","Softmax Output - Max: 0.6080366273848987 Min: 0.3919633726151013 Sum (first example): 1.0\n","ReLU Activation - Max: 0.762831886231845 Min: 0.0\n","Softmax Output - Max: 0.6205574124812198 Min: 0.3794425875187802 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023454460444005114 Min: -0.023454460444005114\n","Layer 0 - Gradient Weights Max: 0.01644654909839697 Min: -0.013104015426948955\n","ReLU Activation - Max: 0.7029926415697116 Min: 0.0\n","Softmax Output - Max: 0.6096475930424659 Min: 0.3903524069575341 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7691691323526588 Min: 0.0\n","Softmax Output - Max: 0.622375638062109 Min: 0.377624361937891 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023581048087704747 Min: -0.023581048087704747\n","Layer 0 - Gradient Weights Max: 0.016528842208641818 Min: -0.013199465228446682\n","ReLU Activation - Max: 0.708779777503525 Min: 0.0\n","Softmax Output - Max: 0.6112763373653568 Min: 0.38872366263464325 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7755420151560521 Min: 0.0\n","Softmax Output - Max: 0.6242135359184828 Min: 0.3757864640815171 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023706549762112974 Min: -0.023706549762112974\n","Layer 0 - Gradient Weights Max: 0.01661555241238821 Min: -0.013291933198778626\n","ReLU Activation - Max: 0.7145963732223993 Min: 0.0\n","Softmax Output - Max: 0.6129231470643162 Min: 0.3870768529356839 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7819489583930077 Min: 0.0\n","Softmax Output - Max: 0.6260713273274594 Min: 0.3739286726725406 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02383090498181963 Min: -0.023830904981819622\n","Layer 0 - Gradient Weights Max: 0.016692151113856284 Min: -0.013369620087227807\n","ReLU Activation - Max: 0.7204468038148278 Min: 0.0\n","Softmax Output - Max: 0.6145871576241347 Min: 0.38541284237586515 Sum (first example): 1.0\n","ReLU Activation - Max: 0.7883935794316717 Min: 0.0\n","Softmax Output - Max: 0.6279481611334806 Min: 0.3720518388665193 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0239540384672848 Min: -0.0239540384672848\n","Layer 0 - Gradient Weights Max: 0.016802931610124944 Min: -0.013436653341841801\n","ReLU Activation - Max: 0.7263334560574363 Min: 0.0\n","Softmax Output - Max: 0.6162688767117481 Min: 0.38373112328825193 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 0.7948703554220761 Min: 0.0\n","Softmax Output - Max: 0.6298444913191593 Min: 0.37015550868084063 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024075883450804872 Min: -0.02407588345080487\n","Layer 0 - Gradient Weights Max: 0.016910629121095522 Min: -0.013524402488704994\n","ReLU Activation - Max: 0.7322561057777545 Min: 0.0\n","Softmax Output - Max: 0.6179683190762324 Min: 0.3820316809237676 Sum (first example): 1.0\n","Epoch 270, Loss: 0.6637810722930763, Test Accuracy: 0.6953571428571429\n","ReLU Activation - Max: 0.8013810158217622 Min: 0.0\n","Softmax Output - Max: 0.6317599329142006 Min: 0.3682400670857994 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024196420449630555 Min: -0.024196420449630555\n","Layer 0 - Gradient Weights Max: 0.017012947450592432 Min: -0.01359157858705793\n","ReLU Activation - Max: 0.7382082918660253 Min: 0.0\n","Softmax Output - Max: 0.6196856238506853 Min: 0.3803143761493147 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8079267831592852 Min: 0.0\n","Softmax Output - Max: 0.6336947079679446 Min: 0.3663052920320553 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024315620205556974 Min: -0.02431562020555698\n","Layer 0 - Gradient Weights Max: 0.01709549652027086 Min: -0.01363463052941415\n","ReLU Activation - Max: 0.7441889121078714 Min: 0.0\n","Softmax Output - Max: 0.6214204026363558 Min: 0.37857959736364416 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8145072931891159 Min: 0.0\n","Softmax Output - Max: 0.6356484749598822 Min: 0.3643515250401178 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024433388727516554 Min: -0.02443338872751656\n","Layer 0 - Gradient Weights Max: 0.01717910044634399 Min: -0.013747450983370553\n","ReLU Activation - Max: 0.7502049031180763 Min: 0.0\n","Softmax Output - Max: 0.6231725109383063 Min: 0.3768274890616938 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8211188365982947 Min: 0.0\n","Softmax Output - Max: 0.6376208241156912 Min: 0.3623791758843089 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024549703167910768 Min: -0.024549703167910764\n","Layer 0 - Gradient Weights Max: 0.017263692562694468 Min: -0.013825730876008474\n","ReLU Activation - Max: 0.7562508078672643 Min: 0.0\n","Softmax Output - Max: 0.6249417400265076 Min: 0.3750582599734925 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8277623451589171 Min: 0.0\n","Softmax Output - Max: 0.6396116204408239 Min: 0.360388379559176 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02466450558907272 Min: -0.02466450558907272\n","Layer 0 - Gradient Weights Max: 0.01734707074426365 Min: -0.013920411868710392\n","ReLU Activation - Max: 0.7623277466650278 Min: 0.0\n","Softmax Output - Max: 0.6267279169132628 Min: 0.3732720830867373 Sum (first example): 1.0\n","ReLU Activation - Max: 0.834435828431063 Min: 0.0\n","Softmax Output - Max: 0.6416204439072012 Min: 0.3583795560927988 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024777755283939346 Min: -0.024777755283939346\n","Layer 0 - Gradient Weights Max: 0.017436055529175907 Min: -0.013996151273958576\n","ReLU Activation - Max: 0.7684364352233197 Min: 0.0\n","Softmax Output - Max: 0.6285305964857192 Min: 0.3714694035142808 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8411453639504464 Min: 0.0\n","Softmax Output - Max: 0.643647040666498 Min: 0.3563529593335019 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024889414920850585 Min: -0.024889414920850585\n","Layer 0 - Gradient Weights Max: 0.017536547579918984 Min: -0.01408378984166548\n","ReLU Activation - Max: 0.7745790052543523 Min: 0.0\n","Softmax Output - Max: 0.6303509739780592 Min: 0.36964902602194083 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 0.8478882273795186 Min: 0.0\n","Softmax Output - Max: 0.6456920541612547 Min: 0.3543079458387452 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024999403279064997 Min: -0.024999403279064997\n","Layer 0 - Gradient Weights Max: 0.017625478418368774 Min: -0.014169335611986997\n","ReLU Activation - Max: 0.7807509173771776 Min: 0.0\n","Softmax Output - Max: 0.6321880188786438 Min: 0.3678119811213561 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 0.8546615940759617 Min: 0.0\n","Softmax Output - Max: 0.6477542359995214 Min: 0.35224576400047847 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02510769854611829 Min: -0.02510769854611829\n","Layer 0 - Gradient Weights Max: 0.017704824016486927 Min: -0.01424505258157874\n","ReLU Activation - Max: 0.7869509530983134 Min: 0.0\n","Softmax Output - Max: 0.6340414615019027 Min: 0.3659585384980973 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8614647920301797 Min: 0.0\n","Softmax Output - Max: 0.6498338809912368 Min: 0.35016611900876315 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02521424299216051 Min: -0.02521424299216051\n","Layer 0 - Gradient Weights Max: 0.017782986416090715 Min: -0.014320123858696288\n","ReLU Activation - Max: 0.7931787076730701 Min: 0.0\n","Softmax Output - Max: 0.6359113104153877 Min: 0.3640886895846123 Sum (first example): 1.0\n","Epoch 280, Loss: 0.6591801054112375, Test Accuracy: 0.6932142857142857\n","ReLU Activation - Max: 0.8682973483199641 Min: 0.0\n","Softmax Output - Max: 0.6519307637766227 Min: 0.34806923622337727 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025318987234956115 Min: -0.025318987234956115\n","Layer 0 - Gradient Weights Max: 0.01784701508578509 Min: -0.014411447389285867\n","ReLU Activation - Max: 0.7994366475882125 Min: 0.0\n","Softmax Output - Max: 0.6377969455273209 Min: 0.3622030544726791 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8751542896905528 Min: 0.0\n","Softmax Output - Max: 0.6540444378766999 Min: 0.3459555621233002 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025421899579986074 Min: -0.025421899579986078\n","Layer 0 - Gradient Weights Max: 0.017939570493835537 Min: -0.014473579561414119\n","ReLU Activation - Max: 0.8057229497020411 Min: 0.0\n","Softmax Output - Max: 0.6396984664160325 Min: 0.36030153358396755 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8820388648677064 Min: 0.0\n","Softmax Output - Max: 0.6561747832686751 Min: 0.343825216731325 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025522929245524847 Min: -0.02552292924552485\n","Layer 0 - Gradient Weights Max: 0.018005479423302287 Min: -0.014549105115958605\n","ReLU Activation - Max: 0.8120355538791093 Min: 0.0\n","Softmax Output - Max: 0.6416162798086429 Min: 0.35838372019135706 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8889518087231536 Min: 0.0\n","Softmax Output - Max: 0.658322219383608 Min: 0.341677780616392 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025621998970298567 Min: -0.025621998970298574\n","Layer 0 - Gradient Weights Max: 0.018072931276620508 Min: -0.014614448963939845\n","ReLU Activation - Max: 0.8183716684056986 Min: 0.0\n","Softmax Output - Max: 0.643549645261832 Min: 0.356450354738168 Sum (first example): 1.0\n","ReLU Activation - Max: 0.8958948273966091 Min: 0.0\n","Softmax Output - Max: 0.6604855801580691 Min: 0.33951441984193087 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025719056807917348 Min: -0.025719056807917345\n","Layer 0 - Gradient Weights Max: 0.01814236337917069 Min: -0.014674253744064088\n","ReLU Activation - Max: 0.8247329827840341 Min: 0.0\n","Softmax Output - Max: 0.6454977801127711 Min: 0.35450221988722885 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9028627492943311 Min: 0.0\n","Softmax Output - Max: 0.6626644814673376 Min: 0.3373355185326624 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02581406478969565 Min: -0.025814064789695657\n","Layer 0 - Gradient Weights Max: 0.01820524118037013 Min: -0.014734593496514962\n","ReLU Activation - Max: 0.8311139784340121 Min: 0.0\n","Softmax Output - Max: 0.6474605986624004 Min: 0.3525394013375997 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9098555415771229 Min: 0.0\n","Softmax Output - Max: 0.6648584417124503 Min: 0.3351415582875497 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02590698275458614 Min: -0.025906982754586142\n","Layer 0 - Gradient Weights Max: 0.01827380461274604 Min: -0.014804330610748328\n","ReLU Activation - Max: 0.8375194460297611 Min: 0.0\n","Softmax Output - Max: 0.6494375674385751 Min: 0.35056243256142494 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9168739300718789 Min: 0.0\n","Softmax Output - Max: 0.6670669791088006 Min: 0.3329330208911993 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02599773344235784 Min: -0.02599773344235785\n","Layer 0 - Gradient Weights Max: 0.01834086597664867 Min: -0.014873238290847683\n","ReLU Activation - Max: 0.8439488771803662 Min: 0.0\n","Softmax Output - Max: 0.6514289563281608 Min: 0.34857104367183933 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9239173271343618 Min: 0.0\n","Softmax Output - Max: 0.6692898908545728 Min: 0.33071010914542714 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02608628640015576 Min: -0.02608628640015576\n","Layer 0 - Gradient Weights Max: 0.018410862380442247 Min: -0.01498816531163371\n","ReLU Activation - Max: 0.8504053182341924 Min: 0.0\n","Softmax Output - Max: 0.6534353998240612 Min: 0.3465646001759389 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9309902761055576 Min: 0.0\n","Softmax Output - Max: 0.6715261240994368 Min: 0.32847387590056326 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02617258941606231 Min: -0.02617258941606231\n","Layer 0 - Gradient Weights Max: 0.018457563659149388 Min: -0.015042827930120932\n","ReLU Activation - Max: 0.8568861744704492 Min: 0.0\n","Softmax Output - Max: 0.6554556816646954 Min: 0.34454431833530463 Sum (first example): 1.0\n","Epoch 290, Loss: 0.6541947227322866, Test Accuracy: 0.6935714285714286\n","ReLU Activation - Max: 0.9380923147347191 Min: 0.0\n","Softmax Output - Max: 0.6737759359072049 Min: 0.3262240640927952 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02625658574144986 Min: -0.02625658574144986\n","Layer 0 - Gradient Weights Max: 0.018504588507604943 Min: -0.015170837842562021\n","ReLU Activation - Max: 0.8633933298122165 Min: 0.0\n","Softmax Output - Max: 0.6574900646421858 Min: 0.34250993535781415 Sum (first example): 1.0\n","ReLU Activation - Max: 0.945219584822328 Min: 0.0\n","Softmax Output - Max: 0.6760390418442322 Min: 0.32396095815576775 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02633825903793957 Min: -0.02633825903793957\n","Layer 0 - Gradient Weights Max: 0.01855378706387089 Min: -0.015257432352396127\n","ReLU Activation - Max: 0.8699208851561795 Min: 0.0\n","Softmax Output - Max: 0.6595389624668474 Min: 0.34046103753315266 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9523676328984617 Min: 0.0\n","Softmax Output - Max: 0.6783166198447336 Min: 0.3216833801552664 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.026417538424555128 Min: -0.02641753842455512\n","Layer 0 - Gradient Weights Max: 0.018570804811132036 Min: -0.015352576080441188\n","ReLU Activation - Max: 0.8764618574446017 Min: 0.0\n","Softmax Output - Max: 0.6616008804982143 Min: 0.3383991195017856 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9595485403927988 Min: 0.0\n","Softmax Output - Max: 0.6806069421666691 Min: 0.31939305783333094 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026494400264933493 Min: -0.026494400264933493\n","Layer 0 - Gradient Weights Max: 0.018610141199681168 Min: -0.015459312291132996\n","ReLU Activation - Max: 0.8830250884960607 Min: 0.0\n","Softmax Output - Max: 0.6636748264814332 Min: 0.3363251735185668 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9667500517952505 Min: 0.0\n","Softmax Output - Max: 0.6829087960436582 Min: 0.31709120395634177 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02656876629632228 Min: -0.02656876629632228\n","Layer 0 - Gradient Weights Max: 0.018671765908507124 Min: -0.015521894492637297\n","ReLU Activation - Max: 0.8896095545711653 Min: 0.0\n","Softmax Output - Max: 0.6657611912063036 Min: 0.3342388087936964 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9739716694333919 Min: 0.0\n","Softmax Output - Max: 0.6852227246174473 Min: 0.3147772753825528 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026640594957578468 Min: -0.02664059495757847\n","Layer 0 - Gradient Weights Max: 0.01873197723165545 Min: -0.015591757492713857\n","ReLU Activation - Max: 0.8962115741800453 Min: 0.0\n","Softmax Output - Max: 0.6678586589496838 Min: 0.3321413410503162 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9812116166966354 Min: 0.0\n","Softmax Output - Max: 0.6875482789767992 Min: 0.31245172102320085 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026709814022971363 Min: -0.026709814022971363\n","Layer 0 - Gradient Weights Max: 0.018807760101206748 Min: -0.015644196866200877\n","ReLU Activation - Max: 0.902835561189176 Min: 0.0\n","Softmax Output - Max: 0.6699678391532808 Min: 0.33003216084671916 Sum (first example): 1.0\n","ReLU Activation - Max: 0.988466300993863 Min: 0.0\n","Softmax Output - Max: 0.689884775020949 Min: 0.31011522497905103 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02677638406206778 Min: -0.02677638406206778\n","Layer 0 - Gradient Weights Max: 0.018880015487765415 Min: -0.015682391441691607\n","ReLU Activation - Max: 0.9094788246341494 Min: 0.0\n","Softmax Output - Max: 0.67208734166357 Min: 0.32791265833642996 Sum (first example): 1.0\n","ReLU Activation - Max: 0.9957414778739002 Min: 0.0\n","Softmax Output - Max: 0.6922308996896475 Min: 0.3077691003103525 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026840263019450836 Min: -0.026840263019450843\n","Layer 0 - Gradient Weights Max: 0.018920023788571876 Min: -0.015749626713541524\n","ReLU Activation - Max: 0.9161379550058217 Min: 0.0\n","Softmax Output - Max: 0.6742174197500953 Min: 0.32578258024990464 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0030329602137675 Min: 0.0\n","Softmax Output - Max: 0.6945867078413009 Min: 0.305413292158699 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.026901423337862204 Min: -0.026901423337862204\n","Layer 0 - Gradient Weights Max: 0.019003723811372407 Min: -0.01580533250085378\n","ReLU Activation - Max: 0.9228157285901599 Min: 0.0\n","Softmax Output - Max: 0.676356341737701 Min: 0.32364365826229896 Sum (first example): 1.0\n","Epoch 300, Loss: 0.6488988409639133, Test Accuracy: 0.6942857142857143\n","ReLU Activation - Max: 1.0103434097812698 Min: 0.0\n","Softmax Output - Max: 0.6969507486106364 Min: 0.3030492513893636 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026959813891351075 Min: -0.026959813891351082\n","Layer 0 - Gradient Weights Max: 0.019040795021781774 Min: -0.01588032179202083\n","ReLU Activation - Max: 0.9295113412722218 Min: 0.0\n","Softmax Output - Max: 0.6785041129558878 Min: 0.32149588704411236 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0176679482615596 Min: 0.0\n","Softmax Output - Max: 0.6993228544682476 Min: 0.3006771455317524 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027015406703412948 Min: -0.02701540670341296\n","Layer 0 - Gradient Weights Max: 0.019083411775295052 Min: -0.015936325708486507\n","ReLU Activation - Max: 0.9362226712114272 Min: 0.0\n","Softmax Output - Max: 0.6806604507267334 Min: 0.31933954927326663 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0250080601771299 Min: 0.0\n","Softmax Output - Max: 0.7017024613899847 Min: 0.2982975386100153 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027068165849907448 Min: -0.027068165849907448\n","Layer 0 - Gradient Weights Max: 0.01909593571880754 Min: -0.015995487102231202\n","ReLU Activation - Max: 0.942949881852446 Min: 0.0\n","Softmax Output - Max: 0.6828255269265515 Min: 0.3171744730734485 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0323625331075819 Min: 0.0\n","Softmax Output - Max: 0.7040892164999056 Min: 0.2959107835000943 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027118046747019196 Min: -0.027118046747019196\n","Layer 0 - Gradient Weights Max: 0.019161279210801786 Min: -0.01605351094899973\n","ReLU Activation - Max: 0.9496926517666562 Min: 0.0\n","Softmax Output - Max: 0.6849988982476357 Min: 0.3150011017523643 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0397326067517716 Min: 0.0\n","Softmax Output - Max: 0.7064826190380453 Min: 0.2935173809619546 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02716501257693413 Min: -0.02716501257693413\n","Layer 0 - Gradient Weights Max: 0.019193756302534774 Min: -0.016094623113673278\n","ReLU Activation - Max: 0.9564444716493052 Min: 0.0\n","Softmax Output - Max: 0.6871804421953469 Min: 0.31281955780465304 Sum (first example): 1.0\n","ReLU Activation - Max: 1.047119352817941 Min: 0.0\n","Softmax Output - Max: 0.708882710486767 Min: 0.291117289513233 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027209038018821713 Min: -0.027209038018821713\n","Layer 0 - Gradient Weights Max: 0.01922538575463089 Min: -0.016116844736837964\n","ReLU Activation - Max: 0.9632064836907354 Min: 0.0\n","Softmax Output - Max: 0.6893689446646764 Min: 0.31063105533532365 Sum (first example): 1.0\n","ReLU Activation - Max: 1.054523331903968 Min: 0.0\n","Softmax Output - Max: 0.7112879802841172 Min: 0.28871201971588284 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027250057614634088 Min: -0.027250057614634088\n","Layer 0 - Gradient Weights Max: 0.01926053996247478 Min: -0.016150365004404543\n","ReLU Activation - Max: 0.9699809893327449 Min: 0.0\n","Softmax Output - Max: 0.6915644678917806 Min: 0.3084355321082194 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0619437486561227 Min: 0.0\n","Softmax Output - Max: 0.7136978800470184 Min: 0.28630211995298166 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027288058150429458 Min: -0.027288058150429458\n","Layer 0 - Gradient Weights Max: 0.019289529663883056 Min: -0.016195985099754798\n","ReLU Activation - Max: 0.9767658020134311 Min: 0.0\n","Softmax Output - Max: 0.6937664920894239 Min: 0.30623350791057613 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0693684030118074 Min: 0.0\n","Softmax Output - Max: 0.7161124486755901 Min: 0.2838875513244099 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02732299471084076 Min: -0.027322994710840764\n","Layer 0 - Gradient Weights Max: 0.019321333312475094 Min: -0.016233921290092386\n","ReLU Activation - Max: 0.9835635817712767 Min: 0.0\n","Softmax Output - Max: 0.6959743101082457 Min: 0.3040256898917543 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.076803652943769 Min: 0.0\n","Softmax Output - Max: 0.7185308702744925 Min: 0.28146912972550747 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027354865214671103 Min: -0.027354865214671103\n","Layer 0 - Gradient Weights Max: 0.01934746326948687 Min: -0.016279509305145164\n","ReLU Activation - Max: 0.990371437912076 Min: 0.0\n","Softmax Output - Max: 0.6981872586397616 Min: 0.3018127413602384 Sum (first example): 0.9999999999999999\n","Epoch 310, Loss: 0.6433888984616768, Test Accuracy: 0.6939285714285715\n","ReLU Activation - Max: 1.0842481417732464 Min: 0.0\n","Softmax Output - Max: 0.7209525963846096 Min: 0.27904740361539027 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02738362388637057 Min: -0.02738362388637057\n","Layer 0 - Gradient Weights Max: 0.0193714454461435 Min: -0.01632374304832306\n","ReLU Activation - Max: 0.997188638764988 Min: 0.0\n","Softmax Output - Max: 0.7004051520757905 Min: 0.29959484792420943 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0917010344569038 Min: 0.0\n","Softmax Output - Max: 0.7233767767394379 Min: 0.2766232232605621 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027409242547377102 Min: -0.027409242547377102\n","Layer 0 - Gradient Weights Max: 0.01940036881287758 Min: -0.016364715836468112\n","ReLU Activation - Max: 1.0040141337253534 Min: 0.0\n","Softmax Output - Max: 0.7026275606899824 Min: 0.2973724393100175 Sum (first example): 1.0\n","ReLU Activation - Max: 1.0991586334144976 Min: 0.0\n","Softmax Output - Max: 0.7258029796161024 Min: 0.2741970203838976 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027431650792554302 Min: -0.027431650792554302\n","Layer 0 - Gradient Weights Max: 0.01940817727403234 Min: -0.01643377222706984\n","ReLU Activation - Max: 1.0108458744025657 Min: 0.0\n","Softmax Output - Max: 0.7048540299195993 Min: 0.2951459700804007 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1066235162651348 Min: 0.0\n","Softmax Output - Max: 0.7282303420479435 Min: 0.27176965795205665 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02745095983041529 Min: -0.02745095983041529\n","Layer 0 - Gradient Weights Max: 0.019402866856993644 Min: -0.01647150385308136\n","ReLU Activation - Max: 1.0176804275873574 Min: 0.0\n","Softmax Output - Max: 0.7070832094202579 Min: 0.292916790579742 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1140931889648258 Min: 0.0\n","Softmax Output - Max: 0.7306581482284474 Min: 0.2693418517715527 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027467076552540565 Min: -0.027467076552540572\n","Layer 0 - Gradient Weights Max: 0.01941792121678522 Min: -0.016510381074299023\n","ReLU Activation - Max: 1.0245212993877792 Min: 0.0\n","Softmax Output - Max: 0.7093141243518046 Min: 0.29068587564819537 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1215678503525381 Min: 0.0\n","Softmax Output - Max: 0.7330855089102228 Min: 0.26691449108977716 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02747999611668697 Min: -0.02747999611668697\n","Layer 0 - Gradient Weights Max: 0.019434978278248416 Min: -0.01646309419721865\n","ReLU Activation - Max: 1.031361995998155 Min: 0.0\n","Softmax Output - Max: 0.7115465979552091 Min: 0.28845340204479075 Sum (first example): 1.0\n","ReLU Activation - Max: 1.129051331096298 Min: 0.0\n","Softmax Output - Max: 0.7355121247298333 Min: 0.2644878752701667 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02748962704888512 Min: -0.027489627048885124\n","Layer 0 - Gradient Weights Max: 0.01943338516362723 Min: -0.016465752359498917\n","ReLU Activation - Max: 1.0382059794853913 Min: 0.0\n","Softmax Output - Max: 0.7137814827706422 Min: 0.28621851722935776 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1365405928784453 Min: 0.0\n","Softmax Output - Max: 0.7379379353822624 Min: 0.2620620646177375 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02749596388465676 Min: -0.027495963884656756\n","Layer 0 - Gradient Weights Max: 0.01944169698684149 Min: -0.016499697158907655\n","ReLU Activation - Max: 1.045053939483305 Min: 0.0\n","Softmax Output - Max: 0.7160172740618588 Min: 0.28398272593814117 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1440322649311851 Min: 0.0\n","Softmax Output - Max: 0.7403617242635849 Min: 0.25963827573641507 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02749902557064832 Min: -0.027499025570648317\n","Layer 0 - Gradient Weights Max: 0.01948404944805572 Min: -0.01650514010610097\n","ReLU Activation - Max: 1.0519057658117097 Min: 0.0\n","Softmax Output - Max: 0.7182537948442393 Min: 0.28174620515576054 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1515275545376011 Min: 0.0\n","Softmax Output - Max: 0.7427835520213323 Min: 0.25721644797866783 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027498790684417622 Min: -0.02749879068441762\n","Layer 0 - Gradient Weights Max: 0.019502604732649673 Min: -0.016540363249545182\n","ReLU Activation - Max: 1.0587619232927425 Min: 0.0\n","Softmax Output - Max: 0.7204899904467132 Min: 0.2795100095532868 Sum (first example): 1.0\n","Epoch 320, Loss: 0.6377824470586279, Test Accuracy: 0.6917857142857143\n","ReLU Activation - Max: 1.1590275111376247 Min: 0.0\n","Softmax Output - Max: 0.7452023799869795 Min: 0.2547976200130205 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.02749524263233065 Min: -0.02749524263233065\n","Layer 0 - Gradient Weights Max: 0.01950759728213167 Min: -0.016539356915768733\n","ReLU Activation - Max: 1.0656168773051113 Min: 0.0\n","Softmax Output - Max: 0.7227257407313377 Min: 0.2772742592686623 Sum (first example): 1.0\n","ReLU Activation - Max: 1.166533430380026 Min: 0.0\n","Softmax Output - Max: 0.7476172655950795 Min: 0.2523827344049205 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.027488368332990953 Min: -0.027488368332990953\n","Layer 0 - Gradient Weights Max: 0.019484693326445656 Min: -0.016537845821941725\n","ReLU Activation - Max: 1.0724666109949128 Min: 0.0\n","Softmax Output - Max: 0.7249597925613016 Min: 0.27504020743869845 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1740428858031813 Min: 0.0\n","Softmax Output - Max: 0.7500268786487185 Min: 0.24997312135128152 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02747818213022494 Min: -0.02747818213022494\n","Layer 0 - Gradient Weights Max: 0.019506271250764054 Min: -0.016546975227216165\n","ReLU Activation - Max: 1.079316408605155 Min: 0.0\n","Softmax Output - Max: 0.7271919636827988 Min: 0.27280803631720113 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.1815555052258897 Min: 0.0\n","Softmax Output - Max: 0.7524313154366185 Min: 0.24756868456338157 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027464678443163488 Min: -0.02746467844316349\n","Layer 0 - Gradient Weights Max: 0.01950072922293505 Min: -0.0166006561250588\n","ReLU Activation - Max: 1.0861664870148349 Min: 0.0\n","Softmax Output - Max: 0.7294216769941105 Min: 0.2705783230058896 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.1890677024040877 Min: 0.0\n","Softmax Output - Max: 0.7548295358720675 Min: 0.24517046412793259 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027447906146608945 Min: -0.027447906146608945\n","Layer 0 - Gradient Weights Max: 0.01950018827778014 Min: -0.016624465230950694\n","ReLU Activation - Max: 1.0930160499668418 Min: 0.0\n","Softmax Output - Max: 0.7316487219422831 Min: 0.2683512780577169 Sum (first example): 1.0\n","ReLU Activation - Max: 1.1965749734527473 Min: 0.0\n","Softmax Output - Max: 0.7572214238923626 Min: 0.24277857610763737 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02742783965011775 Min: -0.027427839650117747\n","Layer 0 - Gradient Weights Max: 0.019488766673202693 Min: -0.016643704828890278\n","ReLU Activation - Max: 1.0998596943705772 Min: 0.0\n","Softmax Output - Max: 0.7339586692557872 Min: 0.2660413307442127 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.2040804066201005 Min: 0.0\n","Softmax Output - Max: 0.7596062701852785 Min: 0.24039372981472154 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02740445299774818 Min: -0.02740445299774818\n","Layer 0 - Gradient Weights Max: 0.01946063599041568 Min: -0.016663301756420636\n","ReLU Activation - Max: 1.1067037883024204 Min: 0.0\n","Softmax Output - Max: 0.7362805576124588 Min: 0.26371944238754114 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.2115773892333381 Min: 0.0\n","Softmax Output - Max: 0.7619832337473376 Min: 0.23801676625266252 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027377773556327607 Min: -0.027377773556327614\n","Layer 0 - Gradient Weights Max: 0.019467174584315558 Min: -0.01670258842659234\n","ReLU Activation - Max: 1.113544977789722 Min: 0.0\n","Softmax Output - Max: 0.738599362822417 Min: 0.2614006371775831 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2190645776729077 Min: 0.0\n","Softmax Output - Max: 0.7643520488344245 Min: 0.23564795116557544 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02734780778196999 Min: -0.02734780778196999\n","Layer 0 - Gradient Weights Max: 0.019453640815126325 Min: -0.016742767766309345\n","ReLU Activation - Max: 1.1203798062283046 Min: 0.0\n","Softmax Output - Max: 0.7409146234832021 Min: 0.2590853765167978 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2265458282096398 Min: 0.0\n","Softmax Output - Max: 0.7667124613105898 Min: 0.23328753868941013 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027314587667272124 Min: -0.027314587667272124\n","Layer 0 - Gradient Weights Max: 0.019406535506763228 Min: -0.01675332568508407\n","ReLU Activation - Max: 1.1272124002129966 Min: 0.0\n","Softmax Output - Max: 0.7432253357520352 Min: 0.2567746642479648 Sum (first example): 1.0\n","Epoch 330, Loss: 0.6322073712805659, Test Accuracy: 0.6914285714285714\n","ReLU Activation - Max: 1.2340227946583215 Min: 0.0\n","Softmax Output - Max: 0.7690635459890511 Min: 0.23093645401094884 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027278081695861444 Min: -0.027278081695861444\n","Layer 0 - Gradient Weights Max: 0.019385124576408626 Min: -0.016767873611701952\n","ReLU Activation - Max: 1.1340387918592647 Min: 0.0\n","Softmax Output - Max: 0.7455310378309782 Min: 0.2544689621690218 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2414908651953047 Min: 0.0\n","Softmax Output - Max: 0.7714047725130475 Min: 0.22859522748695252 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027238333515203064 Min: -0.027238333515203064\n","Layer 0 - Gradient Weights Max: 0.01937997432360174 Min: -0.01676991752650686\n","ReLU Activation - Max: 1.140859368893 Min: 0.0\n","Softmax Output - Max: 0.7478311661280285 Min: 0.25216883387197153 Sum (first example): 1.0\n","ReLU Activation - Max: 1.248948371122512 Min: 0.0\n","Softmax Output - Max: 0.7737347087076901 Min: 0.22626529129230982 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02719537475660626 Min: -0.02719537475660626\n","Layer 0 - Gradient Weights Max: 0.019354090338558665 Min: -0.01676128931299391\n","ReLU Activation - Max: 1.1476714796932292 Min: 0.0\n","Softmax Output - Max: 0.750125008487358 Min: 0.24987499151264211 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2563964604601137 Min: 0.0\n","Softmax Output - Max: 0.7760543359802492 Min: 0.22394566401975083 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027149175256188193 Min: -0.0271491752561882\n","Layer 0 - Gradient Weights Max: 0.019295389652654454 Min: -0.016776019079543267\n","ReLU Activation - Max: 1.1544668737165256 Min: 0.0\n","Softmax Output - Max: 0.7524123227653426 Min: 0.24758767723465738 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2638224679128924 Min: 0.0\n","Softmax Output - Max: 0.7783623489218697 Min: 0.2216376510781303 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02709971845210311 Min: -0.02709971845210311\n","Layer 0 - Gradient Weights Max: 0.01927401192207018 Min: -0.016772508373202037\n","ReLU Activation - Max: 1.161248834637592 Min: 0.0\n","Softmax Output - Max: 0.7546930926956028 Min: 0.2453069073043972 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2712418418349205 Min: 0.0\n","Softmax Output - Max: 0.780657984037877 Min: 0.21934201596212308 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.027047084845698403 Min: -0.027047084845698403\n","Layer 0 - Gradient Weights Max: 0.019241314201430704 Min: -0.016779487548739383\n","ReLU Activation - Max: 1.1680207440909924 Min: 0.0\n","Softmax Output - Max: 0.7569667208242367 Min: 0.2430332791757634 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2786479836729532 Min: 0.0\n","Softmax Output - Max: 0.7829406312704432 Min: 0.21705936872955678 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026991286129669004 Min: -0.026991286129669004\n","Layer 0 - Gradient Weights Max: 0.0192287959370774 Min: -0.016802217386307116\n","ReLU Activation - Max: 1.174786758873649 Min: 0.0\n","Softmax Output - Max: 0.7592323798201145 Min: 0.2407676201798855 Sum (first example): 1.0\n","ReLU Activation - Max: 1.2860442892759805 Min: 0.0\n","Softmax Output - Max: 0.7852103512045296 Min: 0.2147896487954704 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02693232310329054 Min: -0.02693232310329054\n","Layer 0 - Gradient Weights Max: 0.019191764639868658 Min: -0.016806241975195826\n","ReLU Activation - Max: 1.1815412381454382 Min: 0.0\n","Softmax Output - Max: 0.7614885550961742 Min: 0.23851144490382584 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.2934256898677723 Min: 0.0\n","Softmax Output - Max: 0.7874669593150619 Min: 0.21253304068493808 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026870262204982773 Min: -0.026870262204982773\n","Layer 0 - Gradient Weights Max: 0.019150593334291005 Min: -0.016842828618876846\n","ReLU Activation - Max: 1.1882889311146811 Min: 0.0\n","Softmax Output - Max: 0.7637352984051649 Min: 0.23626470159483517 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3007897822634404 Min: 0.0\n","Softmax Output - Max: 0.789710392423738 Min: 0.21028960757626197 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026805115421746656 Min: -0.026805115421746656\n","Layer 0 - Gradient Weights Max: 0.019109240621021362 Min: -0.016843963272095933\n","ReLU Activation - Max: 1.1950236386097226 Min: 0.0\n","Softmax Output - Max: 0.7659726350964112 Min: 0.23402736490358886 Sum (first example): 1.0\n","Epoch 340, Loss: 0.6267949281955761, Test Accuracy: 0.6903571428571429\n","ReLU Activation - Max: 1.3081372936300117 Min: 0.0\n","Softmax Output - Max: 0.7919388653889204 Min: 0.20806113461107956 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026736904305787478 Min: -0.026736904305787478\n","Layer 0 - Gradient Weights Max: 0.019073829884780374 Min: -0.016844127287196517\n","ReLU Activation - Max: 1.201745520361584 Min: 0.0\n","Softmax Output - Max: 0.7681999629779699 Min: 0.23180003702203011 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3154698092387664 Min: 0.0\n","Softmax Output - Max: 0.7941517616134288 Min: 0.20584823838657115 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0266656906226154 Min: -0.0266656906226154\n","Layer 0 - Gradient Weights Max: 0.018998219864399396 Min: -0.01679681237284466\n","ReLU Activation - Max: 1.2084476273618194 Min: 0.0\n","Softmax Output - Max: 0.7704165619621863 Min: 0.22958343803781375 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3227893719565216 Min: 0.0\n","Softmax Output - Max: 0.7963493714209423 Min: 0.20365062857905766 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026591441295486844 Min: -0.026591441295486844\n","Layer 0 - Gradient Weights Max: 0.0189504686255315 Min: -0.016793274993484326\n","ReLU Activation - Max: 1.2151345442782082 Min: 0.0\n","Softmax Output - Max: 0.7726217154306678 Min: 0.22737828456933226 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3300899601161595 Min: 0.0\n","Softmax Output - Max: 0.7985310308368433 Min: 0.20146896916315665 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02651420820769392 Min: -0.026514208207693915\n","Layer 0 - Gradient Weights Max: 0.018886392142530004 Min: -0.01680287488703247\n","ReLU Activation - Max: 1.2218010941920765 Min: 0.0\n","Softmax Output - Max: 0.7748156280778308 Min: 0.22518437192216914 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3373640981526869 Min: 0.0\n","Softmax Output - Max: 0.800696715029711 Min: 0.19930328497028907 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02643403894246552 Min: -0.02643403894246552\n","Layer 0 - Gradient Weights Max: 0.01883447840729409 Min: -0.01679654824086947\n","ReLU Activation - Max: 1.2284510277713743 Min: 0.0\n","Softmax Output - Max: 0.7769975160021737 Min: 0.22300248399782632 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3446176585037992 Min: 0.0\n","Softmax Output - Max: 0.8029560209999648 Min: 0.1970439790000352 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026351032191390535 Min: -0.026351032191390535\n","Layer 0 - Gradient Weights Max: 0.01876745695344164 Min: -0.016887651293455805\n","ReLU Activation - Max: 1.235082028210956 Min: 0.0\n","Softmax Output - Max: 0.7791672920510815 Min: 0.22083270794891852 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3518327929340885 Min: 0.0\n","Softmax Output - Max: 0.8052841276265452 Min: 0.19471587237345475 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02626522399914588 Min: -0.026265223999145885\n","Layer 0 - Gradient Weights Max: 0.01876273010764832 Min: -0.01686628396035473\n","ReLU Activation - Max: 1.2416985389073703 Min: 0.0\n","Softmax Output - Max: 0.7813239498934883 Min: 0.21867605010651167 Sum (first example): 1.0\n","ReLU Activation - Max: 1.359031046947655 Min: 0.0\n","Softmax Output - Max: 0.8075950627112075 Min: 0.19240493728879252 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.026176666681621844 Min: -0.026176666681621847\n","Layer 0 - Gradient Weights Max: 0.01867003036792459 Min: -0.01685801992006179\n","ReLU Activation - Max: 1.248291239132432 Min: 0.0\n","Softmax Output - Max: 0.7834679023069643 Min: 0.2165320976930357 Sum (first example): 1.0\n","ReLU Activation - Max: 1.366204225541053 Min: 0.0\n","Softmax Output - Max: 0.8098881513116513 Min: 0.1901118486883488 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.02608531615153079 Min: -0.02608531615153079\n","Layer 0 - Gradient Weights Max: 0.01861012513275261 Min: -0.01684642769082526\n","ReLU Activation - Max: 1.2548646058559576 Min: 0.0\n","Softmax Output - Max: 0.7855978774371876 Min: 0.21440212256281255 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 1.3733537710239148 Min: 0.0\n","Softmax Output - Max: 0.8121631789478359 Min: 0.18783682105216412 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025991280279346978 Min: -0.02599128027934697\n","Layer 0 - Gradient Weights Max: 0.018535187064407987 Min: -0.016857951358000577\n","ReLU Activation - Max: 1.2614211632181858 Min: 0.0\n","Softmax Output - Max: 0.7877144597830894 Min: 0.21228554021691057 Sum (first example): 0.9999999999999999\n","Epoch 350, Loss: 0.6216611588438422, Test Accuracy: 0.6921428571428572\n","ReLU Activation - Max: 1.3804783713950415 Min: 0.0\n","Softmax Output - Max: 0.8144200393694757 Min: 0.1855799606305242 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0258946266044625 Min: -0.0258946266044625\n","Layer 0 - Gradient Weights Max: 0.01847873497065144 Min: -0.016806588303406884\n","ReLU Activation - Max: 1.2679499684894249 Min: 0.0\n","Softmax Output - Max: 0.7898157600716721 Min: 0.21018423992832794 Sum (first example): 1.0\n","ReLU Activation - Max: 1.3875833785635932 Min: 0.0\n","Softmax Output - Max: 0.8166569975864502 Min: 0.1833430024135499 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025795385359678284 Min: -0.025795385359678284\n","Layer 0 - Gradient Weights Max: 0.01840716332812355 Min: -0.016803666525576577\n","ReLU Activation - Max: 1.274454720268486 Min: 0.0\n","Softmax Output - Max: 0.7919019134677715 Min: 0.20809808653222844 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.3946661851983255 Min: 0.0\n","Softmax Output - Max: 0.8188752858952151 Min: 0.18112471410478487 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025693588484111258 Min: -0.025693588484111258\n","Layer 0 - Gradient Weights Max: 0.018348502492013898 Min: -0.01677831430439815\n","ReLU Activation - Max: 1.2809334926308882 Min: 0.0\n","Softmax Output - Max: 0.7939725877108816 Min: 0.20602741228911825 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4017104184139333 Min: 0.0\n","Softmax Output - Max: 0.8210734273677275 Min: 0.17892657263227257 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025589250374550596 Min: -0.025589250374550596\n","Layer 0 - Gradient Weights Max: 0.018245329836626932 Min: -0.016766507930060362\n","ReLU Activation - Max: 1.2873880684561894 Min: 0.0\n","Softmax Output - Max: 0.7960281220547214 Min: 0.20397187794527868 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4087256685586826 Min: 0.0\n","Softmax Output - Max: 0.8232520395084258 Min: 0.1767479604915742 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02548244431544261 Min: -0.02548244431544261\n","Layer 0 - Gradient Weights Max: 0.018169333403836085 Min: -0.016761047017941046\n","ReLU Activation - Max: 1.293819566231854 Min: 0.0\n","Softmax Output - Max: 0.7980678768946354 Min: 0.2019321231053645 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4157170321237849 Min: 0.0\n","Softmax Output - Max: 0.8254102737770453 Min: 0.17458972622295468 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02537330312330689 Min: -0.02537330312330689\n","Layer 0 - Gradient Weights Max: 0.018074473991302062 Min: -0.016716799858225537\n","ReLU Activation - Max: 1.3002252606775304 Min: 0.0\n","Softmax Output - Max: 0.8000919904109483 Min: 0.19990800958905175 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.422677233831168 Min: 0.0\n","Softmax Output - Max: 0.8275477616063494 Min: 0.17245223839365065 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025261755562199475 Min: -0.025261755562199468\n","Layer 0 - Gradient Weights Max: 0.01806020805927674 Min: -0.016714053554535688\n","ReLU Activation - Max: 1.3066086643355264 Min: 0.0\n","Softmax Output - Max: 0.8021001681493214 Min: 0.19789983185067855 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4296088577107073 Min: 0.0\n","Softmax Output - Max: 0.8296640529389847 Min: 0.17033594706101537 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025147870593406486 Min: -0.025147870593406486\n","Layer 0 - Gradient Weights Max: 0.01794604337127321 Min: -0.016674739381484623\n","ReLU Activation - Max: 1.3129647108164622 Min: 0.0\n","Softmax Output - Max: 0.8040917311380836 Min: 0.19590826886191637 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.436508896057713 Min: 0.0\n","Softmax Output - Max: 0.8317598301019221 Min: 0.16824016989807788 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.025031732344389244 Min: -0.02503173234438924\n","Layer 0 - Gradient Weights Max: 0.017825014308635638 Min: -0.016653032448867112\n","ReLU Activation - Max: 1.3192994409182948 Min: 0.0\n","Softmax Output - Max: 0.8060666122989782 Min: 0.1939333877010218 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4433732765983176 Min: 0.0\n","Softmax Output - Max: 0.8338341484563584 Min: 0.1661658515436415 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024913368141615544 Min: -0.024913368141615544\n","Layer 0 - Gradient Weights Max: 0.017745923053325875 Min: -0.01662736198144156\n","ReLU Activation - Max: 1.3256083269798598 Min: 0.0\n","Softmax Output - Max: 0.8080246006955918 Min: 0.1919753993044081 Sum (first example): 1.0\n","Epoch 360, Loss: 0.6168978126225477, Test Accuracy: 0.6935714285714286\n","ReLU Activation - Max: 1.4502068098742618 Min: 0.0\n","Softmax Output - Max: 0.8358861388254127 Min: 0.16411386117458723 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024792875637002294 Min: -0.0247928756370023\n","Layer 0 - Gradient Weights Max: 0.0176640233858059 Min: -0.016599192103528487\n","ReLU Activation - Max: 1.3318866461735208 Min: 0.0\n","Softmax Output - Max: 0.8099653369399531 Min: 0.19003466306004704 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4569997146899465 Min: 0.0\n","Softmax Output - Max: 0.8379164493117195 Min: 0.16208355068828056 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024670232343331974 Min: -0.024670232343331974\n","Layer 0 - Gradient Weights Max: 0.01757860229153378 Min: -0.016457876850734472\n","ReLU Activation - Max: 1.3381240602165008 Min: 0.0\n","Softmax Output - Max: 0.8118887818010442 Min: 0.18811121819895577 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.4637649214283464 Min: 0.0\n","Softmax Output - Max: 0.8399246548099838 Min: 0.16007534519001623 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024545471168007316 Min: -0.024545471168007316\n","Layer 0 - Gradient Weights Max: 0.017499867361892234 Min: -0.016361451392169737\n","ReLU Activation - Max: 1.3443309147783242 Min: 0.0\n","Softmax Output - Max: 0.8137951533337024 Min: 0.18620484666629747 Sum (first example): 1.0\n","ReLU Activation - Max: 1.470501003291107 Min: 0.0\n","Softmax Output - Max: 0.8419109945647794 Min: 0.15808900543522075 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024418689623263816 Min: -0.024418689623263816\n","Layer 0 - Gradient Weights Max: 0.017437463341149817 Min: -0.016351724231370105\n","ReLU Activation - Max: 1.3505080390147495 Min: 0.0\n","Softmax Output - Max: 0.8156843513294733 Min: 0.1843156486705267 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4772076159157426 Min: 0.0\n","Softmax Output - Max: 0.8438758293481402 Min: 0.15612417065185982 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.024290028283685077 Min: -0.024290028283685077\n","Layer 0 - Gradient Weights Max: 0.017299787753982034 Min: -0.016296118264798633\n","ReLU Activation - Max: 1.3566497543693845 Min: 0.0\n","Softmax Output - Max: 0.81755484907517 Min: 0.18244515092483002 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4838750122642224 Min: 0.0\n","Softmax Output - Max: 0.8458176456526959 Min: 0.1541823543473041 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02415946294063408 Min: -0.02415946294063408\n","Layer 0 - Gradient Weights Max: 0.01721177012130429 Min: -0.016230896002135533\n","ReLU Activation - Max: 1.3627619053598683 Min: 0.0\n","Softmax Output - Max: 0.8194069214009547 Min: 0.18059307859904536 Sum (first example): 1.0\n","ReLU Activation - Max: 1.4905105210487843 Min: 0.0\n","Softmax Output - Max: 0.8477370120136705 Min: 0.15226298798632956 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02402711038448735 Min: -0.02402711038448735\n","Layer 0 - Gradient Weights Max: 0.017129610688118996 Min: -0.016184384094564495\n","ReLU Activation - Max: 1.3688414892081406 Min: 0.0\n","Softmax Output - Max: 0.8212405675571893 Min: 0.17875943244281076 Sum (first example): 1.0\n","ReLU Activation - Max: 1.497116463323669 Min: 0.0\n","Softmax Output - Max: 0.8496336531657156 Min: 0.1503663468342844 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02389303775562423 Min: -0.02389303775562423\n","Layer 0 - Gradient Weights Max: 0.01698724138431211 Min: -0.016205686793452346\n","ReLU Activation - Max: 1.3748859711537096 Min: 0.0\n","Softmax Output - Max: 0.8230565174858888 Min: 0.17694348251411116 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5036972316833956 Min: 0.0\n","Softmax Output - Max: 0.8515082830034021 Min: 0.14849171699659794 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02375744360490897 Min: -0.02375744360490897\n","Layer 0 - Gradient Weights Max: 0.016897216481540467 Min: -0.016206562224020327\n","ReLU Activation - Max: 1.3809017452816053 Min: 0.0\n","Softmax Output - Max: 0.8248542985410515 Min: 0.17514570145894848 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5102413877530994 Min: 0.0\n","Softmax Output - Max: 0.8533605922299256 Min: 0.14663940777007442 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023620260732422446 Min: -0.023620260732422446\n","Layer 0 - Gradient Weights Max: 0.016818075077983406 Min: -0.01617086634144536\n","ReLU Activation - Max: 1.3868903759588604 Min: 0.0\n","Softmax Output - Max: 0.8266334701392293 Min: 0.1733665298607708 Sum (first example): 1.0\n","Epoch 370, Loss: 0.6125725405073482, Test Accuracy: 0.6917857142857143\n","ReLU Activation - Max: 1.5167455460815564 Min: 0.0\n","Softmax Output - Max: 0.8551901720024061 Min: 0.14480982799759382 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023481561530699885 Min: -0.02348156153069989\n","Layer 0 - Gradient Weights Max: 0.016699139932071137 Min: -0.016109670846113377\n","ReLU Activation - Max: 1.3928470356172302 Min: 0.0\n","Softmax Output - Max: 0.8283939299104692 Min: 0.17160607008953063 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5232176281998036 Min: 0.0\n","Softmax Output - Max: 0.8569971299078301 Min: 0.14300287009216991 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.023341352687295078 Min: -0.023341352687295078\n","Layer 0 - Gradient Weights Max: 0.01660342299153381 Min: -0.016067149444724284\n","ReLU Activation - Max: 1.3987723919769959 Min: 0.0\n","Softmax Output - Max: 0.8301355022500321 Min: 0.16986449774996787 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5296528105661962 Min: 0.0\n","Softmax Output - Max: 0.8587809064544478 Min: 0.1412190935455521 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0231997358216791 Min: -0.0231997358216791\n","Layer 0 - Gradient Weights Max: 0.016504978090928733 Min: -0.016075188920101326\n","ReLU Activation - Max: 1.404672598720052 Min: 0.0\n","Softmax Output - Max: 0.8318583752176998 Min: 0.16814162478230016 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.5360521449030555 Min: 0.0\n","Softmax Output - Max: 0.8605420206203067 Min: 0.13945797937969318 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02305681707850943 Min: -0.02305681707850943\n","Layer 0 - Gradient Weights Max: 0.016315293527931707 Min: -0.016160300118717243\n","ReLU Activation - Max: 1.4105436928162451 Min: 0.0\n","Softmax Output - Max: 0.8335618599278537 Min: 0.16643814007214627 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5423960509073018 Min: 0.0\n","Softmax Output - Max: 0.8622806568496227 Min: 0.13771934315037715 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022912634527728366 Min: -0.022912634527728366\n","Layer 0 - Gradient Weights Max: 0.01621701285323156 Min: -0.016119936062119738\n","ReLU Activation - Max: 1.4163825978697249 Min: 0.0\n","Softmax Output - Max: 0.8352460210265065 Min: 0.16475397897349361 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5487021899851452 Min: 0.0\n","Softmax Output - Max: 0.8639963327975847 Min: 0.1360036672024154 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02276724799640302 Min: -0.022767247996403017\n","Layer 0 - Gradient Weights Max: 0.016122195410790567 Min: -0.016071708670718516\n","ReLU Activation - Max: 1.4221880931149185 Min: 0.0\n","Softmax Output - Max: 0.8369116101691739 Min: 0.16308838983082613 Sum (first example): 1.0\n","ReLU Activation - Max: 1.554967506263177 Min: 0.0\n","Softmax Output - Max: 0.8656897581604437 Min: 0.1343102418395564 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022620700901991378 Min: -0.022620700901991378\n","Layer 0 - Gradient Weights Max: 0.015984096612955755 Min: -0.01605781099123041\n","ReLU Activation - Max: 1.4279623681902243 Min: 0.0\n","Softmax Output - Max: 0.8385581774623306 Min: 0.1614418225376693 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.5611981437616504 Min: 0.0\n","Softmax Output - Max: 0.8673611437049743 Min: 0.13263885629502573 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022473101529336917 Min: -0.022473101529336917\n","Layer 0 - Gradient Weights Max: 0.015907144827509184 Min: -0.016026376895329448\n","ReLU Activation - Max: 1.4337068365380652 Min: 0.0\n","Softmax Output - Max: 0.8401851339568271 Min: 0.15981486604317288 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5673907795474362 Min: 0.0\n","Softmax Output - Max: 0.8690108609870207 Min: 0.13098913901297912 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.022324605413657213 Min: -0.02232460541365721\n","Layer 0 - Gradient Weights Max: 0.015805310650397638 Min: -0.016047091660987313\n","ReLU Activation - Max: 1.4394202211397327 Min: 0.0\n","Softmax Output - Max: 0.841793473663033 Min: 0.15820652633696705 Sum (first example): 1.0\n","ReLU Activation - Max: 1.573547952798025 Min: 0.0\n","Softmax Output - Max: 0.8706385079209682 Min: 0.12936149207903183 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.022175194065401707 Min: -0.022175194065401707\n","Layer 0 - Gradient Weights Max: 0.01570278274235579 Min: -0.01600308776228366\n","ReLU Activation - Max: 1.4451000952133997 Min: 0.0\n","Softmax Output - Max: 0.8433826274721489 Min: 0.15661737252785118 Sum (first example): 0.9999999999999999\n","Epoch 380, Loss: 0.6087157400414047, Test Accuracy: 0.6917857142857143\n","ReLU Activation - Max: 1.5796659919173706 Min: 0.0\n","Softmax Output - Max: 0.872243488479726 Min: 0.127756511520274 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.022024807933904128 Min: -0.022024807933904128\n","Layer 0 - Gradient Weights Max: 0.015615467408201817 Min: -0.01592661268755752\n","ReLU Activation - Max: 1.4507526910299164 Min: 0.0\n","Softmax Output - Max: 0.8449524223655109 Min: 0.15504757763448895 Sum (first example): 1.0\n","ReLU Activation - Max: 1.585742038552228 Min: 0.0\n","Softmax Output - Max: 0.8738258420254893 Min: 0.12617415797451076 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02187345777227474 Min: -0.02187345777227474\n","Layer 0 - Gradient Weights Max: 0.015491611706758309 Min: -0.01593338105367775\n","ReLU Activation - Max: 1.4563711360040428 Min: 0.0\n","Softmax Output - Max: 0.8465034362787224 Min: 0.15349656372127765 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5917772973206992 Min: 0.0\n","Softmax Output - Max: 0.8753855167598172 Min: 0.12461448324018276 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02172132758068084 Min: -0.021721327580680835\n","Layer 0 - Gradient Weights Max: 0.015403698326746762 Min: -0.01588233005101184\n","ReLU Activation - Max: 1.4619578930562693 Min: 0.0\n","Softmax Output - Max: 0.8480359595233035 Min: 0.1519640404766965 Sum (first example): 1.0\n","ReLU Activation - Max: 1.5977774347556686 Min: 0.0\n","Softmax Output - Max: 0.8769231723081785 Min: 0.12307682769182143 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021568415812443485 Min: -0.021568415812443485\n","Layer 0 - Gradient Weights Max: 0.015314587169554882 Min: -0.015781501687337225\n","ReLU Activation - Max: 1.4675087625227248 Min: 0.0\n","Softmax Output - Max: 0.84955031650988 Min: 0.15044968349012 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6037397475570183 Min: 0.0\n","Softmax Output - Max: 0.8784397174261745 Min: 0.1215602825738254 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021414835334560667 Min: -0.021414835334560664\n","Layer 0 - Gradient Weights Max: 0.015216919153495956 Min: -0.01580757402434348\n","ReLU Activation - Max: 1.4730266967595542 Min: 0.0\n","Softmax Output - Max: 0.8510461603933432 Min: 0.14895383960665684 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6096651858902677 Min: 0.0\n","Softmax Output - Max: 0.879934591239399 Min: 0.12006540876060094 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021260724017244246 Min: -0.021260724017244246\n","Layer 0 - Gradient Weights Max: 0.015090762413055417 Min: -0.015753736888917007\n","ReLU Activation - Max: 1.4785060548316085 Min: 0.0\n","Softmax Output - Max: 0.8525235666357408 Min: 0.14747643336425922 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.6155455922688673 Min: 0.0\n","Softmax Output - Max: 0.8814089383337727 Min: 0.11859106166622735 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.021105971784890043 Min: -0.021105971784890043\n","Layer 0 - Gradient Weights Max: 0.01494334585224304 Min: -0.01576931397962362\n","ReLU Activation - Max: 1.4839480602950632 Min: 0.0\n","Softmax Output - Max: 0.8539825647839158 Min: 0.1460174352160843 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6213807571549508 Min: 0.0\n","Softmax Output - Max: 0.8828623167407337 Min: 0.11713768325926625 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02095072978551571 Min: -0.020950729785515716\n","Layer 0 - Gradient Weights Max: 0.014834351640167555 Min: -0.01568603284362095\n","ReLU Activation - Max: 1.4893549632381942 Min: 0.0\n","Softmax Output - Max: 0.8554232352016252 Min: 0.14457676479837486 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6271740642036137 Min: 0.0\n","Softmax Output - Max: 0.8842945421079422 Min: 0.1157054578920578 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02079495771966748 Min: -0.02079495771966748\n","Layer 0 - Gradient Weights Max: 0.01472853778030514 Min: -0.015614815569786089\n","ReLU Activation - Max: 1.4947272885260128 Min: 0.0\n","Softmax Output - Max: 0.8568457103189937 Min: 0.14315428968100635 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6329261830839785 Min: 0.0\n","Softmax Output - Max: 0.8857055744182537 Min: 0.11429442558174635 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02063879798348544 Min: -0.02063879798348544\n","Layer 0 - Gradient Weights Max: 0.014617205634305968 Min: -0.015557050339396843\n","ReLU Activation - Max: 1.500063903061667 Min: 0.0\n","Softmax Output - Max: 0.8582497986561565 Min: 0.14175020134384347 Sum (first example): 1.0\n","Epoch 390, Loss: 0.6053275016297072, Test Accuracy: 0.6917857142857143\n","ReLU Activation - Max: 1.6386412252626106 Min: 0.0\n","Softmax Output - Max: 0.8870958147386234 Min: 0.11290418526137658 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02048221470377118 Min: -0.020482214703771186\n","Layer 0 - Gradient Weights Max: 0.01450874257828627 Min: -0.015506134642742066\n","ReLU Activation - Max: 1.5053650379801566 Min: 0.0\n","Softmax Output - Max: 0.8596361318765913 Min: 0.14036386812340876 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6443151733463566 Min: 0.0\n","Softmax Output - Max: 0.8884652258563552 Min: 0.11153477414364484 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.020325312878662526 Min: -0.020325312878662533\n","Layer 0 - Gradient Weights Max: 0.014370453224380646 Min: -0.015479560712747377\n","ReLU Activation - Max: 1.5106298904372175 Min: 0.0\n","Softmax Output - Max: 0.8610043059266665 Min: 0.13899569407333334 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.6499422702160036 Min: 0.0\n","Softmax Output - Max: 0.8898133329032606 Min: 0.11018666709673935 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02016811353541301 Min: -0.02016811353541301\n","Layer 0 - Gradient Weights Max: 0.014274786510570563 Min: -0.01544445187781744\n","ReLU Activation - Max: 1.5158622378998707 Min: 0.0\n","Softmax Output - Max: 0.8623547223695458 Min: 0.13764527763045428 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6555317870370199 Min: 0.0\n","Softmax Output - Max: 0.8911413257028808 Min: 0.10885867429711908 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.02001069130459961 Min: -0.02001069130459961\n","Layer 0 - Gradient Weights Max: 0.01410909412384716 Min: -0.015434156858306724\n","ReLU Activation - Max: 1.5210565483875154 Min: 0.0\n","Softmax Output - Max: 0.8636872188883049 Min: 0.13631278111169515 Sum (first example): 1.0\n","ReLU Activation - Max: 1.661077455629915 Min: 0.0\n","Softmax Output - Max: 0.8924489954456485 Min: 0.10755100455435165 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019853057093058957 Min: -0.01985305709305896\n","Layer 0 - Gradient Weights Max: 0.014007775798453352 Min: -0.015402018672979507\n","ReLU Activation - Max: 1.52621234171797 Min: 0.0\n","Softmax Output - Max: 0.8650016975371217 Min: 0.1349983024628782 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6665762008341831 Min: 0.0\n","Softmax Output - Max: 0.8937364545897044 Min: 0.10626354541029553 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019695376043435204 Min: -0.01969537604343521\n","Layer 0 - Gradient Weights Max: 0.013900381377988331 Min: -0.015359992538834273\n","ReLU Activation - Max: 1.5313441274886075 Min: 0.0\n","Softmax Output - Max: 0.8662988491742386 Min: 0.13370115082576142 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.6720436012614304 Min: 0.0\n","Softmax Output - Max: 0.8950043347883265 Min: 0.10499566521167356 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019537562755396892 Min: -0.019537562755396892\n","Layer 0 - Gradient Weights Max: 0.013746182878246874 Min: -0.015286375612885877\n","ReLU Activation - Max: 1.5364344993011279 Min: 0.0\n","Softmax Output - Max: 0.867578058028924 Min: 0.13242194197107596 Sum (first example): 1.0\n","ReLU Activation - Max: 1.6774694548849949 Min: 0.0\n","Softmax Output - Max: 0.8962522007876469 Min: 0.10374779921235322 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019379632669362084 Min: -0.019379632669362087\n","Layer 0 - Gradient Weights Max: 0.0136449225089363 Min: -0.015374840320061681\n","ReLU Activation - Max: 1.5415088839080717 Min: 0.0\n","Softmax Output - Max: 0.868839611285564 Min: 0.131160388714436 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 1.6828557781417952 Min: 0.0\n","Softmax Output - Max: 0.8974805281138807 Min: 0.10251947188611937 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019221919447103317 Min: -0.019221919447103317\n","Layer 0 - Gradient Weights Max: 0.013535084908574179 Min: -0.015320576670648999\n","ReLU Activation - Max: 1.5465472932411708 Min: 0.0\n","Softmax Output - Max: 0.8700845738918641 Min: 0.1299154261081359 Sum (first example): 1.0\n","ReLU Activation - Max: 1.68820060381702 Min: 0.0\n","Softmax Output - Max: 0.898689900023528 Min: 0.1013100999764721 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.019064221230147862 Min: -0.019064221230147862\n","Layer 0 - Gradient Weights Max: 0.013386634271493603 Min: -0.015284128676454147\n","ReLU Activation - Max: 1.5515474397040925 Min: 0.0\n","Softmax Output - Max: 0.8713124092341337 Min: 0.12868759076586633 Sum (first example): 1.0\n","Epoch 400, Loss: 0.6023910628311175, Test Accuracy: 0.6925\n","ReLU Activation - Max: 1.6935009419625684 Min: 0.0\n","Softmax Output - Max: 0.8998799732925862 Min: 0.10012002670741381 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01890667962934681 Min: -0.018906679629346817\n","Layer 0 - Gradient Weights Max: 0.013238179398720162 Min: -0.015196965303856358\n","ReLU Activation - Max: 1.556508789529697 Min: 0.0\n","Softmax Output - Max: 0.8725233546600712 Min: 0.12747664533992878 Sum (first example): 1.0\n","ReLU Activation - Max: 1.698761291353939 Min: 0.0\n","Softmax Output - Max: 0.9010512193301334 Min: 0.09894878066986658 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018749193948762802 Min: -0.0187491939487628\n","Layer 0 - Gradient Weights Max: 0.013109601477752354 Min: -0.01504971968941212\n","ReLU Activation - Max: 1.561419635500348 Min: 0.0\n","Softmax Output - Max: 0.8737170071416758 Min: 0.12628299285832434 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.7039925276396561 Min: 0.0\n","Softmax Output - Max: 0.9022036412483296 Min: 0.09779635875167032 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018591713616385206 Min: -0.018591713616385206\n","Layer 0 - Gradient Weights Max: 0.012912486933469636 Min: -0.014985286170624001\n","ReLU Activation - Max: 1.5662907865865658 Min: 0.0\n","Softmax Output - Max: 0.874893803646592 Min: 0.12510619635340803 Sum (first example): 1.0\n","ReLU Activation - Max: 1.709176964995448 Min: 0.0\n","Softmax Output - Max: 0.903337886739443 Min: 0.09666211326055689 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.018434372247248428 Min: -0.018434372247248428\n","Layer 0 - Gradient Weights Max: 0.012805295141442383 Min: -0.014915986927943577\n","ReLU Activation - Max: 1.5711208452291892 Min: 0.0\n","Softmax Output - Max: 0.8760543914014244 Min: 0.12394560859857567 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7143163851565981 Min: 0.0\n","Softmax Output - Max: 0.9044543657262524 Min: 0.09554563427374765 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01827733363344038 Min: -0.018277333633440383\n","Layer 0 - Gradient Weights Max: 0.0126520931703058 Min: -0.014902711583705344\n","ReLU Activation - Max: 1.5759095311548248 Min: 0.0\n","Softmax Output - Max: 0.8771987574643978 Min: 0.12280124253560212 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7194139017559218 Min: 0.0\n","Softmax Output - Max: 0.9055529780885586 Min: 0.09444702191144147 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01812074843002903 Min: -0.01812074843002903\n","Layer 0 - Gradient Weights Max: 0.012541117871655802 Min: -0.014913006996726926\n","ReLU Activation - Max: 1.5806754871636282 Min: 0.0\n","Softmax Output - Max: 0.878326959324141 Min: 0.12167304067585898 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7244642242104173 Min: 0.0\n","Softmax Output - Max: 0.9066342366060813 Min: 0.09336576339391862 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017964631823227835 Min: -0.017964631823227828\n","Layer 0 - Gradient Weights Max: 0.012431618954747708 Min: -0.014856475324614125\n","ReLU Activation - Max: 1.5854056309190518 Min: 0.0\n","Softmax Output - Max: 0.8794394963734872 Min: 0.12056050362651285 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7294733824032182 Min: 0.0\n","Softmax Output - Max: 0.9076981937407712 Min: 0.09230180625922879 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017809048108878775 Min: -0.01780904810887878\n","Layer 0 - Gradient Weights Max: 0.012292542371487889 Min: -0.014870940380065088\n","ReLU Activation - Max: 1.590109733134499 Min: 0.0\n","Softmax Output - Max: 0.8805359900405099 Min: 0.1194640099594901 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7344304442555927 Min: 0.0\n","Softmax Output - Max: 0.9087446694551284 Min: 0.09125533054487142 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.01765389188412638 Min: -0.01765389188412638\n","Layer 0 - Gradient Weights Max: 0.01218356568050427 Min: -0.014813979983994049\n","ReLU Activation - Max: 1.5947781732343957 Min: 0.0\n","Softmax Output - Max: 0.8816171712919033 Min: 0.11838282870809669 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7393465706522444 Min: 0.0\n","Softmax Output - Max: 0.9097741067894343 Min: 0.09022589321056579 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017499261379460555 Min: -0.017499261379460562\n","Layer 0 - Gradient Weights Max: 0.012012004752897147 Min: -0.014764631028536397\n","ReLU Activation - Max: 1.5994112596235488 Min: 0.0\n","Softmax Output - Max: 0.8826832562016252 Min: 0.11731674379837466 Sum (first example): 1.0\n","Epoch 410, Loss: 0.5998695430441408, Test Accuracy: 0.6921428571428572\n","ReLU Activation - Max: 1.7442179547591234 Min: 0.0\n","Softmax Output - Max: 0.9107872788793167 Min: 0.08921272112068337 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017345013291307346 Min: -0.017345013291307346\n","Layer 0 - Gradient Weights Max: 0.011904596511286798 Min: -0.01472559475373134\n","ReLU Activation - Max: 1.604010887245411 Min: 0.0\n","Softmax Output - Max: 0.8837338952814745 Min: 0.11626610471852557 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.749050599743299 Min: 0.0\n","Softmax Output - Max: 0.9117839392546364 Min: 0.0882160607453635 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.017191308053869928 Min: -0.017191308053869928\n","Layer 0 - Gradient Weights Max: 0.01180459471988536 Min: -0.014636945851861951\n","ReLU Activation - Max: 1.6085723208263951 Min: 0.0\n","Softmax Output - Max: 0.8847695941950834 Min: 0.11523040580491664 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7538373616902063 Min: 0.0\n","Softmax Output - Max: 0.9127646218696596 Min: 0.0872353781303404 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.01703816409211268 Min: -0.01703816409211268\n","Layer 0 - Gradient Weights Max: 0.011697864785957888 Min: -0.014605154192661536\n","ReLU Activation - Max: 1.6131063929892844 Min: 0.0\n","Softmax Output - Max: 0.8857903259704553 Min: 0.11420967402954486 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7587201582735732 Min: 0.0\n","Softmax Output - Max: 0.9137290294289172 Min: 0.08627097057108288 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016885751642041757 Min: -0.016885751642041754\n","Layer 0 - Gradient Weights Max: 0.011527930102709867 Min: -0.014555804239339429\n","ReLU Activation - Max: 1.6175900661322662 Min: 0.0\n","Softmax Output - Max: 0.8867965849406665 Min: 0.11320341505933357 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 1.7638092146973712 Min: 0.0\n","Softmax Output - Max: 0.9146777369979691 Min: 0.08532226300203093 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016733833414258593 Min: -0.016733833414258593\n","Layer 0 - Gradient Weights Max: 0.011390635922722108 Min: -0.01449101095566842\n","ReLU Activation - Max: 1.6220348660796946 Min: 0.0\n","Softmax Output - Max: 0.8877883870822557 Min: 0.11221161291774438 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7688654262659746 Min: 0.0\n","Softmax Output - Max: 0.915611065727246 Min: 0.08438893427275398 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016582514404415712 Min: -0.016582514404415716\n","Layer 0 - Gradient Weights Max: 0.011288359372742557 Min: -0.014493805417675549\n","ReLU Activation - Max: 1.6264511006068285 Min: 0.0\n","Softmax Output - Max: 0.8887662086466529 Min: 0.11123379135334713 Sum (first example): 1.0\n","ReLU Activation - Max: 1.773884946624536 Min: 0.0\n","Softmax Output - Max: 0.9165296781878676 Min: 0.08347032181213238 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.016431844881708905 Min: -0.016431844881708905\n","Layer 0 - Gradient Weights Max: 0.011172293067697778 Min: -0.014326471506092793\n","ReLU Activation - Max: 1.6308298206563845 Min: 0.0\n","Softmax Output - Max: 0.8897299215200107 Min: 0.11027007847998938 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7788580980838484 Min: 0.0\n","Softmax Output - Max: 0.9174332214023705 Min: 0.08256677859762947 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01628179619705814 Min: -0.01628179619705814\n","Layer 0 - Gradient Weights Max: 0.011066186221954674 Min: -0.01426823988223021\n","ReLU Activation - Max: 1.6351737812014528 Min: 0.0\n","Softmax Output - Max: 0.8906791964250613 Min: 0.10932080357493862 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7837922280678065 Min: 0.0\n","Softmax Output - Max: 0.91832157807098 Min: 0.08167842192901993 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01613249749682521 Min: -0.0161324974968252\n","Layer 0 - Gradient Weights Max: 0.010912807065605745 Min: -0.014278739130698992\n","ReLU Activation - Max: 1.6394919678632238 Min: 0.0\n","Softmax Output - Max: 0.8916146494785716 Min: 0.10838535052142835 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7886928229368877 Min: 0.0\n","Softmax Output - Max: 0.9191952908538484 Min: 0.08080470914615169 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015983975808552 Min: -0.015983975808551997\n","Layer 0 - Gradient Weights Max: 0.010787470189811929 Min: -0.014208092184146379\n","ReLU Activation - Max: 1.6437748216189785 Min: 0.0\n","Softmax Output - Max: 0.8925364593881334 Min: 0.10746354061186661 Sum (first example): 1.0\n","Epoch 420, Loss: 0.5977167734819359, Test Accuracy: 0.6921428571428572\n","ReLU Activation - Max: 1.7935551692111726 Min: 0.0\n","Softmax Output - Max: 0.9200546608964673 Min: 0.07994533910353271 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01583620905153703 Min: -0.015836209051537025\n","Layer 0 - Gradient Weights Max: 0.010725127558299186 Min: -0.014128716602584091\n","ReLU Activation - Max: 1.648032030752876 Min: 0.0\n","Softmax Output - Max: 0.8934450134339044 Min: 0.10655498656609555 Sum (first example): 1.0\n","ReLU Activation - Max: 1.7983798897461574 Min: 0.0\n","Softmax Output - Max: 0.92089997221816 Min: 0.07910002778184 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015689366831229963 Min: -0.015689366831229963\n","Layer 0 - Gradient Weights Max: 0.010643764225143867 Min: -0.01415493756025216\n","ReLU Activation - Max: 1.6522673399822196 Min: 0.0\n","Softmax Output - Max: 0.8943404343765089 Min: 0.10565956562349099 Sum (first example): 1.0\n","ReLU Activation - Max: 1.803173944024593 Min: 0.0\n","Softmax Output - Max: 0.9217312776363392 Min: 0.0782687223636608 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015543529826072284 Min: -0.015543529826072286\n","Layer 0 - Gradient Weights Max: 0.010540306746090628 Min: -0.014096501810585776\n","ReLU Activation - Max: 1.6564686785869827 Min: 0.0\n","Softmax Output - Max: 0.8952221632981248 Min: 0.10477783670187517 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8079298787599836 Min: 0.0\n","Softmax Output - Max: 0.9225482717683932 Min: 0.07745172823160679 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01539851635848098 Min: -0.01539851635848098\n","Layer 0 - Gradient Weights Max: 0.010488200363805952 Min: -0.014052042089817553\n","ReLU Activation - Max: 1.6606467163039136 Min: 0.0\n","Softmax Output - Max: 0.8960905987586049 Min: 0.10390940124139514 Sum (first example): 1.0\n","ReLU Activation - Max: 1.812644629906142 Min: 0.0\n","Softmax Output - Max: 0.9233523695860626 Min: 0.0766476304139374 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015254415944508755 Min: -0.015254415944508753\n","Layer 0 - Gradient Weights Max: 0.010450572607969309 Min: -0.013925630827009185\n","ReLU Activation - Max: 1.664779039930658 Min: 0.0\n","Softmax Output - Max: 0.896946479020337 Min: 0.10305352097966314 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8173052971075414 Min: 0.0\n","Softmax Output - Max: 0.9241433044092116 Min: 0.0758566955907885 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.015111246394664999 Min: -0.015111246394665004\n","Layer 0 - Gradient Weights Max: 0.010333294095334133 Min: -0.013847747027065059\n","ReLU Activation - Max: 1.6688749611469647 Min: 0.0\n","Softmax Output - Max: 0.897789149692913 Min: 0.10221085030708685 Sum (first example): 1.0\n","ReLU Activation - Max: 1.821928087186612 Min: 0.0\n","Softmax Output - Max: 0.9249209564081157 Min: 0.07507904359188426 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014969004638372497 Min: -0.014969004638372497\n","Layer 0 - Gradient Weights Max: 0.010221825762668764 Min: -0.013709524704393617\n","ReLU Activation - Max: 1.6729372559358824 Min: 0.0\n","Softmax Output - Max: 0.8986193712910506 Min: 0.10138062870894946 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.8265193520937621 Min: 0.0\n","Softmax Output - Max: 0.9256861162380106 Min: 0.07431388376198943 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014827725554499103 Min: -0.014827725554499103\n","Layer 0 - Gradient Weights Max: 0.010185969151965586 Min: -0.013738584193450245\n","ReLU Activation - Max: 1.676980709588771 Min: 0.0\n","Softmax Output - Max: 0.899437512978246 Min: 0.10056248702175397 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8310753175044232 Min: 0.0\n","Softmax Output - Max: 0.9264387678682632 Min: 0.0735612321317369 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01468760843173375 Min: -0.014687608431733747\n","Layer 0 - Gradient Weights Max: 0.01007496809571609 Min: -0.013664872127936675\n","ReLU Activation - Max: 1.6809879816058462 Min: 0.0\n","Softmax Output - Max: 0.9002437101509336 Min: 0.0997562898490663 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8355927707200974 Min: 0.0\n","Softmax Output - Max: 0.9271789945577424 Min: 0.07282100544225754 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.014548415044206666 Min: -0.014548415044206663\n","Layer 0 - Gradient Weights Max: 0.009975150104790572 Min: -0.013606769613137781\n","ReLU Activation - Max: 1.684962599923925 Min: 0.0\n","Softmax Output - Max: 0.9010370616678127 Min: 0.09896293833218733 Sum (first example): 1.0\n","Epoch 430, Loss: 0.5958820103607753, Test Accuracy: 0.6921428571428572\n","ReLU Activation - Max: 1.8400736286066592 Min: 0.0\n","Softmax Output - Max: 0.9279065456373655 Min: 0.07209345436263442 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014410195018868201 Min: -0.014410195018868201\n","Layer 0 - Gradient Weights Max: 0.009856014765728385 Min: -0.013537313384265818\n","ReLU Activation - Max: 1.6889056574001322 Min: 0.0\n","Softmax Output - Max: 0.9018180863123578 Min: 0.09818191368764208 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8445167003235108 Min: 0.0\n","Softmax Output - Max: 0.9286219870786955 Min: 0.07137801292130443 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014272929863043451 Min: -0.01427292986304345\n","Layer 0 - Gradient Weights Max: 0.009725771648811972 Min: -0.01347022832703278\n","ReLU Activation - Max: 1.692816785972915 Min: 0.0\n","Softmax Output - Max: 0.9025870123256716 Min: 0.09741298767432849 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8489320400162554 Min: 0.0\n","Softmax Output - Max: 0.9293257035340401 Min: 0.07067429646595987 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014136487611448888 Min: -0.014136487611448888\n","Layer 0 - Gradient Weights Max: 0.009691424943375621 Min: -0.013379525641488123\n","ReLU Activation - Max: 1.6966884787024883 Min: 0.0\n","Softmax Output - Max: 0.9033440164438102 Min: 0.09665598355618972 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.8532987137385186 Min: 0.0\n","Softmax Output - Max: 0.9300176737618536 Min: 0.06998232623814644 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.014000969191194361 Min: -0.01400096919119436\n","Layer 0 - Gradient Weights Max: 0.009579677894974856 Min: -0.013358507462954586\n","ReLU Activation - Max: 1.7005261643983953 Min: 0.0\n","Softmax Output - Max: 0.904089417388956 Min: 0.09591058261104397 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8576339805346682 Min: 0.0\n","Softmax Output - Max: 0.9306977249538028 Min: 0.0693022750461973 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013866533030669032 Min: -0.013866533030669028\n","Layer 0 - Gradient Weights Max: 0.009466425819541071 Min: -0.013312276444860974\n","ReLU Activation - Max: 1.7043308162716917 Min: 0.0\n","Softmax Output - Max: 0.9048233862587591 Min: 0.09517661374124092 Sum (first example): 1.0\n","ReLU Activation - Max: 1.861944453631228 Min: 0.0\n","Softmax Output - Max: 0.9313663933586109 Min: 0.06863360664138911 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013733150312876679 Min: -0.013733150312876677\n","Layer 0 - Gradient Weights Max: 0.009391267470964923 Min: -0.013243863899346963\n","ReLU Activation - Max: 1.7080999637038934 Min: 0.0\n","Softmax Output - Max: 0.9055456979283277 Min: 0.09445430207167228 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8662175007574502 Min: 0.0\n","Softmax Output - Max: 0.9320234813681524 Min: 0.06797651863184766 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013600756795688554 Min: -0.013600756795688554\n","Layer 0 - Gradient Weights Max: 0.009238085319115002 Min: -0.013174237119524625\n","ReLU Activation - Max: 1.7118335820504162 Min: 0.0\n","Softmax Output - Max: 0.90625699801174 Min: 0.09374300198826006 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8704524224555261 Min: 0.0\n","Softmax Output - Max: 0.9326695138568559 Min: 0.06733048614314416 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013469252802209305 Min: -0.013469252802209305\n","Layer 0 - Gradient Weights Max: 0.009093880197417165 Min: -0.013057166543458863\n","ReLU Activation - Max: 1.715535262563375 Min: 0.0\n","Softmax Output - Max: 0.9069581488730809 Min: 0.09304185112691903 Sum (first example): 1.0\n","ReLU Activation - Max: 1.874641829935275 Min: 0.0\n","Softmax Output - Max: 0.9333052870576142 Min: 0.06669471294238594 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013338652419874643 Min: -0.013338652419874643\n","Layer 0 - Gradient Weights Max: 0.00897019253410651 Min: -0.013058788943371603\n","ReLU Activation - Max: 1.7192040084116633 Min: 0.0\n","Softmax Output - Max: 0.907648623879924 Min: 0.09235137612007611 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8788002656648493 Min: 0.0\n","Softmax Output - Max: 0.9339305203441552 Min: 0.06606947965584482 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.013209308330789522 Min: -0.013209308330789522\n","Layer 0 - Gradient Weights Max: 0.008833458499142523 Min: -0.01301211313179201\n","ReLU Activation - Max: 1.7228266966564552 Min: 0.0\n","Softmax Output - Max: 0.9083288416855363 Min: 0.09167115831446357 Sum (first example): 1.0\n","Epoch 440, Loss: 0.594316203316795, Test Accuracy: 0.6928571428571428\n","ReLU Activation - Max: 1.8829478705545981 Min: 0.0\n","Softmax Output - Max: 0.9345461915955315 Min: 0.0654538084044685 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01308096907516131 Min: -0.01308096907516131\n","Layer 0 - Gradient Weights Max: 0.008732754662861726 Min: -0.012935225615920599\n","ReLU Activation - Max: 1.726413717320264 Min: 0.0\n","Softmax Output - Max: 0.908998101428943 Min: 0.09100189857105692 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.8870578503857365 Min: 0.0\n","Softmax Output - Max: 0.9351522821705798 Min: 0.06484771782942024 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012953667070966903 Min: -0.012953667070966901\n","Layer 0 - Gradient Weights Max: 0.008644853407258414 Min: -0.01287733455702788\n","ReLU Activation - Max: 1.7299687555285903 Min: 0.0\n","Softmax Output - Max: 0.9096574670866996 Min: 0.0903425329133003 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8911376782395446 Min: 0.0\n","Softmax Output - Max: 0.935748133053821 Min: 0.0642518669461789 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012827505012999531 Min: -0.012827505012999535\n","Layer 0 - Gradient Weights Max: 0.008399370830186166 Min: -0.012949844545968467\n","ReLU Activation - Max: 1.7334987834044961 Min: 0.0\n","Softmax Output - Max: 0.9103069848369898 Min: 0.08969301516301015 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8952029260590328 Min: 0.0\n","Softmax Output - Max: 0.9363343170744733 Min: 0.0636656829255267 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012702466861397522 Min: -0.01270246686139752\n","Layer 0 - Gradient Weights Max: 0.008347550997076483 Min: -0.012902105675134144\n","ReLU Activation - Max: 1.7370043852936161 Min: 0.0\n","Softmax Output - Max: 0.9109467004880831 Min: 0.08905329951191689 Sum (first example): 1.0\n","ReLU Activation - Max: 1.8992348187915902 Min: 0.0\n","Softmax Output - Max: 0.9369107602477905 Min: 0.06308923975220934 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012578574818185648 Min: -0.012578574818185645\n","Layer 0 - Gradient Weights Max: 0.008253560488388894 Min: -0.012851158187231688\n","ReLU Activation - Max: 1.740476666913567 Min: 0.0\n","Softmax Output - Max: 0.9115773657456678 Min: 0.08842263425433204 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.9032332990462386 Min: 0.0\n","Softmax Output - Max: 0.9374781021730623 Min: 0.06252189782693769 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012455821417255528 Min: -0.012455821417255528\n","Layer 0 - Gradient Weights Max: 0.008147795335056461 Min: -0.012882891093163337\n","ReLU Activation - Max: 1.7439382813498088 Min: 0.0\n","Softmax Output - Max: 0.9121985131916543 Min: 0.08780148680834568 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9072170801124357 Min: 0.0\n","Softmax Output - Max: 0.9380359904241082 Min: 0.061964009575891765 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012334382899993935 Min: -0.012334382899993935\n","Layer 0 - Gradient Weights Max: 0.008056532439115707 Min: -0.012866011080726512\n","ReLU Activation - Max: 1.7473757536381962 Min: 0.0\n","Softmax Output - Max: 0.9128111775134125 Min: 0.08718882248658763 Sum (first example): 1.0\n","ReLU Activation - Max: 1.911179035616403 Min: 0.0\n","Softmax Output - Max: 0.9385853801161748 Min: 0.061414619883825274 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01221404044092209 Min: -0.01221404044092209\n","Layer 0 - Gradient Weights Max: 0.007969351141290769 Min: -0.01280921402278396\n","ReLU Activation - Max: 1.7507845629471335 Min: 0.0\n","Softmax Output - Max: 0.9134149981743457 Min: 0.0865850018256544 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9151085192736197 Min: 0.0\n","Softmax Output - Max: 0.9391263676203898 Min: 0.060873632379610236 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.012094836012297903 Min: -0.012094836012297903\n","Layer 0 - Gradient Weights Max: 0.007890438566344292 Min: -0.012797316437555092\n","ReLU Activation - Max: 1.7541731683739274 Min: 0.0\n","Softmax Output - Max: 0.9140095381102357 Min: 0.08599046188976438 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9190017249638276 Min: 0.0\n","Softmax Output - Max: 0.9396580829372828 Min: 0.060341917062717215 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01197690480394606 Min: -0.01197690480394606\n","Layer 0 - Gradient Weights Max: 0.007839606807407978 Min: -0.012817543805930181\n","ReLU Activation - Max: 1.7575414937002587 Min: 0.0\n","Softmax Output - Max: 0.914595448232009 Min: 0.08540455176799099 Sum (first example): 1.0\n","Epoch 450, Loss: 0.5929716165219266, Test Accuracy: 0.6932142857142857\n","ReLU Activation - Max: 1.922860794319584 Min: 0.0\n","Softmax Output - Max: 0.9401810796917475 Min: 0.05981892030825258 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.011860124136269695 Min: -0.011860124136269691\n","Layer 0 - Gradient Weights Max: 0.007682891888344959 Min: -0.012690555427756772\n","ReLU Activation - Max: 1.7608743217536669 Min: 0.0\n","Softmax Output - Max: 0.9151721261965545 Min: 0.0848278738034454 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9266837532435321 Min: 0.0\n","Softmax Output - Max: 0.940695670769493 Min: 0.05930432923050688 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.011744263125174232 Min: -0.011744263125174232\n","Layer 0 - Gradient Weights Max: 0.007553657463599859 Min: -0.012695316333459859\n","ReLU Activation - Max: 1.7641744565023179 Min: 0.0\n","Softmax Output - Max: 0.9157395332156454 Min: 0.08426046678435468 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9304886117012106 Min: 0.0\n","Softmax Output - Max: 0.9412015258341241 Min: 0.058798474165875934 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.011629606308862782 Min: -0.011629606308862778\n","Layer 0 - Gradient Weights Max: 0.007476425304529981 Min: -0.012637126004730376\n","ReLU Activation - Max: 1.7674523700304443 Min: 0.0\n","Softmax Output - Max: 0.9162985660403005 Min: 0.08370143395969952 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.9342642106718926 Min: 0.0\n","Softmax Output - Max: 0.9416989850237181 Min: 0.058301014976281866 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.011516034853666665 Min: -0.011516034853666665\n","Layer 0 - Gradient Weights Max: 0.007368812224019857 Min: -0.012578541330343723\n","ReLU Activation - Max: 1.770701685271188 Min: 0.0\n","Softmax Output - Max: 0.916849293819454 Min: 0.08315070618054596 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 1.9380087008889295 Min: 0.0\n","Softmax Output - Max: 0.9421883423667832 Min: 0.05781165763321698 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.011403580674644033 Min: -0.011403580674644032\n","Layer 0 - Gradient Weights Max: 0.007313691574087812 Min: -0.01251691646520198\n","ReLU Activation - Max: 1.7739336589563517 Min: 0.0\n","Softmax Output - Max: 0.9173921011834439 Min: 0.08260789881655606 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9417198333672996 Min: 0.0\n","Softmax Output - Max: 0.942669666606724 Min: 0.057330333393276076 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.011292298350651006 Min: -0.01129229835065101\n","Layer 0 - Gradient Weights Max: 0.00723287723592408 Min: -0.012460945975343163\n","ReLU Activation - Max: 1.7771387953043352 Min: 0.0\n","Softmax Output - Max: 0.9179272465252956 Min: 0.08207275347470439 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9454004893033507 Min: 0.0\n","Softmax Output - Max: 0.9431438006161219 Min: 0.056856199383878085 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.01118206353051053 Min: -0.011182063530510534\n","Layer 0 - Gradient Weights Max: 0.00716571696469852 Min: -0.01236125467053359\n","ReLU Activation - Max: 1.7803252682013815 Min: 0.0\n","Softmax Output - Max: 0.9184540134495998 Min: 0.08154598655040025 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.9490633416949372 Min: 0.0\n","Softmax Output - Max: 0.9436095601560184 Min: 0.056390439843981545 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.011072937809850053 Min: -0.01107293780985005\n","Layer 0 - Gradient Weights Max: 0.007079601894094964 Min: -0.01244128540681217\n","ReLU Activation - Max: 1.7835008544532178 Min: 0.0\n","Softmax Output - Max: 0.9189731850259172 Min: 0.08102681497408269 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.9527125762867326 Min: 0.0\n","Softmax Output - Max: 0.9440679483877721 Min: 0.05593205161222788 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0109649301907957 Min: -0.0109649301907957\n","Layer 0 - Gradient Weights Max: 0.007020519989128522 Min: -0.012361192066244897\n","ReLU Activation - Max: 1.7866491738310843 Min: 0.0\n","Softmax Output - Max: 0.9194846098279835 Min: 0.08051539017201653 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.9563330731049078 Min: 0.0\n","Softmax Output - Max: 0.9445189399024013 Min: 0.0554810600975986 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.010857904925365378 Min: -0.010857904925365378\n","Layer 0 - Gradient Weights Max: 0.0069499268685275254 Min: -0.012338319383887464\n","ReLU Activation - Max: 1.7897773627240159 Min: 0.0\n","Softmax Output - Max: 0.9199887552305518 Min: 0.08001124476944824 Sum (first example): 1.0\n","Epoch 460, Loss: 0.5918096449747454, Test Accuracy: 0.6935714285714286\n","ReLU Activation - Max: 1.959919670920523 Min: 0.0\n","Softmax Output - Max: 0.9449627285788988 Min: 0.05503727142110122 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.010752000783024462 Min: -0.01075200078302446\n","Layer 0 - Gradient Weights Max: 0.006894024685127308 Min: -0.01223518285294846\n","ReLU Activation - Max: 1.7928780182108026 Min: 0.0\n","Softmax Output - Max: 0.9204858348911373 Min: 0.07951416510886265 Sum (first example): 1.0\n","ReLU Activation - Max: 1.963459730052991 Min: 0.0\n","Softmax Output - Max: 0.9453993798498714 Min: 0.05460062015012858 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.010647080840699312 Min: -0.01064708084069931\n","Layer 0 - Gradient Weights Max: 0.006818491164991855 Min: -0.012253287731550285\n","ReLU Activation - Max: 1.7959644260685592 Min: 0.0\n","Softmax Output - Max: 0.920975187431762 Min: 0.07902481256823797 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.9669619323044047 Min: 0.0\n","Softmax Output - Max: 0.9458286733346805 Min: 0.05417132666531957 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01054358655019911 Min: -0.010543586550199114\n","Layer 0 - Gradient Weights Max: 0.00671645170119087 Min: -0.012278297061728678\n","ReLU Activation - Max: 1.7990299941768193 Min: 0.0\n","Softmax Output - Max: 0.921456730475062 Min: 0.07854326952493801 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 1.9704345688458598 Min: 0.0\n","Softmax Output - Max: 0.9462508090606941 Min: 0.053749190939305906 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.01044130758177209 Min: -0.01044130758177209\n","Layer 0 - Gradient Weights Max: 0.006630993651384938 Min: -0.012200708015285002\n","ReLU Activation - Max: 1.8020629579923557 Min: 0.0\n","Softmax Output - Max: 0.9219315841127054 Min: 0.07806841588729461 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 1.9738863250166763 Min: 0.0\n","Softmax Output - Max: 0.9466664634881962 Min: 0.053333536511803815 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.010340046260304742 Min: -0.01034004626030474\n","Layer 0 - Gradient Weights Max: 0.00655451847437688 Min: -0.012259022393844184\n","ReLU Activation - Max: 1.8050695732782553 Min: 0.0\n","Softmax Output - Max: 0.9223992931084362 Min: 0.0776007068915638 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9773167447442792 Min: 0.0\n","Softmax Output - Max: 0.9470751757880047 Min: 0.05292482421199532 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.010240139256490439 Min: -0.01024013925649044\n","Layer 0 - Gradient Weights Max: 0.006477451045378794 Min: -0.012252246536220259\n","ReLU Activation - Max: 1.8080387620068246 Min: 0.0\n","Softmax Output - Max: 0.9228602631692865 Min: 0.07713973683071353 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.9807336909523565 Min: 0.0\n","Softmax Output - Max: 0.9474776779817209 Min: 0.052522322018279194 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.010141376494070889 Min: -0.010141376494070889\n","Layer 0 - Gradient Weights Max: 0.006418705119228298 Min: -0.012224429798822774\n","ReLU Activation - Max: 1.8109900979829092 Min: 0.0\n","Softmax Output - Max: 0.9233145803233577 Min: 0.07668541967664232 Sum (first example): 1.0\n","ReLU Activation - Max: 1.98412994712369 Min: 0.0\n","Softmax Output - Max: 0.9478742220862562 Min: 0.052125777913743704 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.010043749372123191 Min: -0.010043749372123193\n","Layer 0 - Gradient Weights Max: 0.006417989738105109 Min: -0.012163777764294205\n","ReLU Activation - Max: 1.8139122204158398 Min: 0.0\n","Softmax Output - Max: 0.9237623034503786 Min: 0.07623769654962154 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9874984751555047 Min: 0.0\n","Softmax Output - Max: 0.9482648900230967 Min: 0.05173510997690329 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009947056238663488 Min: -0.009947056238663488\n","Layer 0 - Gradient Weights Max: 0.006398039893256886 Min: -0.012104342579620708\n","ReLU Activation - Max: 1.816806412649531 Min: 0.0\n","Softmax Output - Max: 0.9242033089747554 Min: 0.07579669102524461 Sum (first example): 1.0\n","ReLU Activation - Max: 1.9908445625666724 Min: 0.0\n","Softmax Output - Max: 0.9486495609447535 Min: 0.05135043905524639 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009851348436502932 Min: -0.00985134843650293\n","Layer 0 - Gradient Weights Max: 0.006337119152797879 Min: -0.012100743994685725\n","ReLU Activation - Max: 1.8196861777259894 Min: 0.0\n","Softmax Output - Max: 0.9246378884116704 Min: 0.07536211158832962 Sum (first example): 1.0\n","Epoch 470, Loss: 0.5907942945370527, Test Accuracy: 0.6946428571428571\n","ReLU Activation - Max: 1.994169887135782 Min: 0.0\n","Softmax Output - Max: 0.9490278935567845 Min: 0.05097210644321546 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009756874074367136 Min: -0.009756874074367138\n","Layer 0 - Gradient Weights Max: 0.0062418560321931575 Min: -0.012117338090904267\n","ReLU Activation - Max: 1.8225421021460344 Min: 0.0\n","Softmax Output - Max: 0.9250663414474779 Min: 0.07493365855252213 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 1.997481061217504 Min: 0.0\n","Softmax Output - Max: 0.9494003565463315 Min: 0.05059964345366857 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009663587017440092 Min: -0.009663587017440094\n","Layer 0 - Gradient Weights Max: 0.006129183120681498 Min: -0.012029769523620984\n","ReLU Activation - Max: 1.8253767657590756 Min: 0.0\n","Softmax Output - Max: 0.925488771510347 Min: 0.07451122848965296 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0007699190788726 Min: 0.0\n","Softmax Output - Max: 0.9497672803937603 Min: 0.05023271960623975 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009571314537488934 Min: -0.009571314537488936\n","Layer 0 - Gradient Weights Max: 0.005996629440859157 Min: -0.011978645647787074\n","ReLU Activation - Max: 1.8281794065392876 Min: 0.0\n","Softmax Output - Max: 0.9259053323564882 Min: 0.07409466764351186 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.004056165012041 Min: 0.0\n","Softmax Output - Max: 0.9501285512633268 Min: 0.04987144873667315 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009479923376318847 Min: -0.009479923376318847\n","Layer 0 - Gradient Weights Max: 0.005963641068439029 Min: -0.011898706557667805\n","ReLU Activation - Max: 1.830957649000589 Min: 0.0\n","Softmax Output - Max: 0.9263160986359807 Min: 0.07368390136401919 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0073188954735075 Min: 0.0\n","Softmax Output - Max: 0.9504847553134496 Min: 0.0495152446865505 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009389443746111264 Min: -0.00938944374611126\n","Layer 0 - Gradient Weights Max: 0.00587619287323466 Min: -0.011880542096173934\n","ReLU Activation - Max: 1.8337226989003 Min: 0.0\n","Softmax Output - Max: 0.9267207206003371 Min: 0.07327927939966303 Sum (first example): 1.0\n","ReLU Activation - Max: 2.010559042201851 Min: 0.0\n","Softmax Output - Max: 0.9508354735946892 Min: 0.049164526405310736 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009300227830496142 Min: -0.009300227830496142\n","Layer 0 - Gradient Weights Max: 0.005795490135174606 Min: -0.011866587263665934\n","ReLU Activation - Max: 1.8364699911243847 Min: 0.0\n","Softmax Output - Max: 0.9271194874438815 Min: 0.07288051255611852 Sum (first example): 1.0\n","ReLU Activation - Max: 2.013780844534308 Min: 0.0\n","Softmax Output - Max: 0.9511807900252863 Min: 0.048819209974713694 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009212302227008543 Min: -0.009212302227008543\n","Layer 0 - Gradient Weights Max: 0.005730153447663118 Min: -0.011813678638834159\n","ReLU Activation - Max: 1.8391951857963573 Min: 0.0\n","Softmax Output - Max: 0.9275121466075473 Min: 0.07248785339245278 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0169774250352317 Min: 0.0\n","Softmax Output - Max: 0.951520441213107 Min: 0.04847955878689301 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009125458177489814 Min: -0.009125458177489816\n","Layer 0 - Gradient Weights Max: 0.005587731642724597 Min: -0.012000560905008718\n","ReLU Activation - Max: 1.8419213594422845 Min: 0.0\n","Softmax Output - Max: 0.9278994165569918 Min: 0.07210058344300824 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0201612507696933 Min: 0.0\n","Softmax Output - Max: 0.9518550339598673 Min: 0.04814496604013271 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.009040029633732015 Min: -0.009040029633732018\n","Layer 0 - Gradient Weights Max: 0.005492702071422775 Min: -0.01196865262815572\n","ReLU Activation - Max: 1.8446135524116984 Min: 0.0\n","Softmax Output - Max: 0.92828167221896 Min: 0.07171832778103994 Sum (first example): 1.0\n","ReLU Activation - Max: 2.023320852363753 Min: 0.0\n","Softmax Output - Max: 0.9521847270136032 Min: 0.0478152729863967 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008955456471560388 Min: -0.00895545647156039\n","Layer 0 - Gradient Weights Max: 0.005260901773985422 Min: -0.012048122308059022\n","ReLU Activation - Max: 1.8472881065413327 Min: 0.0\n","Softmax Output - Max: 0.9286587334079334 Min: 0.0713412665920665 Sum (first example): 1.0\n","Epoch 480, Loss: 0.5898964883922232, Test Accuracy: 0.695\n","ReLU Activation - Max: 2.0264631131174147 Min: 0.0\n","Softmax Output - Max: 0.9525094047772175 Min: 0.04749059522278255 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008872077137695736 Min: -0.008872077137695732\n","Layer 0 - Gradient Weights Max: 0.0052194480430216555 Min: -0.012072909252629122\n","ReLU Activation - Max: 1.8499332581346148 Min: 0.0\n","Softmax Output - Max: 0.9290306664310193 Min: 0.07096933356898057 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.029584088687812 Min: 0.0\n","Softmax Output - Max: 0.9528295280430497 Min: 0.04717047195695028 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008789857346644416 Min: -0.008789857346644413\n","Layer 0 - Gradient Weights Max: 0.005152504692216755 Min: -0.012010055429516251\n","ReLU Activation - Max: 1.8525524710951664 Min: 0.0\n","Softmax Output - Max: 0.9293976055458599 Min: 0.07060239445414021 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0326868201970196 Min: 0.0\n","Softmax Output - Max: 0.9531447451861236 Min: 0.04685525481387649 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008708501243514664 Min: -0.008708501243514665\n","Layer 0 - Gradient Weights Max: 0.005073649897916767 Min: -0.011884772239017755\n","ReLU Activation - Max: 1.8551477451508016 Min: 0.0\n","Softmax Output - Max: 0.9297596338056947 Min: 0.07024036619430535 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.035747935342694 Min: 0.0\n","Softmax Output - Max: 0.9534553732259006 Min: 0.04654462677409946 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00862792682331615 Min: -0.00862792682331615\n","Layer 0 - Gradient Weights Max: 0.004986381447617573 Min: -0.011862688562985706\n","ReLU Activation - Max: 1.8577183536766744 Min: 0.0\n","Softmax Output - Max: 0.9301161599496527 Min: 0.06988384005034734 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0387892560817025 Min: 0.0\n","Softmax Output - Max: 0.9537615369520284 Min: 0.04623846304797164 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008548348411263677 Min: -0.008548348411263677\n","Layer 0 - Gradient Weights Max: 0.004976273779939202 Min: -0.01184351369664246\n","ReLU Activation - Max: 1.8602692921503452 Min: 0.0\n","Softmax Output - Max: 0.9304679557509443 Min: 0.06953204424905557 Sum (first example): 1.0\n","ReLU Activation - Max: 2.041806675450229 Min: 0.0\n","Softmax Output - Max: 0.9540635374138446 Min: 0.04593646258615535 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008469628470793876 Min: -0.008469628470793876\n","Layer 0 - Gradient Weights Max: 0.004931468893465443 Min: -0.011771460233063066\n","ReLU Activation - Max: 1.8627989167522903 Min: 0.0\n","Softmax Output - Max: 0.9308150451200964 Min: 0.06918495487990362 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0448009483167864 Min: 0.0\n","Softmax Output - Max: 0.954361339370135 Min: 0.04563866062986492 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008391881074400972 Min: -0.008391881074400972\n","Layer 0 - Gradient Weights Max: 0.0048622761880482025 Min: -0.011739574357927699\n","ReLU Activation - Max: 1.8653117020119907 Min: 0.0\n","Softmax Output - Max: 0.9311569815461492 Min: 0.06884301845385067 Sum (first example): 1.0\n","ReLU Activation - Max: 2.047777254529639 Min: 0.0\n","Softmax Output - Max: 0.9546544898457937 Min: 0.04534551015420636 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008315242000748554 Min: -0.008315242000748556\n","Layer 0 - Gradient Weights Max: 0.004805047101137938 Min: -0.011687994931090238\n","ReLU Activation - Max: 1.8678048029706928 Min: 0.0\n","Softmax Output - Max: 0.9314932343137587 Min: 0.06850676568624134 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0507309299390943 Min: 0.0\n","Softmax Output - Max: 0.9549431085544489 Min: 0.04505689144555104 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008239518780333554 Min: -0.008239518780333554\n","Layer 0 - Gradient Weights Max: 0.004724745856318558 Min: -0.011647595550609346\n","ReLU Activation - Max: 1.8702769934326897 Min: 0.0\n","Softmax Output - Max: 0.9318251041515577 Min: 0.06817489584844229 Sum (first example): 1.0\n","ReLU Activation - Max: 2.053646724796245 Min: 0.0\n","Softmax Output - Max: 0.9552276143274847 Min: 0.04477238567251539 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.00816463337348026 Min: -0.00816463337348026\n","Layer 0 - Gradient Weights Max: 0.004661299863194184 Min: -0.011560287191622979\n","ReLU Activation - Max: 1.8727306201099425 Min: 0.0\n","Softmax Output - Max: 0.9321520557600287 Min: 0.0678479442399712 Sum (first example): 0.9999999999999999\n","Epoch 490, Loss: 0.5890889157048018, Test Accuracy: 0.6953571428571429\n","ReLU Activation - Max: 2.0565373517033865 Min: 0.0\n","Softmax Output - Max: 0.9555075250345595 Min: 0.044492474965440426 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008090692243886777 Min: -0.008090692243886779\n","Layer 0 - Gradient Weights Max: 0.00463145879294721 Min: -0.011504308766115887\n","ReLU Activation - Max: 1.8751716156855012 Min: 0.0\n","Softmax Output - Max: 0.9324743816920855 Min: 0.06752561830791451 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0594080857461448 Min: 0.0\n","Softmax Output - Max: 0.9557831608159724 Min: 0.044216839184027704 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.008017709717693434 Min: -0.008017709717693436\n","Layer 0 - Gradient Weights Max: 0.004570927869320178 Min: -0.011393978281259902\n","ReLU Activation - Max: 1.8775899260994582 Min: 0.0\n","Softmax Output - Max: 0.9327922250018559 Min: 0.06720777499814412 Sum (first example): 1.0\n","ReLU Activation - Max: 2.062255548679909 Min: 0.0\n","Softmax Output - Max: 0.9560545273633652 Min: 0.043945472636634925 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007945279463193207 Min: -0.007945279463193209\n","Layer 0 - Gradient Weights Max: 0.004575798931754932 Min: -0.01140206263980918\n","ReLU Activation - Max: 1.8800074714095072 Min: 0.0\n","Softmax Output - Max: 0.93310606011443 Min: 0.06689393988557008 Sum (first example): 1.0\n","ReLU Activation - Max: 2.065082298653376 Min: 0.0\n","Softmax Output - Max: 0.9563227315159427 Min: 0.04367726848405725 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007874234496831462 Min: -0.00787423449683146\n","Layer 0 - Gradient Weights Max: 0.004522334555873247 Min: -0.011351407301520915\n","ReLU Activation - Max: 1.8824064737135016 Min: 0.0\n","Softmax Output - Max: 0.9334156242477674 Min: 0.06658437575223258 Sum (first example): 1.0\n","ReLU Activation - Max: 2.067887733370573 Min: 0.0\n","Softmax Output - Max: 0.9565868643863011 Min: 0.04341313561369891 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007803906962483666 Min: -0.007803906962483667\n","Layer 0 - Gradient Weights Max: 0.00443216020006292 Min: -0.011378002603946407\n","ReLU Activation - Max: 1.8847930475595323 Min: 0.0\n","Softmax Output - Max: 0.9337212563866042 Min: 0.06627874361339567 Sum (first example): 1.0\n","ReLU Activation - Max: 2.070686451603309 Min: 0.0\n","Softmax Output - Max: 0.9568474125236297 Min: 0.04315258747637037 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.007734647753371429 Min: -0.007734647753371425\n","Layer 0 - Gradient Weights Max: 0.004372666861480787 Min: -0.011424297295308467\n","ReLU Activation - Max: 1.8871608621596578 Min: 0.0\n","Softmax Output - Max: 0.9340227004714561 Min: 0.06597729952854395 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0734747254772024 Min: 0.0\n","Softmax Output - Max: 0.9571042655038035 Min: 0.04289573449619663 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007666466218310123 Min: -0.0076664662183101254\n","Layer 0 - Gradient Weights Max: 0.0043317989383812915 Min: -0.01135993260206983\n","ReLU Activation - Max: 1.8895094460888162 Min: 0.0\n","Softmax Output - Max: 0.9343200794405765 Min: 0.0656799205594236 Sum (first example): 1.0\n","ReLU Activation - Max: 2.076239278222062 Min: 0.0\n","Softmax Output - Max: 0.9573575136621114 Min: 0.042642486337888565 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007599087060323463 Min: -0.007599087060323465\n","Layer 0 - Gradient Weights Max: 0.00435149843966618 Min: -0.011335488241897493\n","ReLU Activation - Max: 1.8918413008060757 Min: 0.0\n","Softmax Output - Max: 0.9346130215270192 Min: 0.06538697847298076 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0789880823518776 Min: 0.0\n","Softmax Output - Max: 0.9576071416637157 Min: 0.04239285833628433 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0075325757999825105 Min: -0.007532575799982512\n","Layer 0 - Gradient Weights Max: 0.004308948259105632 Min: -0.011269942455040666\n","ReLU Activation - Max: 1.8941528732517277 Min: 0.0\n","Softmax Output - Max: 0.9349017620043368 Min: 0.06509823799566322 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.0817171515501323 Min: 0.0\n","Softmax Output - Max: 0.9578533003375731 Min: 0.04214669966242701 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.007466871306993224 Min: -0.007466871306993225\n","Layer 0 - Gradient Weights Max: 0.004299229495836067 Min: -0.011160131982142624\n","ReLU Activation - Max: 1.8964348650786416 Min: 0.0\n","Softmax Output - Max: 0.9351866764838064 Min: 0.06481332351619364 Sum (first example): 1.0\n","Epoch 500, Loss: 0.5883529996812498, Test Accuracy: 0.6960714285714286\n","ReLU Activation - Max: 2.0844071386754615 Min: 0.0\n","Softmax Output - Max: 0.9580963967074743 Min: 0.04190360329252577 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007401672834099527 Min: -0.007401672834099526\n","Layer 0 - Gradient Weights Max: 0.0042896889474605985 Min: -0.011121020452491494\n","ReLU Activation - Max: 1.898716714355203 Min: 0.0\n","Softmax Output - Max: 0.9354671755181581 Min: 0.06453282448184186 Sum (first example): 1.0\n","ReLU Activation - Max: 2.087063775182882 Min: 0.0\n","Softmax Output - Max: 0.9583358944867857 Min: 0.041664105513214277 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0073373371893195975 Min: -0.007337337189319595\n","Layer 0 - Gradient Weights Max: 0.004312602522132142 Min: -0.011062565889185088\n","ReLU Activation - Max: 1.9009827918664322 Min: 0.0\n","Softmax Output - Max: 0.935743671250138 Min: 0.0642563287498619 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.0897006636852153 Min: 0.0\n","Softmax Output - Max: 0.9585720144911435 Min: 0.04142798550885664 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007273881820996042 Min: -0.0072738818209960405\n","Layer 0 - Gradient Weights Max: 0.0042641482463802866 Min: -0.011013100224219186\n","ReLU Activation - Max: 1.903231973111496 Min: 0.0\n","Softmax Output - Max: 0.9360167488785115 Min: 0.06398325112148846 Sum (first example): 1.0\n","ReLU Activation - Max: 2.092318026770645 Min: 0.0\n","Softmax Output - Max: 0.9588047720805921 Min: 0.04119522791940797 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007211153389402939 Min: -0.0072111533894029375\n","Layer 0 - Gradient Weights Max: 0.004208212284463797 Min: -0.010906413459748574\n","ReLU Activation - Max: 1.9054601868265475 Min: 0.0\n","Softmax Output - Max: 0.9362864005485662 Min: 0.06371359945143376 Sum (first example): 1.0\n","ReLU Activation - Max: 2.0949173175692577 Min: 0.0\n","Softmax Output - Max: 0.9590343692145687 Min: 0.04096563078543125 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007149046744369845 Min: -0.007149046744369846\n","Layer 0 - Gradient Weights Max: 0.0041749725741712694 Min: -0.010851335187954806\n","ReLU Activation - Max: 1.907672156884932 Min: 0.0\n","Softmax Output - Max: 0.9365528789775163 Min: 0.06344712102248366 Sum (first example): 1.0\n","ReLU Activation - Max: 2.097497369156981 Min: 0.0\n","Softmax Output - Max: 0.9592608472891757 Min: 0.04073915271082423 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007087728351912019 Min: -0.007087728351912019\n","Layer 0 - Gradient Weights Max: 0.004159787296073503 Min: -0.010758284185540245\n","ReLU Activation - Max: 1.9098629293735065 Min: 0.0\n","Softmax Output - Max: 0.9368160652612959 Min: 0.06318393473870418 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1000532229029094 Min: 0.0\n","Softmax Output - Max: 0.9594845043851207 Min: 0.040515495614879264 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.007027085955227074 Min: -0.0070270859552270725\n","Layer 0 - Gradient Weights Max: 0.004091764649104304 Min: -0.010672819472756634\n","ReLU Activation - Max: 1.9120299309897546 Min: 0.0\n","Softmax Output - Max: 0.9370755990842471 Min: 0.06292440091575298 Sum (first example): 1.0\n","ReLU Activation - Max: 2.102599021099457 Min: 0.0\n","Softmax Output - Max: 0.9597051660514109 Min: 0.04029483394858903 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0069670093541625185 Min: -0.006967009354162518\n","Layer 0 - Gradient Weights Max: 0.00405562228938085 Min: -0.010706235527255794\n","ReLU Activation - Max: 1.9141892793710396 Min: 0.0\n","Softmax Output - Max: 0.9373316173714958 Min: 0.06266838262850408 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1051204554367136 Min: 0.0\n","Softmax Output - Max: 0.9599231073685227 Min: 0.0400768926314772 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0069076760745250725 Min: -0.006907676074525075\n","Layer 0 - Gradient Weights Max: 0.004010058237668675 Min: -0.010657903928131684\n","ReLU Activation - Max: 1.9163327140542379 Min: 0.0\n","Softmax Output - Max: 0.937583652108157 Min: 0.06241634789184303 Sum (first example): 1.0\n","ReLU Activation - Max: 2.107623427208849 Min: 0.0\n","Softmax Output - Max: 0.9601378703605769 Min: 0.03986212963942323 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006848940399268468 Min: -0.006848940399268466\n","Layer 0 - Gradient Weights Max: 0.003967238799623995 Min: -0.010503559051124435\n","ReLU Activation - Max: 1.9184651990063744 Min: 0.0\n","Softmax Output - Max: 0.9378326920697366 Min: 0.062167307930263406 Sum (first example): 1.0\n","Epoch 510, Loss: 0.587673987323189, Test Accuracy: 0.6964285714285714\n","ReLU Activation - Max: 2.1100953384447263 Min: 0.0\n","Softmax Output - Max: 0.9603499501742019 Min: 0.03965004982579806 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006790687696321905 Min: -0.0067906876963219065\n","Layer 0 - Gradient Weights Max: 0.003953727379619392 Min: -0.01050196172719196\n","ReLU Activation - Max: 1.920587545618346 Min: 0.0\n","Softmax Output - Max: 0.9380784955710018 Min: 0.06192150442899815 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.112547093352756 Min: 0.0\n","Softmax Output - Max: 0.9605591835451868 Min: 0.03944081645481334 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006733278959805259 Min: -0.00673327895980526\n","Layer 0 - Gradient Weights Max: 0.003946326804995417 Min: -0.010510321300732137\n","ReLU Activation - Max: 1.9227092638255556 Min: 0.0\n","Softmax Output - Max: 0.9383213465111606 Min: 0.061678653488839455 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1149779149119596 Min: 0.0\n","Softmax Output - Max: 0.9607655643038905 Min: 0.03923443569610954 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006676778419681498 Min: -0.006676778419681497\n","Layer 0 - Gradient Weights Max: 0.00391312508638626 Min: -0.01053462637943276\n","ReLU Activation - Max: 1.92482361247121 Min: 0.0\n","Softmax Output - Max: 0.9385612763793495 Min: 0.061438723620650476 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.1173936502364534 Min: 0.0\n","Softmax Output - Max: 0.9609692682681067 Min: 0.0390307317318934 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00662109010708346 Min: -0.006621090107083459\n","Layer 0 - Gradient Weights Max: 0.003907569724676099 Min: -0.010453807878799168\n","ReLU Activation - Max: 1.9269194565576555 Min: 0.0\n","Softmax Output - Max: 0.9387980366121065 Min: 0.06120196338789351 Sum (first example): 1.0\n","ReLU Activation - Max: 2.119778386442736 Min: 0.0\n","Softmax Output - Max: 0.9611700344882251 Min: 0.03882996551177485 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006565999542323511 Min: -0.006565999542323512\n","Layer 0 - Gradient Weights Max: 0.003844255253385976 Min: -0.010595504933707896\n","ReLU Activation - Max: 1.9289977501999165 Min: 0.0\n","Softmax Output - Max: 0.9390318878670565 Min: 0.060968112132943494 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.1221862482915697 Min: 0.0\n","Softmax Output - Max: 0.9613681308480237 Min: 0.03863186915197618 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00651174864864538 Min: -0.00651174864864538\n","Layer 0 - Gradient Weights Max: 0.003715568242636505 Min: -0.010497536998452864\n","ReLU Activation - Max: 1.9310478084116611 Min: 0.0\n","Softmax Output - Max: 0.9392625165461296 Min: 0.06073748345387051 Sum (first example): 1.0\n","ReLU Activation - Max: 2.124585111690454 Min: 0.0\n","Softmax Output - Max: 0.9615632085152972 Min: 0.038436791484702866 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00645813407233987 Min: -0.00645813407233987\n","Layer 0 - Gradient Weights Max: 0.0036270091187047843 Min: -0.010421997251253597\n","ReLU Activation - Max: 1.9330722575969717 Min: 0.0\n","Softmax Output - Max: 0.9394902146441112 Min: 0.0605097853558887 Sum (first example): 1.0\n","ReLU Activation - Max: 2.126976584504805 Min: 0.0\n","Softmax Output - Max: 0.9617557700900077 Min: 0.03824422990999235 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006405018855616757 Min: -0.006405018855616756\n","Layer 0 - Gradient Weights Max: 0.003607528895150121 Min: -0.010334818737317472\n","ReLU Activation - Max: 1.935089514275153 Min: 0.0\n","Softmax Output - Max: 0.9397152955388897 Min: 0.0602847044611103 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1293344761012496 Min: 0.0\n","Softmax Output - Max: 0.9619459717169655 Min: 0.0380540282830345 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006352525328031482 Min: -0.006352525328031479\n","Layer 0 - Gradient Weights Max: 0.003555506187056417 Min: -0.010233271397027977\n","ReLU Activation - Max: 1.9370936430226176 Min: 0.0\n","Softmax Output - Max: 0.9399373298747464 Min: 0.06006267012525351 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1316720129557387 Min: 0.0\n","Softmax Output - Max: 0.9621333052982535 Min: 0.03786669470174644 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0063007882145902 Min: -0.0063007882145902005\n","Layer 0 - Gradient Weights Max: 0.003622915631833704 Min: -0.0102553199799816\n","ReLU Activation - Max: 1.9390855906012443 Min: 0.0\n","Softmax Output - Max: 0.9401568341404144 Min: 0.05984316585958557 Sum (first example): 0.9999999999999999\n","Epoch 520, Loss: 0.5870436728515396, Test Accuracy: 0.6960714285714286\n","ReLU Activation - Max: 2.1339965219228865 Min: 0.0\n","Softmax Output - Max: 0.962318136097217 Min: 0.03768186390278312 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006249872047391244 Min: -0.0062498720473912455\n","Layer 0 - Gradient Weights Max: 0.0036137463014991197 Min: -0.01038246251357216\n","ReLU Activation - Max: 1.9410580622187845 Min: 0.0\n","Softmax Output - Max: 0.9403740482419692 Min: 0.0596259517580309 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1363280670743108 Min: 0.0\n","Softmax Output - Max: 0.9625007256648912 Min: 0.037499274335108926 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006199870603235172 Min: -0.006199870603235171\n","Layer 0 - Gradient Weights Max: 0.0036661012290153104 Min: -0.010361597407768984\n","ReLU Activation - Max: 1.9430165346993438 Min: 0.0\n","Softmax Output - Max: 0.9405881513077633 Min: 0.059411848692236834 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.1386349129437634 Min: 0.0\n","Softmax Output - Max: 0.962681218243942 Min: 0.037318781756057844 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006150523154274661 Min: -0.006150523154274661\n","Layer 0 - Gradient Weights Max: 0.0036335896416541656 Min: -0.0102116179055708\n","ReLU Activation - Max: 1.9449499238916874 Min: 0.0\n","Softmax Output - Max: 0.9407996760570564 Min: 0.05920032394294349 Sum (first example): 1.0\n","ReLU Activation - Max: 2.140914061824906 Min: 0.0\n","Softmax Output - Max: 0.9628596421826685 Min: 0.0371403578173314 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0061015644648013 Min: -0.0061015644648013\n","Layer 0 - Gradient Weights Max: 0.003580832104479267 Min: -0.010182626443137586\n","ReLU Activation - Max: 1.9468656684401853 Min: 0.0\n","Softmax Output - Max: 0.9410087139715687 Min: 0.05899128602843129 Sum (first example): 1.0\n","ReLU Activation - Max: 2.143183186211873 Min: 0.0\n","Softmax Output - Max: 0.9630359484447328 Min: 0.03696405155526721 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006053199715039868 Min: -0.006053199715039865\n","Layer 0 - Gradient Weights Max: 0.003629078356352124 Min: -0.010108795934648645\n","ReLU Activation - Max: 1.9487594641439707 Min: 0.0\n","Softmax Output - Max: 0.9412153110096392 Min: 0.05878468899036091 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1454219870176456 Min: 0.0\n","Softmax Output - Max: 0.9632096858143356 Min: 0.03679031418566426 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006005361889350957 Min: -0.006005361889350957\n","Layer 0 - Gradient Weights Max: 0.0036079066763570395 Min: -0.01009152524465649\n","ReLU Activation - Max: 1.9506384545876068 Min: 0.0\n","Softmax Output - Max: 0.9414193193998676 Min: 0.05858068060013242 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.147653327099371 Min: 0.0\n","Softmax Output - Max: 0.9633811605464347 Min: 0.036618839453565186 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005958207228148943 Min: -0.005958207228148944\n","Layer 0 - Gradient Weights Max: 0.0036063050703294152 Min: -0.01002248780259587\n","ReLU Activation - Max: 1.9525166148502882 Min: 0.0\n","Softmax Output - Max: 0.9416209052295991 Min: 0.058379094770400965 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1498599855635576 Min: 0.0\n","Softmax Output - Max: 0.9635505324515391 Min: 0.03644946754846093 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005911551851344912 Min: -0.005911551851344912\n","Layer 0 - Gradient Weights Max: 0.003598312670470279 Min: -0.009933937515032819\n","ReLU Activation - Max: 1.9543831730740793 Min: 0.0\n","Softmax Output - Max: 0.9418204533024078 Min: 0.0581795466975922 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1520421166633628 Min: 0.0\n","Softmax Output - Max: 0.9637177487281187 Min: 0.036282251271881366 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005865187487387306 Min: -0.005865187487387304\n","Layer 0 - Gradient Weights Max: 0.0035643677572408648 Min: -0.009850352256145537\n","ReLU Activation - Max: 1.9562188209650584 Min: 0.0\n","Softmax Output - Max: 0.9420179013620252 Min: 0.05798209863797481 Sum (first example): 1.0\n","ReLU Activation - Max: 2.154207265997233 Min: 0.0\n","Softmax Output - Max: 0.9638827842269556 Min: 0.036117215773044276 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005819229738441288 Min: -0.005819229738441285\n","Layer 0 - Gradient Weights Max: 0.0035278231420422935 Min: -0.009772524751069593\n","ReLU Activation - Max: 1.9580522301313597 Min: 0.0\n","Softmax Output - Max: 0.9422125402071726 Min: 0.05778745979282735 Sum (first example): 1.0\n","Epoch 530, Loss: 0.5864511240728995, Test Accuracy: 0.6967857142857142\n","ReLU Activation - Max: 2.1563463568558126 Min: 0.0\n","Softmax Output - Max: 0.964045562888541 Min: 0.03595443711145905 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005773724297521849 Min: -0.005773724297521849\n","Layer 0 - Gradient Weights Max: 0.0034451013278134946 Min: -0.00979351298098765\n","ReLU Activation - Max: 1.9598672820443275 Min: 0.0\n","Softmax Output - Max: 0.9424048309548001 Min: 0.05759516904519983 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1584781113273257 Min: 0.0\n","Softmax Output - Max: 0.9642062515081014 Min: 0.03579374849189858 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005728772632120249 Min: -0.005728772632120249\n","Layer 0 - Gradient Weights Max: 0.003452454560065199 Min: -0.009739355406286742\n","ReLU Activation - Max: 1.9616692101087287 Min: 0.0\n","Softmax Output - Max: 0.9425947862719628 Min: 0.057405213728037294 Sum (first example): 1.0\n","ReLU Activation - Max: 2.160593484933254 Min: 0.0\n","Softmax Output - Max: 0.9643649034653698 Min: 0.035635096534630144 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00568434556249725 Min: -0.0056843455624972505\n","Layer 0 - Gradient Weights Max: 0.003458025473933624 Min: -0.009783778760335425\n","ReLU Activation - Max: 1.9634545982379144 Min: 0.0\n","Softmax Output - Max: 0.9427822692185565 Min: 0.05721773078144355 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.1626814004799253 Min: 0.0\n","Softmax Output - Max: 0.9645215482060236 Min: 0.03547845179397641 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005640575041507204 Min: -0.005640575041507205\n","Layer 0 - Gradient Weights Max: 0.0034536424969301358 Min: -0.009741726692431792\n","ReLU Activation - Max: 1.965230640555279 Min: 0.0\n","Softmax Output - Max: 0.9429674100239375 Min: 0.05703258997606248 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.1647588078939224 Min: 0.0\n","Softmax Output - Max: 0.9646765965684064 Min: 0.03532340343159351 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005597352902614953 Min: -0.005597352902614953\n","Layer 0 - Gradient Weights Max: 0.003438676810553806 Min: -0.00967306123212862\n","ReLU Activation - Max: 1.9669878350933174 Min: 0.0\n","Softmax Output - Max: 0.9431507189236242 Min: 0.0568492810763757 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1668204790478445 Min: 0.0\n","Softmax Output - Max: 0.9648298878920538 Min: 0.035170112107946246 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005554445812252847 Min: -0.005554445812252847\n","Layer 0 - Gradient Weights Max: 0.003393262261298319 Min: -0.009629528061738675\n","ReLU Activation - Max: 1.9687489234615714 Min: 0.0\n","Softmax Output - Max: 0.9433317833545101 Min: 0.056668216645489905 Sum (first example): 1.0\n","ReLU Activation - Max: 2.168864082248703 Min: 0.0\n","Softmax Output - Max: 0.9649811811332 Min: 0.03501881886679996 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005512029855135286 Min: -0.005512029855135286\n","Layer 0 - Gradient Weights Max: 0.003404846619563249 Min: -0.00963091240864477\n","ReLU Activation - Max: 1.9704897604310885 Min: 0.0\n","Softmax Output - Max: 0.9435109515482025 Min: 0.05648904845179752 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1709023437208317 Min: 0.0\n","Softmax Output - Max: 0.9651304780517751 Min: 0.03486952194822493 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005470203813951517 Min: -0.0054702038139515154\n","Layer 0 - Gradient Weights Max: 0.003426691981854273 Min: -0.009566557066260487\n","ReLU Activation - Max: 1.972215433162764 Min: 0.0\n","Softmax Output - Max: 0.9436883730045683 Min: 0.05631162699543174 Sum (first example): 1.0\n","ReLU Activation - Max: 2.172939457460974 Min: 0.0\n","Softmax Output - Max: 0.9652781408445071 Min: 0.034721859155492814 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00542863645663011 Min: -0.00542863645663011\n","Layer 0 - Gradient Weights Max: 0.003421503836737573 Min: -0.009406239105893173\n","ReLU Activation - Max: 1.9739196892184094 Min: 0.0\n","Softmax Output - Max: 0.9438641920716228 Min: 0.056135807928377184 Sum (first example): 1.0\n","ReLU Activation - Max: 2.174966309929091 Min: 0.0\n","Softmax Output - Max: 0.9654244994666049 Min: 0.034575500533395136 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0053875336063163165 Min: -0.005387533606316319\n","Layer 0 - Gradient Weights Max: 0.0034381337842076328 Min: -0.009370895574109311\n","ReLU Activation - Max: 1.975617763756106 Min: 0.0\n","Softmax Output - Max: 0.9440381839207558 Min: 0.05596181607924422 Sum (first example): 1.0\n","Epoch 540, Loss: 0.5858903075839151, Test Accuracy: 0.6982142857142857\n","ReLU Activation - Max: 2.1769757437398023 Min: 0.0\n","Softmax Output - Max: 0.9655694568470886 Min: 0.034430543152911365 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005346989477468095 Min: -0.0053469894774680945\n","Layer 0 - Gradient Weights Max: 0.0034830915716231735 Min: -0.009380205469505385\n","ReLU Activation - Max: 1.9773123285367673 Min: 0.0\n","Softmax Output - Max: 0.9442099205037463 Min: 0.05579007949625368 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1789629964794135 Min: 0.0\n","Softmax Output - Max: 0.9657127206569955 Min: 0.034287279343004406 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005307289443069172 Min: -0.005307289443069175\n","Layer 0 - Gradient Weights Max: 0.0035081344140634857 Min: -0.009437894452374182\n","ReLU Activation - Max: 1.9789892321276021 Min: 0.0\n","Softmax Output - Max: 0.9443797924559771 Min: 0.055620207544022825 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1809407900674738 Min: 0.0\n","Softmax Output - Max: 0.9658546060487018 Min: 0.03414539395129834 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005268059299540199 Min: -0.005268059299540199\n","Layer 0 - Gradient Weights Max: 0.0035437402928835855 Min: -0.009439658680533765\n","ReLU Activation - Max: 1.9806493071413114 Min: 0.0\n","Softmax Output - Max: 0.9445476827072002 Min: 0.05545231729279984 Sum (first example): 1.0\n","ReLU Activation - Max: 2.182924296468439 Min: 0.0\n","Softmax Output - Max: 0.9659947864596944 Min: 0.03400521354030567 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005229408413476206 Min: -0.005229408413476206\n","Layer 0 - Gradient Weights Max: 0.003551684800334542 Min: -0.009395648164366985\n","ReLU Activation - Max: 1.9822982823164776 Min: 0.0\n","Softmax Output - Max: 0.9447137562176743 Min: 0.05528624378232575 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1848946764259503 Min: 0.0\n","Softmax Output - Max: 0.9661332267353882 Min: 0.03386677326461189 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0051912019922765494 Min: -0.0051912019922765494\n","Layer 0 - Gradient Weights Max: 0.003577794944071301 Min: -0.009332173681684503\n","ReLU Activation - Max: 1.9839326807791622 Min: 0.0\n","Softmax Output - Max: 0.9448778734532023 Min: 0.055122126546797684 Sum (first example): 1.0\n","ReLU Activation - Max: 2.186848300400191 Min: 0.0\n","Softmax Output - Max: 0.9662701155677028 Min: 0.03372988443229723 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005153474612221647 Min: -0.0051534746122216465\n","Layer 0 - Gradient Weights Max: 0.0036541736355784073 Min: -0.009346243638398302\n","ReLU Activation - Max: 1.985563687307355 Min: 0.0\n","Softmax Output - Max: 0.9450410532523437 Min: 0.05495894674765633 Sum (first example): 1.0\n","ReLU Activation - Max: 2.188800436993507 Min: 0.0\n","Softmax Output - Max: 0.9664058258294486 Min: 0.03359417417055145 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005116368266393776 Min: -0.0051163682663937764\n","Layer 0 - Gradient Weights Max: 0.00374202061605263 Min: -0.009376721583704364\n","ReLU Activation - Max: 1.9871809815454073 Min: 0.0\n","Softmax Output - Max: 0.9452026587218415 Min: 0.054797341278158475 Sum (first example): 1.0\n","ReLU Activation - Max: 2.190759889054001 Min: 0.0\n","Softmax Output - Max: 0.9665400509673414 Min: 0.033459949032658504 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005079837779319609 Min: -0.005079837779319609\n","Layer 0 - Gradient Weights Max: 0.0037547919609686423 Min: -0.009363299466789734\n","ReLU Activation - Max: 1.9887933544631662 Min: 0.0\n","Softmax Output - Max: 0.9453620162843048 Min: 0.054637983715695185 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1927100806235056 Min: 0.0\n","Softmax Output - Max: 0.9666726910254304 Min: 0.03332730897456956 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005043700869251182 Min: -0.005043700869251185\n","Layer 0 - Gradient Weights Max: 0.0037844325401401686 Min: -0.009340926950637497\n","ReLU Activation - Max: 1.990387197726223 Min: 0.0\n","Softmax Output - Max: 0.9455197574831825 Min: 0.05448024251681735 Sum (first example): 1.0\n","ReLU Activation - Max: 2.1946472667429084 Min: 0.0\n","Softmax Output - Max: 0.966804019492513 Min: 0.03319598050748682 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005007884657211134 Min: -0.005007884657211133\n","Layer 0 - Gradient Weights Max: 0.003798507283951548 Min: -0.009321947068720925\n","ReLU Activation - Max: 1.9919766302411688 Min: 0.0\n","Softmax Output - Max: 0.9456755928297802 Min: 0.05432440717021989 Sum (first example): 0.9999999999999999\n","Epoch 550, Loss: 0.5853453958557656, Test Accuracy: 0.6985714285714286\n","ReLU Activation - Max: 2.1965870485123973 Min: 0.0\n","Softmax Output - Max: 0.9669337264957685 Min: 0.03306627350423152 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004972564521605831 Min: -0.004972564521605831\n","Layer 0 - Gradient Weights Max: 0.0037502079295315483 Min: -0.009205628497499854\n","ReLU Activation - Max: 1.993564563591747 Min: 0.0\n","Softmax Output - Max: 0.9458293249587292 Min: 0.05417067504127086 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.19851729477323 Min: 0.0\n","Softmax Output - Max: 0.9670617578771511 Min: 0.03293824212284892 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0049375926904985205 Min: -0.004937592690498519\n","Layer 0 - Gradient Weights Max: 0.003794349235234047 Min: -0.00928503921163996\n","ReLU Activation - Max: 1.9951382840855456 Min: 0.0\n","Softmax Output - Max: 0.945981275287495 Min: 0.05401872471250502 Sum (first example): 1.0\n","ReLU Activation - Max: 2.200448643504999 Min: 0.0\n","Softmax Output - Max: 0.9671883344302389 Min: 0.03281166556976119 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004903331679337095 Min: -0.004903331679337096\n","Layer 0 - Gradient Weights Max: 0.003804577225566302 Min: -0.009186683672610461\n","ReLU Activation - Max: 1.9967000078793726 Min: 0.0\n","Softmax Output - Max: 0.9461312353189357 Min: 0.05386876468106431 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2023541292007476 Min: 0.0\n","Softmax Output - Max: 0.9673135493776907 Min: 0.03268645062230919 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004869335728114551 Min: -0.00486933572811455\n","Layer 0 - Gradient Weights Max: 0.003779458964596006 Min: -0.009172953616094343\n","ReLU Activation - Max: 1.998261566700054 Min: 0.0\n","Softmax Output - Max: 0.9462793441486598 Min: 0.05372065585134009 Sum (first example): 1.0\n","ReLU Activation - Max: 2.204251887959415 Min: 0.0\n","Softmax Output - Max: 0.9674371620445436 Min: 0.03256283795545647 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004835873856588032 Min: -0.004835873856588032\n","Layer 0 - Gradient Weights Max: 0.003772639162472776 Min: -0.009179504749016907\n","ReLU Activation - Max: 1.9998193668744708 Min: 0.0\n","Softmax Output - Max: 0.9464257517698963 Min: 0.05357424823010361 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2061325117084407 Min: 0.0\n","Softmax Output - Max: 0.9675594688619215 Min: 0.03244053113807847 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0048027954589254665 Min: -0.004802795458925466\n","Layer 0 - Gradient Weights Max: 0.0038896808985606013 Min: -0.009125941323591666\n","ReLU Activation - Max: 2.0013678604127723 Min: 0.0\n","Softmax Output - Max: 0.9465703029177179 Min: 0.05342969708228207 Sum (first example): 1.0\n","ReLU Activation - Max: 2.207992253510845 Min: 0.0\n","Softmax Output - Max: 0.9676804142347241 Min: 0.032319585765275896 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004770210869067684 Min: -0.004770210869067683\n","Layer 0 - Gradient Weights Max: 0.003923954562599152 Min: -0.009139839175769734\n","ReLU Activation - Max: 2.0029255340451027 Min: 0.0\n","Softmax Output - Max: 0.9467135040907615 Min: 0.05328649590923854 Sum (first example): 1.0\n","ReLU Activation - Max: 2.209825965407547 Min: 0.0\n","Softmax Output - Max: 0.9678002021005861 Min: 0.03219979789941383 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004738024965217106 Min: -0.004738024965217105\n","Layer 0 - Gradient Weights Max: 0.003961322842500441 Min: -0.00907612556575143\n","ReLU Activation - Max: 2.004475506852081 Min: 0.0\n","Softmax Output - Max: 0.9468550776698912 Min: 0.05314492233010873 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2116395624237475 Min: 0.0\n","Softmax Output - Max: 0.9679190527471223 Min: 0.032080947252877565 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0047062954800354785 Min: -0.004706295480035477\n","Layer 0 - Gradient Weights Max: 0.003990957063774836 Min: -0.009091586185677604\n","ReLU Activation - Max: 2.0060151676671434 Min: 0.0\n","Softmax Output - Max: 0.9469950543182721 Min: 0.05300494568172794 Sum (first example): 1.0\n","ReLU Activation - Max: 2.213449063118943 Min: 0.0\n","Softmax Output - Max: 0.9680364960281606 Min: 0.03196350397183947 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004675005214246401 Min: -0.004675005214246401\n","Layer 0 - Gradient Weights Max: 0.004038921719728205 Min: -0.00904820022455185\n","ReLU Activation - Max: 2.0075418845510904 Min: 0.0\n","Softmax Output - Max: 0.9471329648919744 Min: 0.05286703510802564 Sum (first example): 1.0\n","Epoch 560, Loss: 0.5848136130949909, Test Accuracy: 0.6996428571428571\n","ReLU Activation - Max: 2.2152462024900523 Min: 0.0\n","Softmax Output - Max: 0.9681526639316482 Min: 0.031847336068351745 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004644221781611498 Min: -0.004644221781611498\n","Layer 0 - Gradient Weights Max: 0.004140107023112145 Min: -0.0090047842967394\n","ReLU Activation - Max: 2.0090605316730086 Min: 0.0\n","Softmax Output - Max: 0.9472697768002445 Min: 0.052730223199755386 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.217027696990148 Min: 0.0\n","Softmax Output - Max: 0.9682677550758905 Min: 0.03173224492410947 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004613894112841644 Min: -0.004613894112841643\n","Layer 0 - Gradient Weights Max: 0.004151556839769569 Min: -0.00903583657621345\n","ReLU Activation - Max: 2.0105568003556944 Min: 0.0\n","Softmax Output - Max: 0.9474049217898562 Min: 0.05259507821014386 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.2187913883731905 Min: 0.0\n","Softmax Output - Max: 0.9683812936073253 Min: 0.03161870639267481 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004583566070013525 Min: -0.0045835660700135235\n","Layer 0 - Gradient Weights Max: 0.004175379748099541 Min: -0.009170456767251329\n","ReLU Activation - Max: 2.012032731626907 Min: 0.0\n","Softmax Output - Max: 0.9475380079849972 Min: 0.05246199201500271 Sum (first example): 1.0\n","ReLU Activation - Max: 2.220547178671195 Min: 0.0\n","Softmax Output - Max: 0.9684932471366994 Min: 0.03150675286330053 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004553627867536609 Min: -0.004553627867536606\n","Layer 0 - Gradient Weights Max: 0.004241095859107127 Min: -0.009165064757737677\n","ReLU Activation - Max: 2.013499527752211 Min: 0.0\n","Softmax Output - Max: 0.9476699653826938 Min: 0.05233003461730627 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.222292225493034 Min: 0.0\n","Softmax Output - Max: 0.9686039359474128 Min: 0.03139606405258721 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004524195676175525 Min: -0.004524195676175525\n","Layer 0 - Gradient Weights Max: 0.004174621936823125 Min: -0.009184273786629619\n","ReLU Activation - Max: 2.0149570884003882 Min: 0.0\n","Softmax Output - Max: 0.9478003747596195 Min: 0.05219962524038051 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2240223935742787 Min: 0.0\n","Softmax Output - Max: 0.9687136383874928 Min: 0.03128636161250714 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004495084202121197 Min: -0.004495084202121196\n","Layer 0 - Gradient Weights Max: 0.004116679511846785 Min: -0.00919872199598838\n","ReLU Activation - Max: 2.0164089614574645 Min: 0.0\n","Softmax Output - Max: 0.9479302041313649 Min: 0.052069795868635194 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.225748372194249 Min: 0.0\n","Softmax Output - Max: 0.9688222921135368 Min: 0.03117770788646332 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004466488040144292 Min: -0.004466488040144291\n","Layer 0 - Gradient Weights Max: 0.004252394480707284 Min: -0.009179703997689685\n","ReLU Activation - Max: 2.0178639640191482 Min: 0.0\n","Softmax Output - Max: 0.9480586291164985 Min: 0.0519413708835015 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.2274613398731846 Min: 0.0\n","Softmax Output - Max: 0.968929492296899 Min: 0.031070507703100922 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00443868515104719 Min: -0.00443868515104719\n","Layer 0 - Gradient Weights Max: 0.004302368493811652 Min: -0.009170877752438909\n","ReLU Activation - Max: 2.019301964827992 Min: 0.0\n","Softmax Output - Max: 0.9481857829685475 Min: 0.05181421703145249 Sum (first example): 1.0\n","ReLU Activation - Max: 2.229189659844315 Min: 0.0\n","Softmax Output - Max: 0.9690355584071587 Min: 0.03096444159284134 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004411173558838103 Min: -0.004411173558838103\n","Layer 0 - Gradient Weights Max: 0.004331512094802144 Min: -0.009252672675570284\n","ReLU Activation - Max: 2.020730803919038 Min: 0.0\n","Softmax Output - Max: 0.9483116422458867 Min: 0.05168835775411328 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2308988577219426 Min: 0.0\n","Softmax Output - Max: 0.9691403886527142 Min: 0.03085961134728586 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004384028732439328 Min: -0.004384028732439329\n","Layer 0 - Gradient Weights Max: 0.004374115142092145 Min: -0.009265580193331072\n","ReLU Activation - Max: 2.0221697114393415 Min: 0.0\n","Softmax Output - Max: 0.9484353357316336 Min: 0.05156466426836642 Sum (first example): 1.0\n","Epoch 570, Loss: 0.5842861683991106, Test Accuracy: 0.7003571428571429\n","ReLU Activation - Max: 2.2326160610165626 Min: 0.0\n","Softmax Output - Max: 0.9692442353541801 Min: 0.030755764645819885 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004357901259282953 Min: -0.0043579012592829544\n","Layer 0 - Gradient Weights Max: 0.004538464117761595 Min: -0.009292612751301645\n","ReLU Activation - Max: 2.023604021634122 Min: 0.0\n","Softmax Output - Max: 0.9485579167920135 Min: 0.051442083207986544 Sum (first example): 1.0\n","ReLU Activation - Max: 2.234324891605401 Min: 0.0\n","Softmax Output - Max: 0.9693470809190267 Min: 0.03065291908097332 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004332352715605106 Min: -0.004332352715605104\n","Layer 0 - Gradient Weights Max: 0.004496140361469986 Min: -0.009317610685263693\n","ReLU Activation - Max: 2.0250233690671826 Min: 0.0\n","Softmax Output - Max: 0.9486792416122949 Min: 0.05132075838770505 Sum (first example): 1.0\n","ReLU Activation - Max: 2.236039385563655 Min: 0.0\n","Softmax Output - Max: 0.9694487294079552 Min: 0.03055127059204466 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0043071122734508385 Min: -0.004307112273450839\n","Layer 0 - Gradient Weights Max: 0.004477923711109545 Min: -0.009307473840157092\n","ReLU Activation - Max: 2.0264259689332746 Min: 0.0\n","Softmax Output - Max: 0.9487995877360854 Min: 0.05120041226391462 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.237731213602241 Min: 0.0\n","Softmax Output - Max: 0.9695493222387312 Min: 0.030450677761268773 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004282105368725183 Min: -0.004282105368725183\n","Layer 0 - Gradient Weights Max: 0.004514810302668682 Min: -0.009281257451967668\n","ReLU Activation - Max: 2.027813894917996 Min: 0.0\n","Softmax Output - Max: 0.9489192468281199 Min: 0.051080753171880024 Sum (first example): 1.0\n","ReLU Activation - Max: 2.239425535922201 Min: 0.0\n","Softmax Output - Max: 0.9696494035216667 Min: 0.030350596478333203 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004257276889435851 Min: -0.004257276889435853\n","Layer 0 - Gradient Weights Max: 0.00453955629446684 Min: -0.009311092177463145\n","ReLU Activation - Max: 2.029201776503085 Min: 0.0\n","Softmax Output - Max: 0.9490374523799241 Min: 0.050962547620075835 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2411153308822356 Min: 0.0\n","Softmax Output - Max: 0.9697487053020094 Min: 0.03025129469799051 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004232802933492494 Min: -0.004232802933492494\n","Layer 0 - Gradient Weights Max: 0.004652957190197548 Min: -0.009391477526444038\n","ReLU Activation - Max: 2.0305690279824993 Min: 0.0\n","Softmax Output - Max: 0.9491545426667796 Min: 0.05084545733322041 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2428084300582967 Min: 0.0\n","Softmax Output - Max: 0.9698471602661624 Min: 0.03015283973383774 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004208434424207499 Min: -0.0042084344242074995\n","Layer 0 - Gradient Weights Max: 0.004553760572560112 Min: -0.009487754645070524\n","ReLU Activation - Max: 2.0319289248348085 Min: 0.0\n","Softmax Output - Max: 0.9492707584575345 Min: 0.05072924154246548 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.244489822812642 Min: 0.0\n","Softmax Output - Max: 0.9699449363002801 Min: 0.030055063699719914 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004184368838365529 Min: -0.004184368838365531\n","Layer 0 - Gradient Weights Max: 0.0046704164905930325 Min: -0.009482242997338288\n","ReLU Activation - Max: 2.033281028609243 Min: 0.0\n","Softmax Output - Max: 0.9493866419882946 Min: 0.05061335801170554 Sum (first example): 1.0\n","ReLU Activation - Max: 2.246161652985983 Min: 0.0\n","Softmax Output - Max: 0.9700418013017142 Min: 0.029958198698285757 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004160794941757892 Min: -0.00416079494175789\n","Layer 0 - Gradient Weights Max: 0.004750594322522052 Min: -0.00943778824857019\n","ReLU Activation - Max: 2.034625757098555 Min: 0.0\n","Softmax Output - Max: 0.9495016125203216 Min: 0.05049838747967841 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.2478173635477545 Min: 0.0\n","Softmax Output - Max: 0.9701379836794001 Min: 0.029862016320599944 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004137305856784642 Min: -0.004137305856784641\n","Layer 0 - Gradient Weights Max: 0.004807642053272365 Min: -0.009518202161214637\n","ReLU Activation - Max: 2.035958676033403 Min: 0.0\n","Softmax Output - Max: 0.9496152174698487 Min: 0.05038478253015144 Sum (first example): 1.0\n","Epoch 580, Loss: 0.5837601646849548, Test Accuracy: 0.7014285714285714\n","ReLU Activation - Max: 2.24947942537359 Min: 0.0\n","Softmax Output - Max: 0.9702332676144824 Min: 0.02976673238551758 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0041140288985150415 Min: -0.0041140288985150415\n","Layer 0 - Gradient Weights Max: 0.004821306134926901 Min: -0.009584717921671222\n","ReLU Activation - Max: 2.037284248165357 Min: 0.0\n","Softmax Output - Max: 0.9497273398414539 Min: 0.05027266015854611 Sum (first example): 1.0\n","ReLU Activation - Max: 2.251126569814403 Min: 0.0\n","Softmax Output - Max: 0.970327631640664 Min: 0.029672368359335958 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004091190207214883 Min: -0.004091190207214883\n","Layer 0 - Gradient Weights Max: 0.004885446132141958 Min: -0.009639532259262092\n","ReLU Activation - Max: 2.0386022536414248 Min: 0.0\n","Softmax Output - Max: 0.9498383489651004 Min: 0.05016165103489958 Sum (first example): 1.0\n","ReLU Activation - Max: 2.252782313893104 Min: 0.0\n","Softmax Output - Max: 0.9704212183904053 Min: 0.029578781609594783 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004068889051971792 Min: -0.0040688890519717915\n","Layer 0 - Gradient Weights Max: 0.004895357928891642 Min: -0.009675241218688867\n","ReLU Activation - Max: 2.039912895199532 Min: 0.0\n","Softmax Output - Max: 0.9499484899082331 Min: 0.05005151009176686 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2544288509883095 Min: 0.0\n","Softmax Output - Max: 0.9705141724856072 Min: 0.029485827514392708 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004046856495102829 Min: -0.004046856495102831\n","Layer 0 - Gradient Weights Max: 0.004921533341893414 Min: -0.009694446301155141\n","ReLU Activation - Max: 2.0412085707205447 Min: 0.0\n","Softmax Output - Max: 0.9500577229542556 Min: 0.04994227704574432 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2560801279663814 Min: 0.0\n","Softmax Output - Max: 0.9706064273282586 Min: 0.029393572671741402 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004025192211050958 Min: -0.004025192211050958\n","Layer 0 - Gradient Weights Max: 0.004933606663704744 Min: -0.009908594270839574\n","ReLU Activation - Max: 2.0424969805835285 Min: 0.0\n","Softmax Output - Max: 0.9501655715130333 Min: 0.04983442848696679 Sum (first example): 1.0\n","ReLU Activation - Max: 2.257723974754196 Min: 0.0\n","Softmax Output - Max: 0.9706976236228813 Min: 0.029302376377118744 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004003675071124212 Min: -0.004003675071124211\n","Layer 0 - Gradient Weights Max: 0.004950925470537414 Min: -0.009981342013611685\n","ReLU Activation - Max: 2.0437727580537968 Min: 0.0\n","Softmax Output - Max: 0.9502724912459201 Min: 0.04972750875407987 Sum (first example): 1.0\n","ReLU Activation - Max: 2.259378588779727 Min: 0.0\n","Softmax Output - Max: 0.9707880728025957 Min: 0.029211927197404328 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003982477919197866 Min: -0.003982477919197866\n","Layer 0 - Gradient Weights Max: 0.004941484932708511 Min: -0.009958057129025154\n","ReLU Activation - Max: 2.0450357283811758 Min: 0.0\n","Softmax Output - Max: 0.9503788381889255 Min: 0.04962116181107437 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2610204770663582 Min: 0.0\n","Softmax Output - Max: 0.9708777183516707 Min: 0.029122281648329328 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0039614994980713214 Min: -0.003961499498071323\n","Layer 0 - Gradient Weights Max: 0.004988819550567376 Min: -0.01003188417154554\n","ReLU Activation - Max: 2.0462995803956137 Min: 0.0\n","Softmax Output - Max: 0.950484247994902 Min: 0.049515752005097946 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2626432927868465 Min: 0.0\n","Softmax Output - Max: 0.9709666483395255 Min: 0.029033351660474493 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003940771638595658 Min: -0.00394077163859566\n","Layer 0 - Gradient Weights Max: 0.005079304055680071 Min: -0.010003823962724203\n","ReLU Activation - Max: 2.0475564486696305 Min: 0.0\n","Softmax Output - Max: 0.9505888958288259 Min: 0.049411104171174135 Sum (first example): 1.0\n","ReLU Activation - Max: 2.264253188809826 Min: 0.0\n","Softmax Output - Max: 0.971054805421245 Min: 0.028945194578755044 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003920260828830607 Min: -0.003920260828830609\n","Layer 0 - Gradient Weights Max: 0.005121430444081061 Min: -0.009893792846506762\n","ReLU Activation - Max: 2.0488181269064434 Min: 0.0\n","Softmax Output - Max: 0.9506924669555195 Min: 0.04930753304448054 Sum (first example): 1.0\n","Epoch 590, Loss: 0.5832317137875969, Test Accuracy: 0.7003571428571429\n","ReLU Activation - Max: 2.265851227458194 Min: 0.0\n","Softmax Output - Max: 0.9711422859921286 Min: 0.028857714007871262 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038999569792563115 Min: -0.0038999569792563115\n","Layer 0 - Gradient Weights Max: 0.005140416022776536 Min: -0.009847454776349206\n","ReLU Activation - Max: 2.0500705064112417 Min: 0.0\n","Softmax Output - Max: 0.9507953372911118 Min: 0.049204662708888285 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.2674510080297376 Min: 0.0\n","Softmax Output - Max: 0.9712291621897617 Min: 0.028770837810238162 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038801034558871438 Min: -0.0038801034558871455\n","Layer 0 - Gradient Weights Max: 0.00516319244353226 Min: -0.009862188972188225\n","ReLU Activation - Max: 2.0513157962448663 Min: 0.0\n","Softmax Output - Max: 0.9508977415477727 Min: 0.049102258452227314 Sum (first example): 1.0\n","ReLU Activation - Max: 2.269043032050603 Min: 0.0\n","Softmax Output - Max: 0.9713153071356886 Min: 0.0286846928643113 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003860505316812247 Min: -0.0038605053168122485\n","Layer 0 - Gradient Weights Max: 0.005179284578148255 Min: -0.009887055988838375\n","ReLU Activation - Max: 2.0525489719712398 Min: 0.0\n","Softmax Output - Max: 0.9509988764812636 Min: 0.04900112351873636 Sum (first example): 1.0\n","ReLU Activation - Max: 2.27062428886061 Min: 0.0\n","Softmax Output - Max: 0.9714005743359982 Min: 0.02859942566400167 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038410890747871624 Min: -0.0038410890747871632\n","Layer 0 - Gradient Weights Max: 0.005232429365868183 Min: -0.009835961940952981\n","ReLU Activation - Max: 2.053777801816148 Min: 0.0\n","Softmax Output - Max: 0.9510997941057767 Min: 0.048900205894223274 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2721994448838227 Min: 0.0\n","Softmax Output - Max: 0.9714851078926524 Min: 0.028514892107347655 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003821972444718102 Min: -0.0038219724447181016\n","Layer 0 - Gradient Weights Max: 0.00524179476712037 Min: -0.009702580785622491\n","ReLU Activation - Max: 2.055004003826129 Min: 0.0\n","Softmax Output - Max: 0.9512003684342489 Min: 0.04879963156575104 Sum (first example): 1.0\n","ReLU Activation - Max: 2.273776218541843 Min: 0.0\n","Softmax Output - Max: 0.9715692083532821 Min: 0.028430791646718013 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00380307037317463 Min: -0.0038030703731746305\n","Layer 0 - Gradient Weights Max: 0.005465957647704737 Min: -0.00978185008725555\n","ReLU Activation - Max: 2.056237369489409 Min: 0.0\n","Softmax Output - Max: 0.9513002140611628 Min: 0.048699785938837135 Sum (first example): 1.0\n","ReLU Activation - Max: 2.275337221796969 Min: 0.0\n","Softmax Output - Max: 0.9716523177459765 Min: 0.028347682254023475 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003784442949986035 Min: -0.003784442949986035\n","Layer 0 - Gradient Weights Max: 0.005455389413085579 Min: -0.009685099871230939\n","ReLU Activation - Max: 2.057458344262147 Min: 0.0\n","Softmax Output - Max: 0.9513996933918218 Min: 0.04860030660817819 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.2769007934997427 Min: 0.0\n","Softmax Output - Max: 0.9717350284085988 Min: 0.028264971591401105 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003765833152956336 Min: -0.003765833152956334\n","Layer 0 - Gradient Weights Max: 0.005536725328663111 Min: -0.009696053499665177\n","ReLU Activation - Max: 2.058674714902716 Min: 0.0\n","Softmax Output - Max: 0.9514979644881553 Min: 0.048502035511844656 Sum (first example): 1.0\n","ReLU Activation - Max: 2.278472193610252 Min: 0.0\n","Softmax Output - Max: 0.9718172654419457 Min: 0.028182734558054305 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003747573105848802 Min: -0.003747573105848801\n","Layer 0 - Gradient Weights Max: 0.005554769737031954 Min: -0.009668338428452704\n","ReLU Activation - Max: 2.059887454896307 Min: 0.0\n","Softmax Output - Max: 0.9515954982994064 Min: 0.048404501700593586 Sum (first example): 1.0\n","ReLU Activation - Max: 2.280024389071376 Min: 0.0\n","Softmax Output - Max: 0.9718986551143673 Min: 0.028101344885632647 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003729261741167071 Min: -0.003729261741167071\n","Layer 0 - Gradient Weights Max: 0.005552160104424465 Min: -0.00976841660708648\n","ReLU Activation - Max: 2.0611005202352177 Min: 0.0\n","Softmax Output - Max: 0.951691680704264 Min: 0.04830831929573599 Sum (first example): 1.0\n","Epoch 600, Loss: 0.5827065273829329, Test Accuracy: 0.7003571428571429\n","ReLU Activation - Max: 2.2815592048016042 Min: 0.0\n","Softmax Output - Max: 0.9719790261099971 Min: 0.02802097389000288 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037114011598358646 Min: -0.0037114011598358655\n","Layer 0 - Gradient Weights Max: 0.005562336853108415 Min: -0.009837371239379129\n","ReLU Activation - Max: 2.0623086832232667 Min: 0.0\n","Softmax Output - Max: 0.9517869202181493 Min: 0.0482130797818508 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2830783421235625 Min: 0.0\n","Softmax Output - Max: 0.9720588012968037 Min: 0.02794119870319641 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036937719599101763 Min: -0.0036937719599101746\n","Layer 0 - Gradient Weights Max: 0.005648255083197474 Min: -0.009813471062288645\n","ReLU Activation - Max: 2.063520400489957 Min: 0.0\n","Softmax Output - Max: 0.9518817375469257 Min: 0.04811826245307424 Sum (first example): 1.0\n","ReLU Activation - Max: 2.284581218344059 Min: 0.0\n","Softmax Output - Max: 0.9721380028814097 Min: 0.027861997118590307 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003676264346817269 Min: -0.003676264346817269\n","Layer 0 - Gradient Weights Max: 0.005662133089157126 Min: -0.009803317356427652\n","ReLU Activation - Max: 2.064721531336294 Min: 0.0\n","Softmax Output - Max: 0.9519760660283743 Min: 0.04802393397162568 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.286098191084352 Min: 0.0\n","Softmax Output - Max: 0.9722167494321279 Min: 0.027783250567872016 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036591199844740866 Min: -0.003659119984474085\n","Layer 0 - Gradient Weights Max: 0.0057786284371962255 Min: -0.009815632873027785\n","ReLU Activation - Max: 2.0659165407535385 Min: 0.0\n","Softmax Output - Max: 0.9520692531783271 Min: 0.047930746821672865 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2876061992128447 Min: 0.0\n","Softmax Output - Max: 0.9722945309838934 Min: 0.027705469016106713 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036425285470347126 Min: -0.0036425285470347113\n","Layer 0 - Gradient Weights Max: 0.005807551794298498 Min: -0.009829256023188696\n","ReLU Activation - Max: 2.0671059499286057 Min: 0.0\n","Softmax Output - Max: 0.9521746770803363 Min: 0.04782532291966375 Sum (first example): 1.0\n","ReLU Activation - Max: 2.289113602809402 Min: 0.0\n","Softmax Output - Max: 0.9723716913751508 Min: 0.02762830862484924 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036262087058962504 Min: -0.0036262087058962513\n","Layer 0 - Gradient Weights Max: 0.005755006962319918 Min: -0.00975276426457383\n","ReLU Activation - Max: 2.068292914485826 Min: 0.0\n","Softmax Output - Max: 0.9523208251454329 Min: 0.047679174854567095 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2906301110471463 Min: 0.0\n","Softmax Output - Max: 0.9724486678977028 Min: 0.02755133210229715 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003610215492370297 Min: -0.003610215492370297\n","Layer 0 - Gradient Weights Max: 0.005800790554117839 Min: -0.009777784003714004\n","ReLU Activation - Max: 2.0694721755439445 Min: 0.0\n","Softmax Output - Max: 0.9524660846693593 Min: 0.047533915330640705 Sum (first example): 1.0\n","ReLU Activation - Max: 2.292123615337911 Min: 0.0\n","Softmax Output - Max: 0.9725250569207787 Min: 0.027474943079221388 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003594193147309174 Min: -0.0035941931473091756\n","Layer 0 - Gradient Weights Max: 0.005968442507491756 Min: -0.009830657523835027\n","ReLU Activation - Max: 2.070645692336348 Min: 0.0\n","Softmax Output - Max: 0.952610469154876 Min: 0.047389530845123956 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2936097396393706 Min: 0.0\n","Softmax Output - Max: 0.9726006843478779 Min: 0.027399315652121982 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003578156763442829 Min: -0.003578156763442828\n","Layer 0 - Gradient Weights Max: 0.005939513582975838 Min: -0.009864882025225554\n","ReLU Activation - Max: 2.071811514062029 Min: 0.0\n","Softmax Output - Max: 0.9527537706952572 Min: 0.04724622930474281 Sum (first example): 1.0\n","ReLU Activation - Max: 2.2950920426374335 Min: 0.0\n","Softmax Output - Max: 0.9726756691858993 Min: 0.027324330814100635 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003562145988044156 Min: -0.0035621459880441552\n","Layer 0 - Gradient Weights Max: 0.005983054232518125 Min: -0.009893986084452138\n","ReLU Activation - Max: 2.0729762564423355 Min: 0.0\n","Softmax Output - Max: 0.95289623861471 Min: 0.0471037613852901 Sum (first example): 1.0\n","Epoch 610, Loss: 0.5821797381323677, Test Accuracy: 0.7010714285714286\n","ReLU Activation - Max: 2.296569315902329 Min: 0.0\n","Softmax Output - Max: 0.9727503211087231 Min: 0.0272496788912768 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003546625702956384 Min: -0.0035466257029563835\n","Layer 0 - Gradient Weights Max: 0.005995733470590714 Min: -0.01002135735019532\n","ReLU Activation - Max: 2.074144000656564 Min: 0.0\n","Softmax Output - Max: 0.9530378879023549 Min: 0.04696211209764507 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.298039166321907 Min: 0.0\n","Softmax Output - Max: 0.9728241739135646 Min: 0.02717582608643542 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003531537062765112 Min: -0.003531537062765112\n","Layer 0 - Gradient Weights Max: 0.006004477302980736 Min: -0.009982321504180698\n","ReLU Activation - Max: 2.0752913399032717 Min: 0.0\n","Softmax Output - Max: 0.9531781482654947 Min: 0.046821851734505225 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.2995212775568064 Min: 0.0\n","Softmax Output - Max: 0.9728976585831149 Min: 0.027102341416885126 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003516771996362766 Min: -0.003516771996362766\n","Layer 0 - Gradient Weights Max: 0.006005560484982151 Min: -0.009918176359604217\n","ReLU Activation - Max: 2.076441201431953 Min: 0.0\n","Softmax Output - Max: 0.9533178796703464 Min: 0.04668212032965362 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3009886696541297 Min: 0.0\n","Softmax Output - Max: 0.9729705980838136 Min: 0.02702940191618639 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003502250228149293 Min: -0.003502250228149294\n","Layer 0 - Gradient Weights Max: 0.006131995827056894 Min: -0.009891085770282549\n","ReLU Activation - Max: 2.077584275263227 Min: 0.0\n","Softmax Output - Max: 0.953456681591194 Min: 0.046543318408805906 Sum (first example): 1.0\n","ReLU Activation - Max: 2.30244326183891 Min: 0.0\n","Softmax Output - Max: 0.9730431456946235 Min: 0.026956854305376494 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00348775501304735 Min: -0.00348775501304735\n","Layer 0 - Gradient Weights Max: 0.006115316690637886 Min: -0.010026214066146318\n","ReLU Activation - Max: 2.0787304279413945 Min: 0.0\n","Softmax Output - Max: 0.9535949835306724 Min: 0.04640501646932753 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.303887268818876 Min: 0.0\n","Softmax Output - Max: 0.973115148109812 Min: 0.026884851890188053 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003473582405651165 Min: -0.0034735824056511657\n","Layer 0 - Gradient Weights Max: 0.006165020992535071 Min: -0.010024979729289412\n","ReLU Activation - Max: 2.0798550515087793 Min: 0.0\n","Softmax Output - Max: 0.9537315874217469 Min: 0.046268412578253225 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3053386582766127 Min: 0.0\n","Softmax Output - Max: 0.973186346722762 Min: 0.02681365327723781 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034593918093869754 Min: -0.0034593918093869754\n","Layer 0 - Gradient Weights Max: 0.006193908960286252 Min: -0.01007031508345017\n","ReLU Activation - Max: 2.0809767603708273 Min: 0.0\n","Softmax Output - Max: 0.9538674777977808 Min: 0.04613252220221923 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3067804970980603 Min: 0.0\n","Softmax Output - Max: 0.9732568855027456 Min: 0.02674311449725437 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034454452925690034 Min: -0.0034454452925690034\n","Layer 0 - Gradient Weights Max: 0.006182042973784484 Min: -0.01010777351489029\n","ReLU Activation - Max: 2.082090223942015 Min: 0.0\n","Softmax Output - Max: 0.9540024434811356 Min: 0.04599755651886433 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3082179613569975 Min: 0.0\n","Softmax Output - Max: 0.9733270842458259 Min: 0.026672915754174013 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.003431881797333455 Min: -0.003431881797333455\n","Layer 0 - Gradient Weights Max: 0.006264163215890386 Min: -0.010031491640403564\n","ReLU Activation - Max: 2.0832025582355262 Min: 0.0\n","Softmax Output - Max: 0.9541364684234 Min: 0.04586353157659996 Sum (first example): 1.0\n","ReLU Activation - Max: 2.309665398072498 Min: 0.0\n","Softmax Output - Max: 0.9733967824381794 Min: 0.026603217561820477 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034187174785372714 Min: -0.0034187174785372714\n","Layer 0 - Gradient Weights Max: 0.006261199396726618 Min: -0.010014116438481145\n","ReLU Activation - Max: 2.0842947748794987 Min: 0.0\n","Softmax Output - Max: 0.9542689500648799 Min: 0.045731049935120165 Sum (first example): 0.9999999999999999\n","Epoch 620, Loss: 0.5816455333907367, Test Accuracy: 0.7025\n","ReLU Activation - Max: 2.3111102783029347 Min: 0.0\n","Softmax Output - Max: 0.9734660213025067 Min: 0.026533978697493323 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034051591799309153 Min: -0.003405159179930916\n","Layer 0 - Gradient Weights Max: 0.006334212642293374 Min: -0.010089526096563722\n","ReLU Activation - Max: 2.085382049495574 Min: 0.0\n","Softmax Output - Max: 0.9544006757967972 Min: 0.04559932420320284 Sum (first example): 1.0\n","ReLU Activation - Max: 2.312548269801899 Min: 0.0\n","Softmax Output - Max: 0.973534761739292 Min: 0.026465238260707916 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0033916986784651706 Min: -0.003391698678465172\n","Layer 0 - Gradient Weights Max: 0.006406596518261904 Min: -0.010099436334450186\n","ReLU Activation - Max: 2.0864556790328574 Min: 0.0\n","Softmax Output - Max: 0.9545313096179326 Min: 0.04546869038206741 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.3139870596075696 Min: 0.0\n","Softmax Output - Max: 0.9736028812713827 Min: 0.026397118728617254 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003378461977124468 Min: -0.0033784619771244687\n","Layer 0 - Gradient Weights Max: 0.006347639396532519 Min: -0.010121840007733703\n","ReLU Activation - Max: 2.0875314365729376 Min: 0.0\n","Softmax Output - Max: 0.9546614461300399 Min: 0.04533855386996004 Sum (first example): 1.0\n","ReLU Activation - Max: 2.31544735981928 Min: 0.0\n","Softmax Output - Max: 0.9736705755474339 Min: 0.026329424452566107 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033657346935689197 Min: -0.00336573469356892\n","Layer 0 - Gradient Weights Max: 0.006411847466569137 Min: -0.010107539879770302\n","ReLU Activation - Max: 2.088601040178334 Min: 0.0\n","Softmax Output - Max: 0.954790721730937 Min: 0.04520927826906301 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.316902854525011 Min: 0.0\n","Softmax Output - Max: 0.9737380933459029 Min: 0.026261906654097195 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.00335306885458274 Min: -0.00335306885458274\n","Layer 0 - Gradient Weights Max: 0.006286760476375559 Min: -0.010070896614504393\n","ReLU Activation - Max: 2.089662259827927 Min: 0.0\n","Softmax Output - Max: 0.954919298485347 Min: 0.04508070151465304 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3183515273713646 Min: 0.0\n","Softmax Output - Max: 0.9738052506318848 Min: 0.02619474936811503 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003340268159687969 Min: -0.003340268159687968\n","Layer 0 - Gradient Weights Max: 0.00627386270683878 Min: -0.01006269551348893\n","ReLU Activation - Max: 2.0907182980429178 Min: 0.0\n","Softmax Output - Max: 0.9550471533390372 Min: 0.044952846660962775 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.3198202784955324 Min: 0.0\n","Softmax Output - Max: 0.9738720202346717 Min: 0.02612797976532824 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033278384323871084 Min: -0.0033278384323871067\n","Layer 0 - Gradient Weights Max: 0.006293101432813936 Min: -0.010045444554081575\n","ReLU Activation - Max: 2.0917750997104236 Min: 0.0\n","Softmax Output - Max: 0.9551744078529996 Min: 0.04482559214700039 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.3212829300582203 Min: 0.0\n","Softmax Output - Max: 0.973938433279724 Min: 0.026061566720275996 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0033156102173161417 Min: -0.0033156102173161443\n","Layer 0 - Gradient Weights Max: 0.0063121539111584725 Min: -0.010017149268296733\n","ReLU Activation - Max: 2.092811138523267 Min: 0.0\n","Softmax Output - Max: 0.9553005011384755 Min: 0.04469949886152456 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3227523053877546 Min: 0.0\n","Softmax Output - Max: 0.9740042271369413 Min: 0.025995772863058708 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003303420008357587 Min: -0.0033034200083575872\n","Layer 0 - Gradient Weights Max: 0.006220482407164737 Min: -0.010060063476541564\n","ReLU Activation - Max: 2.0938449024699484 Min: 0.0\n","Softmax Output - Max: 0.9554260063772372 Min: 0.04457399362276294 Sum (first example): 1.0\n","ReLU Activation - Max: 2.324214131220808 Min: 0.0\n","Softmax Output - Max: 0.9740694469631958 Min: 0.025930553036804267 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003291352110168279 Min: -0.003291352110168279\n","Layer 0 - Gradient Weights Max: 0.0062775849728231425 Min: -0.010048759148165326\n","ReLU Activation - Max: 2.0948758172306503 Min: 0.0\n","Softmax Output - Max: 0.9555508845968955 Min: 0.04444911540310451 Sum (first example): 1.0\n","Epoch 630, Loss: 0.5811082027354095, Test Accuracy: 0.7025\n","ReLU Activation - Max: 2.325661406228788 Min: 0.0\n","Softmax Output - Max: 0.9741343370961086 Min: 0.025865662903891497 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003279283308672987 Min: -0.003279283308672987\n","Layer 0 - Gradient Weights Max: 0.006330797948025106 Min: -0.009997258450652557\n","ReLU Activation - Max: 2.0959021395237296 Min: 0.0\n","Softmax Output - Max: 0.9556752333019758 Min: 0.04432476669802414 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.3271020981255153 Min: 0.0\n","Softmax Output - Max: 0.9741986926338533 Min: 0.025801307366146685 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032673296410643607 Min: -0.0032673296410643594\n","Layer 0 - Gradient Weights Max: 0.006400033695369236 Min: -0.010041615788609695\n","ReLU Activation - Max: 2.096933504864287 Min: 0.0\n","Softmax Output - Max: 0.9557992508682529 Min: 0.044200749131747145 Sum (first example): 1.0\n","ReLU Activation - Max: 2.328555459875414 Min: 0.0\n","Softmax Output - Max: 0.9742625191175353 Min: 0.025737480882464712 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003255793891162706 Min: -0.0032557938911627064\n","Layer 0 - Gradient Weights Max: 0.006436639295464818 Min: -0.010186510014987245\n","ReLU Activation - Max: 2.0979562170165633 Min: 0.0\n","Softmax Output - Max: 0.9559224958886083 Min: 0.04407750411139164 Sum (first example): 1.0\n","ReLU Activation - Max: 2.330005989020394 Min: 0.0\n","Softmax Output - Max: 0.9743257923149894 Min: 0.025674207685010667 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003244600001815313 Min: -0.0032446000018153142\n","Layer 0 - Gradient Weights Max: 0.00646961089323468 Min: -0.010150368511431512\n","ReLU Activation - Max: 2.098974748718223 Min: 0.0\n","Softmax Output - Max: 0.9560449407002239 Min: 0.043955059299776035 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3314506630781437 Min: 0.0\n","Softmax Output - Max: 0.9743888295298371 Min: 0.025611170470162918 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032334819195718924 Min: -0.0032334819195718937\n","Layer 0 - Gradient Weights Max: 0.0065915201525787925 Min: -0.010117784916166949\n","ReLU Activation - Max: 2.099991431854385 Min: 0.0\n","Softmax Output - Max: 0.956166858626774 Min: 0.043833141373225956 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3328782475923657 Min: 0.0\n","Softmax Output - Max: 0.9744512552893211 Min: 0.025548744710679073 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003222390315464085 Min: -0.0032223903154640844\n","Layer 0 - Gradient Weights Max: 0.006683815796402738 Min: -0.010170828821568728\n","ReLU Activation - Max: 2.1010037415492797 Min: 0.0\n","Softmax Output - Max: 0.9562882555877804 Min: 0.043711744412219575 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3342996730684575 Min: 0.0\n","Softmax Output - Max: 0.974513086401529 Min: 0.02548691359847104 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003211366203895983 Min: -0.0032113662038959815\n","Layer 0 - Gradient Weights Max: 0.006637742646764671 Min: -0.010171383173944738\n","ReLU Activation - Max: 2.1020013723113906 Min: 0.0\n","Softmax Output - Max: 0.9564087206831721 Min: 0.043591279316827936 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3357110376911367 Min: 0.0\n","Softmax Output - Max: 0.9745744838986783 Min: 0.02542551610132165 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0032003539055951626 Min: -0.0032003539055951613\n","Layer 0 - Gradient Weights Max: 0.006680295261255627 Min: -0.010216505033365164\n","ReLU Activation - Max: 2.1029899651637876 Min: 0.0\n","Softmax Output - Max: 0.9565284728174058 Min: 0.04347152718259408 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3371169779414767 Min: 0.0\n","Softmax Output - Max: 0.9746353865505176 Min: 0.02536461344948233 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003189596904098911 Min: -0.00318959690409891\n","Layer 0 - Gradient Weights Max: 0.00668732506876868 Min: -0.01026466708372376\n","ReLU Activation - Max: 2.1039743161995554 Min: 0.0\n","Softmax Output - Max: 0.9566475392061016 Min: 0.04335246079389847 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.338517034012661 Min: 0.0\n","Softmax Output - Max: 0.9746958557360977 Min: 0.02530414426390237 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031789192589691518 Min: -0.0031789192589691518\n","Layer 0 - Gradient Weights Max: 0.006638063996482657 Min: -0.010293249645684437\n","ReLU Activation - Max: 2.1049579877461033 Min: 0.0\n","Softmax Output - Max: 0.9567661319890348 Min: 0.0432338680109651 Sum (first example): 0.9999999999999999\n","Epoch 640, Loss: 0.5805686602446688, Test Accuracy: 0.7021428571428572\n","ReLU Activation - Max: 2.3399073823249767 Min: 0.0\n","Softmax Output - Max: 0.9747558168216577 Min: 0.025244183178342355 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031684957121699614 Min: -0.0031684957121699614\n","Layer 0 - Gradient Weights Max: 0.006702449622252312 Min: -0.010407393921006071\n","ReLU Activation - Max: 2.1059424602972574 Min: 0.0\n","Softmax Output - Max: 0.9568843913037105 Min: 0.04311560869628949 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.3412875092008494 Min: 0.0\n","Softmax Output - Max: 0.9748149325377643 Min: 0.02518506746223571 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0031582570701177235 Min: -0.0031582570701177235\n","Layer 0 - Gradient Weights Max: 0.006681006047435948 Min: -0.010349965248321661\n","ReLU Activation - Max: 2.106918866424283 Min: 0.0\n","Softmax Output - Max: 0.9570018076071087 Min: 0.04299819239289121 Sum (first example): 1.0\n","ReLU Activation - Max: 2.342666949480866 Min: 0.0\n","Softmax Output - Max: 0.9748741073732865 Min: 0.02512589262671348 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031483618793935212 Min: -0.003148361879393522\n","Layer 0 - Gradient Weights Max: 0.006609712107563292 Min: -0.010224404993196032\n","ReLU Activation - Max: 2.107893682139409 Min: 0.0\n","Softmax Output - Max: 0.9571187065039487 Min: 0.04288129349605121 Sum (first example): 1.0\n","ReLU Activation - Max: 2.344049864026942 Min: 0.0\n","Softmax Output - Max: 0.9749331057100236 Min: 0.025066894289976546 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003138608627843061 Min: -0.0031386086278430607\n","Layer 0 - Gradient Weights Max: 0.006572642065000303 Min: -0.010263333279341164\n","ReLU Activation - Max: 2.1088695685242698 Min: 0.0\n","Softmax Output - Max: 0.9572350736646653 Min: 0.042764926335334734 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3454137037012766 Min: 0.0\n","Softmax Output - Max: 0.9749920033287583 Min: 0.0250079966712415 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003133605700404547 Min: -0.003133605700404547\n","Layer 0 - Gradient Weights Max: 0.006613044639909963 Min: -0.01026853838916861\n","ReLU Activation - Max: 2.1098450939498967 Min: 0.0\n","Softmax Output - Max: 0.9573509856806002 Min: 0.04264901431939986 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3467705102204572 Min: 0.0\n","Softmax Output - Max: 0.975050482179734 Min: 0.0249495178202661 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031382935045495574 Min: -0.003138293504549556\n","Layer 0 - Gradient Weights Max: 0.006681898390218515 Min: -0.010273519776909375\n","ReLU Activation - Max: 2.110822706001641 Min: 0.0\n","Softmax Output - Max: 0.9574663689188959 Min: 0.042533631081103994 Sum (first example): 1.0\n","ReLU Activation - Max: 2.348133438284941 Min: 0.0\n","Softmax Output - Max: 0.9751083391719017 Min: 0.02489166082809842 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031429607168275728 Min: -0.003142960716827573\n","Layer 0 - Gradient Weights Max: 0.006677289411333814 Min: -0.010242018497552876\n","ReLU Activation - Max: 2.111785279241293 Min: 0.0\n","Softmax Output - Max: 0.9575807731299926 Min: 0.04241922687000739 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.34950107272366 Min: 0.0\n","Softmax Output - Max: 0.9751656917348267 Min: 0.024834308265173393 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031476362230699307 Min: -0.003147636223069931\n","Layer 0 - Gradient Weights Max: 0.00662572084106276 Min: -0.01018429702702437\n","ReLU Activation - Max: 2.1127420740321528 Min: 0.0\n","Softmax Output - Max: 0.9576944805395475 Min: 0.04230551946045253 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3508655319681266 Min: 0.0\n","Softmax Output - Max: 0.9752227605443233 Min: 0.024777239455676725 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031523157987365493 Min: -0.0031523157987365493\n","Layer 0 - Gradient Weights Max: 0.006768858016364251 Min: -0.010172626787566598\n","ReLU Activation - Max: 2.113680264876429 Min: 0.0\n","Softmax Output - Max: 0.9578070430977307 Min: 0.04219295690226924 Sum (first example): 1.0\n","ReLU Activation - Max: 2.35224584554226 Min: 0.0\n","Softmax Output - Max: 0.9752794768783221 Min: 0.024720523121677884 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003157054351233811 Min: -0.0031570543512338105\n","Layer 0 - Gradient Weights Max: 0.00682894194991459 Min: -0.01020521046576553\n","ReLU Activation - Max: 2.114634125854883 Min: 0.0\n","Softmax Output - Max: 0.9579197302389272 Min: 0.042080269761072826 Sum (first example): 1.0\n","Epoch 650, Loss: 0.5800224762442799, Test Accuracy: 0.7028571428571428\n","ReLU Activation - Max: 2.3536183159223487 Min: 0.0\n","Softmax Output - Max: 0.97533570424918 Min: 0.024664295750820114 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031617848259866055 Min: -0.0031617848259866063\n","Layer 0 - Gradient Weights Max: 0.0069207691495865 Min: -0.01019586549996387\n","ReLU Activation - Max: 2.1155819489621406 Min: 0.0\n","Softmax Output - Max: 0.9580317176708775 Min: 0.041968282329122565 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.355001063420882 Min: 0.0\n","Softmax Output - Max: 0.9753912384739231 Min: 0.024608761526076994 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003166588145577345 Min: -0.003166588145577344\n","Layer 0 - Gradient Weights Max: 0.006873785037646297 Min: -0.010256117427873933\n","ReLU Activation - Max: 2.1165179078531677 Min: 0.0\n","Softmax Output - Max: 0.9581430375995607 Min: 0.041856962400439415 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3563550832257243 Min: 0.0\n","Softmax Output - Max: 0.9754465380280402 Min: 0.024553461971959813 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003171425428710432 Min: -0.0031714254287104325\n","Layer 0 - Gradient Weights Max: 0.006940713403156583 Min: -0.010199775369415352\n","ReLU Activation - Max: 2.1174499539878324 Min: 0.0\n","Softmax Output - Max: 0.9582537955520416 Min: 0.04174620444795827 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.357703535221051 Min: 0.0\n","Softmax Output - Max: 0.9755014972804652 Min: 0.02449850271953485 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031762997694388625 Min: -0.003176299769438864\n","Layer 0 - Gradient Weights Max: 0.006942495302033882 Min: -0.010345359262925893\n","ReLU Activation - Max: 2.1183775934040034 Min: 0.0\n","Softmax Output - Max: 0.9583639764583333 Min: 0.0416360235416668 Sum (first example): 1.0\n","ReLU Activation - Max: 2.359048451072169 Min: 0.0\n","Softmax Output - Max: 0.97555611554533 Min: 0.024443884454670065 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031812373536093987 Min: -0.0031812373536093987\n","Layer 0 - Gradient Weights Max: 0.007015147212817861 Min: -0.010332933245242424\n","ReLU Activation - Max: 2.119307352611867 Min: 0.0\n","Softmax Output - Max: 0.9584738271380029 Min: 0.041526172861997133 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.360384400824862 Min: 0.0\n","Softmax Output - Max: 0.9756104952169514 Min: 0.024389504783048695 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0031861732302892877 Min: -0.0031861732302892877\n","Layer 0 - Gradient Weights Max: 0.007025230770712884 Min: -0.010308213258156673\n","ReLU Activation - Max: 2.1202382672021045 Min: 0.0\n","Softmax Output - Max: 0.9585834786725923 Min: 0.04141652132740766 Sum (first example): 1.0\n","ReLU Activation - Max: 2.361721050541309 Min: 0.0\n","Softmax Output - Max: 0.9756645243703375 Min: 0.024335475629662456 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003191099651965845 Min: -0.003191099651965845\n","Layer 0 - Gradient Weights Max: 0.00705191398220737 Min: -0.010302475984286548\n","ReLU Activation - Max: 2.121165265370389 Min: 0.0\n","Softmax Output - Max: 0.9586926668494317 Min: 0.04130733315056844 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3630330910057746 Min: 0.0\n","Softmax Output - Max: 0.9757185134786843 Min: 0.024281486521315565 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003196046012918917 Min: -0.0031960460129189165\n","Layer 0 - Gradient Weights Max: 0.007050397449257252 Min: -0.010286704014845797\n","ReLU Activation - Max: 2.1220849267045874 Min: 0.0\n","Softmax Output - Max: 0.9588010602438003 Min: 0.04119893975619982 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3643425140533423 Min: 0.0\n","Softmax Output - Max: 0.9757722062141517 Min: 0.024227793785848172 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032010287763554095 Min: -0.003201028776355409\n","Layer 0 - Gradient Weights Max: 0.006963916460757401 Min: -0.010366823242005585\n","ReLU Activation - Max: 2.122989545970557 Min: 0.0\n","Softmax Output - Max: 0.9589085473006214 Min: 0.04109145269937877 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3656554527932365 Min: 0.0\n","Softmax Output - Max: 0.9758256765078189 Min: 0.024174323492181054 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032058810874303597 Min: -0.0032058810874303592\n","Layer 0 - Gradient Weights Max: 0.007016571328189268 Min: -0.01041712822355554\n","ReLU Activation - Max: 2.123890289882134 Min: 0.0\n","Softmax Output - Max: 0.959015436546478 Min: 0.04098456345352201 Sum (first example): 1.0000000000000002\n","Epoch 660, Loss: 0.5794693343585567, Test Accuracy: 0.705\n","ReLU Activation - Max: 2.366967207763802 Min: 0.0\n","Softmax Output - Max: 0.9758789044543891 Min: 0.024121095545610805 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032107892376027204 Min: -0.0032107892376027204\n","Layer 0 - Gradient Weights Max: 0.0070391058947247955 Min: -0.010433593882470402\n","ReLU Activation - Max: 2.1247796900443543 Min: 0.0\n","Softmax Output - Max: 0.9591215992980352 Min: 0.04087840070196477 Sum (first example): 1.0\n","ReLU Activation - Max: 2.368280670181207 Min: 0.0\n","Softmax Output - Max: 0.9759321710049476 Min: 0.02406782899505248 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003215738362186065 Min: -0.0032157383621860642\n","Layer 0 - Gradient Weights Max: 0.007031030045904731 Min: -0.010412697054799839\n","ReLU Activation - Max: 2.1256687549732733 Min: 0.0\n","Softmax Output - Max: 0.9592272570255223 Min: 0.040772742974477805 Sum (first example): 1.0\n","ReLU Activation - Max: 2.369586362830585 Min: 0.0\n","Softmax Output - Max: 0.9759853853263749 Min: 0.024014614673625156 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003220728760111547 Min: -0.003220728760111547\n","Layer 0 - Gradient Weights Max: 0.007136699547895455 Min: -0.010428315470316401\n","ReLU Activation - Max: 2.1265569118345216 Min: 0.0\n","Softmax Output - Max: 0.9593325019815518 Min: 0.04066749801844818 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3708847663680235 Min: 0.0\n","Softmax Output - Max: 0.9760385475160924 Min: 0.023961452483907523 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003225665957162962 Min: -0.003225665957162962\n","Layer 0 - Gradient Weights Max: 0.00723547146513815 Min: -0.01047662208539196\n","ReLU Activation - Max: 2.1274331069970307 Min: 0.0\n","Softmax Output - Max: 0.9594370100438004 Min: 0.040562989956199544 Sum (first example): 1.0\n","ReLU Activation - Max: 2.372174351932112 Min: 0.0\n","Softmax Output - Max: 0.9760915518631413 Min: 0.023908448136858617 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003230600918547463 Min: -0.003230600918547463\n","Layer 0 - Gradient Weights Max: 0.007205100878493723 Min: -0.010613534037001387\n","ReLU Activation - Max: 2.1282987863408813 Min: 0.0\n","Softmax Output - Max: 0.9595408280059502 Min: 0.0404591719940499 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3734639077922832 Min: 0.0\n","Softmax Output - Max: 0.976144066752215 Min: 0.02385593324778507 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032354218204119537 Min: -0.003235421820411953\n","Layer 0 - Gradient Weights Max: 0.00721989056464291 Min: -0.010572279469623845\n","ReLU Activation - Max: 2.1291606839178217 Min: 0.0\n","Softmax Output - Max: 0.959644124137414 Min: 0.04035587586258608 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3747483811473185 Min: 0.0\n","Softmax Output - Max: 0.9761965170183518 Min: 0.023803482981648267 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003240236963344663 Min: -0.003240236963344663\n","Layer 0 - Gradient Weights Max: 0.007399909193400895 Min: -0.010530534844005986\n","ReLU Activation - Max: 2.1300119514887292 Min: 0.0\n","Softmax Output - Max: 0.95974656423651 Min: 0.040253435763489984 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.376070630331054 Min: 0.0\n","Softmax Output - Max: 0.9762489285641975 Min: 0.023751071435802608 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032451243158594075 Min: -0.0032451243158594075\n","Layer 0 - Gradient Weights Max: 0.007423752151106163 Min: -0.010582312818444559\n","ReLU Activation - Max: 2.1308577296110363 Min: 0.0\n","Softmax Output - Max: 0.9598485155395643 Min: 0.04015148446043566 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3773901021629174 Min: 0.0\n","Softmax Output - Max: 0.9763011176715226 Min: 0.02369888232847734 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003249942986854226 Min: -0.003249942986854226\n","Layer 0 - Gradient Weights Max: 0.007427192670327336 Min: -0.010649023481104697\n","ReLU Activation - Max: 2.131690194520564 Min: 0.0\n","Softmax Output - Max: 0.9599496420220656 Min: 0.04005035797793435 Sum (first example): 1.0\n","ReLU Activation - Max: 2.378698855695868 Min: 0.0\n","Softmax Output - Max: 0.976352944168443 Min: 0.023647055831557084 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00325476313583064 Min: -0.00325476313583064\n","Layer 0 - Gradient Weights Max: 0.007538006235883345 Min: -0.010680921349894125\n","ReLU Activation - Max: 2.132525192811729 Min: 0.0\n","Softmax Output - Max: 0.960050520186674 Min: 0.03994947981332605 Sum (first example): 1.0\n","Epoch 670, Loss: 0.5789087361311942, Test Accuracy: 0.705\n","ReLU Activation - Max: 2.3799893036726116 Min: 0.0\n","Softmax Output - Max: 0.9764044003456944 Min: 0.023595599654305566 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003259629316813474 Min: -0.0032596293168134748\n","Layer 0 - Gradient Weights Max: 0.007524525164960395 Min: -0.01070509916122926\n","ReLU Activation - Max: 2.133351987345691 Min: 0.0\n","Softmax Output - Max: 0.9601506191899666 Min: 0.03984938081003351 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.3812753554053865 Min: 0.0\n","Softmax Output - Max: 0.9764557280084448 Min: 0.023544271991555094 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003264480134183918 Min: -0.003264480134183918\n","Layer 0 - Gradient Weights Max: 0.0074541344049859366 Min: -0.010793553442318096\n","ReLU Activation - Max: 2.134174516225025 Min: 0.0\n","Softmax Output - Max: 0.960250049245336 Min: 0.039749950754664094 Sum (first example): 1.0\n","ReLU Activation - Max: 2.382553995856772 Min: 0.0\n","Softmax Output - Max: 0.9765068215623259 Min: 0.023493178437674112 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003269265529406652 Min: -0.0032692655294066526\n","Layer 0 - Gradient Weights Max: 0.007441818228392414 Min: -0.010766712346027562\n","ReLU Activation - Max: 2.134995593428258 Min: 0.0\n","Softmax Output - Max: 0.9603490314611274 Min: 0.03965096853887258 Sum (first example): 1.0\n","ReLU Activation - Max: 2.383840672256037 Min: 0.0\n","Softmax Output - Max: 0.9765573327817606 Min: 0.023442667218239326 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032740787007990583 Min: -0.0032740787007990583\n","Layer 0 - Gradient Weights Max: 0.007465758630288375 Min: -0.010744717068009982\n","ReLU Activation - Max: 2.135813239746469 Min: 0.0\n","Softmax Output - Max: 0.9604476120270429 Min: 0.03955238797295711 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3851189562491557 Min: 0.0\n","Softmax Output - Max: 0.9766074297087816 Min: 0.023392570291218426 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032789267063626504 Min: -0.003278926706362651\n","Layer 0 - Gradient Weights Max: 0.007428095051491238 Min: -0.010758592045828618\n","ReLU Activation - Max: 2.1366291964198254 Min: 0.0\n","Softmax Output - Max: 0.9605456746195339 Min: 0.039454325380466164 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.386389359152302 Min: 0.0\n","Softmax Output - Max: 0.9766570746996747 Min: 0.02334292530032531 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0032837544255786963 Min: -0.0032837544255786963\n","Layer 0 - Gradient Weights Max: 0.0074529559724707765 Min: -0.010703967395678443\n","ReLU Activation - Max: 2.1374418783880254 Min: 0.0\n","Softmax Output - Max: 0.9606432318827896 Min: 0.03935676811721034 Sum (first example): 1.0\n","ReLU Activation - Max: 2.387654881337256 Min: 0.0\n","Softmax Output - Max: 0.976706630744935 Min: 0.02329336925506498 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003288658540204878 Min: -0.003288658540204879\n","Layer 0 - Gradient Weights Max: 0.007432811367862074 Min: -0.010736615623650319\n","ReLU Activation - Max: 2.138251571353734 Min: 0.0\n","Softmax Output - Max: 0.9607402373985648 Min: 0.03925976260143515 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3888935349577056 Min: 0.0\n","Softmax Output - Max: 0.97675593200794 Min: 0.02324406799206005 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0032935723550848177 Min: -0.003293572355084818\n","Layer 0 - Gradient Weights Max: 0.007434914790193429 Min: -0.010714392404371394\n","ReLU Activation - Max: 2.1390522442925244 Min: 0.0\n","Softmax Output - Max: 0.9608365285689598 Min: 0.03916347143104008 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.3901304544334594 Min: 0.0\n","Softmax Output - Max: 0.9768049363457967 Min: 0.023195063654203305 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003298446036515941 Min: -0.003298446036515941\n","Layer 0 - Gradient Weights Max: 0.0074131080290343945 Min: -0.010736022088575298\n","ReLU Activation - Max: 2.139847686328403 Min: 0.0\n","Softmax Output - Max: 0.9609321176530617 Min: 0.039067882346938254 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.391359652945759 Min: 0.0\n","Softmax Output - Max: 0.9768536916718716 Min: 0.023146308328128355 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003303310428332097 Min: -0.003303310428332097\n","Layer 0 - Gradient Weights Max: 0.007391344174785735 Min: -0.01080883824463007\n","ReLU Activation - Max: 2.140644225981862 Min: 0.0\n","Softmax Output - Max: 0.9610273804667231 Min: 0.038972619533277034 Sum (first example): 1.0000000000000002\n","Epoch 680, Loss: 0.5783379407642956, Test Accuracy: 0.7064285714285714\n","ReLU Activation - Max: 2.392573243915592 Min: 0.0\n","Softmax Output - Max: 0.9769021213482286 Min: 0.023097878651771375 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003308206797832468 Min: -0.003308206797832467\n","Layer 0 - Gradient Weights Max: 0.007502080254893712 Min: -0.010780946007992188\n","ReLU Activation - Max: 2.1414376569951545 Min: 0.0\n","Softmax Output - Max: 0.9611221812212658 Min: 0.03887781877873424 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3937823739617543 Min: 0.0\n","Softmax Output - Max: 0.9769503978441855 Min: 0.02304960215581461 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003313089142121815 Min: -0.003313089142121815\n","Layer 0 - Gradient Weights Max: 0.007531096341315291 Min: -0.010716960636688444\n","ReLU Activation - Max: 2.142227715175066 Min: 0.0\n","Softmax Output - Max: 0.9612164554860566 Min: 0.03878354451394347 Sum (first example): 1.0\n","ReLU Activation - Max: 2.3949880150606453 Min: 0.0\n","Softmax Output - Max: 0.9769986662094353 Min: 0.023001333790564682 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003317888575011973 Min: -0.003317888575011974\n","Layer 0 - Gradient Weights Max: 0.0076137808089954675 Min: -0.010643307334273665\n","ReLU Activation - Max: 2.143015261642838 Min: 0.0\n","Softmax Output - Max: 0.9613105585358696 Min: 0.038689441464130565 Sum (first example): 1.0\n","ReLU Activation - Max: 2.396186904339337 Min: 0.0\n","Softmax Output - Max: 0.9770468067540846 Min: 0.022953193245915392 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003322629920067207 Min: -0.003322629920067207\n","Layer 0 - Gradient Weights Max: 0.007711948023315617 Min: -0.010741804863194931\n","ReLU Activation - Max: 2.143799910093662 Min: 0.0\n","Softmax Output - Max: 0.9614042692299615 Min: 0.03859573077003855 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.3973854324630257 Min: 0.0\n","Softmax Output - Max: 0.9771047169334645 Min: 0.022895283066535578 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033273468831293185 Min: -0.003327346883129319\n","Layer 0 - Gradient Weights Max: 0.007829868731598175 Min: -0.010580184477830622\n","ReLU Activation - Max: 2.1445845841522564 Min: 0.0\n","Softmax Output - Max: 0.9614973666266072 Min: 0.03850263337339269 Sum (first example): 1.0\n","ReLU Activation - Max: 2.398582574939871 Min: 0.0\n","Softmax Output - Max: 0.9771707695173781 Min: 0.022829230482621858 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033320208779657363 Min: -0.0033320208779657368\n","Layer 0 - Gradient Weights Max: 0.007916063441524753 Min: -0.01059588986808996\n","ReLU Activation - Max: 2.1453563925572214 Min: 0.0\n","Softmax Output - Max: 0.9615897526949753 Min: 0.0384102473050248 Sum (first example): 1.0\n","ReLU Activation - Max: 2.399789407317196 Min: 0.0\n","Softmax Output - Max: 0.9772364724782486 Min: 0.02276352752175133 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003336688253585365 Min: -0.003336688253585365\n","Layer 0 - Gradient Weights Max: 0.007934570962273407 Min: -0.010626324706540433\n","ReLU Activation - Max: 2.146122902085573 Min: 0.0\n","Softmax Output - Max: 0.9616814398955524 Min: 0.03831856010444762 Sum (first example): 1.0\n","ReLU Activation - Max: 2.400999581791228 Min: 0.0\n","Softmax Output - Max: 0.9773020739045716 Min: 0.022697926095428415 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033413334766739036 Min: -0.003341333476673903\n","Layer 0 - Gradient Weights Max: 0.007912655541988159 Min: -0.010703250950632745\n","ReLU Activation - Max: 2.1469039185936585 Min: 0.0\n","Softmax Output - Max: 0.9617733046644682 Min: 0.03822669533553186 Sum (first example): 1.0\n","ReLU Activation - Max: 2.40219139660779 Min: 0.0\n","Softmax Output - Max: 0.977367433014833 Min: 0.022632566985166896 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033477817914537183 Min: -0.0033477817914537196\n","Layer 0 - Gradient Weights Max: 0.00803953758217705 Min: -0.010781289368218424\n","ReLU Activation - Max: 2.1476820246616977 Min: 0.0\n","Softmax Output - Max: 0.9618648272850929 Min: 0.03813517271490702 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4033788786693617 Min: 0.0\n","Softmax Output - Max: 0.9774324328409609 Min: 0.022567567159039154 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033554864504565618 Min: -0.0033554864504565626\n","Layer 0 - Gradient Weights Max: 0.008017799809512463 Min: -0.010663281379817622\n","ReLU Activation - Max: 2.1484698199242187 Min: 0.0\n","Softmax Output - Max: 0.9619562818334152 Min: 0.0380437181665848 Sum (first example): 1.0\n","Epoch 690, Loss: 0.5777559623505178, Test Accuracy: 0.7057142857142857\n","ReLU Activation - Max: 2.40455644857791 Min: 0.0\n","Softmax Output - Max: 0.9774971275842279 Min: 0.022502872415772198 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003362911405491895 Min: -0.0033629114054918936\n","Layer 0 - Gradient Weights Max: 0.00813934250428516 Min: -0.01066252310289384\n","ReLU Activation - Max: 2.1492433939843334 Min: 0.0\n","Softmax Output - Max: 0.9620468117196475 Min: 0.03795318828035247 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.405718782894866 Min: 0.0\n","Softmax Output - Max: 0.977561723661655 Min: 0.02243827633834515 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033706282458583975 Min: -0.0033706282458583975\n","Layer 0 - Gradient Weights Max: 0.008220687490735469 Min: -0.010759162591013343\n","ReLU Activation - Max: 2.1500197124468405 Min: 0.0\n","Softmax Output - Max: 0.96213708712571 Min: 0.037862912874289965 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4068678899734515 Min: 0.0\n","Softmax Output - Max: 0.9776257325886339 Min: 0.02237426741136606 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003378763503438386 Min: -0.003378763503438385\n","Layer 0 - Gradient Weights Max: 0.008231404960428991 Min: -0.01064842318563923\n","ReLU Activation - Max: 2.1507925257037326 Min: 0.0\n","Softmax Output - Max: 0.9622270367613767 Min: 0.03777296323862334 Sum (first example): 1.0\n","ReLU Activation - Max: 2.408012419600374 Min: 0.0\n","Softmax Output - Max: 0.9776898934399793 Min: 0.022310106560020866 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0033866463223593906 Min: -0.0033866463223593906\n","Layer 0 - Gradient Weights Max: 0.008258301064063505 Min: -0.01070443718985056\n","ReLU Activation - Max: 2.1515635024533957 Min: 0.0\n","Softmax Output - Max: 0.9623166240446518 Min: 0.037683375955348146 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4091632125525617 Min: 0.0\n","Softmax Output - Max: 0.9777536214023931 Min: 0.022246378597606857 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003394568944896024 Min: -0.0033945689448960227\n","Layer 0 - Gradient Weights Max: 0.008293054678391715 Min: -0.010707962119021803\n","ReLU Activation - Max: 2.1523340040242878 Min: 0.0\n","Softmax Output - Max: 0.9624059428146592 Min: 0.03759405718534089 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4103120879332995 Min: 0.0\n","Softmax Output - Max: 0.9778171004985873 Min: 0.02218289950141275 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003402477531942581 Min: -0.00340247753194258\n","Layer 0 - Gradient Weights Max: 0.008416329230401138 Min: -0.010669342340631223\n","ReLU Activation - Max: 2.1531017910740795 Min: 0.0\n","Softmax Output - Max: 0.9624948752178948 Min: 0.03750512478210521 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4114704515060468 Min: 0.0\n","Softmax Output - Max: 0.9778801957044317 Min: 0.02211980429556835 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034104611931023918 Min: -0.0034104611931023918\n","Layer 0 - Gradient Weights Max: 0.008419581946559772 Min: -0.010676650186762416\n","ReLU Activation - Max: 2.1538634824409915 Min: 0.0\n","Softmax Output - Max: 0.9625832575392557 Min: 0.037416742460744404 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4126429245735204 Min: 0.0\n","Softmax Output - Max: 0.9779431513183505 Min: 0.022056848681649435 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003418398089445206 Min: -0.0034183980894452056\n","Layer 0 - Gradient Weights Max: 0.008449294601485809 Min: -0.010702546845494135\n","ReLU Activation - Max: 2.1546236546433524 Min: 0.0\n","Softmax Output - Max: 0.9626713156668563 Min: 0.03732868433314363 Sum (first example): 1.0\n","ReLU Activation - Max: 2.413819424142786 Min: 0.0\n","Softmax Output - Max: 0.9780056497236606 Min: 0.02199435027633931 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003426280460969661 Min: -0.003426280460969663\n","Layer 0 - Gradient Weights Max: 0.008468817013470242 Min: -0.01072492394071083\n","ReLU Activation - Max: 2.1553807493818913 Min: 0.0\n","Softmax Output - Max: 0.9627576019557015 Min: 0.037242398044298505 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4149745936897062 Min: 0.0\n","Softmax Output - Max: 0.9780679397742144 Min: 0.021932060225785626 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034341357456357763 Min: -0.0034341357456357763\n","Layer 0 - Gradient Weights Max: 0.008561055308935049 Min: -0.010769299503450553\n","ReLU Activation - Max: 2.156116079001767 Min: 0.0\n","Softmax Output - Max: 0.9628429977267402 Min: 0.03715700227325975 Sum (first example): 1.0\n","Epoch 700, Loss: 0.577157507315305, Test Accuracy: 0.7075\n","ReLU Activation - Max: 2.416136785890468 Min: 0.0\n","Softmax Output - Max: 0.9781297542634353 Min: 0.02187024573656477 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0034421956372896875 Min: -0.0034421956372896875\n","Layer 0 - Gradient Weights Max: 0.008571218079282126 Min: -0.010781425672325725\n","ReLU Activation - Max: 2.15683818852248 Min: 0.0\n","Softmax Output - Max: 0.9629277334356033 Min: 0.03707226656439664 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.417304578515598 Min: 0.0\n","Softmax Output - Max: 0.9781912550049765 Min: 0.02180874499502338 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034501241636870164 Min: -0.0034501241636870172\n","Layer 0 - Gradient Weights Max: 0.008681567999366306 Min: -0.010871229973749643\n","ReLU Activation - Max: 2.157559745458145 Min: 0.0\n","Softmax Output - Max: 0.9630121468108447 Min: 0.036987853189155324 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4184660442483454 Min: 0.0\n","Softmax Output - Max: 0.9782523294735108 Min: 0.02174767052648908 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034580935170494586 Min: -0.003458093517049459\n","Layer 0 - Gradient Weights Max: 0.008672339545603288 Min: -0.010865970903407102\n","ReLU Activation - Max: 2.1582821523478986 Min: 0.0\n","Softmax Output - Max: 0.9630962764360245 Min: 0.03690372356397546 Sum (first example): 1.0\n","ReLU Activation - Max: 2.419619705238473 Min: 0.0\n","Softmax Output - Max: 0.978313017901018 Min: 0.02168698209898196 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003465953646423961 Min: -0.003465953646423961\n","Layer 0 - Gradient Weights Max: 0.008727174702017108 Min: -0.010913091667221412\n","ReLU Activation - Max: 2.159005275137686 Min: 0.0\n","Softmax Output - Max: 0.9631800356289081 Min: 0.03681996437109198 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.42075532745369 Min: 0.0\n","Softmax Output - Max: 0.9783730857917992 Min: 0.021626914208200895 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003473906706337341 Min: -0.0034739067063373407\n","Layer 0 - Gradient Weights Max: 0.008791331534282134 Min: -0.010849313028745679\n","ReLU Activation - Max: 2.1597296960075414 Min: 0.0\n","Softmax Output - Max: 0.9632636731455964 Min: 0.03673632685440361 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4218804519399093 Min: 0.0\n","Softmax Output - Max: 0.9784329876510286 Min: 0.021567012348971522 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003481674397864076 Min: -0.003481674397864076\n","Layer 0 - Gradient Weights Max: 0.008897985747847385 Min: -0.010783374380545276\n","ReLU Activation - Max: 2.16044924585104 Min: 0.0\n","Softmax Output - Max: 0.9633469840590198 Min: 0.03665301594098031 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.4229962910960667 Min: 0.0\n","Softmax Output - Max: 0.9784927481911845 Min: 0.021507251808815422 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0034893857946783394 Min: -0.003489385794678339\n","Layer 0 - Gradient Weights Max: 0.008817724897873457 Min: -0.010716457087791124\n","ReLU Activation - Max: 2.161166406387539 Min: 0.0\n","Softmax Output - Max: 0.9634298864695945 Min: 0.036570113530405485 Sum (first example): 1.0\n","ReLU Activation - Max: 2.424119279186537 Min: 0.0\n","Softmax Output - Max: 0.9785522495146142 Min: 0.02144775048538583 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003497028748078259 Min: -0.003497028748078259\n","Layer 0 - Gradient Weights Max: 0.008750576962744477 Min: -0.010793785599336568\n","ReLU Activation - Max: 2.1618738568826426 Min: 0.0\n","Softmax Output - Max: 0.9635122110616902 Min: 0.03648778893830988 Sum (first example): 1.0\n","ReLU Activation - Max: 2.425234348069419 Min: 0.0\n","Softmax Output - Max: 0.9786118002797598 Min: 0.021388199720240184 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0035048108737867206 Min: -0.0035048108737867206\n","Layer 0 - Gradient Weights Max: 0.008770393407711805 Min: -0.010837417358139265\n","ReLU Activation - Max: 2.1625853335985052 Min: 0.0\n","Softmax Output - Max: 0.9635945322300756 Min: 0.036405467769924316 Sum (first example): 1.0\n","ReLU Activation - Max: 2.426341410632589 Min: 0.0\n","Softmax Output - Max: 0.9786712573416592 Min: 0.021328742658340852 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0035125178373727377 Min: -0.0035125178373727364\n","Layer 0 - Gradient Weights Max: 0.008848505633068637 Min: -0.010893312933534413\n","ReLU Activation - Max: 2.1632960402164714 Min: 0.0\n","Softmax Output - Max: 0.9636764968999547 Min: 0.03632350310004533 Sum (first example): 1.0\n","Epoch 710, Loss: 0.5765488971309347, Test Accuracy: 0.7089285714285715\n","ReLU Activation - Max: 2.4274429428707025 Min: 0.0\n","Softmax Output - Max: 0.9787304980948274 Min: 0.021269501905172593 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003520502839535639 Min: -0.0035205028395356376\n","Layer 0 - Gradient Weights Max: 0.008826449172532736 Min: -0.010938634280359216\n","ReLU Activation - Max: 2.1640045713054437 Min: 0.0\n","Softmax Output - Max: 0.9637582376594952 Min: 0.036241762340504864 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4285409713885224 Min: 0.0\n","Softmax Output - Max: 0.9787899683366227 Min: 0.02121003166337724 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003528605249228368 Min: -0.0035286052492283686\n","Layer 0 - Gradient Weights Max: 0.008803339843231098 Min: -0.010946108678435587\n","ReLU Activation - Max: 2.1647247813825348 Min: 0.0\n","Softmax Output - Max: 0.9638401266001377 Min: 0.03615987339986237 Sum (first example): 1.0\n","ReLU Activation - Max: 2.429614447715625 Min: 0.0\n","Softmax Output - Max: 0.978849164867004 Min: 0.02115083513299592 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0035367716074784492 Min: -0.0035367716074784492\n","Layer 0 - Gradient Weights Max: 0.008819600128146962 Min: -0.010956293958877013\n","ReLU Activation - Max: 2.1654464390158137 Min: 0.0\n","Softmax Output - Max: 0.9639219335053011 Min: 0.036078066494698906 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.430689147388698 Min: 0.0\n","Softmax Output - Max: 0.9789078515924032 Min: 0.021092148407596652 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0035448169982480898 Min: -0.003544816998248089\n","Layer 0 - Gradient Weights Max: 0.008886889311997407 Min: -0.010935147327192168\n","ReLU Activation - Max: 2.166166465518211 Min: 0.0\n","Softmax Output - Max: 0.9640033642368142 Min: 0.03599663576318574 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.431755191902711 Min: 0.0\n","Softmax Output - Max: 0.9789659958555991 Min: 0.021034004144401045 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003552502833119585 Min: -0.0035525028331195863\n","Layer 0 - Gradient Weights Max: 0.008907599069094621 Min: -0.010855321694084293\n","ReLU Activation - Max: 2.166887680413221 Min: 0.0\n","Softmax Output - Max: 0.9640845188996751 Min: 0.035915481100324824 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4328163567733427 Min: 0.0\n","Softmax Output - Max: 0.979023960480367 Min: 0.020976039519633106 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003560087863315155 Min: -0.003560087863315155\n","Layer 0 - Gradient Weights Max: 0.00888194184817006 Min: -0.010871009942075849\n","ReLU Activation - Max: 2.167595700427713 Min: 0.0\n","Softmax Output - Max: 0.9641650020108606 Min: 0.03583499798913944 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4338834321683165 Min: 0.0\n","Softmax Output - Max: 0.9790814052040632 Min: 0.020918594795936808 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0035679684378804952 Min: -0.0035679684378804952\n","Layer 0 - Gradient Weights Max: 0.008860928450722772 Min: -0.010896624763095887\n","ReLU Activation - Max: 2.1683014307274453 Min: 0.0\n","Softmax Output - Max: 0.9642449479363474 Min: 0.03575505206365261 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4349469337699263 Min: 0.0\n","Softmax Output - Max: 0.9791389371426847 Min: 0.020861062857315173 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00357567947120606 Min: -0.00357567947120606\n","Layer 0 - Gradient Weights Max: 0.00886272381072534 Min: -0.010896174675414984\n","ReLU Activation - Max: 2.1690020103556327 Min: 0.0\n","Softmax Output - Max: 0.9643244016657301 Min: 0.035675598334269906 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.436005782102549 Min: 0.0\n","Softmax Output - Max: 0.9791964914708311 Min: 0.02080350852916887 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003583492995192539 Min: -0.003583492995192539\n","Layer 0 - Gradient Weights Max: 0.008853689392736501 Min: -0.010841491637096422\n","ReLU Activation - Max: 2.1697003653484144 Min: 0.0\n","Softmax Output - Max: 0.9644035121532508 Min: 0.03559648784674921 Sum (first example): 1.0\n","ReLU Activation - Max: 2.437061083323359 Min: 0.0\n","Softmax Output - Max: 0.9792536714523328 Min: 0.020746328547667153 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003591072046482484 Min: -0.0035910720464824852\n","Layer 0 - Gradient Weights Max: 0.008987399543820072 Min: -0.010856701781899116\n","ReLU Activation - Max: 2.170400907696627 Min: 0.0\n","Softmax Output - Max: 0.9644825012498685 Min: 0.03551749875013142 Sum (first example): 1.0\n","Epoch 720, Loss: 0.5759335123549328, Test Accuracy: 0.7096428571428571\n","ReLU Activation - Max: 2.438108374624046 Min: 0.0\n","Softmax Output - Max: 0.9793104206374202 Min: 0.020689579362579733 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0035986114460460814 Min: -0.0035986114460460814\n","Layer 0 - Gradient Weights Max: 0.009009756507624657 Min: -0.010867210085692818\n","ReLU Activation - Max: 2.1710991030153695 Min: 0.0\n","Softmax Output - Max: 0.9645608689484222 Min: 0.03543913105157776 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4391520438723893 Min: 0.0\n","Softmax Output - Max: 0.9793666721924338 Min: 0.020633327807566354 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036060641176523926 Min: -0.0036060641176523926\n","Layer 0 - Gradient Weights Max: 0.009092725972802767 Min: -0.010874837307363251\n","ReLU Activation - Max: 2.171789805149042 Min: 0.0\n","Softmax Output - Max: 0.9646387791617268 Min: 0.03536122083827321 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4401824188747896 Min: 0.0\n","Softmax Output - Max: 0.979422541411941 Min: 0.020577458588059087 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036137510034437055 Min: -0.003613751003443705\n","Layer 0 - Gradient Weights Max: 0.009222188188065451 Min: -0.010902092036053756\n","ReLU Activation - Max: 2.1724868595576794 Min: 0.0\n","Softmax Output - Max: 0.9647166922400635 Min: 0.035283307759936516 Sum (first example): 1.0\n","ReLU Activation - Max: 2.441203305199989 Min: 0.0\n","Softmax Output - Max: 0.9794781267854803 Min: 0.020521873214519745 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036216547263985565 Min: -0.0036216547263985556\n","Layer 0 - Gradient Weights Max: 0.009184646633005764 Min: -0.010931916348738272\n","ReLU Activation - Max: 2.173177345681019 Min: 0.0\n","Softmax Output - Max: 0.9647942537625691 Min: 0.035205746237430766 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.4422052520752535 Min: 0.0\n","Softmax Output - Max: 0.9795336929896155 Min: 0.020466307010384665 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036296887323991533 Min: -0.0036296887323991546\n","Layer 0 - Gradient Weights Max: 0.0091722480155107 Min: -0.010956202213220344\n","ReLU Activation - Max: 2.173865619884644 Min: 0.0\n","Softmax Output - Max: 0.964871585534332 Min: 0.035128414465667855 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4432037123979713 Min: 0.0\n","Softmax Output - Max: 0.9795889181766511 Min: 0.02041108182334896 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036376483066621473 Min: -0.0036376483066621473\n","Layer 0 - Gradient Weights Max: 0.00917606296604869 Min: -0.010941720933762004\n","ReLU Activation - Max: 2.1745599866745096 Min: 0.0\n","Softmax Output - Max: 0.9649488373773047 Min: 0.03505116262269539 Sum (first example): 1.0\n","ReLU Activation - Max: 2.444192377160075 Min: 0.0\n","Softmax Output - Max: 0.9796439071690697 Min: 0.020356092830930386 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036454755153214357 Min: -0.003645475515321435\n","Layer 0 - Gradient Weights Max: 0.009234320948375545 Min: -0.011016806768508918\n","ReLU Activation - Max: 2.175245397271704 Min: 0.0\n","Softmax Output - Max: 0.9650257180553458 Min: 0.034974281944654316 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4451882913801084 Min: 0.0\n","Softmax Output - Max: 0.9796983075965603 Min: 0.02030169240343964 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003653546889260751 Min: -0.0036535468892607524\n","Layer 0 - Gradient Weights Max: 0.009277195691134018 Min: -0.011029714015739902\n","ReLU Activation - Max: 2.175927875179414 Min: 0.0\n","Softmax Output - Max: 0.9651023393995071 Min: 0.034897660600492916 Sum (first example): 1.0\n","ReLU Activation - Max: 2.446155972942333 Min: 0.0\n","Softmax Output - Max: 0.9797524688811027 Min: 0.020247531118897367 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003661942273761702 Min: -0.0036619422737617013\n","Layer 0 - Gradient Weights Max: 0.009259660859482724 Min: -0.011070209359914101\n","ReLU Activation - Max: 2.1765970121536107 Min: 0.0\n","Softmax Output - Max: 0.965178449082059 Min: 0.03482155091794092 Sum (first example): 1.0\n","ReLU Activation - Max: 2.44711054600472 Min: 0.0\n","Softmax Output - Max: 0.9798062383065372 Min: 0.020193761693462795 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036704092590462414 Min: -0.0036704092590462423\n","Layer 0 - Gradient Weights Max: 0.009330552888472503 Min: -0.01112283509250647\n","ReLU Activation - Max: 2.1772652731598177 Min: 0.0\n","Softmax Output - Max: 0.96525437885918 Min: 0.03474562114081994 Sum (first example): 1.0\n","Epoch 730, Loss: 0.5753083919455402, Test Accuracy: 0.7121428571428572\n","ReLU Activation - Max: 2.4480487631960277 Min: 0.0\n","Softmax Output - Max: 0.9798595569984521 Min: 0.02014044300154787 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003678996234313586 Min: -0.003678996234313585\n","Layer 0 - Gradient Weights Max: 0.009348195773790195 Min: -0.011126525117528588\n","ReLU Activation - Max: 2.1779315341696672 Min: 0.0\n","Softmax Output - Max: 0.9653300260166007 Min: 0.03466997398339942 Sum (first example): 1.0\n","ReLU Activation - Max: 2.448983998260823 Min: 0.0\n","Softmax Output - Max: 0.9799128667333269 Min: 0.02008713326667303 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003687537552729047 Min: -0.003687537552729047\n","Layer 0 - Gradient Weights Max: 0.009308724611588388 Min: -0.011077236044156311\n","ReLU Activation - Max: 2.178598020163838 Min: 0.0\n","Softmax Output - Max: 0.9654055196442186 Min: 0.034594480355781436 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4499092503625217 Min: 0.0\n","Softmax Output - Max: 0.9799661278787442 Min: 0.020033872121255902 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0036959444635167316 Min: -0.0036959444635167316\n","Layer 0 - Gradient Weights Max: 0.009261568668808383 Min: -0.011093704784250698\n","ReLU Activation - Max: 2.1792656069020726 Min: 0.0\n","Softmax Output - Max: 0.965480763499116 Min: 0.03451923650088406 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4508289706410524 Min: 0.0\n","Softmax Output - Max: 0.9800191113348656 Min: 0.019980888665134335 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037044888488915566 Min: -0.003704488848891557\n","Layer 0 - Gradient Weights Max: 0.009300116519309056 Min: -0.011109136138028677\n","ReLU Activation - Max: 2.1799269294202506 Min: 0.0\n","Softmax Output - Max: 0.9655558342942429 Min: 0.03444416570575721 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.451746481128322 Min: 0.0\n","Softmax Output - Max: 0.9800719296895887 Min: 0.019928070310411324 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0037128424263686327 Min: -0.0037128424263686327\n","Layer 0 - Gradient Weights Max: 0.009161273034614866 Min: -0.01103032652999337\n","ReLU Activation - Max: 2.1805846933665984 Min: 0.0\n","Softmax Output - Max: 0.9656307677321049 Min: 0.03436923226789503 Sum (first example): 1.0\n","ReLU Activation - Max: 2.452648909924842 Min: 0.0\n","Softmax Output - Max: 0.9801248923552482 Min: 0.019875107644751755 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037208496645958064 Min: -0.0037208496645958064\n","Layer 0 - Gradient Weights Max: 0.00926213250545856 Min: -0.01108203562757689\n","ReLU Activation - Max: 2.181244110267711 Min: 0.0\n","Softmax Output - Max: 0.9657055671038566 Min: 0.03429443289614337 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.453537871327051 Min: 0.0\n","Softmax Output - Max: 0.9801771602117816 Min: 0.019822839788218433 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003728967630724936 Min: -0.0037289676307249347\n","Layer 0 - Gradient Weights Max: 0.009234403843629885 Min: -0.011102984572876009\n","ReLU Activation - Max: 2.1818969601454437 Min: 0.0\n","Softmax Output - Max: 0.9657799891947109 Min: 0.034220010805289164 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4544172102138293 Min: 0.0\n","Softmax Output - Max: 0.9802290026007766 Min: 0.019770997399223323 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003737156813534391 Min: -0.0037371568135343924\n","Layer 0 - Gradient Weights Max: 0.00925551981805662 Min: -0.011115419201650923\n","ReLU Activation - Max: 2.1825656434771528 Min: 0.0\n","Softmax Output - Max: 0.9658547037746517 Min: 0.034145296225348336 Sum (first example): 1.0\n","ReLU Activation - Max: 2.455272246768482 Min: 0.0\n","Softmax Output - Max: 0.9802805311758869 Min: 0.019719468824113087 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037454847281989533 Min: -0.0037454847281989537\n","Layer 0 - Gradient Weights Max: 0.009278486990901535 Min: -0.011154684441600381\n","ReLU Activation - Max: 2.1832319183083095 Min: 0.0\n","Softmax Output - Max: 0.9659292066856234 Min: 0.0340707933143767 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.456149156845348 Min: 0.0\n","Softmax Output - Max: 0.9803313844965873 Min: 0.019668615503412593 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037538122861987253 Min: -0.0037538122861987253\n","Layer 0 - Gradient Weights Max: 0.009230118284603845 Min: -0.011198900058476818\n","ReLU Activation - Max: 2.1838968536835157 Min: 0.0\n","Softmax Output - Max: 0.9660035519471862 Min: 0.03399644805281376 Sum (first example): 1.0\n","Epoch 740, Loss: 0.5746715755222532, Test Accuracy: 0.7125\n","ReLU Activation - Max: 2.4570076486471617 Min: 0.0\n","Softmax Output - Max: 0.9803818852089686 Min: 0.019618114791031406 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037620540661212044 Min: -0.003762054066121205\n","Layer 0 - Gradient Weights Max: 0.009231686901788052 Min: -0.011189676694721694\n","ReLU Activation - Max: 2.1845628670898876 Min: 0.0\n","Softmax Output - Max: 0.9660776569587413 Min: 0.03392234304125878 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4578652667087435 Min: 0.0\n","Softmax Output - Max: 0.9804321307212527 Min: 0.019567869278747346 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037701679104510197 Min: -0.0037701679104510197\n","Layer 0 - Gradient Weights Max: 0.009294700096064768 Min: -0.011183668261339085\n","ReLU Activation - Max: 2.185268066970066 Min: 0.0\n","Softmax Output - Max: 0.9661526333716774 Min: 0.03384736662832252 Sum (first example): 1.0\n","ReLU Activation - Max: 2.458698350441849 Min: 0.0\n","Softmax Output - Max: 0.9804820072435226 Min: 0.01951799275647738 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.003778246926522187 Min: -0.003778246926522187\n","Layer 0 - Gradient Weights Max: 0.009268184466275824 Min: -0.011176395005128816\n","ReLU Activation - Max: 2.185982318196922 Min: 0.0\n","Softmax Output - Max: 0.9662275159732553 Min: 0.03377248402674476 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.459528424730219 Min: 0.0\n","Softmax Output - Max: 0.980531805950689 Min: 0.019468194049310994 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037863448648915946 Min: -0.0037863448648915933\n","Layer 0 - Gradient Weights Max: 0.009184977573068435 Min: -0.011207985877534254\n","ReLU Activation - Max: 2.1866929257914607 Min: 0.0\n","Softmax Output - Max: 0.9663021229919464 Min: 0.03369787700805361 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4603552076977877 Min: 0.0\n","Softmax Output - Max: 0.9805813001539443 Min: 0.01941869984605572 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0037944400296941235 Min: -0.0037944400296941235\n","Layer 0 - Gradient Weights Max: 0.009181024955874504 Min: -0.01119569830072181\n","ReLU Activation - Max: 2.1874024944450055 Min: 0.0\n","Softmax Output - Max: 0.9663766105447498 Min: 0.03362338945525022 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4611720511583686 Min: 0.0\n","Softmax Output - Max: 0.9806306964164399 Min: 0.019369303583560118 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00380254411478569 Min: -0.0038025441147856893\n","Layer 0 - Gradient Weights Max: 0.009137925567055576 Min: -0.011251558469059075\n","ReLU Activation - Max: 2.1881163569553905 Min: 0.0\n","Softmax Output - Max: 0.9664510101601864 Min: 0.03354898983981351 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4619794423151165 Min: 0.0\n","Softmax Output - Max: 0.9806800805565363 Min: 0.019319919443463635 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003810876266186539 Min: -0.003810876266186539\n","Layer 0 - Gradient Weights Max: 0.009122085038446092 Min: -0.011237198672231509\n","ReLU Activation - Max: 2.1888190469017657 Min: 0.0\n","Softmax Output - Max: 0.9665249805920069 Min: 0.03347501940799318 Sum (first example): 1.0\n","ReLU Activation - Max: 2.462797264684334 Min: 0.0\n","Softmax Output - Max: 0.9807296903038698 Min: 0.019270309696130136 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0038192110648699097 Min: -0.00381921106486991\n","Layer 0 - Gradient Weights Max: 0.009179309047787307 Min: -0.01136930398868383\n","ReLU Activation - Max: 2.189506430827302 Min: 0.0\n","Softmax Output - Max: 0.9665984365358959 Min: 0.03340156346410413 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.463606773057795 Min: 0.0\n","Softmax Output - Max: 0.9807791141770659 Min: 0.01922088582293412 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038280126589701523 Min: -0.0038280126589701523\n","Layer 0 - Gradient Weights Max: 0.009128162054909928 Min: -0.011431889479304496\n","ReLU Activation - Max: 2.1901916908384065 Min: 0.0\n","Softmax Output - Max: 0.9666715690134304 Min: 0.033328430986569506 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.464413958291353 Min: 0.0\n","Softmax Output - Max: 0.9808283880202089 Min: 0.019171611979791225 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038369351411263455 Min: -0.003836935141126345\n","Layer 0 - Gradient Weights Max: 0.009140709309168948 Min: -0.01146103266731\n","ReLU Activation - Max: 2.1908731530244574 Min: 0.0\n","Softmax Output - Max: 0.9667443953691339 Min: 0.03325560463086612 Sum (first example): 1.0\n","Epoch 750, Loss: 0.5740214151515338, Test Accuracy: 0.7128571428571429\n","ReLU Activation - Max: 2.465223634863624 Min: 0.0\n","Softmax Output - Max: 0.9808776046626939 Min: 0.01912239533730607 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038461297826021066 Min: -0.003846129782602108\n","Layer 0 - Gradient Weights Max: 0.009074103424415092 Min: -0.011552055347424527\n","ReLU Activation - Max: 2.191557620001008 Min: 0.0\n","Softmax Output - Max: 0.9668169995400274 Min: 0.03318300045997256 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4660292579877074 Min: 0.0\n","Softmax Output - Max: 0.9809267124522535 Min: 0.019073287547746468 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038554100575837386 Min: -0.003855410057583738\n","Layer 0 - Gradient Weights Max: 0.00906990490064302 Min: -0.01148668563999235\n","ReLU Activation - Max: 2.1922312413194467 Min: 0.0\n","Softmax Output - Max: 0.9668891662051367 Min: 0.03311083379486336 Sum (first example): 1.0\n","ReLU Activation - Max: 2.466855783907099 Min: 0.0\n","Softmax Output - Max: 0.98097560070204 Min: 0.019024399297959897 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038645368421795613 Min: -0.003864536842179561\n","Layer 0 - Gradient Weights Max: 0.00910884798802756 Min: -0.0114524601953516\n","ReLU Activation - Max: 2.1928953582853317 Min: 0.0\n","Softmax Output - Max: 0.9669607141513193 Min: 0.033039285848680516 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4676866664573653 Min: 0.0\n","Softmax Output - Max: 0.9810242747821004 Min: 0.018975725217899597 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003873660295546085 Min: -0.0038736602955460843\n","Layer 0 - Gradient Weights Max: 0.00916283266487956 Min: -0.011532766862828605\n","ReLU Activation - Max: 2.1935502764309978 Min: 0.0\n","Softmax Output - Max: 0.967031769988691 Min: 0.03296823001130887 Sum (first example): 1.0\n","ReLU Activation - Max: 2.468530541077179 Min: 0.0\n","Softmax Output - Max: 0.9810723118591351 Min: 0.018927688140864964 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038832330733540223 Min: -0.0038832330733540223\n","Layer 0 - Gradient Weights Max: 0.009176691302163812 Min: -0.011629932508740585\n","ReLU Activation - Max: 2.1942031686251395 Min: 0.0\n","Softmax Output - Max: 0.9671026390155928 Min: 0.03289736098440709 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4693718288647646 Min: 0.0\n","Softmax Output - Max: 0.9811199619442953 Min: 0.018880038055704634 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0038928046245364045 Min: -0.003892804624536403\n","Layer 0 - Gradient Weights Max: 0.009195566684978031 Min: -0.011617860279243058\n","ReLU Activation - Max: 2.194852640781018 Min: 0.0\n","Softmax Output - Max: 0.967173401685099 Min: 0.032826598314901 Sum (first example): 1.0\n","ReLU Activation - Max: 2.470193546244596 Min: 0.0\n","Softmax Output - Max: 0.9811677098716377 Min: 0.018832290128362277 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003902183049424013 Min: -0.003902183049424013\n","Layer 0 - Gradient Weights Max: 0.009231987369356688 Min: -0.011574682714272226\n","ReLU Activation - Max: 2.1954804086525623 Min: 0.0\n","Softmax Output - Max: 0.9672434103960383 Min: 0.032756589603961585 Sum (first example): 1.0\n","ReLU Activation - Max: 2.471023708061623 Min: 0.0\n","Softmax Output - Max: 0.9812152011318851 Min: 0.018784798868114908 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003911604480876672 Min: -0.0039116044808766735\n","Layer 0 - Gradient Weights Max: 0.009235333177665702 Min: -0.011600252718081523\n","ReLU Activation - Max: 2.196103718600853 Min: 0.0\n","Softmax Output - Max: 0.9673130616286907 Min: 0.032686938371309475 Sum (first example): 1.0\n","ReLU Activation - Max: 2.471849012011618 Min: 0.0\n","Softmax Output - Max: 0.9812627264603966 Min: 0.018737273539603434 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003921060113161047 Min: -0.003921060113161045\n","Layer 0 - Gradient Weights Max: 0.009289097653865772 Min: -0.011652884699840793\n","ReLU Activation - Max: 2.196725192344327 Min: 0.0\n","Softmax Output - Max: 0.9673825072010769 Min: 0.03261749279892298 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4726717113638284 Min: 0.0\n","Softmax Output - Max: 0.9813097080874347 Min: 0.018690291912565174 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003930667493424564 Min: -0.003930667493424566\n","Layer 0 - Gradient Weights Max: 0.009343333728283528 Min: -0.011622822786000839\n","ReLU Activation - Max: 2.1973360645135234 Min: 0.0\n","Softmax Output - Max: 0.9674516851424769 Min: 0.03254831485752305 Sum (first example): 1.0\n","Epoch 760, Loss: 0.5733572185569316, Test Accuracy: 0.7128571428571429\n","ReLU Activation - Max: 2.473510162574049 Min: 0.0\n","Softmax Output - Max: 0.9813566244298659 Min: 0.01864337557013413 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003940027993456356 Min: -0.003940027993456356\n","Layer 0 - Gradient Weights Max: 0.009287365853454748 Min: -0.011597705810837616\n","ReLU Activation - Max: 2.197950433862362 Min: 0.0\n","Softmax Output - Max: 0.9675207469216079 Min: 0.032479253078391994 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4743382473263837 Min: 0.0\n","Softmax Output - Max: 0.9814036925434568 Min: 0.018596307456543115 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.00394922373492683 Min: -0.003949223734926829\n","Layer 0 - Gradient Weights Max: 0.009362313357245414 Min: -0.011620892783868369\n","ReLU Activation - Max: 2.1985640801761415 Min: 0.0\n","Softmax Output - Max: 0.967589621828813 Min: 0.03241037817118705 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.475165178427911 Min: 0.0\n","Softmax Output - Max: 0.9814505423167436 Min: 0.018549457683256526 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003958371657422414 Min: -0.003958371657422416\n","Layer 0 - Gradient Weights Max: 0.009216744109367486 Min: -0.01159987222538355\n","ReLU Activation - Max: 2.1991840233534896 Min: 0.0\n","Softmax Output - Max: 0.9676583662183871 Min: 0.03234163378161285 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.47598743009991 Min: 0.0\n","Softmax Output - Max: 0.9814973812228063 Min: 0.018502618777193693 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003967214089861116 Min: -0.003967214089861116\n","Layer 0 - Gradient Weights Max: 0.00921946965044515 Min: -0.011643338237482025\n","ReLU Activation - Max: 2.199792044895817 Min: 0.0\n","Softmax Output - Max: 0.9677267628684069 Min: 0.03227323713159317 Sum (first example): 1.0\n","ReLU Activation - Max: 2.476807826198726 Min: 0.0\n","Softmax Output - Max: 0.9815438269378453 Min: 0.018456173062154784 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003976299052687203 Min: -0.003976299052687204\n","Layer 0 - Gradient Weights Max: 0.009326773922822239 Min: -0.011729359551742728\n","ReLU Activation - Max: 2.2004085429354068 Min: 0.0\n","Softmax Output - Max: 0.9677951555609796 Min: 0.032204844439020414 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4776235852550683 Min: 0.0\n","Softmax Output - Max: 0.9815899237916289 Min: 0.01841007620837111 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003985632014158204 Min: -0.003985632014158208\n","Layer 0 - Gradient Weights Max: 0.00935427572214865 Min: -0.011715374041419844\n","ReLU Activation - Max: 2.2010308229791007 Min: 0.0\n","Softmax Output - Max: 0.9678635904353865 Min: 0.03213640956461342 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.4784282123665644 Min: 0.0\n","Softmax Output - Max: 0.981635649430521 Min: 0.01836435056947914 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.003994771477493137 Min: -0.003994771477493137\n","Layer 0 - Gradient Weights Max: 0.00932988618704236 Min: -0.011680425691114212\n","ReLU Activation - Max: 2.2016546326987743 Min: 0.0\n","Softmax Output - Max: 0.967931918899441 Min: 0.0320680811005591 Sum (first example): 1.0\n","ReLU Activation - Max: 2.479229818483867 Min: 0.0\n","Softmax Output - Max: 0.9816814532460583 Min: 0.01831854675394177 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004003772412879346 Min: -0.004003772412879349\n","Layer 0 - Gradient Weights Max: 0.009402900541019063 Min: -0.011723763452657774\n","ReLU Activation - Max: 2.2022701157057094 Min: 0.0\n","Softmax Output - Max: 0.9679998008153827 Min: 0.03200019918461732 Sum (first example): 1.0\n","ReLU Activation - Max: 2.480027721095147 Min: 0.0\n","Softmax Output - Max: 0.981726946851979 Min: 0.01827305314802096 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004012870308910448 Min: -0.004012870308910449\n","Layer 0 - Gradient Weights Max: 0.009449050707236811 Min: -0.011727847033728193\n","ReLU Activation - Max: 2.2028872800443815 Min: 0.0\n","Softmax Output - Max: 0.9680676287443085 Min: 0.03193237125569153 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4808256352506257 Min: 0.0\n","Softmax Output - Max: 0.9817726018717388 Min: 0.018227398128261272 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004021826951289362 Min: -0.004021826951289362\n","Layer 0 - Gradient Weights Max: 0.009456002069625588 Min: -0.011775325126755618\n","ReLU Activation - Max: 2.203495917240417 Min: 0.0\n","Softmax Output - Max: 0.968135240531705 Min: 0.03186475946829513 Sum (first example): 1.0\n","Epoch 770, Loss: 0.5726845871801048, Test Accuracy: 0.715\n","ReLU Activation - Max: 2.481626476421531 Min: 0.0\n","Softmax Output - Max: 0.9818182094616835 Min: 0.018181790538316513 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004030792623857146 Min: -0.004030792623857146\n","Layer 0 - Gradient Weights Max: 0.009443031805794368 Min: -0.011754656089412853\n","ReLU Activation - Max: 2.2041003137866473 Min: 0.0\n","Softmax Output - Max: 0.9682027167638277 Min: 0.031797283236172307 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4824078284719753 Min: 0.0\n","Softmax Output - Max: 0.9818637773498332 Min: 0.018136222650166754 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004039764969794045 Min: -0.004039764969794045\n","Layer 0 - Gradient Weights Max: 0.009694818955765207 Min: -0.011758492021226885\n","ReLU Activation - Max: 2.204708362491047 Min: 0.0\n","Softmax Output - Max: 0.9682700772256764 Min: 0.03172992277432364 Sum (first example): 1.0\n","ReLU Activation - Max: 2.483180717095869 Min: 0.0\n","Softmax Output - Max: 0.981908966283486 Min: 0.018091033716514132 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004048941220248995 Min: -0.0040489412202489965\n","Layer 0 - Gradient Weights Max: 0.009809700596559195 Min: -0.01180172293983397\n","ReLU Activation - Max: 2.2053160966742893 Min: 0.0\n","Softmax Output - Max: 0.9683372788211483 Min: 0.031662721178851834 Sum (first example): 1.0\n","ReLU Activation - Max: 2.483934986181878 Min: 0.0\n","Softmax Output - Max: 0.9819537030698657 Min: 0.018046296930134226 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00405833936016244 Min: -0.00405833936016244\n","Layer 0 - Gradient Weights Max: 0.00982963874286931 Min: -0.011783186435372317\n","ReLU Activation - Max: 2.205919387047538 Min: 0.0\n","Softmax Output - Max: 0.9684042086855584 Min: 0.03159579131444149 Sum (first example): 1.0\n","ReLU Activation - Max: 2.484696639682486 Min: 0.0\n","Softmax Output - Max: 0.981998143166838 Min: 0.018001856833162022 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004067664841274068 Min: -0.004067664841274068\n","Layer 0 - Gradient Weights Max: 0.009711204857429052 Min: -0.011804109123815219\n","ReLU Activation - Max: 2.206519091507782 Min: 0.0\n","Softmax Output - Max: 0.9684708336885677 Min: 0.03152916631143232 Sum (first example): 1.0\n","ReLU Activation - Max: 2.485445798751281 Min: 0.0\n","Softmax Output - Max: 0.9820425560649477 Min: 0.017957443935052265 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0040770926178729525 Min: -0.0040770926178729525\n","Layer 0 - Gradient Weights Max: 0.009937858569831983 Min: -0.0118310518095163\n","ReLU Activation - Max: 2.2071212569961074 Min: 0.0\n","Softmax Output - Max: 0.9685374151817754 Min: 0.031462584818224705 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4861863154410258 Min: 0.0\n","Softmax Output - Max: 0.9820866159727941 Min: 0.01791338402720601 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004086817655916408 Min: -0.0040868176559164075\n","Layer 0 - Gradient Weights Max: 0.010179374696224876 Min: -0.011806949343639054\n","ReLU Activation - Max: 2.2077193953321435 Min: 0.0\n","Softmax Output - Max: 0.968603760017463 Min: 0.03139623998253703 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4869302264396542 Min: 0.0\n","Softmax Output - Max: 0.9821306508934248 Min: 0.017869349106575286 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004096534593910731 Min: -0.00409653459391073\n","Layer 0 - Gradient Weights Max: 0.010111192376070518 Min: -0.01175206914945515\n","ReLU Activation - Max: 2.2083163750973855 Min: 0.0\n","Softmax Output - Max: 0.9686699892342615 Min: 0.03133001076573841 Sum (first example): 1.0\n","ReLU Activation - Max: 2.48767789389378 Min: 0.0\n","Softmax Output - Max: 0.9821746038388746 Min: 0.017825396161125513 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004106101232523303 Min: -0.004106101232523303\n","Layer 0 - Gradient Weights Max: 0.010083646537705446 Min: -0.011704679336766298\n","ReLU Activation - Max: 2.208911946866516 Min: 0.0\n","Softmax Output - Max: 0.9687360332906089 Min: 0.03126396670939113 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4884236185506223 Min: 0.0\n","Softmax Output - Max: 0.9822185346756527 Min: 0.01778146532434726 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0041154833151845855 Min: -0.0041154833151845855\n","Layer 0 - Gradient Weights Max: 0.010105351290896392 Min: -0.011688306042375791\n","ReLU Activation - Max: 2.2095123398198613 Min: 0.0\n","Softmax Output - Max: 0.9688019418342941 Min: 0.03119805816570588 Sum (first example): 0.9999999999999999\n","Epoch 780, Loss: 0.5719936408966638, Test Accuracy: 0.7160714285714286\n","ReLU Activation - Max: 2.489155020601494 Min: 0.0\n","Softmax Output - Max: 0.9822625792055848 Min: 0.017737420794415346 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004125038284680797 Min: -0.0041250382846807964\n","Layer 0 - Gradient Weights Max: 0.010159816620884832 Min: -0.011794470412311426\n","ReLU Activation - Max: 2.210112992809854 Min: 0.0\n","Softmax Output - Max: 0.9688677113147924 Min: 0.03113228868520764 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.4898815939537413 Min: 0.0\n","Softmax Output - Max: 0.9823066087979784 Min: 0.017693391202021694 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004135028700812659 Min: -0.004135028700812657\n","Layer 0 - Gradient Weights Max: 0.010152066856566569 Min: -0.011749113964263145\n","ReLU Activation - Max: 2.2107089029976326 Min: 0.0\n","Softmax Output - Max: 0.9689333039785295 Min: 0.031066696021470545 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4906070279376538 Min: 0.0\n","Softmax Output - Max: 0.9823507464113644 Min: 0.017649253588635557 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00414527179124234 Min: -0.004145271791242339\n","Layer 0 - Gradient Weights Max: 0.010111208150616566 Min: -0.011814238584672638\n","ReLU Activation - Max: 2.2113050854335095 Min: 0.0\n","Softmax Output - Max: 0.9689988505630606 Min: 0.03100114943693942 Sum (first example): 1.0\n","ReLU Activation - Max: 2.491363807605279 Min: 0.0\n","Softmax Output - Max: 0.982394758062755 Min: 0.017605241937244942 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004155674258110316 Min: -0.004155674258110316\n","Layer 0 - Gradient Weights Max: 0.01009513975607951 Min: -0.011937274324836605\n","ReLU Activation - Max: 2.2118938696374624 Min: 0.0\n","Softmax Output - Max: 0.9690639762696789 Min: 0.030936023730321064 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.4921259602801995 Min: 0.0\n","Softmax Output - Max: 0.9824384973731894 Min: 0.017561502626810416 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004166318223237969 Min: -0.00416631822323797\n","Layer 0 - Gradient Weights Max: 0.01004150440401404 Min: -0.011845157977796315\n","ReLU Activation - Max: 2.2124753659883116 Min: 0.0\n","Softmax Output - Max: 0.9691285951111075 Min: 0.030871404888892513 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.492891106408006 Min: 0.0\n","Softmax Output - Max: 0.9824824837101344 Min: 0.017517516289865507 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004176736273605846 Min: -0.004176736273605846\n","Layer 0 - Gradient Weights Max: 0.009960467130969885 Min: -0.011758979666689484\n","ReLU Activation - Max: 2.213055546627176 Min: 0.0\n","Softmax Output - Max: 0.9691928151100343 Min: 0.030807184889965682 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4936543774430633 Min: 0.0\n","Softmax Output - Max: 0.9825265478036087 Min: 0.017473452196391164 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0041868244402440535 Min: -0.004186824440244053\n","Layer 0 - Gradient Weights Max: 0.009958961466760619 Min: -0.011784340391736212\n","ReLU Activation - Max: 2.2136324719191056 Min: 0.0\n","Softmax Output - Max: 0.9692568627926115 Min: 0.030743137207388496 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.4944258026118424 Min: 0.0\n","Softmax Output - Max: 0.982570009502621 Min: 0.017429990497378936 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004196666945671825 Min: -0.004196666945671827\n","Layer 0 - Gradient Weights Max: 0.009963122387896572 Min: -0.011726834076516176\n","ReLU Activation - Max: 2.214196981335414 Min: 0.0\n","Softmax Output - Max: 0.9693204911124369 Min: 0.030679508887563163 Sum (first example): 1.0\n","ReLU Activation - Max: 2.495200585313166 Min: 0.0\n","Softmax Output - Max: 0.9826134861395844 Min: 0.017386513860415505 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004206524043228163 Min: -0.004206524043228163\n","Layer 0 - Gradient Weights Max: 0.010022427395485957 Min: -0.011745598534681552\n","ReLU Activation - Max: 2.214763421426487 Min: 0.0\n","Softmax Output - Max: 0.9693838405755906 Min: 0.03061615942440949 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4959679950277605 Min: 0.0\n","Softmax Output - Max: 0.9826565792504256 Min: 0.01734342074957434 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004216530070102973 Min: -0.004216530070102973\n","Layer 0 - Gradient Weights Max: 0.009979989697835118 Min: -0.011785848069809196\n","ReLU Activation - Max: 2.2153252657800424 Min: 0.0\n","Softmax Output - Max: 0.9694467961715049 Min: 0.030553203828495107 Sum (first example): 1.0\n","Epoch 790, Loss: 0.5712835156642811, Test Accuracy: 0.7160714285714286\n","ReLU Activation - Max: 2.496732130423344 Min: 0.0\n","Softmax Output - Max: 0.9826995371527119 Min: 0.01730046284728805 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004226575773800806 Min: -0.004226575773800806\n","Layer 0 - Gradient Weights Max: 0.010005162430359506 Min: -0.011841171536612752\n","ReLU Activation - Max: 2.2158823582358407 Min: 0.0\n","Softmax Output - Max: 0.9695095443666341 Min: 0.030490455633366036 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4974873588222377 Min: 0.0\n","Softmax Output - Max: 0.9827422321580962 Min: 0.01725776784190384 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004236649812607985 Min: -0.004236649812607985\n","Layer 0 - Gradient Weights Max: 0.01012120214130741 Min: -0.011936161260533259\n","ReLU Activation - Max: 2.2164384244531576 Min: 0.0\n","Softmax Output - Max: 0.969572155700021 Min: 0.03042784429997898 Sum (first example): 1.0\n","ReLU Activation - Max: 2.498240841964084 Min: 0.0\n","Softmax Output - Max: 0.9827845875816299 Min: 0.017215412418370123 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00424697678093509 Min: -0.004246976780935089\n","Layer 0 - Gradient Weights Max: 0.01016244373304392 Min: -0.011986589225237513\n","ReLU Activation - Max: 2.2169914736642142 Min: 0.0\n","Softmax Output - Max: 0.9696344583511968 Min: 0.03036554164880329 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4990197473663787 Min: 0.0\n","Softmax Output - Max: 0.9828266221729167 Min: 0.017173377827083405 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004257409814075533 Min: -0.004257409814075531\n","Layer 0 - Gradient Weights Max: 0.01017313015057292 Min: -0.011951345398980198\n","ReLU Activation - Max: 2.2175558150714867 Min: 0.0\n","Softmax Output - Max: 0.9696968797148665 Min: 0.030303120285133364 Sum (first example): 1.0\n","ReLU Activation - Max: 2.49979350795417 Min: 0.0\n","Softmax Output - Max: 0.9828687620915435 Min: 0.01713123790845641 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004267694192269409 Min: -0.0042676941922694086\n","Layer 0 - Gradient Weights Max: 0.010091283506710359 Min: -0.011803647246699175\n","ReLU Activation - Max: 2.2181190932325063 Min: 0.0\n","Softmax Output - Max: 0.969759432308687 Min: 0.030240567691312975 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5005654831182427 Min: 0.0\n","Softmax Output - Max: 0.9829107949068367 Min: 0.017089205093163393 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004277422286611163 Min: -0.004277422286611163\n","Layer 0 - Gradient Weights Max: 0.010075186330457713 Min: -0.011776690480781057\n","ReLU Activation - Max: 2.2186790219802908 Min: 0.0\n","Softmax Output - Max: 0.9698218420290767 Min: 0.030178157970923325 Sum (first example): 1.0\n","ReLU Activation - Max: 2.501340738373444 Min: 0.0\n","Softmax Output - Max: 0.9829525396529758 Min: 0.01704746034702414 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004287108317180744 Min: -0.004287108317180745\n","Layer 0 - Gradient Weights Max: 0.01010597683006512 Min: -0.011736658504488781\n","ReLU Activation - Max: 2.2192378389481546 Min: 0.0\n","Softmax Output - Max: 0.9698842447163296 Min: 0.030115755283670456 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5021140894418115 Min: 0.0\n","Softmax Output - Max: 0.9829946346371073 Min: 0.017005365362892592 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00429684819705146 Min: -0.004296848197051459\n","Layer 0 - Gradient Weights Max: 0.010237357314613421 Min: -0.011766604699647349\n","ReLU Activation - Max: 2.219796834930806 Min: 0.0\n","Softmax Output - Max: 0.9699462148582253 Min: 0.030053785141774616 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.502904855916463 Min: 0.0\n","Softmax Output - Max: 0.9830365810238643 Min: 0.016963418976135708 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0043066912728368005 Min: -0.0043066912728368005\n","Layer 0 - Gradient Weights Max: 0.010249929376473057 Min: -0.011796855544925024\n","ReLU Activation - Max: 2.2203565653810777 Min: 0.0\n","Softmax Output - Max: 0.9700080689766397 Min: 0.029991931023360396 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.503680653736744 Min: 0.0\n","Softmax Output - Max: 0.9830786955427425 Min: 0.016921304457257348 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004316476916621867 Min: -0.004316476916621867\n","Layer 0 - Gradient Weights Max: 0.01021248181935963 Min: -0.011789617981258795\n","ReLU Activation - Max: 2.2209120571769843 Min: 0.0\n","Softmax Output - Max: 0.9700697428740601 Min: 0.029930257125939896 Sum (first example): 1.0\n","Epoch 800, Loss: 0.5705610908143702, Test Accuracy: 0.7160714285714286\n","ReLU Activation - Max: 2.5044681690855923 Min: 0.0\n","Softmax Output - Max: 0.9831207913071176 Min: 0.016879208692882352 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004326190027727608 Min: -0.004326190027727607\n","Layer 0 - Gradient Weights Max: 0.01025829770446025 Min: -0.011785333996161544\n","ReLU Activation - Max: 2.221466510014542 Min: 0.0\n","Softmax Output - Max: 0.9701313551175624 Min: 0.029868644882437716 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5052538904968107 Min: 0.0\n","Softmax Output - Max: 0.9831626880511463 Min: 0.016837311948853628 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004335740239046162 Min: -0.004335740239046162\n","Layer 0 - Gradient Weights Max: 0.010350910759278185 Min: -0.011762561727071219\n","ReLU Activation - Max: 2.2220181263709837 Min: 0.0\n","Softmax Output - Max: 0.9701927252738298 Min: 0.0298072747261703 Sum (first example): 1.0\n","ReLU Activation - Max: 2.506031417860114 Min: 0.0\n","Softmax Output - Max: 0.983204567217382 Min: 0.016795432782618077 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004345475383487302 Min: -0.004345475383487302\n","Layer 0 - Gradient Weights Max: 0.010411171625543175 Min: -0.011824586070580248\n","ReLU Activation - Max: 2.222572812674742 Min: 0.0\n","Softmax Output - Max: 0.9702539339822942 Min: 0.02974606601770591 Sum (first example): 1.0\n","ReLU Activation - Max: 2.506813513390034 Min: 0.0\n","Softmax Output - Max: 0.9832460962706886 Min: 0.0167539037293115 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004355407804687635 Min: -0.004355407804687635\n","Layer 0 - Gradient Weights Max: 0.010298433312488645 Min: -0.011825285788121191\n","ReLU Activation - Max: 2.223131586926297 Min: 0.0\n","Softmax Output - Max: 0.9703149148539376 Min: 0.02968508514606234 Sum (first example): 1.0\n","ReLU Activation - Max: 2.507591825018868 Min: 0.0\n","Softmax Output - Max: 0.9832874306796707 Min: 0.016712569320329298 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00436500142501256 Min: -0.004365001425012561\n","Layer 0 - Gradient Weights Max: 0.010379143367155879 Min: -0.011794475699147808\n","ReLU Activation - Max: 2.223673806419396 Min: 0.0\n","Softmax Output - Max: 0.9703753866503123 Min: 0.029624613349687676 Sum (first example): 1.0\n","ReLU Activation - Max: 2.508398755892478 Min: 0.0\n","Softmax Output - Max: 0.9833284302022282 Min: 0.01667156979777186 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004374514596976971 Min: -0.004374514596976972\n","Layer 0 - Gradient Weights Max: 0.010376939097671189 Min: -0.01179080162778241\n","ReLU Activation - Max: 2.224214904290871 Min: 0.0\n","Softmax Output - Max: 0.9704355341727597 Min: 0.029564465827240302 Sum (first example): 1.0\n","ReLU Activation - Max: 2.509203574272808 Min: 0.0\n","Softmax Output - Max: 0.9833693858300291 Min: 0.016630614169970845 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004383879864841072 Min: -0.004383879864841073\n","Layer 0 - Gradient Weights Max: 0.010329121089931195 Min: -0.011735365951960085\n","ReLU Activation - Max: 2.2247523017630684 Min: 0.0\n","Softmax Output - Max: 0.9704953569044901 Min: 0.02950464309550987 Sum (first example): 1.0\n","ReLU Activation - Max: 2.510009949491959 Min: 0.0\n","Softmax Output - Max: 0.9834104293152397 Min: 0.016589570684760392 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004393012677955193 Min: -0.004393012677955192\n","Layer 0 - Gradient Weights Max: 0.010393878070944872 Min: -0.011722320339144348\n","ReLU Activation - Max: 2.2252915307229286 Min: 0.0\n","Softmax Output - Max: 0.9705550647140887 Min: 0.029444935285911203 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5108087787591638 Min: 0.0\n","Softmax Output - Max: 0.9834512814042721 Min: 0.01654871859572805 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004402217822037762 Min: -0.0044022178220377624\n","Layer 0 - Gradient Weights Max: 0.010418256169829441 Min: -0.011734797205938311\n","ReLU Activation - Max: 2.2258391007070397 Min: 0.0\n","Softmax Output - Max: 0.9706147289575668 Min: 0.029385271042433173 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.511611361722327 Min: 0.0\n","Softmax Output - Max: 0.9834918888423215 Min: 0.0165081111576784 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004411235976213608 Min: -0.004411235976213608\n","Layer 0 - Gradient Weights Max: 0.010328316067508274 Min: -0.011758771888303082\n","ReLU Activation - Max: 2.226389120255903 Min: 0.0\n","Softmax Output - Max: 0.9706741782156593 Min: 0.029325821784340775 Sum (first example): 1.0\n","Epoch 810, Loss: 0.5698313583703496, Test Accuracy: 0.7171428571428572\n","ReLU Activation - Max: 2.512404930706505 Min: 0.0\n","Softmax Output - Max: 0.9835322664695539 Min: 0.01646773353044609 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004420217998026341 Min: -0.004420217998026341\n","Layer 0 - Gradient Weights Max: 0.010398436924412776 Min: -0.011739811078459093\n","ReLU Activation - Max: 2.226937994166172 Min: 0.0\n","Softmax Output - Max: 0.9707333589884132 Min: 0.029266641011586908 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5131961906054316 Min: 0.0\n","Softmax Output - Max: 0.9835724982699762 Min: 0.01642750173002375 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0044291788993417085 Min: -0.004429178899341708\n","Layer 0 - Gradient Weights Max: 0.010343360818859565 Min: -0.01172952081859456\n","ReLU Activation - Max: 2.2274883032658885 Min: 0.0\n","Softmax Output - Max: 0.9707923053294325 Min: 0.029207694670567497 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5139682481139287 Min: 0.0\n","Softmax Output - Max: 0.9836124823203468 Min: 0.016387517679653144 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004437925205993526 Min: -0.0044379252059935275\n","Layer 0 - Gradient Weights Max: 0.010321471384739014 Min: -0.01173085685600614\n","ReLU Activation - Max: 2.2280322393633276 Min: 0.0\n","Softmax Output - Max: 0.9708508115842949 Min: 0.02914918841570515 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5147460961919874 Min: 0.0\n","Softmax Output - Max: 0.9836521304883472 Min: 0.016347869511652877 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004446454097895659 Min: -0.004446454097895659\n","Layer 0 - Gradient Weights Max: 0.0104237720874933 Min: -0.011782848770464644\n","ReLU Activation - Max: 2.228573760392832 Min: 0.0\n","Softmax Output - Max: 0.9709092014338242 Min: 0.029090798566175826 Sum (first example): 1.0\n","ReLU Activation - Max: 2.515528546576593 Min: 0.0\n","Softmax Output - Max: 0.9836913456986459 Min: 0.016308654301354003 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004455224054235887 Min: -0.004455224054235887\n","Layer 0 - Gradient Weights Max: 0.010525638050954637 Min: -0.011770831161387491\n","ReLU Activation - Max: 2.2291200137747236 Min: 0.0\n","Softmax Output - Max: 0.9709678337360979 Min: 0.02903216626390219 Sum (first example): 1.0\n","ReLU Activation - Max: 2.516297608203268 Min: 0.0\n","Softmax Output - Max: 0.9837305678816356 Min: 0.016269432118364503 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004463930922778776 Min: -0.004463930922778775\n","Layer 0 - Gradient Weights Max: 0.010487221696852686 Min: -0.011789509294363364\n","ReLU Activation - Max: 2.2296681641272804 Min: 0.0\n","Softmax Output - Max: 0.9710264998121653 Min: 0.028973500187834747 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5170540809587982 Min: 0.0\n","Softmax Output - Max: 0.9837662507571754 Min: 0.016233749242824665 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0044725787508738965 Min: -0.004472578750873898\n","Layer 0 - Gradient Weights Max: 0.010461552702901118 Min: -0.011753456463093984\n","ReLU Activation - Max: 2.2302089718891662 Min: 0.0\n","Softmax Output - Max: 0.9710848195580701 Min: 0.028915180441929814 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5178009093736025 Min: 0.0\n","Softmax Output - Max: 0.983801963538728 Min: 0.016198036461272047 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004480918331253965 Min: -0.004480918331253963\n","Layer 0 - Gradient Weights Max: 0.010629428383131235 Min: -0.011681242086024323\n","ReLU Activation - Max: 2.230737435805977 Min: 0.0\n","Softmax Output - Max: 0.9711426653058155 Min: 0.028857334694184594 Sum (first example): 1.0\n","ReLU Activation - Max: 2.518578586291633 Min: 0.0\n","Softmax Output - Max: 0.9838377285703309 Min: 0.01616227142966913 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004489231770463216 Min: -0.004489231770463216\n","Layer 0 - Gradient Weights Max: 0.010569336158794953 Min: -0.011792905665490645\n","ReLU Activation - Max: 2.2312535908760314 Min: 0.0\n","Softmax Output - Max: 0.9712001493707725 Min: 0.028799850629227536 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5193594392348047 Min: 0.0\n","Softmax Output - Max: 0.9838733262863986 Min: 0.016126673713601415 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004497687448494757 Min: -0.004497687448494756\n","Layer 0 - Gradient Weights Max: 0.010644419958806082 Min: -0.01177532398935505\n","ReLU Activation - Max: 2.2317772871038426 Min: 0.0\n","Softmax Output - Max: 0.9712577272996646 Min: 0.028742272700335338 Sum (first example): 0.9999999999999999\n","Epoch 820, Loss: 0.5690895594266525, Test Accuracy: 0.7175\n","ReLU Activation - Max: 2.520101096673261 Min: 0.0\n","Softmax Output - Max: 0.983908764080419 Min: 0.01609123591958086 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00450618702768013 Min: -0.004506187027680129\n","Layer 0 - Gradient Weights Max: 0.010689640992187213 Min: -0.01177501841845512\n","ReLU Activation - Max: 2.2323041185674777 Min: 0.0\n","Softmax Output - Max: 0.9713151927788802 Min: 0.02868480722111989 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5208436434575985 Min: 0.0\n","Softmax Output - Max: 0.9839440131674105 Min: 0.016055986832589332 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004514773090787206 Min: -0.0045147730907872064\n","Layer 0 - Gradient Weights Max: 0.010743595644362912 Min: -0.011797743523805387\n","ReLU Activation - Max: 2.2328166236405904 Min: 0.0\n","Softmax Output - Max: 0.9713721932783493 Min: 0.028627806721650703 Sum (first example): 1.0\n","ReLU Activation - Max: 2.521605805023086 Min: 0.0\n","Softmax Output - Max: 0.9839790455564029 Min: 0.016020954443597052 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004523314832263558 Min: -0.004523314832263556\n","Layer 0 - Gradient Weights Max: 0.010801241153736272 Min: -0.011818741022813002\n","ReLU Activation - Max: 2.2333316635715894 Min: 0.0\n","Softmax Output - Max: 0.9714290124057571 Min: 0.02857098759424297 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5223674765886788 Min: 0.0\n","Softmax Output - Max: 0.9840138113793164 Min: 0.015986188620683607 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004532115404747333 Min: -0.004532115404747331\n","Layer 0 - Gradient Weights Max: 0.010878241823990316 Min: -0.01187481837861514\n","ReLU Activation - Max: 2.233833595265924 Min: 0.0\n","Softmax Output - Max: 0.9714852537410995 Min: 0.02851474625890047 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5231284524858233 Min: 0.0\n","Softmax Output - Max: 0.9840480801405901 Min: 0.015951919859409913 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0045408705983164335 Min: -0.004540870598316433\n","Layer 0 - Gradient Weights Max: 0.010912199940866011 Min: -0.011933252182729476\n","ReLU Activation - Max: 2.2343360496166476 Min: 0.0\n","Softmax Output - Max: 0.9715415777309182 Min: 0.02845842226908178 Sum (first example): 1.0\n","ReLU Activation - Max: 2.523888879944435 Min: 0.0\n","Softmax Output - Max: 0.9840821336869787 Min: 0.015917866313021355 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004549912030587301 Min: -0.0045499120305873\n","Layer 0 - Gradient Weights Max: 0.010800307808082742 Min: -0.011993650289988942\n","ReLU Activation - Max: 2.2348328409462197 Min: 0.0\n","Softmax Output - Max: 0.971597451651905 Min: 0.02840254834809492 Sum (first example): 1.0\n","ReLU Activation - Max: 2.524639269065887 Min: 0.0\n","Softmax Output - Max: 0.9841161250794359 Min: 0.015883874920564166 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004559024547001591 Min: -0.004559024547001591\n","Layer 0 - Gradient Weights Max: 0.010689892351366354 Min: -0.012048077133432478\n","ReLU Activation - Max: 2.235340021471119 Min: 0.0\n","Softmax Output - Max: 0.9716535532202281 Min: 0.028346446779771856 Sum (first example): 1.0\n","ReLU Activation - Max: 2.525388215276434 Min: 0.0\n","Softmax Output - Max: 0.9841500951182212 Min: 0.015849904881778915 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0045679030912512755 Min: -0.004567903091251276\n","Layer 0 - Gradient Weights Max: 0.010649359974320274 Min: -0.01203418396311562\n","ReLU Activation - Max: 2.235845179476112 Min: 0.0\n","Softmax Output - Max: 0.9717094848102493 Min: 0.028290515189750672 Sum (first example): 1.0\n","ReLU Activation - Max: 2.52611979463351 Min: 0.0\n","Softmax Output - Max: 0.9841840375814639 Min: 0.015815962418536122 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004576731821676208 Min: -0.004576731821676208\n","Layer 0 - Gradient Weights Max: 0.010621243596560673 Min: -0.012014718400565451\n","ReLU Activation - Max: 2.2363504088311843 Min: 0.0\n","Softmax Output - Max: 0.9717653219897041 Min: 0.02823467801029594 Sum (first example): 1.0\n","ReLU Activation - Max: 2.526861381382946 Min: 0.0\n","Softmax Output - Max: 0.9842178378004811 Min: 0.015782162199518945 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004585496197037964 Min: -0.004585496197037965\n","Layer 0 - Gradient Weights Max: 0.01059634792087377 Min: -0.012014953187467722\n","ReLU Activation - Max: 2.236866644794027 Min: 0.0\n","Softmax Output - Max: 0.9718212766773472 Min: 0.028178723322652737 Sum (first example): 1.0\n","Epoch 830, Loss: 0.5683319112367342, Test Accuracy: 0.7178571428571429\n","ReLU Activation - Max: 2.527585137162297 Min: 0.0\n","Softmax Output - Max: 0.9842515072877083 Min: 0.015748492712291715 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004594263066088985 Min: -0.004594263066088987\n","Layer 0 - Gradient Weights Max: 0.010579647823057974 Min: -0.01199988561528848\n","ReLU Activation - Max: 2.2373774471708767 Min: 0.0\n","Softmax Output - Max: 0.9718770726558345 Min: 0.028122927344165577 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5282851947921023 Min: 0.0\n","Softmax Output - Max: 0.9842851812869543 Min: 0.015714818713045697 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004603130032360188 Min: -0.004603130032360189\n","Layer 0 - Gradient Weights Max: 0.010600827009390976 Min: -0.012029251363903997\n","ReLU Activation - Max: 2.2378880787479765 Min: 0.0\n","Softmax Output - Max: 0.9719328345840977 Min: 0.028067165415902286 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5289853987139432 Min: 0.0\n","Softmax Output - Max: 0.9843188753659029 Min: 0.01568112463409717 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0046119804695846965 Min: -0.004611980469584694\n","Layer 0 - Gradient Weights Max: 0.010425901029936353 Min: -0.0120595179422158\n","ReLU Activation - Max: 2.2383934435949793 Min: 0.0\n","Softmax Output - Max: 0.9719884663832162 Min: 0.02801153361678381 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5297045368840982 Min: 0.0\n","Softmax Output - Max: 0.984352305465285 Min: 0.015647694534715133 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004620854915106945 Min: -0.004620854915106945\n","Layer 0 - Gradient Weights Max: 0.010358841608091657 Min: -0.012064996628160369\n","ReLU Activation - Max: 2.23888853379515 Min: 0.0\n","Softmax Output - Max: 0.9720436833886565 Min: 0.027956316611343363 Sum (first example): 1.0\n","ReLU Activation - Max: 2.530427418575267 Min: 0.0\n","Softmax Output - Max: 0.9843856408908188 Min: 0.01561435910918114 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004629832526313679 Min: -0.004629832526313677\n","Layer 0 - Gradient Weights Max: 0.010340245917437 Min: -0.01205666054000105\n","ReLU Activation - Max: 2.239382723893462 Min: 0.0\n","Softmax Output - Max: 0.9720986826159641 Min: 0.02790131738403586 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5311513957560416 Min: 0.0\n","Softmax Output - Max: 0.9844190028000178 Min: 0.015580997199982126 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0046387393614596775 Min: -0.004638739361459677\n","Layer 0 - Gradient Weights Max: 0.010381543647562004 Min: -0.012031684273132675\n","ReLU Activation - Max: 2.2398810866654997 Min: 0.0\n","Softmax Output - Max: 0.9721536147035317 Min: 0.027846385296468292 Sum (first example): 1.0\n","ReLU Activation - Max: 2.53186016998802 Min: 0.0\n","Softmax Output - Max: 0.9844523240511914 Min: 0.015547675948808614 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004647610791638577 Min: -0.0046476107916385775\n","Layer 0 - Gradient Weights Max: 0.010380203470163597 Min: -0.011983263416860377\n","ReLU Activation - Max: 2.240384739185823 Min: 0.0\n","Softmax Output - Max: 0.9722084091548433 Min: 0.027791590845156654 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5325720330308985 Min: 0.0\n","Softmax Output - Max: 0.9844857555892307 Min: 0.015514244410769163 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004656415225523369 Min: -0.0046564152255233705\n","Layer 0 - Gradient Weights Max: 0.01039949916993127 Min: -0.012088513936860314\n","ReLU Activation - Max: 2.2408985290643226 Min: 0.0\n","Softmax Output - Max: 0.9722631282028176 Min: 0.027736871797182365 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5332700601826357 Min: 0.0\n","Softmax Output - Max: 0.9845188981838199 Min: 0.015481101816180034 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004665417039315998 Min: -0.0046654170393159965\n","Layer 0 - Gradient Weights Max: 0.010376116716122714 Min: -0.012069806764062432\n","ReLU Activation - Max: 2.241410360580676 Min: 0.0\n","Softmax Output - Max: 0.972317647391765 Min: 0.02768235260823513 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5339683502801957 Min: 0.0\n","Softmax Output - Max: 0.9845519464651672 Min: 0.01544805353483292 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004674320704623023 Min: -0.004674320704623021\n","Layer 0 - Gradient Weights Max: 0.010353925245963812 Min: -0.012045093681284542\n","ReLU Activation - Max: 2.241914360307218 Min: 0.0\n","Softmax Output - Max: 0.9723718878655603 Min: 0.027628112134439876 Sum (first example): 1.0\n","Epoch 840, Loss: 0.5675632328863588, Test Accuracy: 0.7185714285714285\n","ReLU Activation - Max: 2.534660289585285 Min: 0.0\n","Softmax Output - Max: 0.9845848417680031 Min: 0.015415158231996854 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004683220596393426 Min: -0.004683220596393427\n","Layer 0 - Gradient Weights Max: 0.01039680333096056 Min: -0.011988877298808822\n","ReLU Activation - Max: 2.2424032528027924 Min: 0.0\n","Softmax Output - Max: 0.9724256378402247 Min: 0.02757436215977534 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5353617316458634 Min: 0.0\n","Softmax Output - Max: 0.984617844293665 Min: 0.015382155706334985 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004692116680459364 Min: -0.004692116680459366\n","Layer 0 - Gradient Weights Max: 0.010440216643310756 Min: -0.012049791689899286\n","ReLU Activation - Max: 2.2428887652906306 Min: 0.0\n","Softmax Output - Max: 0.9724793062682752 Min: 0.027520693731724773 Sum (first example): 1.0\n","ReLU Activation - Max: 2.536053033342041 Min: 0.0\n","Softmax Output - Max: 0.984650692545921 Min: 0.015349307454078968 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004701052604700859 Min: -0.004701052604700859\n","Layer 0 - Gradient Weights Max: 0.010462685729723965 Min: -0.012027985426599939\n","ReLU Activation - Max: 2.243376278445811 Min: 0.0\n","Softmax Output - Max: 0.9725328908924413 Min: 0.027467109107558618 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5367250747711827 Min: 0.0\n","Softmax Output - Max: 0.984683610442974 Min: 0.015316389557025918 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004710096566415254 Min: -0.004710096566415253\n","Layer 0 - Gradient Weights Max: 0.01042373491010154 Min: -0.01207929696550046\n","ReLU Activation - Max: 2.2438433811765206 Min: 0.0\n","Softmax Output - Max: 0.9725858646674286 Min: 0.0274141353325714 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5374065858436063 Min: 0.0\n","Softmax Output - Max: 0.9847163872936429 Min: 0.015283612706357025 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004719312125728534 Min: -0.004719312125728532\n","Layer 0 - Gradient Weights Max: 0.010505603771915003 Min: -0.011946799256341506\n","ReLU Activation - Max: 2.244304225386062 Min: 0.0\n","Softmax Output - Max: 0.97263855079046 Min: 0.02736144920954 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.538094440813939 Min: 0.0\n","Softmax Output - Max: 0.9847490101756121 Min: 0.01525098982438789 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004728506569426304 Min: -0.004728506569426306\n","Layer 0 - Gradient Weights Max: 0.01048973812619242 Min: -0.011976987853733622\n","ReLU Activation - Max: 2.2447627823724634 Min: 0.0\n","Softmax Output - Max: 0.9726913372960345 Min: 0.027308662703965494 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5387730250842346 Min: 0.0\n","Softmax Output - Max: 0.9847818378194595 Min: 0.015218162180540474 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004737616772421407 Min: -0.004737616772421407\n","Layer 0 - Gradient Weights Max: 0.010480134970239013 Min: -0.012074209073525992\n","ReLU Activation - Max: 2.245219309276141 Min: 0.0\n","Softmax Output - Max: 0.9727439170439665 Min: 0.027256082956033566 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.53944847454186 Min: 0.0\n","Softmax Output - Max: 0.9848145592706322 Min: 0.015185440729367774 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004746935480511763 Min: -0.004746935480511762\n","Layer 0 - Gradient Weights Max: 0.010442168212496479 Min: -0.0121532112227699\n","ReLU Activation - Max: 2.245680562240469 Min: 0.0\n","Softmax Output - Max: 0.9727964059707024 Min: 0.027203594029297572 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5401235071425363 Min: 0.0\n","Softmax Output - Max: 0.984847126148734 Min: 0.01515287385126594 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0047563454404518 Min: -0.0047563454404518\n","Layer 0 - Gradient Weights Max: 0.010436961746052334 Min: -0.01212377676993272\n","ReLU Activation - Max: 2.246146622834095 Min: 0.0\n","Softmax Output - Max: 0.9728488557915067 Min: 0.02715114420849326 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5407933407583023 Min: 0.0\n","Softmax Output - Max: 0.984879686903168 Min: 0.015120313096832027 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0047655375944823004 Min: -0.0047655375944823004\n","Layer 0 - Gradient Weights Max: 0.010500106168875917 Min: -0.01210646683546212\n","ReLU Activation - Max: 2.246611130974531 Min: 0.0\n","Softmax Output - Max: 0.9729013355560048 Min: 0.02709866444399527 Sum (first example): 1.0\n","Epoch 850, Loss: 0.5667799024125119, Test Accuracy: 0.7192857142857143\n","ReLU Activation - Max: 2.541469850381336 Min: 0.0\n","Softmax Output - Max: 0.9849121465838505 Min: 0.015087853416149448 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004774521793348154 Min: -0.004774521793348154\n","Layer 0 - Gradient Weights Max: 0.01044872749105235 Min: -0.012206975235678973\n","ReLU Activation - Max: 2.247068539284947 Min: 0.0\n","Softmax Output - Max: 0.97295365222224 Min: 0.027046347777759994 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5421546823825025 Min: 0.0\n","Softmax Output - Max: 0.9849445694765869 Min: 0.015055430523412966 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004783663787985282 Min: -0.00478366378798528\n","Layer 0 - Gradient Weights Max: 0.010476820210668612 Min: -0.012216279794572257\n","ReLU Activation - Max: 2.247532088721999 Min: 0.0\n","Softmax Output - Max: 0.9730059588249703 Min: 0.026994041175029678 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5428459530477543 Min: 0.0\n","Softmax Output - Max: 0.9849770387535471 Min: 0.015022961246452797 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00479270815315404 Min: -0.0047927081531540395\n","Layer 0 - Gradient Weights Max: 0.010286905197258561 Min: -0.012217358960877733\n","ReLU Activation - Max: 2.2479723066806905 Min: 0.0\n","Softmax Output - Max: 0.9730576067059936 Min: 0.026942393294006363 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5435433786793773 Min: 0.0\n","Softmax Output - Max: 0.9850096056702907 Min: 0.01499039432970939 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004801772618266445 Min: -0.004801772618266445\n","Layer 0 - Gradient Weights Max: 0.010258917570475793 Min: -0.012226089625158984\n","ReLU Activation - Max: 2.248406473727827 Min: 0.0\n","Softmax Output - Max: 0.9731089915687376 Min: 0.026891008431262357 Sum (first example): 1.0\n","ReLU Activation - Max: 2.544243539177527 Min: 0.0\n","Softmax Output - Max: 0.9850422286701956 Min: 0.014957771329804297 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004810686540916011 Min: -0.00481068654091601\n","Layer 0 - Gradient Weights Max: 0.010233207151468054 Min: -0.012102809107525864\n","ReLU Activation - Max: 2.2488388426836727 Min: 0.0\n","Softmax Output - Max: 0.9731602348564501 Min: 0.026839765143549917 Sum (first example): 1.0\n","ReLU Activation - Max: 2.544939740627253 Min: 0.0\n","Softmax Output - Max: 0.9850747706532942 Min: 0.014925229346705789 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0048194699473359534 Min: -0.0048194699473359534\n","Layer 0 - Gradient Weights Max: 0.010280760112200714 Min: -0.012118024400824188\n","ReLU Activation - Max: 2.2492712477977195 Min: 0.0\n","Softmax Output - Max: 0.9732113110198372 Min: 0.02678868898016292 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.545633110092728 Min: 0.0\n","Softmax Output - Max: 0.9851070283696762 Min: 0.014892971630323678 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004828240825722205 Min: -0.004828240825722206\n","Layer 0 - Gradient Weights Max: 0.010246280352925467 Min: -0.01206896685758382\n","ReLU Activation - Max: 2.249703051708404 Min: 0.0\n","Softmax Output - Max: 0.9732620762361207 Min: 0.026737923763879345 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5463251425900832 Min: 0.0\n","Softmax Output - Max: 0.985139312712589 Min: 0.01486068728741106 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004836934071075302 Min: -0.004836934071075303\n","Layer 0 - Gradient Weights Max: 0.010259713494059894 Min: -0.012069248393943547\n","ReLU Activation - Max: 2.2501358017925286 Min: 0.0\n","Softmax Output - Max: 0.9733126793669408 Min: 0.02668732063305919 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.547025461792069 Min: 0.0\n","Softmax Output - Max: 0.9851719599995854 Min: 0.014828040000414506 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004845526791960374 Min: -0.004845526791960374\n","Layer 0 - Gradient Weights Max: 0.010261087731325285 Min: -0.012040067385106238\n","ReLU Activation - Max: 2.250563561231224 Min: 0.0\n","Softmax Output - Max: 0.9733631061218059 Min: 0.02663689387819405 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5477300798679745 Min: 0.0\n","Softmax Output - Max: 0.9852045245932344 Min: 0.014795475406765653 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004853591779134686 Min: -0.004853591779134686\n","Layer 0 - Gradient Weights Max: 0.010137947869233733 Min: -0.011926003844300067\n","ReLU Activation - Max: 2.251793861557346 Min: 0.0\n","Softmax Output - Max: 0.9734131776383601 Min: 0.026586822361639808 Sum (first example): 1.0\n","Epoch 860, Loss: 0.5659908820917686, Test Accuracy: 0.7192857142857143\n","ReLU Activation - Max: 2.5484419399373386 Min: 0.0\n","Softmax Output - Max: 0.9852371412962359 Min: 0.014762858703764136 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004861237294993332 Min: -0.004861237294993331\n","Layer 0 - Gradient Weights Max: 0.009927157886849934 Min: -0.011935425744841601\n","ReLU Activation - Max: 2.253180759649549 Min: 0.0\n","Softmax Output - Max: 0.973463188729592 Min: 0.026536811270407887 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5491747113877308 Min: 0.0\n","Softmax Output - Max: 0.9852695282440416 Min: 0.014730471755958478 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0048686523023721276 Min: -0.0048686523023721276\n","Layer 0 - Gradient Weights Max: 0.009953011260807967 Min: -0.011980527281054322\n","ReLU Activation - Max: 2.254553478114762 Min: 0.0\n","Softmax Output - Max: 0.9735130259722821 Min: 0.026486974027717842 Sum (first example): 1.0\n","ReLU Activation - Max: 2.549907239135264 Min: 0.0\n","Softmax Output - Max: 0.985301666898944 Min: 0.014698333101055845 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004876124446005906 Min: -0.004876124446005907\n","Layer 0 - Gradient Weights Max: 0.009971351388563721 Min: -0.011972790345585084\n","ReLU Activation - Max: 2.2559230286204572 Min: 0.0\n","Softmax Output - Max: 0.9735625620800223 Min: 0.026437437919977626 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5506380376676736 Min: 0.0\n","Softmax Output - Max: 0.985333701331496 Min: 0.01466629866850394 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004883575465627483 Min: -0.004883575465627484\n","Layer 0 - Gradient Weights Max: 0.009969681498197514 Min: -0.011970392669399761\n","ReLU Activation - Max: 2.2573107548945988 Min: 0.0\n","Softmax Output - Max: 0.9736121488942765 Min: 0.026387851105723396 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.55137430388297 Min: 0.0\n","Softmax Output - Max: 0.9853653682991298 Min: 0.014634631700870215 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004890784615325746 Min: -0.004890784615325744\n","Layer 0 - Gradient Weights Max: 0.009959200017065338 Min: -0.011819628932042013\n","ReLU Activation - Max: 2.2586999190926607 Min: 0.0\n","Softmax Output - Max: 0.9736616325103586 Min: 0.026338367489641478 Sum (first example): 1.0\n","ReLU Activation - Max: 2.552113737717969 Min: 0.0\n","Softmax Output - Max: 0.9853970991347233 Min: 0.014602900865276628 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00489730623489075 Min: -0.00489730623489075\n","Layer 0 - Gradient Weights Max: 0.009905677642546698 Min: -0.011862953592298258\n","ReLU Activation - Max: 2.2600764008521477 Min: 0.0\n","Softmax Output - Max: 0.9737110746291647 Min: 0.02628892537083527 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5528526464554666 Min: 0.0\n","Softmax Output - Max: 0.9854286462665125 Min: 0.014571353733487374 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004903725278897769 Min: -0.004903725278897768\n","Layer 0 - Gradient Weights Max: 0.009933790074373491 Min: -0.011816164453584693\n","ReLU Activation - Max: 2.2614622535005635 Min: 0.0\n","Softmax Output - Max: 0.9737602459089729 Min: 0.02623975409102714 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.5536023365957736 Min: 0.0\n","Softmax Output - Max: 0.9854600733082587 Min: 0.014539926691741315 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004910153426729665 Min: -0.004910153426729664\n","Layer 0 - Gradient Weights Max: 0.009935233263851023 Min: -0.011836601382089596\n","ReLU Activation - Max: 2.2628368108983734 Min: 0.0\n","Softmax Output - Max: 0.973809315159427 Min: 0.026190684840572867 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.554358168631817 Min: 0.0\n","Softmax Output - Max: 0.9854914071198239 Min: 0.014508592880176061 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004916630483124965 Min: -0.004916630483124965\n","Layer 0 - Gradient Weights Max: 0.009975350724550639 Min: -0.011838690782678424\n","ReLU Activation - Max: 2.264213430303667 Min: 0.0\n","Softmax Output - Max: 0.9738581995369803 Min: 0.02614180046301979 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5551107926253365 Min: 0.0\n","Softmax Output - Max: 0.9855227760889289 Min: 0.014477223911071169 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0049231375573882295 Min: -0.004923137557388229\n","Layer 0 - Gradient Weights Max: 0.009956386354934448 Min: -0.011846739346728957\n","ReLU Activation - Max: 2.265603388015271 Min: 0.0\n","Softmax Output - Max: 0.9739066469446674 Min: 0.026093353055332513 Sum (first example): 1.0\n","Epoch 870, Loss: 0.5652026159126243, Test Accuracy: 0.7207142857142858\n","ReLU Activation - Max: 2.5558469221081683 Min: 0.0\n","Softmax Output - Max: 0.9855538809695845 Min: 0.014446119030415372 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004929707362182773 Min: -0.0049297073621827735\n","Layer 0 - Gradient Weights Max: 0.009940391140937055 Min: -0.011807563780587782\n","ReLU Activation - Max: 2.266994053117575 Min: 0.0\n","Softmax Output - Max: 0.9739552913912186 Min: 0.026044708608781355 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5565843111091673 Min: 0.0\n","Softmax Output - Max: 0.9855849006656933 Min: 0.0144150993343066 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004936077402686036 Min: -0.004936077402686037\n","Layer 0 - Gradient Weights Max: 0.009904517752274501 Min: -0.01172339373659729\n","ReLU Activation - Max: 2.2683616561688202 Min: 0.0\n","Softmax Output - Max: 0.9740039280103406 Min: 0.02599607198965935 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5573010405874514 Min: 0.0\n","Softmax Output - Max: 0.9856156371219003 Min: 0.014384362878099754 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004942132430649459 Min: -0.004942132430649459\n","Layer 0 - Gradient Weights Max: 0.009925989327481774 Min: -0.011751938225054246\n","ReLU Activation - Max: 2.2697219663638477 Min: 0.0\n","Softmax Output - Max: 0.9740522807466897 Min: 0.025947719253310405 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5580143939176136 Min: 0.0\n","Softmax Output - Max: 0.9856460235813408 Min: 0.014353976418659244 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004948376390902236 Min: -0.004948376390902236\n","Layer 0 - Gradient Weights Max: 0.00988778998579336 Min: -0.011783932419245925\n","ReLU Activation - Max: 2.2710909501186514 Min: 0.0\n","Softmax Output - Max: 0.9741005651211601 Min: 0.02589943487883991 Sum (first example): 1.0\n","ReLU Activation - Max: 2.55871710071856 Min: 0.0\n","Softmax Output - Max: 0.9856763671064204 Min: 0.014323632893579677 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0049544084899348665 Min: -0.004954408489934866\n","Layer 0 - Gradient Weights Max: 0.00993313454426716 Min: -0.011791637402451444\n","ReLU Activation - Max: 2.2724613559953117 Min: 0.0\n","Softmax Output - Max: 0.9741484735236039 Min: 0.025851526476396155 Sum (first example): 1.0\n","ReLU Activation - Max: 2.559435267341846 Min: 0.0\n","Softmax Output - Max: 0.9857063987651806 Min: 0.014293601234819257 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004960230812613855 Min: -0.004960230812613857\n","Layer 0 - Gradient Weights Max: 0.009894558149416682 Min: -0.0117544881754964\n","ReLU Activation - Max: 2.2738240301223915 Min: 0.0\n","Softmax Output - Max: 0.9741962792688369 Min: 0.02580372073116303 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5601506703116743 Min: 0.0\n","Softmax Output - Max: 0.9857361143682359 Min: 0.014263885631764159 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004965788453243517 Min: -0.004965788453243517\n","Layer 0 - Gradient Weights Max: 0.009934461666838507 Min: -0.011754753876485155\n","ReLU Activation - Max: 2.2751900917083328 Min: 0.0\n","Softmax Output - Max: 0.9742437804208984 Min: 0.02575621957910157 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5608632550212347 Min: 0.0\n","Softmax Output - Max: 0.9857655947663356 Min: 0.014234405233664257 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004971631838805032 Min: -0.004971631838805034\n","Layer 0 - Gradient Weights Max: 0.009932012414310496 Min: -0.011759963207539652\n","ReLU Activation - Max: 2.2765553673414765 Min: 0.0\n","Softmax Output - Max: 0.9742911433280051 Min: 0.025708856671994854 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5615823312220236 Min: 0.0\n","Softmax Output - Max: 0.9857946336893604 Min: 0.014205366310639674 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.004977384754657684 Min: -0.004977384754657687\n","Layer 0 - Gradient Weights Max: 0.009906311724349075 Min: -0.01170685263650197\n","ReLU Activation - Max: 2.2779133022193325 Min: 0.0\n","Softmax Output - Max: 0.9743381816059407 Min: 0.025661818394059257 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5623111511540615 Min: 0.0\n","Softmax Output - Max: 0.9858235491821359 Min: 0.014176450817864221 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004982901443449135 Min: -0.004982901443449135\n","Layer 0 - Gradient Weights Max: 0.009887094893995052 Min: -0.011701099471047815\n","ReLU Activation - Max: 2.279284292025892 Min: 0.0\n","Softmax Output - Max: 0.9743852339604403 Min: 0.0256147660395596 Sum (first example): 1.0\n","Epoch 880, Loss: 0.5644080751525756, Test Accuracy: 0.7207142857142858\n","ReLU Activation - Max: 2.563017777855833 Min: 0.0\n","Softmax Output - Max: 0.9858525069051826 Min: 0.014147493094817559 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0049884204031454575 Min: -0.0049884204031454575\n","Layer 0 - Gradient Weights Max: 0.009867012956341327 Min: -0.011699727598890059\n","ReLU Activation - Max: 2.280650829734361 Min: 0.0\n","Softmax Output - Max: 0.9744322572509574 Min: 0.025567742749042683 Sum (first example): 1.0\n","ReLU Activation - Max: 2.563718197035234 Min: 0.0\n","Softmax Output - Max: 0.9858813225402829 Min: 0.014118677459717226 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004993885583290161 Min: -0.004993885583290161\n","Layer 0 - Gradient Weights Max: 0.009967255406909967 Min: -0.01160900769413858\n","ReLU Activation - Max: 2.2820191770340683 Min: 0.0\n","Softmax Output - Max: 0.9744790084712843 Min: 0.02552099152871583 Sum (first example): 1.0\n","ReLU Activation - Max: 2.564418636929405 Min: 0.0\n","Softmax Output - Max: 0.9859098981805381 Min: 0.014090101819461841 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.004998953556394383 Min: -0.004998953556394383\n","Layer 0 - Gradient Weights Max: 0.009917478521508389 Min: -0.011607793155388905\n","ReLU Activation - Max: 2.2833835334329162 Min: 0.0\n","Softmax Output - Max: 0.9745257021677541 Min: 0.02547429783224582 Sum (first example): 1.0\n","ReLU Activation - Max: 2.565117579498009 Min: 0.0\n","Softmax Output - Max: 0.9859383328562633 Min: 0.01406166714373663 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005003907512351723 Min: -0.005003907512351722\n","Layer 0 - Gradient Weights Max: 0.00972364042348729 Min: -0.01165143038943777\n","ReLU Activation - Max: 2.284738277430607 Min: 0.0\n","Softmax Output - Max: 0.9745724024388088 Min: 0.025427597561191197 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5658143844694488 Min: 0.0\n","Softmax Output - Max: 0.9859668159000282 Min: 0.014033184099971683 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005008853622384317 Min: -0.005008853622384316\n","Layer 0 - Gradient Weights Max: 0.009737802798602754 Min: -0.011620309476413373\n","ReLU Activation - Max: 2.286094603959905 Min: 0.0\n","Softmax Output - Max: 0.9746191767029267 Min: 0.02538082329707326 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5665017016557505 Min: 0.0\n","Softmax Output - Max: 0.985995208047867 Min: 0.014004791952133078 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005013929013299065 Min: -0.0050139290132990635\n","Layer 0 - Gradient Weights Max: 0.009635139163077918 Min: -0.011624836009076527\n","ReLU Activation - Max: 2.287459853579388 Min: 0.0\n","Softmax Output - Max: 0.9746659454616634 Min: 0.025334054538336617 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5671686054047815 Min: 0.0\n","Softmax Output - Max: 0.9860236067664616 Min: 0.013976393233538362 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005018836136971839 Min: -0.005018836136971841\n","Layer 0 - Gradient Weights Max: 0.009599754901137689 Min: -0.011618357880170651\n","ReLU Activation - Max: 2.2888298478073748 Min: 0.0\n","Softmax Output - Max: 0.9747123680589688 Min: 0.02528763194103121 Sum (first example): 1.0\n","ReLU Activation - Max: 2.567846875265747 Min: 0.0\n","Softmax Output - Max: 0.9860520394683348 Min: 0.013947960531665167 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005023662707098197 Min: -0.0050236627070981985\n","Layer 0 - Gradient Weights Max: 0.009571076606867148 Min: -0.011612957997408585\n","ReLU Activation - Max: 2.290191518684573 Min: 0.0\n","Softmax Output - Max: 0.974748269419573 Min: 0.02525173058042694 Sum (first example): 1.0\n","ReLU Activation - Max: 2.568539737967928 Min: 0.0\n","Softmax Output - Max: 0.9860806077285599 Min: 0.013919392271440246 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005028401189976066 Min: -0.005028401189976066\n","Layer 0 - Gradient Weights Max: 0.00957386698658973 Min: -0.01158786436226549\n","ReLU Activation - Max: 2.291527146676669 Min: 0.0\n","Softmax Output - Max: 0.9747778217318268 Min: 0.025222178268173155 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5692438113920244 Min: 0.0\n","Softmax Output - Max: 0.9861089166095923 Min: 0.013891083390407651 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005033230708255631 Min: -0.005033230708255632\n","Layer 0 - Gradient Weights Max: 0.009612136182338575 Min: -0.011637383838943234\n","ReLU Activation - Max: 2.2928589208928667 Min: 0.0\n","Softmax Output - Max: 0.9748072724446091 Min: 0.02519272755539102 Sum (first example): 1.0\n","Epoch 890, Loss: 0.5636105521825152, Test Accuracy: 0.7210714285714286\n","ReLU Activation - Max: 2.5699464763673627 Min: 0.0\n","Softmax Output - Max: 0.9861370547097433 Min: 0.013862945290256737 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005038313527145768 Min: -0.005038313527145766\n","Layer 0 - Gradient Weights Max: 0.00955988416559522 Min: -0.0115993475477872\n","ReLU Activation - Max: 2.2941919658833188 Min: 0.0\n","Softmax Output - Max: 0.9748367705032895 Min: 0.025163229496710522 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.570660101367886 Min: 0.0\n","Softmax Output - Max: 0.9861651350271862 Min: 0.013834864972813884 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005043214992289132 Min: -0.005043214992289131\n","Layer 0 - Gradient Weights Max: 0.009543535777346845 Min: -0.011523644444562976\n","ReLU Activation - Max: 2.2955211944837446 Min: 0.0\n","Softmax Output - Max: 0.9748664405428191 Min: 0.025133559457180946 Sum (first example): 1.0\n","ReLU Activation - Max: 2.571372355453795 Min: 0.0\n","Softmax Output - Max: 0.9861934525525189 Min: 0.01380654744748119 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005047862611669031 Min: -0.005047862611669031\n","Layer 0 - Gradient Weights Max: 0.00949934021743029 Min: -0.011547156695816704\n","ReLU Activation - Max: 2.2968324178333774 Min: 0.0\n","Softmax Output - Max: 0.974896241838969 Min: 0.025103758161031036 Sum (first example): 1.0\n","ReLU Activation - Max: 2.572079715800801 Min: 0.0\n","Softmax Output - Max: 0.9862216839576309 Min: 0.013778316042368982 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005052397215344932 Min: -0.005052397215344931\n","Layer 0 - Gradient Weights Max: 0.009467044204447784 Min: -0.011523083580995916\n","ReLU Activation - Max: 2.2981692772986664 Min: 0.0\n","Softmax Output - Max: 0.9749258822351557 Min: 0.025074117764844292 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5727831907862435 Min: 0.0\n","Softmax Output - Max: 0.9862496537761177 Min: 0.013750346223882342 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005056845008103553 Min: -0.0050568450081035525\n","Layer 0 - Gradient Weights Max: 0.009513268069502406 Min: -0.011485963653679553\n","ReLU Activation - Max: 2.2995015119636704 Min: 0.0\n","Softmax Output - Max: 0.9749555705950922 Min: 0.025044429404907857 Sum (first example): 1.0\n","ReLU Activation - Max: 2.573488261075323 Min: 0.0\n","Softmax Output - Max: 0.9862777785845167 Min: 0.013722221415483264 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005061128277064097 Min: -0.005061128277064096\n","Layer 0 - Gradient Weights Max: 0.009571981137062549 Min: -0.011489458402036357\n","ReLU Activation - Max: 2.300841667070484 Min: 0.0\n","Softmax Output - Max: 0.9749852421027924 Min: 0.025014757897207696 Sum (first example): 1.0\n","ReLU Activation - Max: 2.574198776224767 Min: 0.0\n","Softmax Output - Max: 0.9863058058833677 Min: 0.013694194116632202 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00506534921503384 Min: -0.005065349215033839\n","Layer 0 - Gradient Weights Max: 0.009487691185071999 Min: -0.011467673375395717\n","ReLU Activation - Max: 2.3021740575101006 Min: 0.0\n","Softmax Output - Max: 0.9750149716821052 Min: 0.02498502831789483 Sum (first example): 1.0\n","ReLU Activation - Max: 2.574901288234485 Min: 0.0\n","Softmax Output - Max: 0.9863339105771837 Min: 0.013666089422816309 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005069507648979579 Min: -0.005069507648979579\n","Layer 0 - Gradient Weights Max: 0.009447978638197407 Min: -0.011485789329983434\n","ReLU Activation - Max: 2.3035102610596216 Min: 0.0\n","Softmax Output - Max: 0.9750447473773662 Min: 0.024955252622633775 Sum (first example): 1.0\n","ReLU Activation - Max: 2.575613248678568 Min: 0.0\n","Softmax Output - Max: 0.986361888030578 Min: 0.013638111969421907 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005073623564690705 Min: -0.005073623564690704\n","Layer 0 - Gradient Weights Max: 0.00946191289483796 Min: -0.011510966915927814\n","ReLU Activation - Max: 2.3048373765010433 Min: 0.0\n","Softmax Output - Max: 0.9750742917141116 Min: 0.024925708285888457 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5763288996749525 Min: 0.0\n","Softmax Output - Max: 0.9863898097228411 Min: 0.013610190277158877 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005078083120982281 Min: -0.005078083120982281\n","Layer 0 - Gradient Weights Max: 0.009489298751872126 Min: -0.011512931317749288\n","ReLU Activation - Max: 2.306155128142681 Min: 0.0\n","Softmax Output - Max: 0.9751038658918028 Min: 0.02489613410819722 Sum (first example): 0.9999999999999999\n","Epoch 900, Loss: 0.5628094436506588, Test Accuracy: 0.72\n","ReLU Activation - Max: 2.5770401062525212 Min: 0.0\n","Softmax Output - Max: 0.9864177847490923 Min: 0.013582215250907803 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0050824469031206885 Min: -0.0050824469031206885\n","Layer 0 - Gradient Weights Max: 0.009502061764633184 Min: -0.011483263211876734\n","ReLU Activation - Max: 2.307468364464875 Min: 0.0\n","Softmax Output - Max: 0.9751331077081957 Min: 0.02486689229180429 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5777524674426897 Min: 0.0\n","Softmax Output - Max: 0.9864456499965735 Min: 0.013554350003426504 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00508674020750534 Min: -0.00508674020750534\n","Layer 0 - Gradient Weights Max: 0.009622680976368151 Min: -0.01133063508484058\n","ReLU Activation - Max: 2.308791999666223 Min: 0.0\n","Softmax Output - Max: 0.9751615334127196 Min: 0.024838466587280485 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5784547272521516 Min: 0.0\n","Softmax Output - Max: 0.9864737202241868 Min: 0.013526279775813301 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005090726108714803 Min: -0.005090726108714804\n","Layer 0 - Gradient Weights Max: 0.00974817497858149 Min: -0.01130416315287448\n","ReLU Activation - Max: 2.310149591321549 Min: 0.0\n","Softmax Output - Max: 0.9751894714700562 Min: 0.02481052852994378 Sum (first example): 1.0\n","ReLU Activation - Max: 2.579169337731123 Min: 0.0\n","Softmax Output - Max: 0.9865016941091873 Min: 0.01349830589081259 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005094708123320621 Min: -0.005094708123320619\n","Layer 0 - Gradient Weights Max: 0.009799947635377388 Min: -0.011269577708905224\n","ReLU Activation - Max: 2.311505682931613 Min: 0.0\n","Softmax Output - Max: 0.9752171487051904 Min: 0.02478285129480961 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5798817764885444 Min: 0.0\n","Softmax Output - Max: 0.9865297344572452 Min: 0.01347026554275472 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005098678408240313 Min: -0.005098678408240311\n","Layer 0 - Gradient Weights Max: 0.009759974058300444 Min: -0.01121398329912327\n","ReLU Activation - Max: 2.3128654630072574 Min: 0.0\n","Softmax Output - Max: 0.9752448821092815 Min: 0.024755117890718447 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5805917334089585 Min: 0.0\n","Softmax Output - Max: 0.9865577493465095 Min: 0.013442250653490521 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005102261421020046 Min: -0.005102261421020046\n","Layer 0 - Gradient Weights Max: 0.00971695080052334 Min: -0.011158042515277024\n","ReLU Activation - Max: 2.314228281271117 Min: 0.0\n","Softmax Output - Max: 0.9752723965923951 Min: 0.024727603407604822 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5813076540489988 Min: 0.0\n","Softmax Output - Max: 0.9865858482504467 Min: 0.013414151749553346 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005105700389232282 Min: -0.00510570038923228\n","Layer 0 - Gradient Weights Max: 0.009768028519645477 Min: -0.011114528886539845\n","ReLU Activation - Max: 2.3155944067993124 Min: 0.0\n","Softmax Output - Max: 0.9752997210695985 Min: 0.0247002789304016 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5820222182520816 Min: 0.0\n","Softmax Output - Max: 0.9866138229473775 Min: 0.013386177052622412 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00510905696670289 Min: -0.005109056966702891\n","Layer 0 - Gradient Weights Max: 0.009736247175568176 Min: -0.011077392941619716\n","ReLU Activation - Max: 2.3169614273375423 Min: 0.0\n","Softmax Output - Max: 0.9753269903832882 Min: 0.024673009616711844 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5827410475430104 Min: 0.0\n","Softmax Output - Max: 0.9866419236490214 Min: 0.013358076350978628 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00511224198767522 Min: -0.005112241987675221\n","Layer 0 - Gradient Weights Max: 0.009707422685723704 Min: -0.011083068185946085\n","ReLU Activation - Max: 2.3183246593278164 Min: 0.0\n","Softmax Output - Max: 0.9753541980551445 Min: 0.02464580194485563 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5834586959744517 Min: 0.0\n","Softmax Output - Max: 0.9866699751380426 Min: 0.013330024861957411 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005115236391510487 Min: -0.005115236391510486\n","Layer 0 - Gradient Weights Max: 0.009642881064468542 Min: -0.011075197494075763\n","ReLU Activation - Max: 2.3196776534547205 Min: 0.0\n","Softmax Output - Max: 0.9753812942764893 Min: 0.024618705723510735 Sum (first example): 0.9999999999999999\n","Epoch 910, Loss: 0.5620023751375846, Test Accuracy: 0.7192857142857143\n","ReLU Activation - Max: 2.5841782833819993 Min: 0.0\n","Softmax Output - Max: 0.986698006700784 Min: 0.013301993299216075 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00511829505839357 Min: -0.005118295058393568\n","Layer 0 - Gradient Weights Max: 0.009647242273566524 Min: -0.01107810484009427\n","ReLU Activation - Max: 2.3210231290702543 Min: 0.0\n","Softmax Output - Max: 0.9754077213534373 Min: 0.024592278646562632 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5848986339709232 Min: 0.0\n","Softmax Output - Max: 0.9867261591743078 Min: 0.013273840825692227 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005121273849127193 Min: -0.005121273849127193\n","Layer 0 - Gradient Weights Max: 0.009675717817326608 Min: -0.011061917611901066\n","ReLU Activation - Max: 2.3223567834572623 Min: 0.0\n","Softmax Output - Max: 0.9754342664914342 Min: 0.024565733508565824 Sum (first example): 1.0\n","ReLU Activation - Max: 2.585600461328066 Min: 0.0\n","Softmax Output - Max: 0.986754122956732 Min: 0.013245877043268042 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005124218015799981 Min: -0.005124218015799983\n","Layer 0 - Gradient Weights Max: 0.009704057912158306 Min: -0.011057200432354386\n","ReLU Activation - Max: 2.323688207183137 Min: 0.0\n","Softmax Output - Max: 0.9754611525964272 Min: 0.024538847403572782 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5863131174975162 Min: 0.0\n","Softmax Output - Max: 0.9867818321909967 Min: 0.013218167809003369 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005127026035387821 Min: -0.005127026035387821\n","Layer 0 - Gradient Weights Max: 0.009622394413012407 Min: -0.011088839735852932\n","ReLU Activation - Max: 2.325022896929647 Min: 0.0\n","Softmax Output - Max: 0.975488246644497 Min: 0.024511753355502947 Sum (first example): 1.0\n","ReLU Activation - Max: 2.58701310945604 Min: 0.0\n","Softmax Output - Max: 0.9868095171880487 Min: 0.013190482811951286 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005129796819410653 Min: -0.005129796819410653\n","Layer 0 - Gradient Weights Max: 0.009629879493971552 Min: -0.011089280004618946\n","ReLU Activation - Max: 2.3263464544142676 Min: 0.0\n","Softmax Output - Max: 0.9755152539737325 Min: 0.02448474602626748 Sum (first example): 1.0\n","ReLU Activation - Max: 2.587715200794436 Min: 0.0\n","Softmax Output - Max: 0.9868371497024322 Min: 0.013162850297567875 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005132591985487126 Min: -0.005132591985487123\n","Layer 0 - Gradient Weights Max: 0.009646846703273832 Min: -0.01105349844951214\n","ReLU Activation - Max: 2.327675867077669 Min: 0.0\n","Softmax Output - Max: 0.9755423772246473 Min: 0.024457622775352798 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5884405316897783 Min: 0.0\n","Softmax Output - Max: 0.9868648393951516 Min: 0.013135160604848482 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005135328493130693 Min: -0.005135328493130694\n","Layer 0 - Gradient Weights Max: 0.00962916616181358 Min: -0.011071207892467157\n","ReLU Activation - Max: 2.3290172646488703 Min: 0.0\n","Softmax Output - Max: 0.9755694759994193 Min: 0.024430524000580635 Sum (first example): 1.0\n","ReLU Activation - Max: 2.589152303165338 Min: 0.0\n","Softmax Output - Max: 0.9868924885452522 Min: 0.01310751145474773 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005138136082770116 Min: -0.005138136082770116\n","Layer 0 - Gradient Weights Max: 0.009580909308742681 Min: -0.011050743399992393\n","ReLU Activation - Max: 2.3303685963014895 Min: 0.0\n","Softmax Output - Max: 0.975595725441614 Min: 0.024404274558385963 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5898731629881757 Min: 0.0\n","Softmax Output - Max: 0.986919760720364 Min: 0.013080239279635905 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005140768726399027 Min: -0.005140768726399026\n","Layer 0 - Gradient Weights Max: 0.009445511491907383 Min: -0.011032294672858052\n","ReLU Activation - Max: 2.331718090069206 Min: 0.0\n","Softmax Output - Max: 0.9756221400130104 Min: 0.0243778599869895 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5905974276834645 Min: 0.0\n","Softmax Output - Max: 0.9869469350597493 Min: 0.013053064940250709 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0051432675305970715 Min: -0.005143267530597071\n","Layer 0 - Gradient Weights Max: 0.009449660097433943 Min: -0.011023254634700337\n","ReLU Activation - Max: 2.3330686552290367 Min: 0.0\n","Softmax Output - Max: 0.9756485627816345 Min: 0.024351437218365485 Sum (first example): 0.9999999999999999\n","Epoch 920, Loss: 0.5611918694663438, Test Accuracy: 0.7185714285714285\n","ReLU Activation - Max: 2.5913163513952653 Min: 0.0\n","Softmax Output - Max: 0.9869740237913057 Min: 0.013025976208694326 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0051455279949990575 Min: -0.005145527994999059\n","Layer 0 - Gradient Weights Max: 0.00942342240617567 Min: -0.01092804201981034\n","ReLU Activation - Max: 2.334431261432927 Min: 0.0\n","Softmax Output - Max: 0.9756748956210485 Min: 0.024325104378951492 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5920234599822005 Min: 0.0\n","Softmax Output - Max: 0.9870009913567379 Min: 0.012999008643262167 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0051473260279861184 Min: -0.00514732602798612\n","Layer 0 - Gradient Weights Max: 0.00943641372327616 Min: -0.010920913269243807\n","ReLU Activation - Max: 2.335797187648902 Min: 0.0\n","Softmax Output - Max: 0.9757014305705277 Min: 0.02429856942947226 Sum (first example): 1.0\n","ReLU Activation - Max: 2.592717343181074 Min: 0.0\n","Softmax Output - Max: 0.9870278887716488 Min: 0.012972111228351247 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005149274897898284 Min: -0.005149274897898283\n","Layer 0 - Gradient Weights Max: 0.00942704352522264 Min: -0.01090709321551452\n","ReLU Activation - Max: 2.3371728004835104 Min: 0.0\n","Softmax Output - Max: 0.9757277049680471 Min: 0.024272295031952985 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5934045150964464 Min: 0.0\n","Softmax Output - Max: 0.9870547611012799 Min: 0.012945238898720231 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005151105477947501 Min: -0.005151105477947501\n","Layer 0 - Gradient Weights Max: 0.009467649111066984 Min: -0.01086119283082226\n","ReLU Activation - Max: 2.338547534090406 Min: 0.0\n","Softmax Output - Max: 0.9757536513204954 Min: 0.0242463486795045 Sum (first example): 1.0\n","ReLU Activation - Max: 2.594086545028683 Min: 0.0\n","Softmax Output - Max: 0.9870815278135592 Min: 0.012918472186440865 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005152871479958599 Min: -0.005152871479958596\n","Layer 0 - Gradient Weights Max: 0.009461486886763924 Min: -0.010853574837264667\n","ReLU Activation - Max: 2.33991466658717 Min: 0.0\n","Softmax Output - Max: 0.9757793492129742 Min: 0.024220650787025767 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5947658811756664 Min: 0.0\n","Softmax Output - Max: 0.9871079565077663 Min: 0.012892043492233642 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00516068940883053 Min: -0.00516068940883053\n","Layer 0 - Gradient Weights Max: 0.009584691601670135 Min: -0.01079353101798725\n","ReLU Activation - Max: 2.341288234958908 Min: 0.0\n","Softmax Output - Max: 0.975804393094274 Min: 0.02419560690572605 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5954361666098236 Min: 0.0\n","Softmax Output - Max: 0.9871339252362831 Min: 0.012866074763717 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005178049455089297 Min: -0.005178049455089296\n","Layer 0 - Gradient Weights Max: 0.009604630531288082 Min: -0.0108596869037932\n","ReLU Activation - Max: 2.3426683025486184 Min: 0.0\n","Softmax Output - Max: 0.9758293270680256 Min: 0.02417067293197424 Sum (first example): 1.0\n","ReLU Activation - Max: 2.596118929776718 Min: 0.0\n","Softmax Output - Max: 0.9871600913814538 Min: 0.012839908618546239 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005195502126137345 Min: -0.005195502126137345\n","Layer 0 - Gradient Weights Max: 0.00961290635533646 Min: -0.010871748602192586\n","ReLU Activation - Max: 2.344042057537226 Min: 0.0\n","Softmax Output - Max: 0.9758540329813586 Min: 0.02414596701864147 Sum (first example): 1.0\n","ReLU Activation - Max: 2.5967963897778095 Min: 0.0\n","Softmax Output - Max: 0.9871862342091579 Min: 0.01281376579084219 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005212888274413627 Min: -0.005212888274413627\n","Layer 0 - Gradient Weights Max: 0.009615455563138258 Min: -0.010846726512816788\n","ReLU Activation - Max: 2.3454203697391827 Min: 0.0\n","Softmax Output - Max: 0.9758788149880316 Min: 0.024121185011968364 Sum (first example): 1.0\n","ReLU Activation - Max: 2.597468764326912 Min: 0.0\n","Softmax Output - Max: 0.9872123366822109 Min: 0.012787663317789131 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005230122063258584 Min: -0.005230122063258584\n","Layer 0 - Gradient Weights Max: 0.009755372757736774 Min: -0.010864092362379476\n","ReLU Activation - Max: 2.3467949854123655 Min: 0.0\n","Softmax Output - Max: 0.9759033067876433 Min: 0.024096693212356798 Sum (first example): 0.9999999999999999\n","Epoch 930, Loss: 0.5603765703164213, Test Accuracy: 0.7196428571428571\n","ReLU Activation - Max: 2.5981320937977035 Min: 0.0\n","Softmax Output - Max: 0.987238322441168 Min: 0.012761677558832039 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005247482659505314 Min: -0.0052474826595053144\n","Layer 0 - Gradient Weights Max: 0.009790333196009107 Min: -0.01085714191358816\n","ReLU Activation - Max: 2.3481670316969585 Min: 0.0\n","Softmax Output - Max: 0.9759276761278347 Min: 0.02407232387216528 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.598791634402433 Min: 0.0\n","Softmax Output - Max: 0.9872641462016368 Min: 0.01273585379836323 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0052647462771138525 Min: -0.0052647462771138525\n","Layer 0 - Gradient Weights Max: 0.00991309771803978 Min: -0.010883136110750253\n","ReLU Activation - Max: 2.3495299095985693 Min: 0.0\n","Softmax Output - Max: 0.9759515395423657 Min: 0.024048460457634254 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.5994503056020077 Min: 0.0\n","Softmax Output - Max: 0.9872898650265761 Min: 0.012710134973423949 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005282068229681653 Min: -0.005282068229681653\n","Layer 0 - Gradient Weights Max: 0.009895360068312924 Min: -0.010894768485020042\n","ReLU Activation - Max: 2.3509225095411286 Min: 0.0\n","Softmax Output - Max: 0.975975702405226 Min: 0.024024297594773977 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6000576519214387 Min: 0.0\n","Softmax Output - Max: 0.9873154215302068 Min: 0.01268457846979312 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00529941868606821 Min: -0.005299418686068211\n","Layer 0 - Gradient Weights Max: 0.009865012819319044 Min: -0.010920937438745445\n","ReLU Activation - Max: 2.3523365011827364 Min: 0.0\n","Softmax Output - Max: 0.9759995035659753 Min: 0.024000496434024604 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.6006519758442717 Min: 0.0\n","Softmax Output - Max: 0.9873407075657629 Min: 0.012659292434237216 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005316944742313718 Min: -0.005316944742313718\n","Layer 0 - Gradient Weights Max: 0.00986896431981172 Min: -0.010872942937481525\n","ReLU Activation - Max: 2.353745012354641 Min: 0.0\n","Softmax Output - Max: 0.9760233331002297 Min: 0.02397666689977038 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6012470236865064 Min: 0.0\n","Softmax Output - Max: 0.9873662214119785 Min: 0.012633778588021413 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005334531507334612 Min: -0.005334531507334613\n","Layer 0 - Gradient Weights Max: 0.009837814137715405 Min: -0.010895421088354543\n","ReLU Activation - Max: 2.355149898025721 Min: 0.0\n","Softmax Output - Max: 0.9760472096205263 Min: 0.02395279037947372 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6018418935635723 Min: 0.0\n","Softmax Output - Max: 0.9873920263834046 Min: 0.012607973616595288 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005351993958223198 Min: -0.005351993958223199\n","Layer 0 - Gradient Weights Max: 0.009785666431813016 Min: -0.01094328849782239\n","ReLU Activation - Max: 2.356547668798603 Min: 0.0\n","Softmax Output - Max: 0.9760708973444724 Min: 0.02392910265552769 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6024276754899898 Min: 0.0\n","Softmax Output - Max: 0.9874176915548681 Min: 0.012582308445131914 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005369331564396043 Min: -0.005369331564396044\n","Layer 0 - Gradient Weights Max: 0.009760311638755651 Min: -0.010993508436719505\n","ReLU Activation - Max: 2.3579457606582754 Min: 0.0\n","Softmax Output - Max: 0.9760945739026846 Min: 0.023905426097315417 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6030258571147367 Min: 0.0\n","Softmax Output - Max: 0.987443314474092 Min: 0.012556685525908037 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005386491412773548 Min: -0.005386491412773548\n","Layer 0 - Gradient Weights Max: 0.00977196104699248 Min: -0.011016568960722761\n","ReLU Activation - Max: 2.359347612703529 Min: 0.0\n","Softmax Output - Max: 0.9761179617294816 Min: 0.02388203827051844 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6036237142596907 Min: 0.0\n","Softmax Output - Max: 0.9874684997637597 Min: 0.012531500236240141 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005403638979524224 Min: -0.005403638979524226\n","Layer 0 - Gradient Weights Max: 0.009818534307101338 Min: -0.011033080449142687\n","ReLU Activation - Max: 2.3607556223611867 Min: 0.0\n","Softmax Output - Max: 0.9761409889863512 Min: 0.023859011013648915 Sum (first example): 1.0000000000000002\n","Epoch 940, Loss: 0.5595509975153581, Test Accuracy: 0.7210714285714286\n","ReLU Activation - Max: 2.6042249994359152 Min: 0.0\n","Softmax Output - Max: 0.9874933885848014 Min: 0.012506611415198638 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005420959882228757 Min: -0.005420959882228757\n","Layer 0 - Gradient Weights Max: 0.009834043304983796 Min: -0.011038436012277188\n","ReLU Activation - Max: 2.3621612379584804 Min: 0.0\n","Softmax Output - Max: 0.9761639374615035 Min: 0.023836062538496558 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6048262477787003 Min: 0.0\n","Softmax Output - Max: 0.9875182010929524 Min: 0.012481798907047637 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005438360770279112 Min: -0.005438360770279111\n","Layer 0 - Gradient Weights Max: 0.009827106222613458 Min: -0.011024515617237106\n","ReLU Activation - Max: 2.3635690565913574 Min: 0.0\n","Softmax Output - Max: 0.9761868743727417 Min: 0.02381312562725835 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6054204562614243 Min: 0.0\n","Softmax Output - Max: 0.9875430427107939 Min: 0.012456957289206247 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005455788346816245 Min: -0.005455788346816245\n","Layer 0 - Gradient Weights Max: 0.009818989092209709 Min: -0.011032557269089673\n","ReLU Activation - Max: 2.364966181589584 Min: 0.0\n","Softmax Output - Max: 0.976210073288382 Min: 0.02378992671161786 Sum (first example): 1.0\n","ReLU Activation - Max: 2.606003454692042 Min: 0.0\n","Softmax Output - Max: 0.9875678902426898 Min: 0.012432109757310303 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0054730812518756266 Min: -0.005473081251875627\n","Layer 0 - Gradient Weights Max: 0.009858164781554124 Min: -0.01107348624279958\n","ReLU Activation - Max: 2.3663597622930475 Min: 0.0\n","Softmax Output - Max: 0.9762332003148422 Min: 0.023766799685157717 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6065860537859415 Min: 0.0\n","Softmax Output - Max: 0.9875925923624306 Min: 0.012407407637569456 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005490401254108591 Min: -0.005490401254108591\n","Layer 0 - Gradient Weights Max: 0.009876049489589861 Min: -0.011073631846434394\n","ReLU Activation - Max: 2.367766005815859 Min: 0.0\n","Softmax Output - Max: 0.9762564791763784 Min: 0.023743520823621748 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.607174781253864 Min: 0.0\n","Softmax Output - Max: 0.9876172701666085 Min: 0.012382729833391466 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005507733988299329 Min: -0.0055077339882993286\n","Layer 0 - Gradient Weights Max: 0.009915169602988053 Min: -0.011036054810984172\n","ReLU Activation - Max: 2.369172501797019 Min: 0.0\n","Softmax Output - Max: 0.9762792967643462 Min: 0.023720703235653864 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6077650859649593 Min: 0.0\n","Softmax Output - Max: 0.9876417887936434 Min: 0.012358211206356688 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005525226382139651 Min: -0.005525226382139651\n","Layer 0 - Gradient Weights Max: 0.009950790336119212 Min: -0.011010069408563913\n","ReLU Activation - Max: 2.3705722780353047 Min: 0.0\n","Softmax Output - Max: 0.9763015802967334 Min: 0.02369841970326653 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6083578520558017 Min: 0.0\n","Softmax Output - Max: 0.9876658437517031 Min: 0.01233415624829694 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005542766895012086 Min: -0.005542766895012086\n","Layer 0 - Gradient Weights Max: 0.009967819359989829 Min: -0.010982209817934905\n","ReLU Activation - Max: 2.371982119369961 Min: 0.0\n","Softmax Output - Max: 0.9763238124996951 Min: 0.023676187500304906 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6089626675888278 Min: 0.0\n","Softmax Output - Max: 0.9876902443568005 Min: 0.012309755643199497 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005560390834672451 Min: -0.00556039083467245\n","Layer 0 - Gradient Weights Max: 0.00999100150348561 Min: -0.010955738667339003\n","ReLU Activation - Max: 2.3733908778988746 Min: 0.0\n","Softmax Output - Max: 0.9763459041121982 Min: 0.023654095887801853 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6095727756535103 Min: 0.0\n","Softmax Output - Max: 0.9877144570356552 Min: 0.012285542964344871 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005577995055527281 Min: -0.005577995055527281\n","Layer 0 - Gradient Weights Max: 0.009969563002602431 Min: -0.01099134129452697\n","ReLU Activation - Max: 2.3748024629753495 Min: 0.0\n","Softmax Output - Max: 0.9763682541486226 Min: 0.0236317458513774 Sum (first example): 1.0\n","Epoch 950, Loss: 0.5587182591511519, Test Accuracy: 0.7214285714285714\n","ReLU Activation - Max: 2.6101824751419254 Min: 0.0\n","Softmax Output - Max: 0.9877384712940285 Min: 0.012261528705971596 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005595579444327335 Min: -0.005595579444327335\n","Layer 0 - Gradient Weights Max: 0.0100385746783012 Min: -0.010955991289659092\n","ReLU Activation - Max: 2.376206670333631 Min: 0.0\n","Softmax Output - Max: 0.9763902806460285 Min: 0.023609719353971454 Sum (first example): 1.0\n","ReLU Activation - Max: 2.610801009590926 Min: 0.0\n","Softmax Output - Max: 0.9877624094488944 Min: 0.012237590551105725 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005613226046026572 Min: -0.005613226046026572\n","Layer 0 - Gradient Weights Max: 0.010013299204671379 Min: -0.010940075079432324\n","ReLU Activation - Max: 2.37761828366833 Min: 0.0\n","Softmax Output - Max: 0.9764121082662898 Min: 0.023587891733710207 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6114069862375917 Min: 0.0\n","Softmax Output - Max: 0.9877862889045232 Min: 0.012213711095476757 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005630827172365116 Min: -0.005630827172365116\n","Layer 0 - Gradient Weights Max: 0.009999975442854643 Min: -0.010959936620580354\n","ReLU Activation - Max: 2.3790264417864173 Min: 0.0\n","Softmax Output - Max: 0.9764341939101822 Min: 0.023565806089817874 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.612015294262101 Min: 0.0\n","Softmax Output - Max: 0.9878102399592064 Min: 0.01218976004079362 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005648300791647048 Min: -0.0056483007916470485\n","Layer 0 - Gradient Weights Max: 0.010033163750148923 Min: -0.010956707185112704\n","ReLU Activation - Max: 2.3804319593415264 Min: 0.0\n","Softmax Output - Max: 0.9764566211552396 Min: 0.023543378844760356 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6126406336921346 Min: 0.0\n","Softmax Output - Max: 0.9878344170572848 Min: 0.012165582942715041 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0056656637532701 Min: -0.0056656637532701\n","Layer 0 - Gradient Weights Max: 0.010048495172857297 Min: -0.010900635143817738\n","ReLU Activation - Max: 2.3818340181634907 Min: 0.0\n","Softmax Output - Max: 0.9764788498302743 Min: 0.02352115016972567 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6132655514812884 Min: 0.0\n","Softmax Output - Max: 0.9878585385545386 Min: 0.01214146144546145 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005683067156967327 Min: -0.005683067156967327\n","Layer 0 - Gradient Weights Max: 0.010094718132014296 Min: -0.010860705298305632\n","ReLU Activation - Max: 2.3832327324263556 Min: 0.0\n","Softmax Output - Max: 0.9765011167170831 Min: 0.023498883282916997 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6138901011213713 Min: 0.0\n","Softmax Output - Max: 0.987882769881733 Min: 0.0121172301182671 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005700375788089462 Min: -0.005700375788089463\n","Layer 0 - Gradient Weights Max: 0.010170920193070266 Min: -0.010847160456237803\n","ReLU Activation - Max: 2.3846280123133408 Min: 0.0\n","Softmax Output - Max: 0.9765234316680149 Min: 0.023476568331985043 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.614504082083826 Min: 0.0\n","Softmax Output - Max: 0.9879067552141954 Min: 0.012093244785804674 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005717899232084123 Min: -0.005717899232084123\n","Layer 0 - Gradient Weights Max: 0.010184019297780617 Min: -0.010782304173594218\n","ReLU Activation - Max: 2.3860199142014116 Min: 0.0\n","Softmax Output - Max: 0.9765457436232077 Min: 0.02345425637679236 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6151176621031893 Min: 0.0\n","Softmax Output - Max: 0.9879306141612139 Min: 0.01206938583878614 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0057353126259241435 Min: -0.0057353126259241435\n","Layer 0 - Gradient Weights Max: 0.010194722517349195 Min: -0.010737855095537862\n","ReLU Activation - Max: 2.3874156977055696 Min: 0.0\n","Softmax Output - Max: 0.9765675366729931 Min: 0.02343246332700701 Sum (first example): 1.0\n","ReLU Activation - Max: 2.615751332860953 Min: 0.0\n","Softmax Output - Max: 0.9879543494589906 Min: 0.01204565054100924 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00575285262153749 Min: -0.005752852621537489\n","Layer 0 - Gradient Weights Max: 0.010224787725998729 Min: -0.010812746588041457\n","ReLU Activation - Max: 2.388814747587569 Min: 0.0\n","Softmax Output - Max: 0.9765888090257677 Min: 0.023411190974232176 Sum (first example): 1.0\n","Epoch 960, Loss: 0.5578845066655022, Test Accuracy: 0.7217857142857143\n","ReLU Activation - Max: 2.6163785822473 Min: 0.0\n","Softmax Output - Max: 0.9879777789555598 Min: 0.01202222104444025 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005770502291019839 Min: -0.005770502291019839\n","Layer 0 - Gradient Weights Max: 0.010247599970228614 Min: -0.010807183791707071\n","ReLU Activation - Max: 2.3902166528724336 Min: 0.0\n","Softmax Output - Max: 0.9766099300670792 Min: 0.023390069932920764 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6170015387105248 Min: 0.0\n","Softmax Output - Max: 0.9880011968937 Min: 0.01199880310629987 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0057881025557943175 Min: -0.005788102555794318\n","Layer 0 - Gradient Weights Max: 0.010232780440217209 Min: -0.010779510688916055\n","ReLU Activation - Max: 2.3916152502811956 Min: 0.0\n","Softmax Output - Max: 0.9766306321864064 Min: 0.02336936781359358 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6176240874976022 Min: 0.0\n","Softmax Output - Max: 0.9880245907377496 Min: 0.011975409262250455 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005805494740913804 Min: -0.005805494740913803\n","Layer 0 - Gradient Weights Max: 0.010263038021612676 Min: -0.01078099237289694\n","ReLU Activation - Max: 2.3930147623885905 Min: 0.0\n","Softmax Output - Max: 0.9766510836137086 Min: 0.02334891638629155 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.618248788296427 Min: 0.0\n","Softmax Output - Max: 0.9880477742567038 Min: 0.011952225743296336 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005822930947838295 Min: -0.005822930947838295\n","Layer 0 - Gradient Weights Max: 0.010318571342889892 Min: -0.0107704858168952\n","ReLU Activation - Max: 2.3944157408303646 Min: 0.0\n","Softmax Output - Max: 0.9766711429481872 Min: 0.023328857051812806 Sum (first example): 1.0\n","ReLU Activation - Max: 2.618870532482994 Min: 0.0\n","Softmax Output - Max: 0.9880708567110107 Min: 0.011929143288989297 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0058406007833567385 Min: -0.005840600783356739\n","Layer 0 - Gradient Weights Max: 0.01032080752345671 Min: -0.010773514800108198\n","ReLU Activation - Max: 2.3958135255045327 Min: 0.0\n","Softmax Output - Max: 0.9766912187441791 Min: 0.023308781255820896 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6194919621725026 Min: 0.0\n","Softmax Output - Max: 0.9880938064701819 Min: 0.01190619352981821 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005858159162930956 Min: -0.005858159162930957\n","Layer 0 - Gradient Weights Max: 0.010340811723893433 Min: -0.01080016235521208\n","ReLU Activation - Max: 2.3972158918936386 Min: 0.0\n","Softmax Output - Max: 0.9767116051535537 Min: 0.02328839484644636 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6201219608510846 Min: 0.0\n","Softmax Output - Max: 0.988116534622754 Min: 0.011883465377245961 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0058755261294812 Min: -0.005875526129481201\n","Layer 0 - Gradient Weights Max: 0.010364363722655957 Min: -0.010811315167620223\n","ReLU Activation - Max: 2.3986066424458707 Min: 0.0\n","Softmax Output - Max: 0.976732025696962 Min: 0.023267974303038053 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.620753812310385 Min: 0.0\n","Softmax Output - Max: 0.9881393021037718 Min: 0.011860697896228284 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005892775095901788 Min: -0.005892775095901788\n","Layer 0 - Gradient Weights Max: 0.010397602668763573 Min: -0.010838017223360216\n","ReLU Activation - Max: 2.3999918402831404 Min: 0.0\n","Softmax Output - Max: 0.9767526288340382 Min: 0.023247371165961907 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6214018090519406 Min: 0.0\n","Softmax Output - Max: 0.9881623170203463 Min: 0.011837682979653628 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005910048043487203 Min: -0.0059100480434872045\n","Layer 0 - Gradient Weights Max: 0.010407323568356944 Min: -0.010807375978205573\n","ReLU Activation - Max: 2.4013698842788496 Min: 0.0\n","Softmax Output - Max: 0.9767732635630433 Min: 0.023226736436956837 Sum (first example): 1.0\n","ReLU Activation - Max: 2.622046086694685 Min: 0.0\n","Softmax Output - Max: 0.9881853581620784 Min: 0.011814641837921462 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005927302495752169 Min: -0.005927302495752171\n","Layer 0 - Gradient Weights Max: 0.010445615771707523 Min: -0.010820885335727316\n","ReLU Activation - Max: 2.402747754002919 Min: 0.0\n","Softmax Output - Max: 0.9767938972771634 Min: 0.023206102722836644 Sum (first example): 0.9999999999999999\n","Epoch 970, Loss: 0.557045218090932, Test Accuracy: 0.7228571428571429\n","ReLU Activation - Max: 2.6226869754166406 Min: 0.0\n","Softmax Output - Max: 0.988208225102636 Min: 0.011791774897364065 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.005944550495900996 Min: -0.005944550495900996\n","Layer 0 - Gradient Weights Max: 0.010454944495572733 Min: -0.010885921719630058\n","ReLU Activation - Max: 2.4041288239938265 Min: 0.0\n","Softmax Output - Max: 0.9768142386596559 Min: 0.023185761340344196 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6233356017811875 Min: 0.0\n","Softmax Output - Max: 0.9882310291957055 Min: 0.011768970804294492 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005961842379207248 Min: -0.005961842379207249\n","Layer 0 - Gradient Weights Max: 0.010501605071394894 Min: -0.010858276320383694\n","ReLU Activation - Max: 2.4055066148540285 Min: 0.0\n","Softmax Output - Max: 0.9768344269995152 Min: 0.023165573000484857 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6239832538710797 Min: 0.0\n","Softmax Output - Max: 0.9882540828866764 Min: 0.011745917113323533 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005979140179783996 Min: -0.0059791401797839976\n","Layer 0 - Gradient Weights Max: 0.01049289635453678 Min: -0.010866209804543085\n","ReLU Activation - Max: 2.406876978565114 Min: 0.0\n","Softmax Output - Max: 0.9768544514811123 Min: 0.023145548518887813 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6246291815527893 Min: 0.0\n","Softmax Output - Max: 0.9882768329264595 Min: 0.01172316707354058 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.005996212291451485 Min: -0.005996212291451485\n","Layer 0 - Gradient Weights Max: 0.010487715314590123 Min: -0.01089078253289049\n","ReLU Activation - Max: 2.4082841371982573 Min: 0.0\n","Softmax Output - Max: 0.9768743736910676 Min: 0.023125626308932414 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6252747606947415 Min: 0.0\n","Softmax Output - Max: 0.9882996204964984 Min: 0.011700379503501506 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006013256771095212 Min: -0.006013256771095212\n","Layer 0 - Gradient Weights Max: 0.010541100577525707 Min: -0.01082741648064151\n","ReLU Activation - Max: 2.409709731535036 Min: 0.0\n","Softmax Output - Max: 0.9768939702091628 Min: 0.023106029790837274 Sum (first example): 1.0\n","ReLU Activation - Max: 2.625926083889383 Min: 0.0\n","Softmax Output - Max: 0.9883223203556292 Min: 0.011677679644370898 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00603047128027882 Min: -0.00603047128027882\n","Layer 0 - Gradient Weights Max: 0.010531548879879353 Min: -0.01078995603873811\n","ReLU Activation - Max: 2.4111319623282563 Min: 0.0\n","Softmax Output - Max: 0.9769133693792735 Min: 0.02308663062072647 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6265764011946677 Min: 0.0\n","Softmax Output - Max: 0.9883447090636746 Min: 0.011655290936325412 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.006047598203540114 Min: -0.006047598203540114\n","Layer 0 - Gradient Weights Max: 0.010520935638427953 Min: -0.010753433212606673\n","ReLU Activation - Max: 2.4125533954264524 Min: 0.0\n","Softmax Output - Max: 0.976932948942218 Min: 0.023067051057781816 Sum (first example): 1.0\n","ReLU Activation - Max: 2.627256217219412 Min: 0.0\n","Softmax Output - Max: 0.9883671737405175 Min: 0.011632826259482488 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006064603949548031 Min: -0.006064603949548033\n","Layer 0 - Gradient Weights Max: 0.010557934113959382 Min: -0.010769630825137044\n","ReLU Activation - Max: 2.4139718440116718 Min: 0.0\n","Softmax Output - Max: 0.9769522267157074 Min: 0.023047773284292698 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.627928406264279 Min: 0.0\n","Softmax Output - Max: 0.9883893714618275 Min: 0.011610628538172534 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00608176247297893 Min: -0.00608176247297893\n","Layer 0 - Gradient Weights Max: 0.010622858854857394 Min: -0.010736994704401186\n","ReLU Activation - Max: 2.415389334714005 Min: 0.0\n","Softmax Output - Max: 0.9769714450842825 Min: 0.023028554915717467 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6286120491231726 Min: 0.0\n","Softmax Output - Max: 0.988411350948155 Min: 0.011588649051844913 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006099029687413712 Min: -0.0060990296874137125\n","Layer 0 - Gradient Weights Max: 0.010620738939146553 Min: -0.010703669248082439\n","ReLU Activation - Max: 2.4168101192255307 Min: 0.0\n","Softmax Output - Max: 0.9769906603751752 Min: 0.023009339624824723 Sum (first example): 1.0\n","Epoch 980, Loss: 0.556204911412588, Test Accuracy: 0.7221428571428572\n","ReLU Activation - Max: 2.6292824861471162 Min: 0.0\n","Softmax Output - Max: 0.988433372735356 Min: 0.011566627264643931 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.0061161924973846 Min: -0.0061161924973846\n","Layer 0 - Gradient Weights Max: 0.01065449385065537 Min: -0.01068848159042796\n","ReLU Activation - Max: 2.418227698606221 Min: 0.0\n","Softmax Output - Max: 0.9770098263988973 Min: 0.022990173601102604 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6299516500933238 Min: 0.0\n","Softmax Output - Max: 0.9884554449461646 Min: 0.011544555053835429 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.006133337266437098 Min: -0.006133337266437098\n","Layer 0 - Gradient Weights Max: 0.010683277986130562 Min: -0.01076809837439498\n","ReLU Activation - Max: 2.4196500339906457 Min: 0.0\n","Softmax Output - Max: 0.9770288775911373 Min: 0.0229711224088626 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.630621273913521 Min: 0.0\n","Softmax Output - Max: 0.9884772655494797 Min: 0.011522734450520362 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.0061503944236751045 Min: -0.006150394423675103\n","Layer 0 - Gradient Weights Max: 0.010689951418200642 Min: -0.010753679032320818\n","ReLU Activation - Max: 2.421065299434244 Min: 0.0\n","Softmax Output - Max: 0.9770483519441476 Min: 0.02295164805585233 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6312938443972946 Min: 0.0\n","Softmax Output - Max: 0.9884991976121809 Min: 0.011500802387819156 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006167398404174568 Min: -0.00616739840417457\n","Layer 0 - Gradient Weights Max: 0.01067867354215692 Min: -0.01072163794633702\n","ReLU Activation - Max: 2.4224788466128473 Min: 0.0\n","Softmax Output - Max: 0.9770679444154947 Min: 0.02293205558450536 Sum (first example): 1.0\n","ReLU Activation - Max: 2.631962019252901 Min: 0.0\n","Softmax Output - Max: 0.9885211233951605 Min: 0.011478876604839571 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006184193433977017 Min: -0.006184193433977017\n","Layer 0 - Gradient Weights Max: 0.01069451618277372 Min: -0.01072417028954817\n","ReLU Activation - Max: 2.423907549134941 Min: 0.0\n","Softmax Output - Max: 0.9770881020092234 Min: 0.02291189799077666 Sum (first example): 1.0\n","ReLU Activation - Max: 2.632601248743959 Min: 0.0\n","Softmax Output - Max: 0.9885429787746138 Min: 0.01145702122538615 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006201069040132707 Min: -0.006201069040132707\n","Layer 0 - Gradient Weights Max: 0.010730119415848812 Min: -0.010600569161141606\n","ReLU Activation - Max: 2.4253243713823736 Min: 0.0\n","Softmax Output - Max: 0.9771083310635157 Min: 0.022891668936484307 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.6332431045655253 Min: 0.0\n","Softmax Output - Max: 0.9885649257486837 Min: 0.011435074251316367 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006217838113673641 Min: -0.0062178381136736415\n","Layer 0 - Gradient Weights Max: 0.010773084399917095 Min: -0.010591588156838489\n","ReLU Activation - Max: 2.426724393818783 Min: 0.0\n","Softmax Output - Max: 0.977128439837725 Min: 0.022871560162274995 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.633872249843372 Min: 0.0\n","Softmax Output - Max: 0.988587138245067 Min: 0.011412861754932987 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006234501600407473 Min: -0.006234501600407471\n","Layer 0 - Gradient Weights Max: 0.01079607553008371 Min: -0.010606647400726919\n","ReLU Activation - Max: 2.428121124821661 Min: 0.0\n","Softmax Output - Max: 0.9771484789308349 Min: 0.022851521069165017 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.6345000503532265 Min: 0.0\n","Softmax Output - Max: 0.9886093357786209 Min: 0.011390664221379087 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006251227654042515 Min: -0.006251227654042516\n","Layer 0 - Gradient Weights Max: 0.010812330771010136 Min: -0.010634551217343169\n","ReLU Activation - Max: 2.4295204865513855 Min: 0.0\n","Softmax Output - Max: 0.9771684290720909 Min: 0.022831570927909002 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.6351216051358906 Min: 0.0\n","Softmax Output - Max: 0.988631490336858 Min: 0.011368509663142063 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006268042838590494 Min: -0.006268042838590494\n","Layer 0 - Gradient Weights Max: 0.010834982567540563 Min: -0.010628001659403578\n","ReLU Activation - Max: 2.4309166195569847 Min: 0.0\n","Softmax Output - Max: 0.9771881808574636 Min: 0.022811819142536528 Sum (first example): 1.0\n","Epoch 990, Loss: 0.5553657721260521, Test Accuracy: 0.7232142857142857\n","ReLU Activation - Max: 2.635742012178307 Min: 0.0\n","Softmax Output - Max: 0.988653331207346 Min: 0.011346668792654082 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006284896870879002 Min: -0.006284896870879002\n","Layer 0 - Gradient Weights Max: 0.010855285045931017 Min: -0.010579344130982936\n","ReLU Activation - Max: 2.4323186867391438 Min: 0.0\n","Softmax Output - Max: 0.9772077840699355 Min: 0.022792215930064485 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6363522386165923 Min: 0.0\n","Softmax Output - Max: 0.9886751732091991 Min: 0.011324826790800959 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006301707806940343 Min: -0.0063017078069403445\n","Layer 0 - Gradient Weights Max: 0.010850766164925712 Min: -0.010594317369074471\n","ReLU Activation - Max: 2.4337147102106376 Min: 0.0\n","Softmax Output - Max: 0.9772272903855682 Min: 0.022772709614431882 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6369602213368966 Min: 0.0\n","Softmax Output - Max: 0.9886970639036542 Min: 0.01130293609634582 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006318432070106825 Min: -0.006318432070106823\n","Layer 0 - Gradient Weights Max: 0.01085092960694313 Min: -0.010610657865315206\n","ReLU Activation - Max: 2.435116803325469 Min: 0.0\n","Softmax Output - Max: 0.9772469942349965 Min: 0.022753005765003578 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.637567775529037 Min: 0.0\n","Softmax Output - Max: 0.9887187574501498 Min: 0.01128124254985023 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00633523772652714 Min: -0.00633523772652714\n","Layer 0 - Gradient Weights Max: 0.010873034527729074 Min: -0.010570609340425065\n","ReLU Activation - Max: 2.436516038099882 Min: 0.0\n","Softmax Output - Max: 0.9772671496846494 Min: 0.022732850315350498 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6381833089122595 Min: 0.0\n","Softmax Output - Max: 0.9887402668499756 Min: 0.011259733150024397 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.00635194366332424 Min: -0.006351943663324238\n","Layer 0 - Gradient Weights Max: 0.010851004853646455 Min: -0.01055991622772002\n","ReLU Activation - Max: 2.437920733227435 Min: 0.0\n","Softmax Output - Max: 0.9772872124538639 Min: 0.022712787546136166 Sum (first example): 1.0\n","ReLU Activation - Max: 2.638795822231558 Min: 0.0\n","Softmax Output - Max: 0.988761751877592 Min: 0.011238248122408017 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006368627010684596 Min: -0.006368627010684596\n","Layer 0 - Gradient Weights Max: 0.010842540586659963 Min: -0.010523693077712169\n","ReLU Activation - Max: 2.439327346195729 Min: 0.0\n","Softmax Output - Max: 0.9773070436494001 Min: 0.022692956350599928 Sum (first example): 0.9999999999999999\n","ReLU Activation - Max: 2.6394090377272623 Min: 0.0\n","Softmax Output - Max: 0.9887834749598482 Min: 0.011216525040151852 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.006385131263678316 Min: -0.006385131263678314\n","Layer 0 - Gradient Weights Max: 0.010846093148718481 Min: -0.010513431750767962\n","ReLU Activation - Max: 2.4407302456695614 Min: 0.0\n","Softmax Output - Max: 0.9773268263275224 Min: 0.022673173672477592 Sum (first example): 1.0000000000000002\n","ReLU Activation - Max: 2.640016238459089 Min: 0.0\n","Softmax Output - Max: 0.9888049207856279 Min: 0.01119507921437219 Sum (first example): 1.0\n","Layer 1 - Gradient Weights Max: 0.006401474628956768 Min: -0.006401474628956768\n","Layer 0 - Gradient Weights Max: 0.010833869335975292 Min: -0.010540240341304508\n","ReLU Activation - Max: 2.4421121155855467 Min: 0.0\n","Softmax Output - Max: 0.9773464082349641 Min: 0.022653591765035785 Sum (first example): 1.0\n","ReLU Activation - Max: 2.6406452396231854 Min: 0.0\n","Softmax Output - Max: 0.9888260577322086 Min: 0.011173942267791401 Sum (first example): 0.9999999999999999\n","Layer 1 - Gradient Weights Max: 0.006417616091719113 Min: -0.006417616091719112\n","Layer 0 - Gradient Weights Max: 0.010857621423150526 Min: -0.010501990617443469\n","ReLU Activation - Max: 2.4434907743479566 Min: 0.0\n","Softmax Output - Max: 0.9773657914749654 Min: 0.02263420852503468 Sum (first example): 1.0\n"]}],"source":["training_losses, test_accuracies = train(\n","    model=model,\n","    X_train=X_train_normalized,\n","    y_train=y_train_one_hot,\n","    X_test=X_test_normalized,\n","    y_test=y_test_one_hot,\n","    epochs=1000\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":630,"status":"ok","timestamp":1711355478136,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"O_bLObSzuest","outputId":"b5520fb0-3045-4219-ebc3-91678eba4797"},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU Activation - Max: 2.641272872749901 Min: 0.0\n","Softmax Output - Max: 0.9888474012320201 Min: 0.011152598767979818 Sum (first example): 1.0\n","ReLU Activation - Max: 2.4434907743479566 Min: 0.0\n","Softmax Output - Max: 0.9773657914749654 Min: 0.02263420852503468 Sum (first example): 1.0\n","Training Accuracy: 71.88%\n","Test Accuracy: 72.32%\n"]}],"source":["def predict(model, X):\n","    \"\"\"\n","    Predicts the class labels for the given input X using the model.\n","\n","    Parameters:\n","    - model: An instance of the FullyConnectedNeuralNetwork class.\n","    - X: Input data, a Numpy array of shape (n_samples, n_features).\n","\n","    Returns:\n","    - predictions: An array of predicted class labels.\n","    \"\"\"\n","    softmax_outputs, _ = model.forward_pass(X)\n","    predictions = np.argmax(softmax_outputs, axis=1)\n","    return predictions\n","\n","def calculate_accuracy(predictions, y_true):\n","    \"\"\"\n","    Calculates the accuracy given predictions and true labels.\n","\n","    Parameters:\n","    - predictions: An array of predicted class labels.\n","    - y_true: True class labels, a Numpy array of shape (n_samples, ).\n","              Assumes y_true is not one-hot encoded.\n","\n","    Returns:\n","    - accuracy: Classification accuracy as a float.\n","    \"\"\"\n","    y_true_labels = np.argmax(y_true, axis=1)  # Convert one-hot encoding to class labels\n","    accuracy = np.mean(predictions == y_true_labels)\n","    return accuracy\n","\n","# Use the predict function to get predictions for training and test sets\n","train_predictions = predict(model, X_train_normalized)\n","test_predictions = predict(model, X_test_normalized)\n","\n","# Calculate accuracy\n","train_accuracy = calculate_accuracy(train_predictions, y_train_one_hot)\n","test_accuracy = calculate_accuracy(test_predictions, y_test_one_hot)\n","\n","# Print the accuracies\n","print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1711355556746,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"_S0d-BE9vIqB","outputId":"f3762526-9a73-41d5-cb45-cf4135c4c729"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix for the Test Set:\n","[[1052  403]\n"," [ 372  973]]\n"]}],"source":["def confusion_matrix(y_true, predictions, num_classes):\n","    \"\"\"\n","    Constructs a confusion matrix.\n","\n","    Parameters:\n","    - y_true: True class labels, a Numpy array of shape (n_samples, ). Assumes labels are integers.\n","    - predictions: Predicted class labels, a Numpy array of shape (n_samples, ).\n","    - num_classes: The number of classes.\n","\n","    Returns:\n","    - cm: The confusion matrix, a 2D Numpy array of shape (num_classes, num_classes).\n","    \"\"\"\n","    cm = np.zeros((num_classes, num_classes), dtype=int)\n","    for true_label, predicted_label in zip(y_true, predictions):\n","        cm[true_label, predicted_label] += 1\n","    return cm\n","\n","y_test_labels = np.argmax(y_test_one_hot, axis=1)  # Convert one-hot encoding to class labels\n","\n","# Number of classes\n","num_classes = y_test_one_hot.shape[1]\n","\n","# Generate confusion matrix for the test set\n","test_cm = confusion_matrix(y_test_labels, test_predictions, num_classes)\n","\n","# Print the confusion matrix\n","print(\"Confusion Matrix for the Test Set:\")\n","print(test_cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1711355603860,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"5WdJFX9Cvcy5","outputId":"760b1dd8-0ed8-4e4b-9935-7c1981790562"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class 0: F1 Score = 0.7308\n","Class 1: F1 Score = 0.7152\n"]}],"source":["def classwise_f1_scores(confusion_matrix):\n","    \"\"\"\n","    Calculates F1 scores for each class based on the confusion matrix.\n","\n","    Parameters:\n","    - confusion_matrix: The confusion matrix as a 2D Numpy array.\n","\n","    Returns:\n","    - f1_scores: A Numpy array containing the F1 score for each class.\n","    \"\"\"\n","    num_classes = confusion_matrix.shape[0]\n","    f1_scores = np.zeros(num_classes)\n","\n","    for i in range(num_classes):\n","        tp = confusion_matrix[i, i]\n","        fp = np.sum(confusion_matrix[:, i]) - tp\n","        fn = np.sum(confusion_matrix[i, :]) - tp\n","        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        f1_scores[i] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    return f1_scores\n","\n","f1_scores = classwise_f1_scores(test_cm)\n","\n","# Print class-wise F1 scores\n","for i, score in enumerate(f1_scores):\n","    print(f\"Class {i}: F1 Score = {score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"executionInfo":{"elapsed":1303,"status":"ok","timestamp":1711355714910,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"OpDTE_dbv8qX","outputId":"aa36c54f-2e67-4132-b9a6-b3de542be229"},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU Activation - Max: 2.4434907743479566 Min: 0.0\n","Softmax Output - Max: 0.9773657914749654 Min: 0.02263420852503468 Sum (first example): 1.0\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 800x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def compute_tpr_fpr(y_true, scores, threshold):\n","    \"\"\"\n","    Compute True Positive Rate and False Positive Rate for a given threshold.\n","    \"\"\"\n","    # Predict positive if score >= threshold\n","    predictions = (scores >= threshold).astype(int)\n","\n","    tp = np.sum((y_true == 1) & (predictions == 1))\n","    fn = np.sum((y_true == 1) & (predictions == 0))\n","    fp = np.sum((y_true == 0) & (predictions == 1))\n","    tn = np.sum((y_true == 0) & (predictions == 0))\n","\n","    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","\n","    return tpr, fpr\n","\n","# Class 0 is the \"positive\" class\n","y_true_binary = (np.argmax(y_test_one_hot, axis=1) == 0).astype(int)\n","scores = model.forward_pass(X_test_normalized)[0][:, 0]  # Softmax scores for class 0\n","\n","# Select 10 thresholds spaced across the score range\n","thresholds = np.linspace(scores.min(), scores.max(), 10)\n","tprs = []\n","fprs = []\n","\n","for threshold in thresholds:\n","    tpr, fpr = compute_tpr_fpr(y_true_binary, scores, threshold)\n","    tprs.append(tpr)\n","    fprs.append(fpr)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fprs, tprs, marker='o', linestyle='-', color='b')\n","plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random Chance')\n","plt.title('ROC Curve')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.grid(True)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uSN04c8FwOJi"},"source":["Now doing it for MSE loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhV7QA1jwU8l"},"outputs":[],"source":["model_mse = FullyConnectedNeuralNetwork(\n","    input_size=X_train_normalized.shape[1],  # Number of input features\n","    output_size=y_train_one_hot.shape[1],    # Number of output classes\n","    hidden_layers=[32],                      # Example: simplified model with one hidden layer of 32 neurons\n","    loss_function='mse',                     # Using Mean Squared Error as the loss function\n","    learning_rate=0.05                       # Learning rate\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21306,"status":"ok","timestamp":1711357769771,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"uIxfiEHd3tBg","outputId":"1ec11e09-ba76-4234-9602-a2b5b848d5ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU Activation - Max: 0.2019943069868658 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.003551642693575914 Min: -0.011091351975374919\n","Layer 0 - Gradient Weights Max: 0.004176370279166576 Min: -0.003217224811177584\n","ReLU Activation - Max: 0.1780454791443938 Min: 0.0\n","Epoch 0, Loss: 1.0011, Test Accuracy: 0.4196\n","ReLU Activation - Max: 0.20259006139697944 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0032024576351233247 Min: -0.01062256603618147\n","Layer 0 - Gradient Weights Max: 0.004042279423125034 Min: -0.0031576045515993786\n","ReLU Activation - Max: 0.17727783179988524 Min: 0.0\n","ReLU Activation - Max: 0.2032014576960629 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.002879144695372172 Min: -0.010183869944946602\n","Layer 0 - Gradient Weights Max: 0.003909217880912611 Min: -0.003076838086422487\n","ReLU Activation - Max: 0.17658423745305263 Min: 0.0\n","ReLU Activation - Max: 0.20382463322945135 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0025791534049588558 Min: -0.00977264367162106\n","Layer 0 - Gradient Weights Max: 0.0037905699447217966 Min: -0.0029972727149034043\n","ReLU Activation - Max: 0.17595807751531048 Min: 0.0\n","ReLU Activation - Max: 0.2044552585916766 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0023001437109419604 Min: -0.009386459675937929\n","Layer 0 - Gradient Weights Max: 0.003659115453818643 Min: -0.0029248802602948125\n","ReLU Activation - Max: 0.17539188496083666 Min: 0.0\n","ReLU Activation - Max: 0.20508809313830445 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.002040086853377204 Min: -0.009023309289987716\n","Layer 0 - Gradient Weights Max: 0.0035481100504436125 Min: -0.0028428356890940042\n","ReLU Activation - Max: 0.1748784329379275 Min: 0.0\n","ReLU Activation - Max: 0.2057208708732869 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0017971774444327632 Min: -0.008681328810476369\n","Layer 0 - Gradient Weights Max: 0.0034369880299843342 Min: -0.0027530384965098412\n","ReLU Activation - Max: 0.1744124832494363 Min: 0.0\n","ReLU Activation - Max: 0.20635192594389504 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0015698568157134856 Min: -0.008358840902810064\n","Layer 0 - Gradient Weights Max: 0.003316585906080931 Min: -0.0026854573079196304\n","ReLU Activation - Max: 0.1739884304861681 Min: 0.0\n","ReLU Activation - Max: 0.20697822075838604 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0013567417434586513 Min: -0.008054418673529911\n","Layer 0 - Gradient Weights Max: 0.0032060330437967757 Min: -0.0026011398558524854\n","ReLU Activation - Max: 0.1736017808279587 Min: 0.0\n","ReLU Activation - Max: 0.20759680318325305 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.001156625969559717 Min: -0.007766767038730671\n","Layer 0 - Gradient Weights Max: 0.003096016968210106 Min: -0.0025289375021182213\n","ReLU Activation - Max: 0.17324935488237553 Min: 0.0\n","ReLU Activation - Max: 0.20820651785866534 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0009684283838222753 Min: -0.007494698602292842\n","Layer 0 - Gradient Weights Max: 0.002995661775135151 Min: -0.0024517423149919295\n","ReLU Activation - Max: 0.1729274529607243 Min: 0.0\n","Epoch 10, Loss: 0.6784, Test Accuracy: 0.4407\n","ReLU Activation - Max: 0.20880582413682944 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0007911748186131805 Min: -0.007237219923705761\n","Layer 0 - Gradient Weights Max: 0.0028869508936199047 Min: -0.002380319492841641\n","ReLU Activation - Max: 0.17263283685199365 Min: 0.0\n","ReLU Activation - Max: 0.20939312307956284 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0006239942943172794 Min: -0.0069933369002411205\n","Layer 0 - Gradient Weights Max: 0.0027868332318884974 Min: -0.002315397514226726\n","ReLU Activation - Max: 0.17236272041020645 Min: 0.0\n","ReLU Activation - Max: 0.2099671098479266 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.00046612130143607966 Min: -0.006762208471718789\n","Layer 0 - Gradient Weights Max: 0.00268990484954651 Min: -0.002241922661394175\n","ReLU Activation - Max: 0.1721145392160104 Min: 0.0\n","ReLU Activation - Max: 0.21052687548278742 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.0003168509167568974 Min: -0.006543091239158479\n","Layer 0 - Gradient Weights Max: 0.002593051746366415 Min: -0.002181924869402297\n","ReLU Activation - Max: 0.17188621553547276 Min: 0.0\n","ReLU Activation - Max: 0.21107211761740596 Min: 0.0\n","Layer 1 - Gradient Weights Max: -0.00017554946526792564 Min: -0.006335204258756952\n","Layer 0 - Gradient Weights Max: 0.002507889448455508 Min: -0.0021223460052572345\n","ReLU Activation - Max: 0.17167534793721717 Min: 0.0\n","ReLU Activation - Max: 0.2116027597864587 Min: 0.0\n","Layer 1 - Gradient Weights Max: -4.16680764003314e-05 Min: -0.00613791483120815\n","Layer 0 - Gradient Weights Max: 0.002412257123896038 Min: -0.0020599739769070354\n","ReLU Activation - Max: 0.17148085780657304 Min: 0.0\n","ReLU Activation - Max: 0.2121176155572913 Min: 0.0\n","Layer 1 - Gradient Weights Max: 8.534059014692282e-05 Min: -0.005950583149613648\n","Layer 0 - Gradient Weights Max: 0.00242976796733551 Min: -0.002002904930005628\n","ReLU Activation - Max: 0.1713007340569715 Min: 0.0\n","ReLU Activation - Max: 0.21261680510937353 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0002059401520906962 Min: -0.005772651657562092\n","Layer 0 - Gradient Weights Max: 0.002446223668934452 Min: -0.001946422036591676\n","ReLU Activation - Max: 0.17113286823216042 Min: 0.0\n","ReLU Activation - Max: 0.21310120012309675 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00032056221906476756 Min: -0.0056036127967235404\n","Layer 0 - Gradient Weights Max: 0.002448764270234929 Min: -0.0018865045873980163\n","ReLU Activation - Max: 0.17097622819208813 Min: 0.0\n","ReLU Activation - Max: 0.21356966291785467 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0004296131131364057 Min: -0.00544296809232344\n","Layer 0 - Gradient Weights Max: 0.0024575522640564776 Min: -0.0018315173805893853\n","ReLU Activation - Max: 0.17082939513014367 Min: 0.0\n","Epoch 20, Loss: 0.5634, Test Accuracy: 0.4961\n","ReLU Activation - Max: 0.2140214213884003 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.000533452962876724 Min: -0.005290262002485361\n","Layer 0 - Gradient Weights Max: 0.0024663691581129255 Min: -0.0017836669355024547\n","ReLU Activation - Max: 0.1706914470489395 Min: 0.0\n","ReLU Activation - Max: 0.21445748593700914 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0006324134241977562 Min: -0.005145118792078085\n","Layer 0 - Gradient Weights Max: 0.0024681530335324228 Min: -0.0017365899008940369\n","ReLU Activation - Max: 0.1705616056023117 Min: 0.0\n","ReLU Activation - Max: 0.2148784872450783 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.000726800249159045 Min: -0.0050070917615290585\n","Layer 0 - Gradient Weights Max: 0.0024675173005571224 Min: -0.0016788435040504843\n","ReLU Activation - Max: 0.1704388316840438 Min: 0.0\n","ReLU Activation - Max: 0.21528454914289033 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.000816897942245427 Min: -0.0048758143331844635\n","Layer 0 - Gradient Weights Max: 0.00248539631501684 Min: -0.001652194072107987\n","ReLU Activation - Max: 0.17032225467657633 Min: 0.0\n","ReLU Activation - Max: 0.21567617425286026 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0009029806672304327 Min: -0.004750949091034351\n","Layer 0 - Gradient Weights Max: 0.002482751312325548 Min: -0.0016326106327314309\n","ReLU Activation - Max: 0.17021114244683885 Min: 0.0\n","ReLU Activation - Max: 0.21605344010059419 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0009852999313079726 Min: -0.004632153470602044\n","Layer 0 - Gradient Weights Max: 0.002502322016835037 Min: -0.0016205587195348243\n","ReLU Activation - Max: 0.17010512867269764 Min: 0.0\n","ReLU Activation - Max: 0.21641621416190765 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.001064094120389798 Min: -0.004519129324140628\n","Layer 0 - Gradient Weights Max: 0.002513244348209764 Min: -0.001601319630802797\n","ReLU Activation - Max: 0.17000396222598732 Min: 0.0\n","ReLU Activation - Max: 0.21676448138025292 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00113957166064214 Min: -0.004411588221103455\n","Layer 0 - Gradient Weights Max: 0.002507060096975151 Min: -0.0015852115021775985\n","ReLU Activation - Max: 0.16990662804097445 Min: 0.0\n","ReLU Activation - Max: 0.21709916883230826 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0012119211255313522 Min: -0.004309251290410483\n","Layer 0 - Gradient Weights Max: 0.002499428924983158 Min: -0.0015755997105388023\n","ReLU Activation - Max: 0.16981250221097385 Min: 0.0\n","ReLU Activation - Max: 0.21742150236495017 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0012813242941514874 Min: -0.004211863498861071\n","Layer 0 - Gradient Weights Max: 0.0025117624951355717 Min: -0.001561860712541593\n","ReLU Activation - Max: 0.16972140906236646 Min: 0.0\n","Epoch 30, Loss: 0.5223, Test Accuracy: 0.5179\n","ReLU Activation - Max: 0.21773041288327646 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0013479606091127366 Min: -0.0041228185902106584\n","Layer 0 - Gradient Weights Max: 0.002521662515871123 Min: -0.0015459759593707259\n","ReLU Activation - Max: 0.16963336321755326 Min: 0.0\n","ReLU Activation - Max: 0.21802677868686 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0014119919732755281 Min: -0.004054943347117647\n","Layer 0 - Gradient Weights Max: 0.002512515138077564 Min: -0.0015324326546255717\n","ReLU Activation - Max: 0.16954751050263886 Min: 0.0\n","ReLU Activation - Max: 0.2183106311763739 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.001473563768310736 Min: -0.003990477823563606\n","Layer 0 - Gradient Weights Max: 0.0025161016809686905 Min: -0.0015197381303687744\n","ReLU Activation - Max: 0.16946380549068585 Min: 0.0\n","ReLU Activation - Max: 0.21858326955461232 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0015328096111156821 Min: -0.003929270766180483\n","Layer 0 - Gradient Weights Max: 0.0025339158256789426 Min: -0.0015283292194599597\n","ReLU Activation - Max: 0.1693815979101484 Min: 0.0\n","ReLU Activation - Max: 0.21884411856670707 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0015898664166519064 Min: -0.003871174656414904\n","Layer 0 - Gradient Weights Max: 0.0025218577409930306 Min: -0.001542657023762977\n","ReLU Activation - Max: 0.16930078789245792 Min: 0.0\n","ReLU Activation - Max: 0.2190939013110697 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0016448878569335695 Min: -0.0038160526146171886\n","Layer 0 - Gradient Weights Max: 0.0025250570268677704 Min: -0.0015466470899620937\n","ReLU Activation - Max: 0.16922114620759424 Min: 0.0\n","ReLU Activation - Max: 0.21933302129412247 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0016979900901071211 Min: -0.003763771784189126\n","Layer 0 - Gradient Weights Max: 0.002529216700966957 Min: -0.0015557464029808674\n","ReLU Activation - Max: 0.16914231034869145 Min: 0.0\n","ReLU Activation - Max: 0.21956223843096095 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.001749265525594702 Min: -0.003714205095961038\n","Layer 0 - Gradient Weights Max: 0.0025288088889146737 Min: -0.0015633477757045179\n","ReLU Activation - Max: 0.1690641712992722 Min: 0.0\n","ReLU Activation - Max: 0.21978156748097497 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0017988174639617283 Min: -0.0036672370168426894\n","Layer 0 - Gradient Weights Max: 0.0025317090697014 Min: -0.001564820724896862\n","ReLU Activation - Max: 0.168986943509543 Min: 0.0\n","ReLU Activation - Max: 0.21999127890205014 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0018467331490075864 Min: -0.0036227539856328135\n","Layer 0 - Gradient Weights Max: 0.002535347294675837 Min: -0.0015694796406116088\n","ReLU Activation - Max: 0.1689107214042887 Min: 0.0\n","Epoch 40, Loss: 0.5076, Test Accuracy: 0.5296\n","ReLU Activation - Max: 0.22019220900617317 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0018931588167475754 Min: -0.0035806454374477255\n","Layer 0 - Gradient Weights Max: 0.0025532522417151095 Min: -0.0015860415599723361\n","ReLU Activation - Max: 0.16883529513326784 Min: 0.0\n","ReLU Activation - Max: 0.2203846100507587 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0019381717442053929 Min: -0.0035408076893924368\n","Layer 0 - Gradient Weights Max: 0.0025579052192177165 Min: -0.0015980358093502095\n","ReLU Activation - Max: 0.16876050260785885 Min: 0.0\n","ReLU Activation - Max: 0.22056854873733805 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.001981845741301182 Min: -0.003503138607844923\n","Layer 0 - Gradient Weights Max: 0.002551700512266558 Min: -0.001611200901391157\n","ReLU Activation - Max: 0.16868605346967255 Min: 0.0\n","ReLU Activation - Max: 0.22074412016103562 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002024274505845116 Min: -0.0034675425545309143\n","Layer 0 - Gradient Weights Max: 0.00254285146290644 Min: -0.001614633293873851\n","ReLU Activation - Max: 0.1686118694138737 Min: 0.0\n","ReLU Activation - Max: 0.22091212595188892 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0020655277685750267 Min: -0.0034339319887900267\n","Layer 0 - Gradient Weights Max: 0.0025429145707420706 Min: -0.0016251544236800715\n","ReLU Activation - Max: 0.16853780111213976 Min: 0.0\n","ReLU Activation - Max: 0.2210722262555928 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002105658725127579 Min: -0.0034022253661935816\n","Layer 0 - Gradient Weights Max: 0.0025498615098876664 Min: -0.0016332549555995574\n","ReLU Activation - Max: 0.1684636586276722 Min: 0.0\n","ReLU Activation - Max: 0.2212251001255024 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002144754848595226 Min: -0.0033723388730426615\n","Layer 0 - Gradient Weights Max: 0.0025436874541498395 Min: -0.001642099702621504\n","ReLU Activation - Max: 0.16838963923409409 Min: 0.0\n","ReLU Activation - Max: 0.2213709620953625 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0021829083574535953 Min: -0.003344192601056646\n","Layer 0 - Gradient Weights Max: 0.00253978335563281 Min: -0.0016490900860177777\n","ReLU Activation - Max: 0.16831537326314813 Min: 0.0\n","ReLU Activation - Max: 0.22151056802472815 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0022201319844659772 Min: -0.0033177139989611928\n","Layer 0 - Gradient Weights Max: 0.002561549851550953 Min: -0.0016628291519458003\n","ReLU Activation - Max: 0.16824079642731893 Min: 0.0\n","ReLU Activation - Max: 0.22164360597424296 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0022564938062263605 Min: -0.003292833460079248\n","Layer 0 - Gradient Weights Max: 0.002559155281190995 Min: -0.001665947262471524\n","ReLU Activation - Max: 0.16816619965884952 Min: 0.0\n","Epoch 50, Loss: 0.5021, Test Accuracy: 0.5496\n","ReLU Activation - Max: 0.22177039008609928 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0022920752219769904 Min: -0.0032694836511121092\n","Layer 0 - Gradient Weights Max: 0.0025709487102034464 Min: -0.001679196810815353\n","ReLU Activation - Max: 0.16809153917090786 Min: 0.0\n","ReLU Activation - Max: 0.22189119378140096 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0023269240637381725 Min: -0.0032476014038944857\n","Layer 0 - Gradient Weights Max: 0.0025761903721706635 Min: -0.0016920683264427569\n","ReLU Activation - Max: 0.16801671123419812 Min: 0.0\n","ReLU Activation - Max: 0.2220067503842602 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002361100667725078 Min: -0.003227122910292138\n","Layer 0 - Gradient Weights Max: 0.0025965626958912537 Min: -0.0017110848815850318\n","ReLU Activation - Max: 0.16794127329823794 Min: 0.0\n","ReLU Activation - Max: 0.22211787419564685 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002394677751582058 Min: -0.003207998829756554\n","Layer 0 - Gradient Weights Max: 0.0026070940576366107 Min: -0.001723864779438674\n","ReLU Activation - Max: 0.16786561911573356 Min: 0.0\n","ReLU Activation - Max: 0.22222394178190186 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002427669569365587 Min: -0.0031901725188076373\n","Layer 0 - Gradient Weights Max: 0.0026101184703401776 Min: -0.0017323153933646728\n","ReLU Activation - Max: 0.16778971192614517 Min: 0.0\n","ReLU Activation - Max: 0.22232552431020045 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002460064821056673 Min: -0.0031735862503579934\n","Layer 0 - Gradient Weights Max: 0.0026278292115599006 Min: -0.001736361010837119\n","ReLU Activation - Max: 0.16771323273700278 Min: 0.0\n","ReLU Activation - Max: 0.22242265294994448 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024919512622412535 Min: -0.003158187497397476\n","Layer 0 - Gradient Weights Max: 0.0026466959516223522 Min: -0.001744750936448508\n","ReLU Activation - Max: 0.16763659124552752 Min: 0.0\n","ReLU Activation - Max: 0.22251507425050399 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025233789974187307 Min: -0.003157892815366445\n","Layer 0 - Gradient Weights Max: 0.002645161731554286 Min: -0.001751429758502936\n","ReLU Activation - Max: 0.16755948112021637 Min: 0.0\n","ReLU Activation - Max: 0.22260325285447372 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002554421205144669 Min: -0.003159658676782742\n","Layer 0 - Gradient Weights Max: 0.0026702867668499616 Min: -0.0017650743275745135\n","ReLU Activation - Max: 0.16748211162754023 Min: 0.0\n","ReLU Activation - Max: 0.22268740050529173 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025850650897832894 Min: -0.0031623847324081457\n","Layer 0 - Gradient Weights Max: 0.002655426535745763 Min: -0.0017772356724966465\n","ReLU Activation - Max: 0.16740454433837929 Min: 0.0\n","Epoch 60, Loss: 0.5000, Test Accuracy: 0.5632\n","ReLU Activation - Max: 0.2227678006816973 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026153460994618 Min: -0.0031660359789008993\n","Layer 0 - Gradient Weights Max: 0.0026934762591957952 Min: -0.0017898615350242185\n","ReLU Activation - Max: 0.1673262108353442 Min: 0.0\n","ReLU Activation - Max: 0.2228446748392887 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026453050378672602 Min: -0.003170595183897301\n","Layer 0 - Gradient Weights Max: 0.002700392384056309 Min: -0.0018039664448481513\n","ReLU Activation - Max: 0.1672473401024862 Min: 0.0\n","ReLU Activation - Max: 0.2229177577697178 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026749771085985163 Min: -0.0031760302577147964\n","Layer 0 - Gradient Weights Max: 0.0027147420324126277 Min: -0.0018122607180928814\n","ReLU Activation - Max: 0.1671683098085947 Min: 0.0\n","ReLU Activation - Max: 0.2229874544945119 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002704360732142257 Min: -0.003182287713862554\n","Layer 0 - Gradient Weights Max: 0.002732458666447115 Min: -0.0018245155427923823\n","ReLU Activation - Max: 0.1670882872525105 Min: 0.0\n","ReLU Activation - Max: 0.22305408746143873 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027334741609300645 Min: -0.003189327354618147\n","Layer 0 - Gradient Weights Max: 0.002742477105310715 Min: -0.0018365662281820136\n","ReLU Activation - Max: 0.1670084462437798 Min: 0.0\n","ReLU Activation - Max: 0.22311772838576607 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027623822027778603 Min: -0.0031971643761304317\n","Layer 0 - Gradient Weights Max: 0.0027577851257828264 Min: -0.0018479115721843058\n","ReLU Activation - Max: 0.16692825646752932 Min: 0.0\n","ReLU Activation - Max: 0.22317835970224464 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002791102017210617 Min: -0.0032057705109614114\n","Layer 0 - Gradient Weights Max: 0.0027744393129424294 Min: -0.0018559888380723507\n","ReLU Activation - Max: 0.16684771695343772 Min: 0.0\n","ReLU Activation - Max: 0.2232362104543719 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002819676662048576 Min: -0.003215143927805323\n","Layer 0 - Gradient Weights Max: 0.002789622979179465 Min: -0.001865654969771617\n","ReLU Activation - Max: 0.16676643609774097 Min: 0.0\n","ReLU Activation - Max: 0.22329161516340462 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028480820634818977 Min: -0.0032252107397611283\n","Layer 0 - Gradient Weights Max: 0.002806286586800349 Min: -0.0018822874447177755\n","ReLU Activation - Max: 0.16668423699341378 Min: 0.0\n","ReLU Activation - Max: 0.22334457725137682 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028763972378088535 Min: -0.003236016011635986\n","Layer 0 - Gradient Weights Max: 0.002817334053941915 Min: -0.0018965764179094762\n","ReLU Activation - Max: 0.16660109980337162 Min: 0.0\n","Epoch 70, Loss: 0.4991, Test Accuracy: 0.5886\n","ReLU Activation - Max: 0.22339518130061098 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029045991627379016 Min: -0.0032474922126759444\n","Layer 0 - Gradient Weights Max: 0.002823074360037891 Min: -0.0019158518621375162\n","ReLU Activation - Max: 0.16651787800645246 Min: 0.0\n","ReLU Activation - Max: 0.22344370819413942 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029326781082118216 Min: -0.003259592536222233\n","Layer 0 - Gradient Weights Max: 0.002829008924228246 Min: -0.0019246660842346366\n","ReLU Activation - Max: 0.16643380123355017 Min: 0.0\n","ReLU Activation - Max: 0.22348982811351353 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029606739727671937 Min: -0.0032723212239390083\n","Layer 0 - Gradient Weights Max: 0.00283547836048784 Min: -0.0019337460043295138\n","ReLU Activation - Max: 0.16635011600162017 Min: 0.0\n","ReLU Activation - Max: 0.22353393080928394 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002988645275255447 Min: -0.003285703327478151\n","Layer 0 - Gradient Weights Max: 0.002859289549676377 Min: -0.0019462568332511275\n","ReLU Activation - Max: 0.1662659438262078 Min: 0.0\n","ReLU Activation - Max: 0.22357588931738762 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030165747506742725 Min: -0.003299687270946777\n","Layer 0 - Gradient Weights Max: 0.0028844012214989533 Min: -0.0019595135880519374\n","ReLU Activation - Max: 0.16618078228020833 Min: 0.0\n","ReLU Activation - Max: 0.22361589365162834 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030444951936141854 Min: -0.003314277885257541\n","Layer 0 - Gradient Weights Max: 0.002899972036740627 Min: -0.0019692240963306317\n","ReLU Activation - Max: 0.16609471144696694 Min: 0.0\n","ReLU Activation - Max: 0.22365421544739947 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003072397120866514 Min: -0.0033294339916814454\n","Layer 0 - Gradient Weights Max: 0.0029069964161636736 Min: -0.00198290278225221\n","ReLU Activation - Max: 0.16600782980761308 Min: 0.0\n","ReLU Activation - Max: 0.22369071655045417 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003100287359440083 Min: -0.0033451341756133126\n","Layer 0 - Gradient Weights Max: 0.0029167112398511825 Min: -0.001998294099508242\n","ReLU Activation - Max: 0.1659203687102003 Min: 0.0\n","ReLU Activation - Max: 0.2237255389343491 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031282164499751254 Min: -0.003361402948943202\n","Layer 0 - Gradient Weights Max: 0.002916729701690236 Min: -0.0020065097884453945\n","ReLU Activation - Max: 0.16583196290343258 Min: 0.0\n","ReLU Activation - Max: 0.223758853369435 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031561630392518466 Min: -0.0033781955751445597\n","Layer 0 - Gradient Weights Max: 0.0029368001647734373 Min: -0.002023698511714935\n","ReLU Activation - Max: 0.16574274066694306 Min: 0.0\n","Epoch 80, Loss: 0.4985, Test Accuracy: 0.6121\n","ReLU Activation - Max: 0.2237905934889989 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031841554359949152 Min: -0.0033955140665476217\n","Layer 0 - Gradient Weights Max: 0.0029407394791365147 Min: -0.0020375737289932932\n","ReLU Activation - Max: 0.16565287189650107 Min: 0.0\n","ReLU Activation - Max: 0.2238209303451005 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032121759242544584 Min: -0.0034133151738304736\n","Layer 0 - Gradient Weights Max: 0.002968145770723814 Min: -0.0020544484433856402\n","ReLU Activation - Max: 0.1655623673931499 Min: 0.0\n","ReLU Activation - Max: 0.22384996086882333 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032402764104590798 Min: -0.003431630979699345\n","Layer 0 - Gradient Weights Max: 0.0029818288873973676 Min: -0.002069687534387342\n","ReLU Activation - Max: 0.16547124812767652 Min: 0.0\n","ReLU Activation - Max: 0.22387767283146337 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003268462131414197 Min: -0.003450447097270827\n","Layer 0 - Gradient Weights Max: 0.0030226266108307437 Min: -0.0020868121629596765\n","ReLU Activation - Max: 0.16537892292107587 Min: 0.0\n","ReLU Activation - Max: 0.22390425036315134 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032967233182088513 Min: -0.0034697318168384447\n","Layer 0 - Gradient Weights Max: 0.003044609647756442 Min: -0.002110325398075622\n","ReLU Activation - Max: 0.16528637801956178 Min: 0.0\n","ReLU Activation - Max: 0.2239295722991409 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033250959244532207 Min: -0.0034895037208142812\n","Layer 0 - Gradient Weights Max: 0.003042373742565487 Min: -0.0021199067349580865\n","ReLU Activation - Max: 0.16519446490027545 Min: 0.0\n","ReLU Activation - Max: 0.22395365230317196 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033536260799537317 Min: -0.0035097910219912694\n","Layer 0 - Gradient Weights Max: 0.0030564676324590103 Min: -0.0021423688122947284\n","ReLU Activation - Max: 0.1651025641671733 Min: 0.0\n","ReLU Activation - Max: 0.22397666961477394 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003382312905733069 Min: -0.003530576884309804\n","Layer 0 - Gradient Weights Max: 0.003077586505426888 Min: -0.0021531479165138854\n","ReLU Activation - Max: 0.16501020251133466 Min: 0.0\n","ReLU Activation - Max: 0.2239985103672885 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034111145290470503 Min: -0.00355180142323733\n","Layer 0 - Gradient Weights Max: 0.003097009940761317 Min: -0.002169433010097491\n","ReLU Activation - Max: 0.16491698333626112 Min: 0.0\n","ReLU Activation - Max: 0.224019194500472 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034401178629346932 Min: -0.003573539171509517\n","Layer 0 - Gradient Weights Max: 0.003117475512792369 Min: -0.002186234109129026\n","ReLU Activation - Max: 0.16482272608126827 Min: 0.0\n","Epoch 90, Loss: 0.4980, Test Accuracy: 0.6318\n","ReLU Activation - Max: 0.22403880716483293 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034693278312966746 Min: -0.0035957768974692167\n","Layer 0 - Gradient Weights Max: 0.0031278232893411444 Min: -0.0022060050939395177\n","ReLU Activation - Max: 0.16472859032772488 Min: 0.0\n","ReLU Activation - Max: 0.2240575462270813 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034987345627199443 Min: -0.0036184903229126903\n","Layer 0 - Gradient Weights Max: 0.003147944893137444 Min: -0.0022224223897434254\n","ReLU Activation - Max: 0.16463460655149734 Min: 0.0\n","ReLU Activation - Max: 0.22407538709796213 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035282556937739702 Min: -0.003641583256040074\n","Layer 0 - Gradient Weights Max: 0.003168552867934018 Min: -0.002247685487767649\n","ReLU Activation - Max: 0.16453971935655284 Min: 0.0\n","ReLU Activation - Max: 0.22409236408584213 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035579232551439834 Min: -0.00366507549143084\n","Layer 0 - Gradient Weights Max: 0.0031845750590475445 Min: -0.002269423068199085\n","ReLU Activation - Max: 0.16444450207231676 Min: 0.0\n","ReLU Activation - Max: 0.22410834501692073 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035877672339084173 Min: -0.0036889843837961517\n","Layer 0 - Gradient Weights Max: 0.0031989033245152436 Min: -0.002290995092702815\n","ReLU Activation - Max: 0.16434844950990096 Min: 0.0\n","ReLU Activation - Max: 0.22412357927197127 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003617869052881595 Min: -0.003713380463225063\n","Layer 0 - Gradient Weights Max: 0.003238862394198503 Min: -0.0023037426505604234\n","ReLU Activation - Max: 0.1642517329020939 Min: 0.0\n","ReLU Activation - Max: 0.22413797639216573 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003648160227745748 Min: -0.003738181585972614\n","Layer 0 - Gradient Weights Max: 0.0032624094485491527 Min: -0.0023151642125878624\n","ReLU Activation - Max: 0.1641544259735128 Min: 0.0\n","ReLU Activation - Max: 0.22415173024933424 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003678735982309699 Min: -0.0037634741989942067\n","Layer 0 - Gradient Weights Max: 0.003274499765864388 Min: -0.0023346166396869994\n","ReLU Activation - Max: 0.16405670784300497 Min: 0.0\n","ReLU Activation - Max: 0.22416476328218696 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003709574272460876 Min: -0.0037892261618448775\n","Layer 0 - Gradient Weights Max: 0.0032871006040122866 Min: -0.0023545270568505108\n","ReLU Activation - Max: 0.16395835904420214 Min: 0.0\n","ReLU Activation - Max: 0.22417714993981624 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0037406682829167525 Min: -0.003815422371094164\n","Layer 0 - Gradient Weights Max: 0.0033058950843872135 Min: -0.002368366332628092\n","ReLU Activation - Max: 0.16386043037367037 Min: 0.0\n","Epoch 100, Loss: 0.4976, Test Accuracy: 0.6482\n","ReLU Activation - Max: 0.22418886053987427 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0037720152821082748 Min: -0.0038420486558948326\n","Layer 0 - Gradient Weights Max: 0.003308169356301014 Min: -0.0023911042445743766\n","ReLU Activation - Max: 0.16376228481753802 Min: 0.0\n","ReLU Activation - Max: 0.22419999944117586 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038035702244402963 Min: -0.0038690498438600017\n","Layer 0 - Gradient Weights Max: 0.00333070448313221 Min: -0.002411389069421318\n","ReLU Activation - Max: 0.1636633144904186 Min: 0.0\n","ReLU Activation - Max: 0.22421054816325445 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038354172615631763 Min: -0.0038965033227009306\n","Layer 0 - Gradient Weights Max: 0.0033480205888117886 Min: -0.002420746342730862\n","ReLU Activation - Max: 0.1635632536294007 Min: 0.0\n","ReLU Activation - Max: 0.22422054120647017 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003867591805195019 Min: -0.003924437627184803\n","Layer 0 - Gradient Weights Max: 0.0033745642778677024 Min: -0.0024317694313553376\n","ReLU Activation - Max: 0.163462242937041 Min: 0.0\n","ReLU Activation - Max: 0.22423002037467424 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038999507066122727 Min: -0.003952700141662548\n","Layer 0 - Gradient Weights Max: 0.0033885411734657423 Min: -0.00244389226830354\n","ReLU Activation - Max: 0.16336103175917163 Min: 0.0\n","ReLU Activation - Max: 0.2242390132498901 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003932681692541371 Min: -0.003981474035594521\n","Layer 0 - Gradient Weights Max: 0.003417201803870197 Min: -0.00246546040727313\n","ReLU Activation - Max: 0.16325919402627204 Min: 0.0\n","ReLU Activation - Max: 0.22424757372389612 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003965710306505655 Min: -0.004010673897573379\n","Layer 0 - Gradient Weights Max: 0.00342643010460486 Min: -0.0024789097412412633\n","ReLU Activation - Max: 0.16315685044659498 Min: 0.0\n","ReLU Activation - Max: 0.2242556866325379 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003999009495850746 Min: -0.004040266946393814\n","Layer 0 - Gradient Weights Max: 0.00345115630372556 Min: -0.0024945525263285646\n","ReLU Activation - Max: 0.1630534082350442 Min: 0.0\n","ReLU Activation - Max: 0.22426347553486298 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004032497045496059 Min: -0.004070163914011129\n","Layer 0 - Gradient Weights Max: 0.0034779419784004943 Min: -0.002521698541491345\n","ReLU Activation - Max: 0.16294929743765865 Min: 0.0\n","ReLU Activation - Max: 0.22427084093902605 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004066206799266585 Min: -0.0041003914662269755\n","Layer 0 - Gradient Weights Max: 0.003510021259109993 Min: -0.0025399741710995616\n","ReLU Activation - Max: 0.1628442029994895 Min: 0.0\n","Epoch 110, Loss: 0.4970, Test Accuracy: 0.6636\n","ReLU Activation - Max: 0.22427786010508785 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00410016511952246 Min: -0.00413097059199242\n","Layer 0 - Gradient Weights Max: 0.003523032010602316 Min: -0.0025526761633935806\n","ReLU Activation - Max: 0.16328208763779103 Min: 0.0\n","ReLU Activation - Max: 0.22428448930679118 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004134398419090632 Min: -0.004161922248736323\n","Layer 0 - Gradient Weights Max: 0.003538181009579366 Min: -0.0025739866284106585\n","ReLU Activation - Max: 0.1637842566462416 Min: 0.0\n","ReLU Activation - Max: 0.22429075783991323 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004168910351961712 Min: -0.004193245770743261\n","Layer 0 - Gradient Weights Max: 0.0035617462070131525 Min: -0.0025918096362876667\n","ReLU Activation - Max: 0.16428992990018054 Min: 0.0\n","ReLU Activation - Max: 0.22429663379813042 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004203756476778911 Min: -0.004224992437071382\n","Layer 0 - Gradient Weights Max: 0.0035885341317932256 Min: -0.002602493531258023\n","ReLU Activation - Max: 0.16479972503171372 Min: 0.0\n","ReLU Activation - Max: 0.22430210406895398 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004238900758283468 Min: -0.004257119177730407\n","Layer 0 - Gradient Weights Max: 0.003586033406112807 Min: -0.002625397815014022\n","ReLU Activation - Max: 0.16531412117340016 Min: 0.0\n","ReLU Activation - Max: 0.22430731733159565 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0042744330689919625 Min: -0.004289712256723686\n","Layer 0 - Gradient Weights Max: 0.003613377608468649 Min: -0.0026415721027582655\n","ReLU Activation - Max: 0.16583263565660258 Min: 0.0\n","ReLU Activation - Max: 0.22431226889807562 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043103061392036155 Min: -0.0043227181363876\n","Layer 0 - Gradient Weights Max: 0.003642737115545434 Min: -0.002668719136442961\n","ReLU Activation - Max: 0.1663551079272008 Min: 0.0\n","ReLU Activation - Max: 0.22431689102329644 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004346628720191571 Min: -0.004356242582057292\n","Layer 0 - Gradient Weights Max: 0.003668342162806292 Min: -0.002690869825484705\n","ReLU Activation - Max: 0.16688226509051995 Min: 0.0\n","ReLU Activation - Max: 0.22432118969159612 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004383308653345917 Min: -0.004390188814282533\n","Layer 0 - Gradient Weights Max: 0.0036813260077784194 Min: -0.002701840583706362\n","ReLU Activation - Max: 0.16741349079806267 Min: 0.0\n","ReLU Activation - Max: 0.22432519230392578 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004420304856046435 Min: -0.004424510814807104\n","Layer 0 - Gradient Weights Max: 0.003694101592334508 Min: -0.0027271173444634337\n","ReLU Activation - Max: 0.16794938238712326 Min: 0.0\n","Epoch 120, Loss: 0.4963, Test Accuracy: 0.6814\n","ReLU Activation - Max: 0.22432888806487356 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004457595639087994 Min: -0.004459182863909375\n","Layer 0 - Gradient Weights Max: 0.003717152171966607 Min: -0.0027514349452156315\n","ReLU Activation - Max: 0.16848948166063965 Min: 0.0\n","ReLU Activation - Max: 0.2243324175376823 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00449508749906602 Min: -0.004494106639491997\n","Layer 0 - Gradient Weights Max: 0.003728744904684308 Min: -0.0027822764330215433\n","ReLU Activation - Max: 0.16903354928899414 Min: 0.0\n","ReLU Activation - Max: 0.2243356955137746 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004532911315705776 Min: -0.004529411090535961\n","Layer 0 - Gradient Weights Max: 0.003751824640774013 Min: -0.002797010189675116\n","ReLU Activation - Max: 0.1695821855895911 Min: 0.0\n","ReLU Activation - Max: 0.22433873546073277 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004570996928108897 Min: -0.004565021686323865\n","Layer 0 - Gradient Weights Max: 0.003774223786381748 Min: -0.002823656222043027\n","ReLU Activation - Max: 0.17013575869505956 Min: 0.0\n","ReLU Activation - Max: 0.22434147707185617 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004609465723316646 Min: -0.004601057261974973\n","Layer 0 - Gradient Weights Max: 0.0037995918078349814 Min: -0.002845844936484606\n","ReLU Activation - Max: 0.17069233475818654 Min: 0.0\n","ReLU Activation - Max: 0.22434397190997257 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004648419202707878 Min: -0.004637614849248166\n","Layer 0 - Gradient Weights Max: 0.0038191724464625284 Min: -0.002872838480993828\n","ReLU Activation - Max: 0.17125338934234233 Min: 0.0\n","ReLU Activation - Max: 0.2243462271304682 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004687767915492884 Min: -0.004674602183005307\n","Layer 0 - Gradient Weights Max: 0.0038511756653391797 Min: -0.002895544287283939\n","ReLU Activation - Max: 0.1718190001400286 Min: 0.0\n","ReLU Activation - Max: 0.224348257974139 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0047275422195840605 Min: -0.004712047832651874\n","Layer 0 - Gradient Weights Max: 0.0038726526062540303 Min: -0.0029151496990876794\n","ReLU Activation - Max: 0.17239091346562838 Min: 0.0\n","ReLU Activation - Max: 0.22434995815047104 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004767716628354526 Min: -0.004749924183520404\n","Layer 0 - Gradient Weights Max: 0.003891343483779994 Min: -0.002934489439443412\n","ReLU Activation - Max: 0.17296806106934773 Min: 0.0\n","ReLU Activation - Max: 0.2243514400384473 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004808281299916754 Min: -0.004788219024034613\n","Layer 0 - Gradient Weights Max: 0.003906064481597353 Min: -0.0029449582588820496\n","ReLU Activation - Max: 0.17354973558173667 Min: 0.0\n","Epoch 130, Loss: 0.4955, Test Accuracy: 0.6882\n","ReLU Activation - Max: 0.22435271642663687 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004849275622180718 Min: -0.004826966299853146\n","Layer 0 - Gradient Weights Max: 0.003930294463905027 Min: -0.0029618170811400515\n","ReLU Activation - Max: 0.17413661370047764 Min: 0.0\n","ReLU Activation - Max: 0.22435378600778683 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004890657001390201 Min: -0.004866119872200064\n","Layer 0 - Gradient Weights Max: 0.003950401900295771 Min: -0.0029793925470195212\n","ReLU Activation - Max: 0.174726906068889 Min: 0.0\n","ReLU Activation - Max: 0.22435469707040695 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004932361086998185 Min: -0.0049056139422890355\n","Layer 0 - Gradient Weights Max: 0.0039761499538750866 Min: -0.003004683811981053\n","ReLU Activation - Max: 0.17531986954667658 Min: 0.0\n","ReLU Activation - Max: 0.2243553962159643 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004974386804060647 Min: -0.004945445337560498\n","Layer 0 - Gradient Weights Max: 0.004003737703375764 Min: -0.003021529139394781\n","ReLU Activation - Max: 0.17591874682862768 Min: 0.0\n","ReLU Activation - Max: 0.22435597795144643 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005016677382932212 Min: -0.004985556709320803\n","Layer 0 - Gradient Weights Max: 0.004022521622665953 Min: -0.003044719698085751\n","ReLU Activation - Max: 0.17652266209285508 Min: 0.0\n","ReLU Activation - Max: 0.2243563675188691 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005059250494639739 Min: -0.005025960219525763\n","Layer 0 - Gradient Weights Max: 0.00404824969217599 Min: -0.003067276822728446\n","ReLU Activation - Max: 0.17713064643518478 Min: 0.0\n","ReLU Activation - Max: 0.22435657108032958 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0051021284261279195 Min: -0.005066677962240139\n","Layer 0 - Gradient Weights Max: 0.004068341062620085 Min: -0.0030910032728723295\n","ReLU Activation - Max: 0.1777422043603219 Min: 0.0\n","ReLU Activation - Max: 0.22435659260376684 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005145426026585105 Min: -0.005107823726776753\n","Layer 0 - Gradient Weights Max: 0.004108496753369548 Min: -0.0031104135932307057\n","ReLU Activation - Max: 0.1783726795503058 Min: 0.0\n","ReLU Activation - Max: 0.22435643584909742 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005189059029251657 Min: -0.005149309845625159\n","Layer 0 - Gradient Weights Max: 0.004133521271030778 Min: -0.003119681895089611\n","ReLU Activation - Max: 0.179135174051162 Min: 0.0\n","ReLU Activation - Max: 0.2243561045007925 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005233197345189818 Min: -0.005191302200645131\n","Layer 0 - Gradient Weights Max: 0.004168148144816062 Min: -0.0031363602795580923\n","ReLU Activation - Max: 0.17990232969664752 Min: 0.0\n","Epoch 140, Loss: 0.4946, Test Accuracy: 0.6939\n","ReLU Activation - Max: 0.22435562836550785 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005277783161405141 Min: -0.005233741444250336\n","Layer 0 - Gradient Weights Max: 0.004203151718994361 Min: -0.00316185599574788\n","ReLU Activation - Max: 0.18067404867405562 Min: 0.0\n","ReLU Activation - Max: 0.22435499034300088 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005322736526542762 Min: -0.005276547958848253\n","Layer 0 - Gradient Weights Max: 0.004239851776868004 Min: -0.0031946710872191426\n","ReLU Activation - Max: 0.1814531584090849 Min: 0.0\n","ReLU Activation - Max: 0.2243541861926838 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005368101994900032 Min: -0.005319765112548158\n","Layer 0 - Gradient Weights Max: 0.004268356704460242 Min: -0.003223750860474175\n","ReLU Activation - Max: 0.18223709160180185 Min: 0.0\n","ReLU Activation - Max: 0.22435324381994712 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005413864231359087 Min: -0.005363376781931979\n","Layer 0 - Gradient Weights Max: 0.004321830754746828 Min: -0.0032589832492305668\n","ReLU Activation - Max: 0.1830300902321244 Min: 0.0\n","ReLU Activation - Max: 0.22435212221390194 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005459963899825971 Min: -0.005407319634316966\n","Layer 0 - Gradient Weights Max: 0.0043511471680624456 Min: -0.003281430376778405\n","ReLU Activation - Max: 0.18383063035140235 Min: 0.0\n","ReLU Activation - Max: 0.22435085533953347 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005506359058025794 Min: -0.005451549511905749\n","Layer 0 - Gradient Weights Max: 0.004387012516481035 Min: -0.0033117121504483787\n","ReLU Activation - Max: 0.18463707208572158 Min: 0.0\n","ReLU Activation - Max: 0.22434943136500443 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0055531520934879305 Min: -0.005496167412668608\n","Layer 0 - Gradient Weights Max: 0.004427647405555805 Min: -0.0033512131948642133\n","ReLU Activation - Max: 0.18545028288679216 Min: 0.0\n","ReLU Activation - Max: 0.22434787809681367 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005600339608240615 Min: -0.005541169791923689\n","Layer 0 - Gradient Weights Max: 0.0044574589382487895 Min: -0.0033932968163401882\n","ReLU Activation - Max: 0.18627062783788367 Min: 0.0\n","ReLU Activation - Max: 0.22434617274892843 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0056477896007684405 Min: -0.005586425380073822\n","Layer 0 - Gradient Weights Max: 0.004480315568327511 Min: -0.0034146361348680564\n","ReLU Activation - Max: 0.1871017624151044 Min: 0.0\n","ReLU Activation - Max: 0.2243443170080752 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005695515547303199 Min: -0.005631943089586041\n","Layer 0 - Gradient Weights Max: 0.004530925104395555 Min: -0.0034479944645354386\n","ReLU Activation - Max: 0.18793954696385287 Min: 0.0\n","Epoch 150, Loss: 0.4934, Test Accuracy: 0.6957\n","ReLU Activation - Max: 0.2243423230721029 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005743680296727452 Min: -0.005677881693108838\n","Layer 0 - Gradient Weights Max: 0.0045768772611400116 Min: -0.0034810515437175738\n","ReLU Activation - Max: 0.1887835112302865 Min: 0.0\n","ReLU Activation - Max: 0.22434017508250603 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005792164790007119 Min: -0.00572412490261882\n","Layer 0 - Gradient Weights Max: 0.004598190155606087 Min: -0.0035088831959450666\n","ReLU Activation - Max: 0.18963316406885516 Min: 0.0\n","ReLU Activation - Max: 0.22433786648371828 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005840856964359021 Min: -0.005770561118893225\n","Layer 0 - Gradient Weights Max: 0.0046206794039451516 Min: -0.003532932396788306\n","ReLU Activation - Max: 0.19048759911434499 Min: 0.0\n","ReLU Activation - Max: 0.22433542145261756 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005889954137912353 Min: -0.0058173831869102405\n","Layer 0 - Gradient Weights Max: 0.00464767697621254 Min: -0.00356301298270637\n","ReLU Activation - Max: 0.19134772234202602 Min: 0.0\n","ReLU Activation - Max: 0.22433283511176597 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005939394554134698 Min: -0.005864529582492474\n","Layer 0 - Gradient Weights Max: 0.004690410815097149 Min: -0.003594870667725166\n","ReLU Activation - Max: 0.19234206953865185 Min: 0.0\n","ReLU Activation - Max: 0.22433010740264914 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005989108861244257 Min: -0.005911927708580019\n","Layer 0 - Gradient Weights Max: 0.00471269897270449 Min: -0.003643131517623176\n","ReLU Activation - Max: 0.19355859223406371 Min: 0.0\n","ReLU Activation - Max: 0.22505152298709505 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006039156860314833 Min: -0.005959639624478223\n","Layer 0 - Gradient Weights Max: 0.004753338194543126 Min: -0.0036719374981827027\n","ReLU Activation - Max: 0.19478471406655393 Min: 0.0\n","ReLU Activation - Max: 0.22655491775936323 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006089459566337383 Min: -0.006007583681935283\n","Layer 0 - Gradient Weights Max: 0.004788637465576189 Min: -0.0037164064655902784\n","ReLU Activation - Max: 0.19602306913667678 Min: 0.0\n","ReLU Activation - Max: 0.2280746904903558 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006139985614200708 Min: -0.006055730195065935\n","Layer 0 - Gradient Weights Max: 0.004816717684370934 Min: -0.003754114648239801\n","ReLU Activation - Max: 0.19727504799605416 Min: 0.0\n","ReLU Activation - Max: 0.2296083413836671 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006190726866384372 Min: -0.006104070088439423\n","Layer 0 - Gradient Weights Max: 0.004852112872163901 Min: -0.003770024795512841\n","ReLU Activation - Max: 0.19853670305988508 Min: 0.0\n","Epoch 160, Loss: 0.4921, Test Accuracy: 0.6968\n","ReLU Activation - Max: 0.23115483868081999 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006241761460914286 Min: -0.006152675234709435\n","Layer 0 - Gradient Weights Max: 0.004883966932561313 Min: -0.0037852858452320382\n","ReLU Activation - Max: 0.19980950137151915 Min: 0.0\n","ReLU Activation - Max: 0.23271370744362538 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006293042409295524 Min: -0.006201497467895752\n","Layer 0 - Gradient Weights Max: 0.004930319581562461 Min: -0.003811405014853734\n","ReLU Activation - Max: 0.20109063864206814 Min: 0.0\n","ReLU Activation - Max: 0.23428531342928594 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006344526655162607 Min: -0.006250492349034407\n","Layer 0 - Gradient Weights Max: 0.0049536620099401105 Min: -0.0038523131928221875\n","ReLU Activation - Max: 0.20238464555132912 Min: 0.0\n","ReLU Activation - Max: 0.23586962057610558 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006396329679426076 Min: -0.006299775042792828\n","Layer 0 - Gradient Weights Max: 0.004992143084566698 Min: -0.003878129623802149\n","ReLU Activation - Max: 0.20368781482794454 Min: 0.0\n","ReLU Activation - Max: 0.23746630968835836 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006448401533735052 Min: -0.006349295301198382\n","Layer 0 - Gradient Weights Max: 0.005034037429143787 Min: -0.003918146519164978\n","ReLU Activation - Max: 0.20499964109327598 Min: 0.0\n","ReLU Activation - Max: 0.23907707796563973 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006500701514414339 Min: -0.00639901195569862\n","Layer 0 - Gradient Weights Max: 0.005058306370695678 Min: -0.003955297012539408\n","ReLU Activation - Max: 0.20632023827536308 Min: 0.0\n","ReLU Activation - Max: 0.24070204298149742 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006553358464456393 Min: -0.006449053684468995\n","Layer 0 - Gradient Weights Max: 0.005073122736164915 Min: -0.004008544158918396\n","ReLU Activation - Max: 0.20764992762109574 Min: 0.0\n","ReLU Activation - Max: 0.24234135639375437 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006606302674712364 Min: -0.006499352233654209\n","Layer 0 - Gradient Weights Max: 0.0050955506968227825 Min: -0.0040308500566500795\n","ReLU Activation - Max: 0.2089916759700876 Min: 0.0\n","ReLU Activation - Max: 0.24399170020175562 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00665955881218442 Min: -0.006549927295232378\n","Layer 0 - Gradient Weights Max: 0.005135464444021731 Min: -0.004057872138410079\n","ReLU Activation - Max: 0.210343588387376 Min: 0.0\n","ReLU Activation - Max: 0.24565373182346806 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006712997304093598 Min: -0.0066006472062096234\n","Layer 0 - Gradient Weights Max: 0.0051801518269089936 Min: -0.004072353729362028\n","ReLU Activation - Max: 0.2117049298684232 Min: 0.0\n","Epoch 170, Loss: 0.4904, Test Accuracy: 0.6982\n","ReLU Activation - Max: 0.24732970425808032 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00676662364201411 Min: -0.006651516422757989\n","Layer 0 - Gradient Weights Max: 0.005221170457514 Min: -0.0041127463661819645\n","ReLU Activation - Max: 0.21308014986465298 Min: 0.0\n","ReLU Activation - Max: 0.24902024893506997 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006820529056115472 Min: -0.006702623305901513\n","Layer 0 - Gradient Weights Max: 0.0052637302189355185 Min: -0.004124230248428927\n","ReLU Activation - Max: 0.21446940863567185 Min: 0.0\n","ReLU Activation - Max: 0.25072653702763975 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00687480823357987 Min: -0.006754060618231598\n","Layer 0 - Gradient Weights Max: 0.005302940953175282 Min: -0.004172658666027936\n","ReLU Activation - Max: 0.21587414525550577 Min: 0.0\n","ReLU Activation - Max: 0.25244462326186284 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006929439796873037 Min: -0.006805808909107008\n","Layer 0 - Gradient Weights Max: 0.005336702653018968 Min: -0.004213714565585338\n","ReLU Activation - Max: 0.2172876967536053 Min: 0.0\n","ReLU Activation - Max: 0.2541782064653435 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006984509675127401 Min: -0.006857952340686402\n","Layer 0 - Gradient Weights Max: 0.0053660973978906035 Min: -0.004246489435285799\n","ReLU Activation - Max: 0.2187158362554632 Min: 0.0\n","ReLU Activation - Max: 0.2559268512282668 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0070399113793328996 Min: -0.006910381458242548\n","Layer 0 - Gradient Weights Max: 0.005391532502655939 Min: -0.004280841052196309\n","ReLU Activation - Max: 0.22015823803033566 Min: 0.0\n","ReLU Activation - Max: 0.2576872091026293 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00709564027258563 Min: -0.0069630966088019336\n","Layer 0 - Gradient Weights Max: 0.005436387188689711 Min: -0.004313778212907597\n","ReLU Activation - Max: 0.2216116524096526 Min: 0.0\n","ReLU Activation - Max: 0.2594611798878628 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007151606600602649 Min: -0.007016009114719481\n","Layer 0 - Gradient Weights Max: 0.005463144026620262 Min: -0.004352619516798922\n","ReLU Activation - Max: 0.2230762547797532 Min: 0.0\n","ReLU Activation - Max: 0.26124832587519964 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0072077166489386245 Min: -0.007069020874977185\n","Layer 0 - Gradient Weights Max: 0.005505043810936195 Min: -0.0043631601327699535\n","ReLU Activation - Max: 0.22455159286746887 Min: 0.0\n","ReLU Activation - Max: 0.2630479060394197 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007264162456760648 Min: -0.007122319821486042\n","Layer 0 - Gradient Weights Max: 0.005555484838821092 Min: -0.004410779421184571\n","ReLU Activation - Max: 0.22603881355370323 Min: 0.0\n","Epoch 180, Loss: 0.4885, Test Accuracy: 0.7014\n","ReLU Activation - Max: 0.26485958651371116 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00732078853947472 Min: -0.007175752692988916\n","Layer 0 - Gradient Weights Max: 0.0055809356957692915 Min: -0.004437120213945872\n","ReLU Activation - Max: 0.22754146783900545 Min: 0.0\n","ReLU Activation - Max: 0.26668557147456856 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007377570071884332 Min: -0.007229296311296526\n","Layer 0 - Gradient Weights Max: 0.005603731878622512 Min: -0.0044692998253070765\n","ReLU Activation - Max: 0.22906248070946325 Min: 0.0\n","ReLU Activation - Max: 0.2685234356573476 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0074344146498698475 Min: -0.007282859006614027\n","Layer 0 - Gradient Weights Max: 0.00561864087633453 Min: -0.004489676888535241\n","ReLU Activation - Max: 0.23059411363668825 Min: 0.0\n","ReLU Activation - Max: 0.2703739403451404 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0074913689507458275 Min: -0.007336484298332727\n","Layer 0 - Gradient Weights Max: 0.005647694913942854 Min: -0.004537274428693849\n","ReLU Activation - Max: 0.2321395124058332 Min: 0.0\n","ReLU Activation - Max: 0.27223972755465214 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007548307993446771 Min: -0.007390050211590577\n","Layer 0 - Gradient Weights Max: 0.005692169364463494 Min: -0.0045577494170114365\n","ReLU Activation - Max: 0.2337019500042608 Min: 0.0\n","ReLU Activation - Max: 0.2741163953837875 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007605306578618107 Min: -0.007443629541026054\n","Layer 0 - Gradient Weights Max: 0.005717233058958864 Min: -0.004592101200308362\n","ReLU Activation - Max: 0.23527735956289464 Min: 0.0\n","ReLU Activation - Max: 0.2760066619742956 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007662453800645881 Min: -0.007497307598324353\n","Layer 0 - Gradient Weights Max: 0.005769984823521008 Min: -0.004637207328308785\n","ReLU Activation - Max: 0.2368654661289295 Min: 0.0\n","ReLU Activation - Max: 0.27791298988527957 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007719655237788917 Min: -0.007550989530357032\n","Layer 0 - Gradient Weights Max: 0.005807208988558235 Min: -0.004666319463518341\n","ReLU Activation - Max: 0.23846709729502782 Min: 0.0\n","ReLU Activation - Max: 0.27983756246190405 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007777020178452968 Min: -0.0076047833738593935\n","Layer 0 - Gradient Weights Max: 0.005816600132848426 Min: -0.004687495767772384\n","ReLU Activation - Max: 0.24008061840797262 Min: 0.0\n","ReLU Activation - Max: 0.2817799690390736 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007834628520120178 Min: -0.00765876423541033\n","Layer 0 - Gradient Weights Max: 0.005863855468944043 Min: -0.004708787353002156\n","ReLU Activation - Max: 0.24170469133655156 Min: 0.0\n","Epoch 190, Loss: 0.4863, Test Accuracy: 0.7036\n","ReLU Activation - Max: 0.28373722015405034 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007892493884664526 Min: -0.0077129446671886735\n","Layer 0 - Gradient Weights Max: 0.005904623368665597 Min: -0.004750430325226173\n","ReLU Activation - Max: 0.24333924625098544 Min: 0.0\n","ReLU Activation - Max: 0.28570871836592465 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007950448896308528 Min: -0.007767159669032386\n","Layer 0 - Gradient Weights Max: 0.005960969431639994 Min: -0.004795566961002044\n","ReLU Activation - Max: 0.24498441020779704 Min: 0.0\n","ReLU Activation - Max: 0.2876959933917682 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008008428464419343 Min: -0.007821345333807078\n","Layer 0 - Gradient Weights Max: 0.0059828568843333105 Min: -0.004831372535546345\n","ReLU Activation - Max: 0.24664087035294988 Min: 0.0\n","ReLU Activation - Max: 0.2897011752519273 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008066475946886217 Min: -0.007875541179127091\n","Layer 0 - Gradient Weights Max: 0.005997504920180119 Min: -0.004865246414598907\n","ReLU Activation - Max: 0.24831020570770115 Min: 0.0\n","ReLU Activation - Max: 0.291720842249862 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00812459154517158 Min: -0.007929748119162685\n","Layer 0 - Gradient Weights Max: 0.006035368953767925 Min: -0.004887779171710085\n","ReLU Activation - Max: 0.24999084111430572 Min: 0.0\n","ReLU Activation - Max: 0.29375515406629416 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008182732526963249 Min: -0.00798392239100176\n","Layer 0 - Gradient Weights Max: 0.006078939057525449 Min: -0.00494533175984711\n","ReLU Activation - Max: 0.2516853661673825 Min: 0.0\n","ReLU Activation - Max: 0.29580353621989947 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008240854373286664 Min: -0.008038027823122478\n","Layer 0 - Gradient Weights Max: 0.006092343797229753 Min: -0.004985035864631728\n","ReLU Activation - Max: 0.2533933786867907 Min: 0.0\n","ReLU Activation - Max: 0.2978650583706584 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008298982328998018 Min: -0.00809208412656561\n","Layer 0 - Gradient Weights Max: 0.006148191584878424 Min: -0.005023566747304755\n","ReLU Activation - Max: 0.25511775591364644 Min: 0.0\n","ReLU Activation - Max: 0.2999434812400408 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00835716441917198 Min: -0.008146137665607388\n","Layer 0 - Gradient Weights Max: 0.006185463298558083 Min: -0.005040819765691848\n","ReLU Activation - Max: 0.2568566819920461 Min: 0.0\n","ReLU Activation - Max: 0.3020356205175652 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00841533098025145 Min: -0.008200113236382596\n","Layer 0 - Gradient Weights Max: 0.006213481626876164 Min: -0.005072422165056638\n","ReLU Activation - Max: 0.2586080625476019 Min: 0.0\n","Epoch 200, Loss: 0.4837, Test Accuracy: 0.7036\n","ReLU Activation - Max: 0.3041466598891892 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008473557423939402 Min: -0.0082540914095825\n","Layer 0 - Gradient Weights Max: 0.006254106277621953 Min: -0.005110775646205066\n","ReLU Activation - Max: 0.26037193374036294 Min: 0.0\n","ReLU Activation - Max: 0.3062708733121748 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008531746326496668 Min: -0.008307975038545384\n","Layer 0 - Gradient Weights Max: 0.006288052283812024 Min: -0.005153139048772379\n","ReLU Activation - Max: 0.2621472920007537 Min: 0.0\n","ReLU Activation - Max: 0.30841145588198476 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008589881177884704 Min: -0.008361744110920053\n","Layer 0 - Gradient Weights Max: 0.006303465464373955 Min: -0.005178224536132067\n","ReLU Activation - Max: 0.2639376094189021 Min: 0.0\n","ReLU Activation - Max: 0.3105647473223107 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008647946884251478 Min: -0.00841538238003478\n","Layer 0 - Gradient Weights Max: 0.00636052746382702 Min: -0.0052144690270918715\n","ReLU Activation - Max: 0.26574069039552484 Min: 0.0\n","ReLU Activation - Max: 0.3127346311441463 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008705916979731299 Min: -0.008468869300102579\n","Layer 0 - Gradient Weights Max: 0.0063926796974855486 Min: -0.005238695164633697\n","ReLU Activation - Max: 0.26755908811870366 Min: 0.0\n","ReLU Activation - Max: 0.3149161420984497 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008763853900049233 Min: -0.008522263402401439\n","Layer 0 - Gradient Weights Max: 0.006424652049476406 Min: -0.005268126541104902\n","ReLU Activation - Max: 0.2693957076526755 Min: 0.0\n","ReLU Activation - Max: 0.31711078733131154 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008821715267570722 Min: -0.008575522752319233\n","Layer 0 - Gradient Weights Max: 0.006466729841791527 Min: -0.005322130340510847\n","ReLU Activation - Max: 0.27124712721972 Min: 0.0\n","ReLU Activation - Max: 0.31932052013020334 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008879430206757103 Min: -0.008628576929699813\n","Layer 0 - Gradient Weights Max: 0.006512286648000734 Min: -0.005371019576267801\n","ReLU Activation - Max: 0.2731181178682858 Min: 0.0\n","ReLU Activation - Max: 0.3215434092215027 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008936911052201095 Min: -0.008681338909541689\n","Layer 0 - Gradient Weights Max: 0.006543795966293758 Min: -0.0054148948545943215\n","ReLU Activation - Max: 0.2750028323649697 Min: 0.0\n","ReLU Activation - Max: 0.323782860194734 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008994335938014606 Min: -0.00873398241479662\n","Layer 0 - Gradient Weights Max: 0.006563798636153862 Min: -0.005447204647541699\n","ReLU Activation - Max: 0.27690422434761736 Min: 0.0\n","Epoch 210, Loss: 0.4806, Test Accuracy: 0.7011\n","ReLU Activation - Max: 0.32603473549024 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009051625712979799 Min: -0.008786434314051012\n","Layer 0 - Gradient Weights Max: 0.006602987385140874 Min: -0.005481801931219606\n","ReLU Activation - Max: 0.27882025742144595 Min: 0.0\n","ReLU Activation - Max: 0.3283007999200995 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009108776753472634 Min: -0.008838685699812178\n","Layer 0 - Gradient Weights Max: 0.0066082649410651745 Min: -0.005493544395787148\n","ReLU Activation - Max: 0.2807482614140535 Min: 0.0\n","ReLU Activation - Max: 0.3305825691279053 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009165812821655473 Min: -0.008890757563722063\n","Layer 0 - Gradient Weights Max: 0.006650854131551783 Min: -0.00553156985798512\n","ReLU Activation - Max: 0.282688817276997 Min: 0.0\n","ReLU Activation - Max: 0.3328812400995102 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009222604208561816 Min: -0.008942523801812183\n","Layer 0 - Gradient Weights Max: 0.006670888995780397 Min: -0.005579256127500419\n","ReLU Activation - Max: 0.2846434399252684 Min: 0.0\n","ReLU Activation - Max: 0.33519801582765746 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009279171854520669 Min: -0.008994005118819157\n","Layer 0 - Gradient Weights Max: 0.006726826661122259 Min: -0.005587761738875192\n","ReLU Activation - Max: 0.2866133050249794 Min: 0.0\n","ReLU Activation - Max: 0.337527280017934 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009335550628736719 Min: -0.009045233977890553\n","Layer 0 - Gradient Weights Max: 0.006771521779301825 Min: -0.00563205828072448\n","ReLU Activation - Max: 0.28859415475472805 Min: 0.0\n","ReLU Activation - Max: 0.3398756418233909 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00939167539218308 Min: -0.009096150018948248\n","Layer 0 - Gradient Weights Max: 0.006803073101633582 Min: -0.005672327301232356\n","ReLU Activation - Max: 0.29058888953875367 Min: 0.0\n","ReLU Activation - Max: 0.34223661279765977 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009447532342717451 Min: -0.009146744100229014\n","Layer 0 - Gradient Weights Max: 0.006810787596739209 Min: -0.005717482887612245\n","ReLU Activation - Max: 0.2925992100675837 Min: 0.0\n","ReLU Activation - Max: 0.34461134340551897 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009503053656997343 Min: -0.00919694672695117\n","Layer 0 - Gradient Weights Max: 0.006852030525911918 Min: -0.005758939115551848\n","ReLU Activation - Max: 0.2946285088774191 Min: 0.0\n","ReLU Activation - Max: 0.3470018131900676 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00955817320357117 Min: -0.009246689835168529\n","Layer 0 - Gradient Weights Max: 0.006879450598382091 Min: -0.005796966575050923\n","ReLU Activation - Max: 0.29666785582193733 Min: 0.0\n","Epoch 220, Loss: 0.4771, Test Accuracy: 0.7007\n","ReLU Activation - Max: 0.34940742858733514 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009612925149045829 Min: -0.009296011295603075\n","Layer 0 - Gradient Weights Max: 0.006933003703966929 Min: -0.005830474198059416\n","ReLU Activation - Max: 0.2987205996291351 Min: 0.0\n","ReLU Activation - Max: 0.3518289752949988 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009667299074136137 Min: -0.009344896061087331\n","Layer 0 - Gradient Weights Max: 0.0069715987774686865 Min: -0.0058759819095666795\n","ReLU Activation - Max: 0.3007845297156626 Min: 0.0\n","ReLU Activation - Max: 0.354264202368532 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00972119535618053 Min: -0.009393249068682696\n","Layer 0 - Gradient Weights Max: 0.00701423400982878 Min: -0.005906240140221165\n","ReLU Activation - Max: 0.3028662632426186 Min: 0.0\n","ReLU Activation - Max: 0.35671132729848887 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00977459942978728 Min: -0.009441050642708764\n","Layer 0 - Gradient Weights Max: 0.007041405120928821 Min: -0.005952298629349069\n","ReLU Activation - Max: 0.304966519740144 Min: 0.0\n","ReLU Activation - Max: 0.35917184166104565 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009827563492557305 Min: -0.009488357565916337\n","Layer 0 - Gradient Weights Max: 0.0070877796500080316 Min: -0.005994431023019594\n","ReLU Activation - Max: 0.30708524276423427 Min: 0.0\n","ReLU Activation - Max: 0.3616484455062521 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009880163709672932 Min: -0.009535240461040371\n","Layer 0 - Gradient Weights Max: 0.0071355941503293985 Min: -0.0060416163032437605\n","ReLU Activation - Max: 0.30921541120288343 Min: 0.0\n","ReLU Activation - Max: 0.3641332892703102 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00993230363508792 Min: -0.00958160932766653\n","Layer 0 - Gradient Weights Max: 0.0071598209696540314 Min: -0.006058971313193359\n","ReLU Activation - Max: 0.311357016673062 Min: 0.0\n","ReLU Activation - Max: 0.3666306478467438 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009984007691199384 Min: -0.009627486119964022\n","Layer 0 - Gradient Weights Max: 0.007203002011642062 Min: -0.006109635861309688\n","ReLU Activation - Max: 0.3135162832866002 Min: 0.0\n","ReLU Activation - Max: 0.3691416126735481 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010035154443354176 Min: -0.009672749702005422\n","Layer 0 - Gradient Weights Max: 0.007219863937283619 Min: -0.006154846798657734\n","ReLU Activation - Max: 0.3156875554803691 Min: 0.0\n","ReLU Activation - Max: 0.3716699799391374 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01008578917766561 Min: -0.009717448910452547\n","Layer 0 - Gradient Weights Max: 0.007261873970696762 Min: -0.006183398327184077\n","ReLU Activation - Max: 0.31787409183002807 Min: 0.0\n","Epoch 230, Loss: 0.4732, Test Accuracy: 0.7000\n","ReLU Activation - Max: 0.3742092707478188 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010135849611775178 Min: -0.009761524461060983\n","Layer 0 - Gradient Weights Max: 0.007280912970009235 Min: -0.006199716751691175\n","ReLU Activation - Max: 0.3200682572499126 Min: 0.0\n","ReLU Activation - Max: 0.3767599413617422 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010185381824724638 Min: -0.009805017053489686\n","Layer 0 - Gradient Weights Max: 0.007329873730290167 Min: -0.006201018657490397\n","ReLU Activation - Max: 0.32227199385418265 Min: 0.0\n","ReLU Activation - Max: 0.3793259861778296 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010234339645169297 Min: -0.009847878247593334\n","Layer 0 - Gradient Weights Max: 0.007360256860668881 Min: -0.006243995485513688\n","ReLU Activation - Max: 0.3244893350896981 Min: 0.0\n","ReLU Activation - Max: 0.38190584704913305 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010282689349677222 Min: -0.00989008203007228\n","Layer 0 - Gradient Weights Max: 0.007383095926709151 Min: -0.0062722432210934025\n","ReLU Activation - Max: 0.32671718444055 Min: 0.0\n","ReLU Activation - Max: 0.3845006930916656 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010330377619502234 Min: -0.009931572324871157\n","Layer 0 - Gradient Weights Max: 0.007417983360750046 Min: -0.006288514613643205\n","ReLU Activation - Max: 0.3289567890975321 Min: 0.0\n","ReLU Activation - Max: 0.38710846871808 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01037744097363943 Min: -0.009972388461019752\n","Layer 0 - Gradient Weights Max: 0.007442829917269967 Min: -0.006318955550914619\n","ReLU Activation - Max: 0.331209670722847 Min: 0.0\n","ReLU Activation - Max: 0.38972921903965263 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010423718106410833 Min: -0.010012371537771259\n","Layer 0 - Gradient Weights Max: 0.007460378850438606 Min: -0.006347008977109699\n","ReLU Activation - Max: 0.33347734450027494 Min: 0.0\n","ReLU Activation - Max: 0.3923616665250917 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010469254991129258 Min: -0.01005156212666431\n","Layer 0 - Gradient Weights Max: 0.007481816611994188 Min: -0.006361941516899684\n","ReLU Activation - Max: 0.3357575860813881 Min: 0.0\n","ReLU Activation - Max: 0.3950080156562648 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01051408714662579 Min: -0.010090002143629661\n","Layer 0 - Gradient Weights Max: 0.007500169065284236 Min: -0.006392006077054451\n","ReLU Activation - Max: 0.33804719836690655 Min: 0.0\n","ReLU Activation - Max: 0.3976692691657469 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01055815915258802 Min: -0.010127632289637417\n","Layer 0 - Gradient Weights Max: 0.007506794017752061 Min: -0.006426067921403064\n","ReLU Activation - Max: 0.3403532113836647 Min: 0.0\n","Epoch 240, Loss: 0.4689, Test Accuracy: 0.6982\n","ReLU Activation - Max: 0.40034384924572197 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010601432551656393 Min: -0.01016441832017366\n","Layer 0 - Gradient Weights Max: 0.007543099593892158 Min: -0.006467176560963532\n","ReLU Activation - Max: 0.34266776865992654 Min: 0.0\n","ReLU Activation - Max: 0.4030278736640205 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01064393475816203 Min: -0.010200388904789923\n","Layer 0 - Gradient Weights Max: 0.0075768377413680315 Min: -0.006496897114065356\n","ReLU Activation - Max: 0.34499626144060913 Min: 0.0\n","ReLU Activation - Max: 0.40572111090910434 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010685589208750677 Min: -0.010235473113480333\n","Layer 0 - Gradient Weights Max: 0.007593743761875487 Min: -0.006550828654081698\n","ReLU Activation - Max: 0.34733467157820413 Min: 0.0\n","ReLU Activation - Max: 0.4084278255109534 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010726380247532249 Min: -0.0102696565563147\n","Layer 0 - Gradient Weights Max: 0.007627650381364765 Min: -0.006585969078506226\n","ReLU Activation - Max: 0.34968423251874303 Min: 0.0\n","ReLU Activation - Max: 0.4111456502966269 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010766245796281923 Min: -0.010302881136942997\n","Layer 0 - Gradient Weights Max: 0.007650098402125083 Min: -0.006636197749091499\n","ReLU Activation - Max: 0.35204378190790275 Min: 0.0\n","ReLU Activation - Max: 0.4138716038930286 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010805205856923294 Min: -0.010335159060106238\n","Layer 0 - Gradient Weights Max: 0.0076727838260148215 Min: -0.006676427951262414\n","ReLU Activation - Max: 0.3544145398792852 Min: 0.0\n","ReLU Activation - Max: 0.41660711550374846 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010843247565712567 Min: -0.010366475283330262\n","Layer 0 - Gradient Weights Max: 0.007699557990755965 Min: -0.006690282081930834\n","ReLU Activation - Max: 0.3568007070829999 Min: 0.0\n","ReLU Activation - Max: 0.41935181579653413 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010880392349317246 Min: -0.010396856367352108\n","Layer 0 - Gradient Weights Max: 0.007725051699021151 Min: -0.006729846418977082\n","ReLU Activation - Max: 0.3591930487526356 Min: 0.0\n","ReLU Activation - Max: 0.42210506656591934 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010916621231394018 Min: -0.010426287635200999\n","Layer 0 - Gradient Weights Max: 0.0077202737425310095 Min: -0.006758524472953263\n","ReLU Activation - Max: 0.361594949546686 Min: 0.0\n","ReLU Activation - Max: 0.4248653957432551 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01095188837415164 Min: -0.010454730731406047\n","Layer 0 - Gradient Weights Max: 0.0077183447713956325 Min: -0.0068142374257476705\n","ReLU Activation - Max: 0.3640020633496158 Min: 0.0\n","Epoch 250, Loss: 0.4641, Test Accuracy: 0.6979\n","ReLU Activation - Max: 0.42763402091061364 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010986166437820506 Min: -0.010482163665815183\n","Layer 0 - Gradient Weights Max: 0.0077274932339683276 Min: -0.00685254049410154\n","ReLU Activation - Max: 0.3664170739159741 Min: 0.0\n","ReLU Activation - Max: 0.43040887778511544 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011019402671046464 Min: -0.010508528831706951\n","Layer 0 - Gradient Weights Max: 0.007757320252770394 Min: -0.00688967682296653\n","ReLU Activation - Max: 0.3688395315745202 Min: 0.0\n","ReLU Activation - Max: 0.4331930767801392 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01105155822590635 Min: -0.010533787478606536\n","Layer 0 - Gradient Weights Max: 0.007777901390937749 Min: -0.006907415928289518\n","ReLU Activation - Max: 0.37127287693873623 Min: 0.0\n","ReLU Activation - Max: 0.43599100793983475 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011082647756812012 Min: -0.010557956965436092\n","Layer 0 - Gradient Weights Max: 0.0077840151542496495 Min: -0.006937162604086065\n","ReLU Activation - Max: 0.3737157361410631 Min: 0.0\n","ReLU Activation - Max: 0.43879488160871966 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011112624611254795 Min: -0.010580989487560623\n","Layer 0 - Gradient Weights Max: 0.007795606424578859 Min: -0.0069823601422724785\n","ReLU Activation - Max: 0.37617066724680787 Min: 0.0\n","ReLU Activation - Max: 0.44160700550798004 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011141444544805992 Min: -0.010602849389635868\n","Layer 0 - Gradient Weights Max: 0.007805008610314808 Min: -0.007010714688568386\n","ReLU Activation - Max: 0.3786359581464152 Min: 0.0\n","ReLU Activation - Max: 0.4444269197336618 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011169133551152583 Min: -0.010623564103272325\n","Layer 0 - Gradient Weights Max: 0.007837704405903466 Min: -0.007013084585446969\n","ReLU Activation - Max: 0.38111106634822794 Min: 0.0\n","ReLU Activation - Max: 0.44725512101591 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0111956925872561 Min: -0.010643134116141319\n","Layer 0 - Gradient Weights Max: 0.007858189918534923 Min: -0.00707754411677219\n","ReLU Activation - Max: 0.38359579136696464 Min: 0.0\n","ReLU Activation - Max: 0.45008991916638125 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011221083673885161 Min: -0.01066152217078015\n","Layer 0 - Gradient Weights Max: 0.007874629769732211 Min: -0.007137001781114402\n","ReLU Activation - Max: 0.3860895844008657 Min: 0.0\n","ReLU Activation - Max: 0.4529285345301807 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011245263098688032 Min: -0.010690079043935617\n","Layer 0 - Gradient Weights Max: 0.00787966738597378 Min: -0.007146306437834338\n","ReLU Activation - Max: 0.3885960222838833 Min: 0.0\n","Epoch 260, Loss: 0.4590, Test Accuracy: 0.6979\n","ReLU Activation - Max: 0.4557767551098607 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01126823390716607 Min: -0.010723527586613775\n","Layer 0 - Gradient Weights Max: 0.007902981178875633 Min: -0.007175676910785816\n","ReLU Activation - Max: 0.3911146001879499 Min: 0.0\n","ReLU Activation - Max: 0.45863313623508567 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011289960080349746 Min: -0.01075594908837231\n","Layer 0 - Gradient Weights Max: 0.007904702283827615 Min: -0.0072273911573279975\n","ReLU Activation - Max: 0.3936402162464462 Min: 0.0\n","ReLU Activation - Max: 0.46149104276046726 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011310450314601988 Min: -0.010787381190728494\n","Layer 0 - Gradient Weights Max: 0.007888732194992625 Min: -0.007283442265244304\n","ReLU Activation - Max: 0.396164913397252 Min: 0.0\n","ReLU Activation - Max: 0.4643533881597923 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011329685445580353 Min: -0.010817851701705104\n","Layer 0 - Gradient Weights Max: 0.007878097616051205 Min: -0.0073122355734038585\n","ReLU Activation - Max: 0.3986960502534419 Min: 0.0\n","ReLU Activation - Max: 0.4672263282800535 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01134760897881021 Min: -0.010847263651311275\n","Layer 0 - Gradient Weights Max: 0.007874593197627544 Min: -0.007339925967733307\n","ReLU Activation - Max: 0.4012319737140856 Min: 0.0\n","ReLU Activation - Max: 0.4701014420780786 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01136419282871852 Min: -0.010875584908839508\n","Layer 0 - Gradient Weights Max: 0.007860043373992903 Min: -0.00736298591370364\n","ReLU Activation - Max: 0.40377648721952014 Min: 0.0\n","ReLU Activation - Max: 0.47297927085838315 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011379479870331556 Min: -0.010902739352025353\n","Layer 0 - Gradient Weights Max: 0.00785918133098953 Min: -0.0073937013349598835\n","ReLU Activation - Max: 0.4063258650138413 Min: 0.0\n","ReLU Activation - Max: 0.4758587327423177 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011393422393628592 Min: -0.010928702896036107\n","Layer 0 - Gradient Weights Max: 0.00786316741178571 Min: -0.00741565160434076\n","ReLU Activation - Max: 0.40888453954276366 Min: 0.0\n","ReLU Activation - Max: 0.4787411926661588 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011406022719712488 Min: -0.010953465458411953\n","Layer 0 - Gradient Weights Max: 0.00786423114331112 Min: -0.007432944364015732\n","ReLU Activation - Max: 0.4114459915696352 Min: 0.0\n","ReLU Activation - Max: 0.4816293417529387 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011417297663146874 Min: -0.010977028445613629\n","Layer 0 - Gradient Weights Max: 0.007846469683005793 Min: -0.007461687787808286\n","ReLU Activation - Max: 0.4140108827519652 Min: 0.0\n","Epoch 270, Loss: 0.4537, Test Accuracy: 0.6979\n","ReLU Activation - Max: 0.4845281657659477 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011427205574790938 Min: -0.010999382299999813\n","Layer 0 - Gradient Weights Max: 0.007845443691233666 Min: -0.007474245253116173\n","ReLU Activation - Max: 0.4165827396263161 Min: 0.0\n","ReLU Activation - Max: 0.48742828113314224 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011435752554241974 Min: -0.011020424926624504\n","Layer 0 - Gradient Weights Max: 0.00786242838110849 Min: -0.007491424087654758\n","ReLU Activation - Max: 0.4191618190245432 Min: 0.0\n","ReLU Activation - Max: 0.49032785068903706 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011442895914007694 Min: -0.011040212307730609\n","Layer 0 - Gradient Weights Max: 0.00786339072267382 Min: -0.007546304241128258\n","ReLU Activation - Max: 0.42173825082756083 Min: 0.0\n","ReLU Activation - Max: 0.4932255613984239 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011448588106351475 Min: -0.011058799625839357\n","Layer 0 - Gradient Weights Max: 0.007861495384472502 Min: -0.007551458417094842\n","ReLU Activation - Max: 0.4243200710474152 Min: 0.0\n","ReLU Activation - Max: 0.4961183413711464 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011452857896912668 Min: -0.011076056340268543\n","Layer 0 - Gradient Weights Max: 0.007848757978064492 Min: -0.007574569226275638\n","ReLU Activation - Max: 0.4269073661059398 Min: 0.0\n","ReLU Activation - Max: 0.4990084692752523 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0114557045002405 Min: -0.011092016716341843\n","Layer 0 - Gradient Weights Max: 0.007857661643915725 Min: -0.007610477620303131\n","ReLU Activation - Max: 0.4294998960147051 Min: 0.0\n","ReLU Activation - Max: 0.5018989847952913 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01145701973704174 Min: -0.011106665228794553\n","Layer 0 - Gradient Weights Max: 0.007864178577679562 Min: -0.007637257412390017\n","ReLU Activation - Max: 0.4320931762365172 Min: 0.0\n","ReLU Activation - Max: 0.5047859372408328 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011456861710571095 Min: -0.011120066507691739\n","Layer 0 - Gradient Weights Max: 0.007858319897101194 Min: -0.007641466213655287\n","ReLU Activation - Max: 0.4346943725332735 Min: 0.0\n","ReLU Activation - Max: 0.5076711808359613 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011455200739434948 Min: -0.011132116572975463\n","Layer 0 - Gradient Weights Max: 0.007861261587120487 Min: -0.007652446607668164\n","ReLU Activation - Max: 0.43729715224356375 Min: 0.0\n","ReLU Activation - Max: 0.5105590208408125 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011452051922468886 Min: -0.011142768494669388\n","Layer 0 - Gradient Weights Max: 0.007813405275898246 Min: -0.007658120042902789\n","ReLU Activation - Max: 0.43990523747077626 Min: 0.0\n","Epoch 280, Loss: 0.4482, Test Accuracy: 0.6975\n","ReLU Activation - Max: 0.5134476851525692 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0114473660090541 Min: -0.011151950717599425\n","Layer 0 - Gradient Weights Max: 0.007815371910943765 Min: -0.007649743608690229\n","ReLU Activation - Max: 0.44251896249187095 Min: 0.0\n","ReLU Activation - Max: 0.5163322664122972 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011441160320822225 Min: -0.011159718366814845\n","Layer 0 - Gradient Weights Max: 0.00779319235575907 Min: -0.007665752718480665\n","ReLU Activation - Max: 0.44513111088287627 Min: 0.0\n","ReLU Activation - Max: 0.5192166829308411 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01143345760860895 Min: -0.011166028016261402\n","Layer 0 - Gradient Weights Max: 0.007780560727196252 Min: -0.007659899462943017\n","ReLU Activation - Max: 0.4477475151994941 Min: 0.0\n","ReLU Activation - Max: 0.522099714845428 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011424253047608947 Min: -0.011170837358851288\n","Layer 0 - Gradient Weights Max: 0.007755823493581268 Min: -0.007660356919724808\n","ReLU Activation - Max: 0.45036591502854434 Min: 0.0\n","ReLU Activation - Max: 0.5249806018019564 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01141354453186243 Min: -0.011174214039830651\n","Layer 0 - Gradient Weights Max: 0.007701954443978231 Min: -0.00765362144847509\n","ReLU Activation - Max: 0.45298607817885783 Min: 0.0\n","ReLU Activation - Max: 0.5278617674605633 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011401297929319746 Min: -0.011176081914949709\n","Layer 0 - Gradient Weights Max: 0.007684581165850031 Min: -0.007675310962472674\n","ReLU Activation - Max: 0.45560069633413314 Min: 0.0\n","ReLU Activation - Max: 0.5307434730243422 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011387524948116363 Min: -0.011176484636563327\n","Layer 0 - Gradient Weights Max: 0.007654094292087157 Min: -0.007680009599572185\n","ReLU Activation - Max: 0.45821646968587126 Min: 0.0\n","ReLU Activation - Max: 0.5336217008306561 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011372186140852576 Min: -0.011175412458860631\n","Layer 0 - Gradient Weights Max: 0.007621508453642458 Min: -0.007678307009593652\n","ReLU Activation - Max: 0.46083347261946744 Min: 0.0\n","ReLU Activation - Max: 0.5364994139857058 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011355325749088109 Min: -0.011172841903552003\n","Layer 0 - Gradient Weights Max: 0.007585177226404802 Min: -0.007714788080263406\n","ReLU Activation - Max: 0.46344949172075406 Min: 0.0\n","ReLU Activation - Max: 0.5393729797426616 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011336973138406904 Min: -0.01116887349969416\n","Layer 0 - Gradient Weights Max: 0.0075688496909075385 Min: -0.0077300360477586165\n","ReLU Activation - Max: 0.46606073887081323 Min: 0.0\n","Epoch 290, Loss: 0.4427, Test Accuracy: 0.6961\n","ReLU Activation - Max: 0.5422460575729372 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011317127266555024 Min: -0.011163420409507106\n","Layer 0 - Gradient Weights Max: 0.007551594112871258 Min: -0.0077252139709412535\n","ReLU Activation - Max: 0.46867176411739697 Min: 0.0\n","ReLU Activation - Max: 0.5451141413831262 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011295784233532205 Min: -0.011156480610268091\n","Layer 0 - Gradient Weights Max: 0.0075283358694888235 Min: -0.007755558243062868\n","ReLU Activation - Max: 0.4712792242004341 Min: 0.0\n","ReLU Activation - Max: 0.5479804602215845 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011272955533292666 Min: -0.011148091727226622\n","Layer 0 - Gradient Weights Max: 0.007536815174153115 Min: -0.007737909654025924\n","ReLU Activation - Max: 0.4738874792410868 Min: 0.0\n","ReLU Activation - Max: 0.550841630695573 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011248627774179622 Min: -0.011138173480915184\n","Layer 0 - Gradient Weights Max: 0.0075248541391266 Min: -0.007751644625186923\n","ReLU Activation - Max: 0.47649147905467776 Min: 0.0\n","ReLU Activation - Max: 0.5536965785120943 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01122281509538326 Min: -0.0111267832122638\n","Layer 0 - Gradient Weights Max: 0.007493925453651001 Min: -0.007744935004795438\n","ReLU Activation - Max: 0.479086453909544 Min: 0.0\n","ReLU Activation - Max: 0.5565440660206256 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011195492959697231 Min: -0.011113930979718311\n","Layer 0 - Gradient Weights Max: 0.007480493719826954 Min: -0.007731971999623969\n","ReLU Activation - Max: 0.48168365540294983 Min: 0.0\n","ReLU Activation - Max: 0.5593853991911554 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011166640960331682 Min: -0.011099559882637318\n","Layer 0 - Gradient Weights Max: 0.007445646865643282 Min: -0.00771382595689729\n","ReLU Activation - Max: 0.48428090648485567 Min: 0.0\n","ReLU Activation - Max: 0.562220549070986 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011136307308656597 Min: -0.011083699607322224\n","Layer 0 - Gradient Weights Max: 0.007417732728494185 Min: -0.007740452218715334\n","ReLU Activation - Max: 0.48687268999380345 Min: 0.0\n","ReLU Activation - Max: 0.5650509086608088 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011104529115771944 Min: -0.011066448488751074\n","Layer 0 - Gradient Weights Max: 0.007398137977472625 Min: -0.007753664502622036\n","ReLU Activation - Max: 0.4894595178296186 Min: 0.0\n","ReLU Activation - Max: 0.5678699725170002 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011071335689215277 Min: -0.011047733895896942\n","Layer 0 - Gradient Weights Max: 0.007357842288585243 Min: -0.007767235491297491\n","ReLU Activation - Max: 0.4920439682978655 Min: 0.0\n","Epoch 300, Loss: 0.4373, Test Accuracy: 0.6954\n","ReLU Activation - Max: 0.5706790117013467 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011036714342623405 Min: -0.011027573033208901\n","Layer 0 - Gradient Weights Max: 0.007324702319402014 Min: -0.007764384132268624\n","ReLU Activation - Max: 0.49462387862994417 Min: 0.0\n","ReLU Activation - Max: 0.5734794361604805 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.011000665177981013 Min: -0.011005928352440136\n","Layer 0 - Gradient Weights Max: 0.007290116592577816 Min: -0.007744315312203872\n","ReLU Activation - Max: 0.49720394877116064 Min: 0.0\n","ReLU Activation - Max: 0.5762708426847277 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010963216585215948 Min: -0.010982779982593215\n","Layer 0 - Gradient Weights Max: 0.007266125215361876 Min: -0.007725877329017192\n","ReLU Activation - Max: 0.49977948499699526 Min: 0.0\n","ReLU Activation - Max: 0.5790519810743096 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010924371174756092 Min: -0.010958142443300975\n","Layer 0 - Gradient Weights Max: 0.007243368462185012 Min: -0.0077211240220027705\n","ReLU Activation - Max: 0.5023522913003644 Min: 0.0\n","ReLU Activation - Max: 0.5818247576737596 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010884180652408385 Min: -0.010932058139434288\n","Layer 0 - Gradient Weights Max: 0.0071879344479330864 Min: -0.007703199645514468\n","ReLU Activation - Max: 0.5049199985163136 Min: 0.0\n","ReLU Activation - Max: 0.5845910754943467 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010842650046269938 Min: -0.01090453875924837\n","Layer 0 - Gradient Weights Max: 0.007153622429785608 Min: -0.007697558358579101\n","ReLU Activation - Max: 0.5074816230298512 Min: 0.0\n","ReLU Activation - Max: 0.5873493981577125 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010799776333550441 Min: -0.010875605667193616\n","Layer 0 - Gradient Weights Max: 0.007109156570510193 Min: -0.007682819677426457\n","ReLU Activation - Max: 0.5100352546675281 Min: 0.0\n","ReLU Activation - Max: 0.5900989857259539 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010755600762426832 Min: -0.010845264604661368\n","Layer 0 - Gradient Weights Max: 0.007070236631251214 Min: -0.007657813690781429\n","ReLU Activation - Max: 0.5125799160223118 Min: 0.0\n","ReLU Activation - Max: 0.5928393375944329 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010710147856052883 Min: -0.010813500240255187\n","Layer 0 - Gradient Weights Max: 0.0070224201934340385 Min: -0.007648259484618468\n","ReLU Activation - Max: 0.5151118842566522 Min: 0.0\n","ReLU Activation - Max: 0.5955689521554245 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010663407054350978 Min: -0.010780331708174511\n","Layer 0 - Gradient Weights Max: 0.0069918336261325514 Min: -0.007631008437025685\n","ReLU Activation - Max: 0.51763529635087 Min: 0.0\n","Epoch 310, Loss: 0.4323, Test Accuracy: 0.6950\n","ReLU Activation - Max: 0.5982871886479286 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01061538658281461 Min: -0.01074577338018825\n","Layer 0 - Gradient Weights Max: 0.006928343800464433 Min: -0.0076056012293751925\n","ReLU Activation - Max: 0.5201523074600934 Min: 0.0\n","ReLU Activation - Max: 0.6009942416176005 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010566106350241017 Min: -0.010709865084577332\n","Layer 0 - Gradient Weights Max: 0.006890273791879914 Min: -0.007572499994475723\n","ReLU Activation - Max: 0.5226588692096585 Min: 0.0\n","ReLU Activation - Max: 0.6036939091003382 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010515625820436638 Min: -0.01067260670766714\n","Layer 0 - Gradient Weights Max: 0.0068513873975411 Min: -0.007541481428674863\n","ReLU Activation - Max: 0.525159820933232 Min: 0.0\n","ReLU Activation - Max: 0.6063821935011826 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010463934787908542 Min: -0.010633977663697482\n","Layer 0 - Gradient Weights Max: 0.006849986058750337 Min: -0.007528212101861356\n","ReLU Activation - Max: 0.527650193656807 Min: 0.0\n","ReLU Activation - Max: 0.6090531670624946 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010411053825015273 Min: -0.010593990994650672\n","Layer 0 - Gradient Weights Max: 0.0068030125265184605 Min: -0.007531798822174508\n","ReLU Activation - Max: 0.5301287219306039 Min: 0.0\n","ReLU Activation - Max: 0.611712213132045 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010357019048899399 Min: -0.010552757329445666\n","Layer 0 - Gradient Weights Max: 0.006776174564357027 Min: -0.0074976185122017265\n","ReLU Activation - Max: 0.5325976692728555 Min: 0.0\n","ReLU Activation - Max: 0.6143597612665125 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010301882896839958 Min: -0.010510187551904922\n","Layer 0 - Gradient Weights Max: 0.0067331182804580385 Min: -0.007480471810554071\n","ReLU Activation - Max: 0.5350497454525737 Min: 0.0\n","ReLU Activation - Max: 0.6169941990307112 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010246990689394143 Min: -0.010466323757237096\n","Layer 0 - Gradient Weights Max: 0.006686672002603121 Min: -0.007462793353484216\n","ReLU Activation - Max: 0.53748783868186 Min: 0.0\n","ReLU Activation - Max: 0.6196161274110638 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01020187520808624 Min: -0.01042121709054506\n","Layer 0 - Gradient Weights Max: 0.006626628757743143 Min: -0.0074418100070819965\n","ReLU Activation - Max: 0.539917775577014 Min: 0.0\n","ReLU Activation - Max: 0.6222162084762377 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010155549722153497 Min: -0.010374843106935932\n","Layer 0 - Gradient Weights Max: 0.006574726475072104 Min: -0.007397142541214323\n","ReLU Activation - Max: 0.5423407636144029 Min: 0.0\n","Epoch 320, Loss: 0.4276, Test Accuracy: 0.6946\n","ReLU Activation - Max: 0.6248022649428757 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010107994885654817 Min: -0.010327185370956548\n","Layer 0 - Gradient Weights Max: 0.006512757286296504 Min: -0.00738092028655995\n","ReLU Activation - Max: 0.5447506583387388 Min: 0.0\n","ReLU Activation - Max: 0.6273776392035388 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.01005934971153565 Min: -0.010278385050873317\n","Layer 0 - Gradient Weights Max: 0.006458246959458347 Min: -0.00735176871415917\n","ReLU Activation - Max: 0.5471485495759486 Min: 0.0\n","ReLU Activation - Max: 0.6299398624060318 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.010009608383019217 Min: -0.010228439433186383\n","Layer 0 - Gradient Weights Max: 0.006411593717766541 Min: -0.0073595206379262505\n","ReLU Activation - Max: 0.549531986056126 Min: 0.0\n","ReLU Activation - Max: 0.6324884236377641 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009958818609560714 Min: -0.01017739565689156\n","Layer 0 - Gradient Weights Max: 0.006354775031340871 Min: -0.00732407294964064\n","ReLU Activation - Max: 0.5519062551619002 Min: 0.0\n","ReLU Activation - Max: 0.6350220708537583 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009906979513750728 Min: -0.010125249084495764\n","Layer 0 - Gradient Weights Max: 0.0062982069217595955 Min: -0.0073075243224151746\n","ReLU Activation - Max: 0.5542615527615128 Min: 0.0\n","ReLU Activation - Max: 0.637540583942725 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009854150824204983 Min: -0.010072051319887042\n","Layer 0 - Gradient Weights Max: 0.006251382112671848 Min: -0.00729449156101229\n","ReLU Activation - Max: 0.5566012262120048 Min: 0.0\n","ReLU Activation - Max: 0.6400482811376825 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009800388388407083 Min: -0.010017867447519159\n","Layer 0 - Gradient Weights Max: 0.006204888967422074 Min: -0.007254836932170338\n","ReLU Activation - Max: 0.5589275259351104 Min: 0.0\n","ReLU Activation - Max: 0.6425374264476785 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009745602792857386 Min: -0.009962612305540802\n","Layer 0 - Gradient Weights Max: 0.006178274457675695 Min: -0.007233039647590975\n","ReLU Activation - Max: 0.5612409876624753 Min: 0.0\n","ReLU Activation - Max: 0.6450090679162107 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009689816068337078 Min: -0.009906303844712132\n","Layer 0 - Gradient Weights Max: 0.006118190335807836 Min: -0.007191907170883867\n","ReLU Activation - Max: 0.5635438754735522 Min: 0.0\n","ReLU Activation - Max: 0.6474641342897682 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009633076930614066 Min: -0.009848993127325975\n","Layer 0 - Gradient Weights Max: 0.006070726650786615 Min: -0.007147612581812284\n","ReLU Activation - Max: 0.5658343731300541 Min: 0.0\n","Epoch 330, Loss: 0.4233, Test Accuracy: 0.6939\n","ReLU Activation - Max: 0.6499999095551473 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009575438520257043 Min: -0.009790723129244987\n","Layer 0 - Gradient Weights Max: 0.006036481468906594 Min: -0.00711545580085663\n","ReLU Activation - Max: 0.5681090354099526 Min: 0.0\n","ReLU Activation - Max: 0.6525381810040575 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009516952613173667 Min: -0.009731556263810942\n","Layer 0 - Gradient Weights Max: 0.005988144907303492 Min: -0.00709359213221285\n","ReLU Activation - Max: 0.5703700174941854 Min: 0.0\n","ReLU Activation - Max: 0.6550601644617778 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00945765498818027 Min: -0.009671521922161117\n","Layer 0 - Gradient Weights Max: 0.005934369521298736 Min: -0.007090928980491575\n","ReLU Activation - Max: 0.5726155127077257 Min: 0.0\n","ReLU Activation - Max: 0.6575634072965835 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009397553388820461 Min: -0.009610623440840219\n","Layer 0 - Gradient Weights Max: 0.005892999132784801 Min: -0.007048853235528783\n","ReLU Activation - Max: 0.5748521975550256 Min: 0.0\n","ReLU Activation - Max: 0.6600492738474856 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009336601206089257 Min: -0.009548832708077364\n","Layer 0 - Gradient Weights Max: 0.00585849581062935 Min: -0.006988980593164271\n","ReLU Activation - Max: 0.5770710977123179 Min: 0.0\n","ReLU Activation - Max: 0.6625190296467137 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009274878373591915 Min: -0.009486222097944412\n","Layer 0 - Gradient Weights Max: 0.005837846969333855 Min: -0.006947210332280683\n","ReLU Activation - Max: 0.5792768220793734 Min: 0.0\n","ReLU Activation - Max: 0.6649701324288404 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009212407823310238 Min: -0.009422816341130843\n","Layer 0 - Gradient Weights Max: 0.005786067048679106 Min: -0.006915762714391403\n","ReLU Activation - Max: 0.5814660203864005 Min: 0.0\n","ReLU Activation - Max: 0.6674030129744437 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009149191343847916 Min: -0.009358615093581884\n","Layer 0 - Gradient Weights Max: 0.0057375262175452186 Min: -0.006859827439024184\n","ReLU Activation - Max: 0.5836378356316432 Min: 0.0\n","ReLU Activation - Max: 0.6698169020209053 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009085250364886995 Min: -0.009293642785548475\n","Layer 0 - Gradient Weights Max: 0.00568300318304628 Min: -0.0068301489390803485\n","ReLU Activation - Max: 0.5857920807998026 Min: 0.0\n","ReLU Activation - Max: 0.6722122965964058 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.009020709306929317 Min: -0.009228017689773334\n","Layer 0 - Gradient Weights Max: 0.005616752286126452 Min: -0.006795715999615643\n","ReLU Activation - Max: 0.5879261667399183 Min: 0.0\n","Epoch 340, Loss: 0.4196, Test Accuracy: 0.6932\n","ReLU Activation - Max: 0.6745890829740415 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008955594612128782 Min: -0.009161770822795163\n","Layer 0 - Gradient Weights Max: 0.005540979093318256 Min: -0.006758257115480248\n","ReLU Activation - Max: 0.5900434036869899 Min: 0.0\n","ReLU Activation - Max: 0.6769471513702338 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00888991008160168 Min: -0.009094902739684036\n","Layer 0 - Gradient Weights Max: 0.005464731807550349 Min: -0.006742174842764082\n","ReLU Activation - Max: 0.5921457884906163 Min: 0.0\n","ReLU Activation - Max: 0.679286385907568 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008823692210640938 Min: -0.009027451956796469\n","Layer 0 - Gradient Weights Max: 0.005410841278326485 Min: -0.0067252865531738475\n","ReLU Activation - Max: 0.5942249890560151 Min: 0.0\n","ReLU Activation - Max: 0.6816068270216404 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008756959075866797 Min: -0.008959442513888439\n","Layer 0 - Gradient Weights Max: 0.005358202011197767 Min: -0.006682613215330492\n","ReLU Activation - Max: 0.5962809307375246 Min: 0.0\n","ReLU Activation - Max: 0.683908251277346 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008689761785867145 Min: -0.00889091592220276\n","Layer 0 - Gradient Weights Max: 0.005294533428118268 Min: -0.006670954800885399\n","ReLU Activation - Max: 0.5983195872404051 Min: 0.0\n","ReLU Activation - Max: 0.6861874584122858 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008622155335878278 Min: -0.008821935093212043\n","Layer 0 - Gradient Weights Max: 0.005226655448942773 Min: -0.006615010704074438\n","ReLU Activation - Max: 0.6003422972388011 Min: 0.0\n","ReLU Activation - Max: 0.6884460683336395 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008554064685630768 Min: -0.008752424040544087\n","Layer 0 - Gradient Weights Max: 0.00516725385872056 Min: -0.00656153423813085\n","ReLU Activation - Max: 0.6023495945630626 Min: 0.0\n","ReLU Activation - Max: 0.6908903390539948 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008485525497427456 Min: -0.008682422929556326\n","Layer 0 - Gradient Weights Max: 0.0050949307152566205 Min: -0.00652547070736841\n","ReLU Activation - Max: 0.6043375595468257 Min: 0.0\n","ReLU Activation - Max: 0.6934128035061188 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008416632887924946 Min: -0.008612020228315804\n","Layer 0 - Gradient Weights Max: 0.005033368894620378 Min: -0.006479895280580554\n","ReLU Activation - Max: 0.6063079408341207 Min: 0.0\n","ReLU Activation - Max: 0.695916275553506 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008347398594163568 Min: -0.008541226484322362\n","Layer 0 - Gradient Weights Max: 0.00497985426359264 Min: -0.006430800214063585\n","ReLU Activation - Max: 0.6082609771217864 Min: 0.0\n","Epoch 350, Loss: 0.4164, Test Accuracy: 0.6936\n","ReLU Activation - Max: 0.6984000683787956 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008277867391883617 Min: -0.008470094421626615\n","Layer 0 - Gradient Weights Max: 0.004915831965731525 Min: -0.006392233275343527\n","ReLU Activation - Max: 0.6101921580247303 Min: 0.0\n","ReLU Activation - Max: 0.7008617067856229 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008208100880503087 Min: -0.00839867834090081\n","Layer 0 - Gradient Weights Max: 0.004860167927954732 Min: -0.006355061717528613\n","ReLU Activation - Max: 0.6120991109324888 Min: 0.0\n","ReLU Activation - Max: 0.7033008209373336 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008138127567111499 Min: -0.008327002756322182\n","Layer 0 - Gradient Weights Max: 0.004808549628588946 Min: -0.0062838571745929\n","ReLU Activation - Max: 0.6139909627067728 Min: 0.0\n","ReLU Activation - Max: 0.7057233215638057 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.008067918351916946 Min: -0.008255053333796837\n","Layer 0 - Gradient Weights Max: 0.004761892603904482 Min: -0.006230219300540895\n","ReLU Activation - Max: 0.6158672764638997 Min: 0.0\n","ReLU Activation - Max: 0.7081253353195589 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007997526847978963 Min: -0.00818287284619557\n","Layer 0 - Gradient Weights Max: 0.004708184271138842 Min: -0.006199396774470027\n","ReLU Activation - Max: 0.6177212751727945 Min: 0.0\n","ReLU Activation - Max: 0.7105089178470084 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007926944934239325 Min: -0.008110450142371302\n","Layer 0 - Gradient Weights Max: 0.0046623884211138816 Min: -0.0061522448704268646\n","ReLU Activation - Max: 0.6195631135977876 Min: 0.0\n","ReLU Activation - Max: 0.7128769566656736 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007856129598613065 Min: -0.008037754771357675\n","Layer 0 - Gradient Weights Max: 0.004627640119495199 Min: -0.0061091973931749815\n","ReLU Activation - Max: 0.6213840198651817 Min: 0.0\n","ReLU Activation - Max: 0.7152193723614241 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007785132158576447 Min: -0.00796484583640159\n","Layer 0 - Gradient Weights Max: 0.004582113277393534 Min: -0.006072480110983842\n","ReLU Activation - Max: 0.6231943279087079 Min: 0.0\n","ReLU Activation - Max: 0.7175406529086552 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007714010003352215 Min: -0.007891769747817917\n","Layer 0 - Gradient Weights Max: 0.004535181325257466 Min: -0.006045926466245464\n","ReLU Activation - Max: 0.6249857786169672 Min: 0.0\n","ReLU Activation - Max: 0.7198377060288389 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0076427630349103015 Min: -0.007818531111127264\n","Layer 0 - Gradient Weights Max: 0.004481854170872406 Min: -0.006012232213739542\n","ReLU Activation - Max: 0.6267546842583676 Min: 0.0\n","Epoch 360, Loss: 0.4138, Test Accuracy: 0.6932\n","ReLU Activation - Max: 0.7221095730991131 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0075714755408532956 Min: -0.007745225986670631\n","Layer 0 - Gradient Weights Max: 0.004453267570052685 Min: -0.005967884942313464\n","ReLU Activation - Max: 0.6285070189585984 Min: 0.0\n","ReLU Activation - Max: 0.7243643696603362 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007500197128106252 Min: -0.007671892881278707\n","Layer 0 - Gradient Weights Max: 0.004391144401754095 Min: -0.005936052132445149\n","ReLU Activation - Max: 0.6302382794179123 Min: 0.0\n","ReLU Activation - Max: 0.7266059856613224 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0074289599946815625 Min: -0.007598556311061546\n","Layer 0 - Gradient Weights Max: 0.00433645923160942 Min: -0.0059012480262278415\n","ReLU Activation - Max: 0.6319615860312003 Min: 0.0\n","ReLU Activation - Max: 0.7288249822419866 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007357700225763049 Min: -0.0075251678725810085\n","Layer 0 - Gradient Weights Max: 0.0042935281730271625 Min: -0.00588762024423147\n","ReLU Activation - Max: 0.6336713382577426 Min: 0.0\n","ReLU Activation - Max: 0.7310230648148826 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007286452933619165 Min: -0.0074517530627816585\n","Layer 0 - Gradient Weights Max: 0.004238003538921234 Min: -0.005890198835029561\n","ReLU Activation - Max: 0.6353627186256636 Min: 0.0\n","ReLU Activation - Max: 0.7331984306768755 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007215277667273883 Min: -0.007378377261232015\n","Layer 0 - Gradient Weights Max: 0.004185689733509124 Min: -0.005863270666149168\n","ReLU Activation - Max: 0.6370320965703453 Min: 0.0\n","ReLU Activation - Max: 0.7353534730639608 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007144194111747203 Min: -0.007305062161079422\n","Layer 0 - Gradient Weights Max: 0.004137295346545171 Min: -0.005860708285738534\n","ReLU Activation - Max: 0.6386873315086139 Min: 0.0\n","ReLU Activation - Max: 0.7374893408189621 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00707321442588851 Min: -0.007231816498839143\n","Layer 0 - Gradient Weights Max: 0.004085368724675328 Min: -0.005838654260481445\n","ReLU Activation - Max: 0.6403235563421993 Min: 0.0\n","ReLU Activation - Max: 0.7396064139748258 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.007002344827127458 Min: -0.007158651986827389\n","Layer 0 - Gradient Weights Max: 0.004033696227587723 Min: -0.005811695471780685\n","ReLU Activation - Max: 0.6419353857168042 Min: 0.0\n","ReLU Activation - Max: 0.7416984351981764 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006931613787369409 Min: -0.00708558826062661\n","Layer 0 - Gradient Weights Max: 0.003983600191445445 Min: -0.005778746169260543\n","ReLU Activation - Max: 0.6435299310233351 Min: 0.0\n","Epoch 370, Loss: 0.4115, Test Accuracy: 0.6925\n","ReLU Activation - Max: 0.7437725546892789 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006861132321951056 Min: -0.007012740658577565\n","Layer 0 - Gradient Weights Max: 0.003939065148419095 Min: -0.005751710159400528\n","ReLU Activation - Max: 0.645104534096198 Min: 0.0\n","ReLU Activation - Max: 0.7458280543046318 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006790914930106665 Min: -0.006940115002130728\n","Layer 0 - Gradient Weights Max: 0.003894270876936962 Min: -0.005732528806148407\n","ReLU Activation - Max: 0.6466604139428529 Min: 0.0\n","ReLU Activation - Max: 0.7478623217222732 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006720899692041097 Min: -0.006867657695741476\n","Layer 0 - Gradient Weights Max: 0.0038272586144788043 Min: -0.005705521436867665\n","ReLU Activation - Max: 0.6481983224101936 Min: 0.0\n","ReLU Activation - Max: 0.7498807372825768 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006651078987141153 Min: -0.006795370227843361\n","Layer 0 - Gradient Weights Max: 0.003777821211839211 Min: -0.005678491302891311\n","ReLU Activation - Max: 0.6497205081795638 Min: 0.0\n","ReLU Activation - Max: 0.7518814764280537 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00658147762186731 Min: -0.006723273898566973\n","Layer 0 - Gradient Weights Max: 0.00372783389585871 Min: -0.00569966705685886\n","ReLU Activation - Max: 0.6512237435037054 Min: 0.0\n","ReLU Activation - Max: 0.7538611859748978 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006512241749441918 Min: -0.006651513939774371\n","Layer 0 - Gradient Weights Max: 0.0036781676101666364 Min: -0.005695035935048737\n","ReLU Activation - Max: 0.6527152492325209 Min: 0.0\n","ReLU Activation - Max: 0.7558189103718685 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006443393274181326 Min: -0.0065801067961459535\n","Layer 0 - Gradient Weights Max: 0.0036287983895616875 Min: -0.00565950959981606\n","ReLU Activation - Max: 0.6541919683376802 Min: 0.0\n","ReLU Activation - Max: 0.757754686332854 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006374900312206231 Min: -0.006509029143684643\n","Layer 0 - Gradient Weights Max: 0.0035751054831144272 Min: -0.005631240736549657\n","ReLU Activation - Max: 0.6556422046542876 Min: 0.0\n","ReLU Activation - Max: 0.7596649157572193 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0063068101473965224 Min: -0.006438328597917528\n","Layer 0 - Gradient Weights Max: 0.003527999373483244 Min: -0.005590204010415228\n","ReLU Activation - Max: 0.6570722357337854 Min: 0.0\n","ReLU Activation - Max: 0.7615583682509538 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0062391066737419966 Min: -0.006367982323172137\n","Layer 0 - Gradient Weights Max: 0.003470326922562694 Min: -0.005590431542089892\n","ReLU Activation - Max: 0.6584863735520718 Min: 0.0\n","Epoch 380, Loss: 0.4097, Test Accuracy: 0.6921\n","ReLU Activation - Max: 0.7634266458223314 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00617173437384903 Min: -0.006297951608009256\n","Layer 0 - Gradient Weights Max: 0.0034168217416899846 Min: -0.005584984809449836\n","ReLU Activation - Max: 0.6598857024365214 Min: 0.0\n","ReLU Activation - Max: 0.7652753585110538 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006104668079155371 Min: -0.00622821384664393\n","Layer 0 - Gradient Weights Max: 0.0033667734526427427 Min: -0.005543400175578025\n","ReLU Activation - Max: 0.6612702062991098 Min: 0.0\n","ReLU Activation - Max: 0.7671011165309178 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.006037900191171312 Min: -0.006158757261462889\n","Layer 0 - Gradient Weights Max: 0.0033159215106790415 Min: -0.005549380772472792\n","ReLU Activation - Max: 0.6626400416794512 Min: 0.0\n","ReLU Activation - Max: 0.7689082079872565 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005971556485937181 Min: -0.006089704589555153\n","Layer 0 - Gradient Weights Max: 0.0032690746207225617 Min: -0.005527054580533008\n","ReLU Activation - Max: 0.663991235064234 Min: 0.0\n","ReLU Activation - Max: 0.7706936302258508 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005905639942214918 Min: -0.006021059107972795\n","Layer 0 - Gradient Weights Max: 0.003202672534320418 Min: -0.0055011856725768506\n","ReLU Activation - Max: 0.6653312600299421 Min: 0.0\n","ReLU Activation - Max: 0.7724629004029989 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005840133116505781 Min: -0.005952805484647307\n","Layer 0 - Gradient Weights Max: 0.0031528795445355316 Min: -0.00546331017051886\n","ReLU Activation - Max: 0.666655903349518 Min: 0.0\n","ReLU Activation - Max: 0.7742154757830211 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00577511032381306 Min: -0.005885013137802563\n","Layer 0 - Gradient Weights Max: 0.003104493908980199 Min: -0.005421098963445182\n","ReLU Activation - Max: 0.6679668888133337 Min: 0.0\n","ReLU Activation - Max: 0.7759464196393037 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005710500939835533 Min: -0.0058176188663971815\n","Layer 0 - Gradient Weights Max: 0.003062418934437029 Min: -0.005377238269388821\n","ReLU Activation - Max: 0.669262428708321 Min: 0.0\n","ReLU Activation - Max: 0.777659979457 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005646369219706142 Min: -0.005750687942449061\n","Layer 0 - Gradient Weights Max: 0.0030262813151990605 Min: -0.005335114060178516\n","ReLU Activation - Max: 0.6705424198717698 Min: 0.0\n","ReLU Activation - Max: 0.7793558562968491 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005582738980814215 Min: -0.005684239694787996\n","Layer 0 - Gradient Weights Max: 0.0029860932676261014 Min: -0.005321908547215494\n","ReLU Activation - Max: 0.6718054642565472 Min: 0.0\n","Epoch 390, Loss: 0.4082, Test Accuracy: 0.6929\n","ReLU Activation - Max: 0.7810293984144465 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005519574182029611 Min: -0.005618238232222753\n","Layer 0 - Gradient Weights Max: 0.0029468735030411704 Min: -0.005338365052558267\n","ReLU Activation - Max: 0.6730527562720656 Min: 0.0\n","ReLU Activation - Max: 0.7826835355338904 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005456929697938363 Min: -0.00555274061707769\n","Layer 0 - Gradient Weights Max: 0.0029033136562437688 Min: -0.005273493570436711\n","ReLU Activation - Max: 0.6742742242328513 Min: 0.0\n","ReLU Activation - Max: 0.7843178095290179 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005394843217154448 Min: -0.005487773163389768\n","Layer 0 - Gradient Weights Max: 0.0028653885641042568 Min: -0.005299226456504601\n","ReLU Activation - Max: 0.6754751528698802 Min: 0.0\n","ReLU Activation - Max: 0.7859368265638769 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005333267358783651 Min: -0.005423297092593555\n","Layer 0 - Gradient Weights Max: 0.0028224569496074054 Min: -0.005284434406061421\n","ReLU Activation - Max: 0.6766557757573706 Min: 0.0\n","ReLU Activation - Max: 0.7875350323143127 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005272200290672571 Min: -0.005359319246195764\n","Layer 0 - Gradient Weights Max: 0.0027793741418710716 Min: -0.005259705137754518\n","ReLU Activation - Max: 0.6778242664305132 Min: 0.0\n","ReLU Activation - Max: 0.7891116598016539 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005211675024965824 Min: -0.005295871634398586\n","Layer 0 - Gradient Weights Max: 0.0027420631140259704 Min: -0.005231992864953884\n","ReLU Activation - Max: 0.6789786632326439 Min: 0.0\n","ReLU Activation - Max: 0.7906714127337227 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005151723919262076 Min: -0.005232992564005057\n","Layer 0 - Gradient Weights Max: 0.0027065051478880105 Min: -0.00526623275986855\n","ReLU Activation - Max: 0.6801173659724055 Min: 0.0\n","ReLU Activation - Max: 0.7922131302119171 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005092354450140239 Min: -0.005170687546782003\n","Layer 0 - Gradient Weights Max: 0.002678474117453211 Min: -0.0052428175395093386\n","ReLU Activation - Max: 0.6812460453153626 Min: 0.0\n","ReLU Activation - Max: 0.7937404753323296 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.005033616881288714 Min: -0.005109001047262937\n","Layer 0 - Gradient Weights Max: 0.0026379178390443674 Min: -0.0052566489796841915\n","ReLU Activation - Max: 0.6823644646468072 Min: 0.0\n","ReLU Activation - Max: 0.7952470574742406 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0049754729051365775 Min: -0.00504789650701914\n","Layer 0 - Gradient Weights Max: 0.0025878848304262824 Min: -0.005225915886001811\n","ReLU Activation - Max: 0.6834707785385307 Min: 0.0\n","Epoch 400, Loss: 0.4070, Test Accuracy: 0.6936\n","ReLU Activation - Max: 0.796737438179048 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004917943768844092 Min: -0.004987396492395515\n","Layer 0 - Gradient Weights Max: 0.0025622371324497246 Min: -0.005202089810444342\n","ReLU Activation - Max: 0.6845589375570837 Min: 0.0\n","ReLU Activation - Max: 0.7982156406043692 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0048610168425055385 Min: -0.004927485485403572\n","Layer 0 - Gradient Weights Max: 0.002488692428509274 Min: -0.00516807836892365\n","ReLU Activation - Max: 0.6856326851164121 Min: 0.0\n","ReLU Activation - Max: 0.7996758430256141 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004804692302705357 Min: -0.0048681717832375765\n","Layer 0 - Gradient Weights Max: 0.002451638695740554 Min: -0.005143610841077148\n","ReLU Activation - Max: 0.6866922255881266 Min: 0.0\n","ReLU Activation - Max: 0.8011181498398744 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004748981245436341 Min: -0.004809467806181068\n","Layer 0 - Gradient Weights Max: 0.0024177892061042357 Min: -0.005122948946150846\n","ReLU Activation - Max: 0.6877418356795744 Min: 0.0\n","ReLU Activation - Max: 0.8025413119364162 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0046938349426616024 Min: -0.004751321395474757\n","Layer 0 - Gradient Weights Max: 0.002379996690955311 Min: -0.005154348676961176\n","ReLU Activation - Max: 0.6887778454362663 Min: 0.0\n","ReLU Activation - Max: 0.8039492495626748 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004639237220950287 Min: -0.00469371805846788\n","Layer 0 - Gradient Weights Max: 0.002367829752986837 Min: -0.005141761859812537\n","ReLU Activation - Max: 0.6898029120793713 Min: 0.0\n","ReLU Activation - Max: 0.805337972595021 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004585217921982806 Min: -0.004636686035846947\n","Layer 0 - Gradient Weights Max: 0.002382623375302835 Min: -0.005137017932123635\n","ReLU Activation - Max: 0.6908183386949712 Min: 0.0\n","ReLU Activation - Max: 0.8067073491264269 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004531811385795503 Min: -0.004580259134130021\n","Layer 0 - Gradient Weights Max: 0.002377115686382995 Min: -0.005147618247881913\n","ReLU Activation - Max: 0.6918165284988923 Min: 0.0\n","ReLU Activation - Max: 0.8080688059678905 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004479105103768865 Min: -0.004524520432463809\n","Layer 0 - Gradient Weights Max: 0.0023979480678474004 Min: -0.00511260371350171\n","ReLU Activation - Max: 0.6927988087793817 Min: 0.0\n","ReLU Activation - Max: 0.8094149206453721 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004426984184048908 Min: -0.004469361104907561\n","Layer 0 - Gradient Weights Max: 0.002400413144481366 Min: -0.0051388708928162856\n","ReLU Activation - Max: 0.6937766615228262 Min: 0.0\n","Epoch 410, Loss: 0.4060, Test Accuracy: 0.6932\n","ReLU Activation - Max: 0.8107439207603415 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004375407570790294 Min: -0.004414755519058551\n","Layer 0 - Gradient Weights Max: 0.0024323284249971964 Min: -0.0051277888806358085\n","ReLU Activation - Max: 0.694740882841432 Min: 0.0\n","ReLU Activation - Max: 0.8120569021951141 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004324454828509818 Min: -0.004360776730627117\n","Layer 0 - Gradient Weights Max: 0.002487958728276907 Min: -0.005116452132537752\n","ReLU Activation - Max: 0.6956960099111764 Min: 0.0\n","ReLU Activation - Max: 0.8133496001397855 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004274120712807432 Min: -0.0043074114213806835\n","Layer 0 - Gradient Weights Max: 0.0025168232449555698 Min: -0.0051289904128130745\n","ReLU Activation - Max: 0.6966383678031214 Min: 0.0\n","ReLU Activation - Max: 0.8146285036937857 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004224419189869956 Min: -0.004254676937298399\n","Layer 0 - Gradient Weights Max: 0.002544887036455525 Min: -0.005120617835230041\n","ReLU Activation - Max: 0.6975675733122679 Min: 0.0\n","ReLU Activation - Max: 0.8158918810900972 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004175380507055466 Min: -0.0042026106361475225\n","Layer 0 - Gradient Weights Max: 0.002561633627804946 Min: -0.005088926832465241\n","ReLU Activation - Max: 0.6984804062915492 Min: 0.0\n","ReLU Activation - Max: 0.8171417111283406 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004126948614577456 Min: -0.004151158365678502\n","Layer 0 - Gradient Weights Max: 0.002562117588990251 Min: -0.0051037272142202715\n","ReLU Activation - Max: 0.6993810355838598 Min: 0.0\n","ReLU Activation - Max: 0.8183768463027472 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004079142275046723 Min: -0.004100330387889244\n","Layer 0 - Gradient Weights Max: 0.002575833660251304 Min: -0.005087651230904442\n","ReLU Activation - Max: 0.7002701448135686 Min: 0.0\n","ReLU Activation - Max: 0.8195973261031609 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004031927698858948 Min: -0.004050100525133486\n","Layer 0 - Gradient Weights Max: 0.002599727247335801 Min: -0.005051162351146959\n","ReLU Activation - Max: 0.7011481828284517 Min: 0.0\n","ReLU Activation - Max: 0.8208018146610743 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003985294857737916 Min: -0.004000456397227878\n","Layer 0 - Gradient Weights Max: 0.002623165241302734 Min: -0.0050173224883070305\n","ReLU Activation - Max: 0.7020227853412654 Min: 0.0\n","ReLU Activation - Max: 0.821987195710504 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003939241600685731 Min: -0.003951405811106292\n","Layer 0 - Gradient Weights Max: 0.002608159788830615 Min: -0.004988978426862386\n","ReLU Activation - Max: 0.7028816146255423 Min: 0.0\n","Epoch 420, Loss: 0.4052, Test Accuracy: 0.6929\n","ReLU Activation - Max: 0.8231613874273945 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003893798551510586 Min: -0.003902961328341745\n","Layer 0 - Gradient Weights Max: 0.002606853563104833 Min: -0.004959790359773773\n","ReLU Activation - Max: 0.7037299287161423 Min: 0.0\n","ReLU Activation - Max: 0.8243247580280308 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003848945328694352 Min: -0.003855106330662276\n","Layer 0 - Gradient Weights Max: 0.002684792760044093 Min: -0.004967155028238761\n","ReLU Activation - Max: 0.7045518430087613 Min: 0.0\n","ReLU Activation - Max: 0.8254703610374009 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003804784579969808 Min: -0.0038079361133050347\n","Layer 0 - Gradient Weights Max: 0.0026970559108018283 Min: -0.004965165599273267\n","ReLU Activation - Max: 0.7053595760616441 Min: 0.0\n","ReLU Activation - Max: 0.8266033077325502 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0037611923466530518 Min: -0.0037613437355437478\n","Layer 0 - Gradient Weights Max: 0.002711913014111482 Min: -0.004925920920853699\n","ReLU Activation - Max: 0.7061565213409231 Min: 0.0\n","ReLU Activation - Max: 0.827720031006744 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0037181766961059937 Min: -0.003715336044446717\n","Layer 0 - Gradient Weights Max: 0.00272383303368903 Min: -0.004897692545825561\n","ReLU Activation - Max: 0.7069540652626243 Min: 0.0\n","ReLU Activation - Max: 0.8288264557736533 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003675664674253601 Min: -0.0036698548709680357\n","Layer 0 - Gradient Weights Max: 0.0027592358120266717 Min: -0.004873040666216593\n","ReLU Activation - Max: 0.7077392178101319 Min: 0.0\n","ReLU Activation - Max: 0.8299165234034939 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0036337341307232807 Min: -0.0036249651173917456\n","Layer 0 - Gradient Weights Max: 0.002770363772356932 Min: -0.00484340970471488\n","ReLU Activation - Max: 0.7085145747718512 Min: 0.0\n","ReLU Activation - Max: 0.8309975683383496 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003592382856331702 Min: -0.0035806552242764627\n","Layer 0 - Gradient Weights Max: 0.0027925037276052833 Min: -0.004827550264541113\n","ReLU Activation - Max: 0.709273754571085 Min: 0.0\n","ReLU Activation - Max: 0.8320641056286389 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035516506598017006 Min: -0.0035369638371077735\n","Layer 0 - Gradient Weights Max: 0.0028431608516612614 Min: -0.00480626834735502\n","ReLU Activation - Max: 0.7100169307030253 Min: 0.0\n","ReLU Activation - Max: 0.8331233655708357 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003511494686301498 Min: -0.003493863075476926\n","Layer 0 - Gradient Weights Max: 0.0028516264497249954 Min: -0.004796828596642143\n","ReLU Activation - Max: 0.7107431178024022 Min: 0.0\n","Epoch 430, Loss: 0.4045, Test Accuracy: 0.6932\n","ReLU Activation - Max: 0.8341692995466106 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003471849903740923 Min: -0.0034512888997142673\n","Layer 0 - Gradient Weights Max: 0.0028696562419666355 Min: -0.004796635184282538\n","ReLU Activation - Max: 0.71145430042036 Min: 0.0\n","ReLU Activation - Max: 0.8352007061327956 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034328125466914715 Min: -0.003409323122092733\n","Layer 0 - Gradient Weights Max: 0.002872350011531072 Min: -0.0048284512396046\n","ReLU Activation - Max: 0.7121565413754138 Min: 0.0\n","ReLU Activation - Max: 0.8362159897290957 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033943770958770178 Min: -0.003367967456688124\n","Layer 0 - Gradient Weights Max: 0.0029405936814070697 Min: -0.004819155631183007\n","ReLU Activation - Max: 0.7128509185883383 Min: 0.0\n","ReLU Activation - Max: 0.8372208112315628 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033565661836397944 Min: -0.0033272466741857266\n","Layer 0 - Gradient Weights Max: 0.0029875754580471446 Min: -0.004796127518126541\n","ReLU Activation - Max: 0.713537276212499 Min: 0.0\n","ReLU Activation - Max: 0.8382081855459653 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033192584402981446 Min: -0.003287032617890929\n","Layer 0 - Gradient Weights Max: 0.0030026003093715295 Min: -0.004818127542681644\n","ReLU Activation - Max: 0.7142159926345177 Min: 0.0\n","ReLU Activation - Max: 0.839184522993111 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003282501004073934 Min: -0.0032473805934887124\n","Layer 0 - Gradient Weights Max: 0.0029957134828292515 Min: -0.004796071490497123\n","ReLU Activation - Max: 0.7148849221180393 Min: 0.0\n","ReLU Activation - Max: 0.8401487936263485 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032462427607775598 Min: -0.003208234018321097\n","Layer 0 - Gradient Weights Max: 0.003021818423721562 Min: -0.004791939122613847\n","ReLU Activation - Max: 0.7155420989474502 Min: 0.0\n","ReLU Activation - Max: 0.8411016830711736 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003210528550143612 Min: -0.003169633718729958\n","Layer 0 - Gradient Weights Max: 0.0030610214858464093 Min: -0.004805740083794493\n","ReLU Activation - Max: 0.7161922720033302 Min: 0.0\n","ReLU Activation - Max: 0.842043054888374 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003175307618211566 Min: -0.003131537080787532\n","Layer 0 - Gradient Weights Max: 0.003068747241759951 Min: -0.0047753587900410815\n","ReLU Activation - Max: 0.716829781248718 Min: 0.0\n","ReLU Activation - Max: 0.8429707591235088 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003140683733293068 Min: -0.0030940550516685658\n","Layer 0 - Gradient Weights Max: 0.003090453177423246 Min: -0.004765831013631157\n","ReLU Activation - Max: 0.7174409461974873 Min: 0.0\n","Epoch 440, Loss: 0.4039, Test Accuracy: 0.6943\n","ReLU Activation - Max: 0.8438869622969878 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031066680094606744 Min: -0.003057180747920656\n","Layer 0 - Gradient Weights Max: 0.003095143234700198 Min: -0.004797382462425043\n","ReLU Activation - Max: 0.718044810553188 Min: 0.0\n","ReLU Activation - Max: 0.8447903505142681 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030731549848110194 Min: -0.0030208239771418364\n","Layer 0 - Gradient Weights Max: 0.00311067021514134 Min: -0.00477438341385314\n","ReLU Activation - Max: 0.7186337167234977 Min: 0.0\n","ReLU Activation - Max: 0.8456816208494424 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030400491134845355 Min: -0.0029848878683660087\n","Layer 0 - Gradient Weights Max: 0.003123420307318672 Min: -0.004771548717809331\n","ReLU Activation - Max: 0.7192074316760217 Min: 0.0\n","ReLU Activation - Max: 0.8465609145032666 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030074319882785424 Min: -0.002949451419794722\n","Layer 0 - Gradient Weights Max: 0.003123820696955966 Min: -0.004804851854635866\n","ReLU Activation - Max: 0.7197797474102254 Min: 0.0\n","ReLU Activation - Max: 0.8474314421478943 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029753734320665946 Min: -0.002914593471151518\n","Layer 0 - Gradient Weights Max: 0.0031392606829455 Min: -0.0047742974471414205\n","ReLU Activation - Max: 0.7203479520250553 Min: 0.0\n","ReLU Activation - Max: 0.8482909995052416 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029438802818233657 Min: -0.002898278628358928\n","Layer 0 - Gradient Weights Max: 0.003138501806465182 Min: -0.004780936500708102\n","ReLU Activation - Max: 0.7208997746452231 Min: 0.0\n","ReLU Activation - Max: 0.8491331976731357 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002913003301717818 Min: -0.002882671046975411\n","Layer 0 - Gradient Weights Max: 0.0031517273111357066 Min: -0.004792262583058319\n","ReLU Activation - Max: 0.7214433423806524 Min: 0.0\n","ReLU Activation - Max: 0.8499649666508718 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028825722219186917 Min: -0.0028673260335275773\n","Layer 0 - Gradient Weights Max: 0.0031919652786080396 Min: -0.004795410000100308\n","ReLU Activation - Max: 0.721982836348362 Min: 0.0\n","ReLU Activation - Max: 0.8507827445757613 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028526263820480243 Min: -0.0028522351012194255\n","Layer 0 - Gradient Weights Max: 0.003195418538913325 Min: -0.004802114884005448\n","ReLU Activation - Max: 0.7225171125873727 Min: 0.0\n","ReLU Activation - Max: 0.8515933098176012 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028230660912736895 Min: -0.0028374578549378218\n","Layer 0 - Gradient Weights Max: 0.0031984641891344883 Min: -0.00481001091339692\n","ReLU Activation - Max: 0.7230420392546044 Min: 0.0\n","Epoch 450, Loss: 0.4034, Test Accuracy: 0.6943\n","ReLU Activation - Max: 0.8523987400695283 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002794066194123581 Min: -0.0028231349728631383\n","Layer 0 - Gradient Weights Max: 0.003205001807882091 Min: -0.004825046849925486\n","ReLU Activation - Max: 0.7235552536934761 Min: 0.0\n","ReLU Activation - Max: 0.8531878178008326 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002765625039711368 Min: -0.0028091502765360847\n","Layer 0 - Gradient Weights Max: 0.003226982754919324 Min: -0.0048814911968764405\n","ReLU Activation - Max: 0.7240572513624048 Min: 0.0\n","ReLU Activation - Max: 0.853965838805113 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027506979167997185 Min: -0.0027955431374767\n","Layer 0 - Gradient Weights Max: 0.00325007719488401 Min: -0.004810160388618879\n","ReLU Activation - Max: 0.7245544946729626 Min: 0.0\n","ReLU Activation - Max: 0.8547324958797708 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027382274926743403 Min: -0.002782203222093925\n","Layer 0 - Gradient Weights Max: 0.0032429984164126843 Min: -0.004820489120495046\n","ReLU Activation - Max: 0.7250503700073033 Min: 0.0\n","ReLU Activation - Max: 0.855485494948234 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002725916410443103 Min: -0.002769037644387318\n","Layer 0 - Gradient Weights Max: 0.00324900004805679 Min: -0.004763055014525556\n","ReLU Activation - Max: 0.7255423656295764 Min: 0.0\n","ReLU Activation - Max: 0.8562288005648429 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027138917492761385 Min: -0.0027561793347928867\n","Layer 0 - Gradient Weights Max: 0.003273650490775597 Min: -0.00476029625433028\n","ReLU Activation - Max: 0.7260303914422295 Min: 0.0\n","ReLU Activation - Max: 0.856964027918298 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002702174959898929 Min: -0.0027436384357831098\n","Layer 0 - Gradient Weights Max: 0.0032915896493489693 Min: -0.004808152919866361\n","ReLU Activation - Max: 0.7265142747755788 Min: 0.0\n","ReLU Activation - Max: 0.857696403250505 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002690849688224577 Min: -0.0027315026030636826\n","Layer 0 - Gradient Weights Max: 0.003329672935480545 Min: -0.004822842581785171\n","ReLU Activation - Max: 0.726989610775322 Min: 0.0\n","ReLU Activation - Max: 0.8584191209852607 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002679919127554987 Min: -0.0027197721921945377\n","Layer 0 - Gradient Weights Max: 0.0033681066409435453 Min: -0.004836850711308298\n","ReLU Activation - Max: 0.7274504110849546 Min: 0.0\n","ReLU Activation - Max: 0.859131494797747 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026693157855422955 Min: -0.0027083840187699655\n","Layer 0 - Gradient Weights Max: 0.003400461459261258 Min: -0.004812370901038929\n","ReLU Activation - Max: 0.7279010825726437 Min: 0.0\n","Epoch 460, Loss: 0.4029, Test Accuracy: 0.6943\n","ReLU Activation - Max: 0.8598355314705319 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002659037067846867 Min: -0.002697333870842333\n","Layer 0 - Gradient Weights Max: 0.003410726067261138 Min: -0.004853605986824677\n","ReLU Activation - Max: 0.7283395967816093 Min: 0.0\n","ReLU Activation - Max: 0.8605383064635831 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002649080280261688 Min: -0.0026866173193226114\n","Layer 0 - Gradient Weights Max: 0.003447589897357741 Min: -0.004813813567210814\n","ReLU Activation - Max: 0.7287696876234262 Min: 0.0\n","ReLU Activation - Max: 0.8612330832309261 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002639434409580625 Min: -0.00267622488122265\n","Layer 0 - Gradient Weights Max: 0.003474792276912962 Min: -0.00484056246057126\n","ReLU Activation - Max: 0.7291934563988675 Min: 0.0\n","ReLU Activation - Max: 0.8619195363158103 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026301344709387187 Min: -0.002666188535824044\n","Layer 0 - Gradient Weights Max: 0.003498561447161184 Min: -0.004818860559186832\n","ReLU Activation - Max: 0.7296003865992139 Min: 0.0\n","ReLU Activation - Max: 0.8625992163974497 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026210878281414905 Min: -0.002656416677568935\n","Layer 0 - Gradient Weights Max: 0.0035169509099763005 Min: -0.004835525594284681\n","ReLU Activation - Max: 0.7300051392962481 Min: 0.0\n","ReLU Activation - Max: 0.863270571843354 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026122310286318903 Min: -0.0026468397127481897\n","Layer 0 - Gradient Weights Max: 0.003523409608351923 Min: -0.004839761901540001\n","ReLU Activation - Max: 0.7304156530748396 Min: 0.0\n","ReLU Activation - Max: 0.8639394543707151 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026035968366790517 Min: -0.0026374902810525304\n","Layer 0 - Gradient Weights Max: 0.00355248606715544 Min: -0.0047887548530612545\n","ReLU Activation - Max: 0.7308128962450583 Min: 0.0\n","ReLU Activation - Max: 0.8646033537607519 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025951552752344976 Min: -0.00262834369881225\n","Layer 0 - Gradient Weights Max: 0.0035592610627654428 Min: -0.004782651379782838\n","ReLU Activation - Max: 0.7312053243849486 Min: 0.0\n","ReLU Activation - Max: 0.8652622799118372 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025869247849949315 Min: -0.0026194153426366273\n","Layer 0 - Gradient Weights Max: 0.0035906622954379203 Min: -0.004786651425018568\n","ReLU Activation - Max: 0.7315966870106781 Min: 0.0\n","ReLU Activation - Max: 0.8659052888254176 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025789581341193854 Min: -0.002610760671385977\n","Layer 0 - Gradient Weights Max: 0.003645391839451588 Min: -0.0047812028188844264\n","ReLU Activation - Max: 0.7319789315147308 Min: 0.0\n","Epoch 470, Loss: 0.4024, Test Accuracy: 0.6943\n","ReLU Activation - Max: 0.8665357479864262 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025711761623864577 Min: -0.002602303666779723\n","Layer 0 - Gradient Weights Max: 0.0036718487464206953 Min: -0.0047802178195973455\n","ReLU Activation - Max: 0.7323525532274815 Min: 0.0\n","ReLU Activation - Max: 0.8671564304426737 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025635455143746782 Min: -0.00259401680238352\n","Layer 0 - Gradient Weights Max: 0.0036636253675463677 Min: -0.004755196945566578\n","ReLU Activation - Max: 0.732722982028693 Min: 0.0\n","ReLU Activation - Max: 0.8677708673672461 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025561847968531624 Min: -0.002586013842900216\n","Layer 0 - Gradient Weights Max: 0.0037419710023340537 Min: -0.004786337622728658\n","ReLU Activation - Max: 0.7330966155929067 Min: 0.0\n","ReLU Activation - Max: 0.8683845523610649 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025490367698096325 Min: -0.0025782298722967106\n","Layer 0 - Gradient Weights Max: 0.003750484705531375 Min: -0.004773596058837338\n","ReLU Activation - Max: 0.7334564169692666 Min: 0.0\n","ReLU Activation - Max: 0.8689861795531977 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002542072998151596 Min: -0.0025706426899080232\n","Layer 0 - Gradient Weights Max: 0.003778694436614126 Min: -0.004764288432013265\n","ReLU Activation - Max: 0.7338153646280781 Min: 0.0\n","ReLU Activation - Max: 0.869575926010947 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002535362620190249 Min: -0.0025633225574396906\n","Layer 0 - Gradient Weights Max: 0.0038038360132921062 Min: -0.0047695556980825345\n","ReLU Activation - Max: 0.7341722740440095 Min: 0.0\n","ReLU Activation - Max: 0.8701585424028702 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002528975191404322 Min: -0.0025563648661572863\n","Layer 0 - Gradient Weights Max: 0.0038347637163687696 Min: -0.004793732060537445\n","ReLU Activation - Max: 0.7345264721168209 Min: 0.0\n","ReLU Activation - Max: 0.8707269093683369 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002522890636298656 Min: -0.00254972677544418\n","Layer 0 - Gradient Weights Max: 0.0038499093286899376 Min: -0.004815834491893017\n","ReLU Activation - Max: 0.734868065592639 Min: 0.0\n","ReLU Activation - Max: 0.8712826619910183 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025171183246709607 Min: -0.0025434138450225836\n","Layer 0 - Gradient Weights Max: 0.003868561087886762 Min: -0.004803376177786663\n","ReLU Activation - Max: 0.7352079165146531 Min: 0.0\n","ReLU Activation - Max: 0.8718293719343432 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002511558333214367 Min: -0.002537317865047476\n","Layer 0 - Gradient Weights Max: 0.0038863265175173287 Min: -0.004790063984911466\n","ReLU Activation - Max: 0.7355473416589893 Min: 0.0\n","Epoch 480, Loss: 0.4020, Test Accuracy: 0.6954\n","ReLU Activation - Max: 0.8723692946993958 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002506145733761884 Min: -0.0025313867246823133\n","Layer 0 - Gradient Weights Max: 0.0039025503196643825 Min: -0.004762165733308163\n","ReLU Activation - Max: 0.7358815399441053 Min: 0.0\n","ReLU Activation - Max: 0.8729032626690292 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025009910262625643 Min: -0.0025257340734771297\n","Layer 0 - Gradient Weights Max: 0.003941415241975066 Min: -0.004776208242804907\n","ReLU Activation - Max: 0.7362126701594534 Min: 0.0\n","ReLU Activation - Max: 0.8734313532547822 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024961069281241232 Min: -0.0025203594687692606\n","Layer 0 - Gradient Weights Max: 0.003969010583935548 Min: -0.004764714048299362\n","ReLU Activation - Max: 0.7365312903170159 Min: 0.0\n","ReLU Activation - Max: 0.8739570186092283 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024913225739727654 Min: -0.002515102459833455\n","Layer 0 - Gradient Weights Max: 0.00398150191414495 Min: -0.004759965055120439\n","ReLU Activation - Max: 0.7368449562839261 Min: 0.0\n","ReLU Activation - Max: 0.8744765882386067 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002486768388947006 Min: -0.0025100992642955794\n","Layer 0 - Gradient Weights Max: 0.003900263946728466 Min: -0.00473235959489381\n","ReLU Activation - Max: 0.7371518178043726 Min: 0.0\n","ReLU Activation - Max: 0.8749907879466301 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002482408920325828 Min: -0.0025053022605481118\n","Layer 0 - Gradient Weights Max: 0.003935440640987023 Min: -0.004756329608690311\n","ReLU Activation - Max: 0.7374568210363627 Min: 0.0\n","ReLU Activation - Max: 0.8754995381543001 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024782354331543137 Min: -0.002500700797080083\n","Layer 0 - Gradient Weights Max: 0.003971681674295578 Min: -0.004733496150882117\n","ReLU Activation - Max: 0.7377432400084455 Min: 0.0\n","ReLU Activation - Max: 0.8760026732818853 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002474254754177891 Min: -0.002496317175562875\n","Layer 0 - Gradient Weights Max: 0.003957081296925007 Min: -0.00469725999645711\n","ReLU Activation - Max: 0.7380258076620392 Min: 0.0\n","ReLU Activation - Max: 0.8764946087806 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024705027899268443 Min: -0.002492180333424287\n","Layer 0 - Gradient Weights Max: 0.00397898253369878 Min: -0.004697431803722811\n","ReLU Activation - Max: 0.7383005301139732 Min: 0.0\n","ReLU Activation - Max: 0.8769831171763187 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024669574888534843 Min: -0.0024882739876308083\n","Layer 0 - Gradient Weights Max: 0.0039821920732035125 Min: -0.0047030437459750505\n","ReLU Activation - Max: 0.7385665586545287 Min: 0.0\n","Epoch 490, Loss: 0.4016, Test Accuracy: 0.6961\n","ReLU Activation - Max: 0.8774725431250225 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024636441976986947 Min: -0.0024846277533160134\n","Layer 0 - Gradient Weights Max: 0.004044163232035119 Min: -0.004691299429077575\n","ReLU Activation - Max: 0.7388407867755167 Min: 0.0\n","ReLU Activation - Max: 0.877952401364163 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024604511017830076 Min: -0.0024811182719786575\n","Layer 0 - Gradient Weights Max: 0.0040613698342591145 Min: -0.004730299249177234\n","ReLU Activation - Max: 0.7391086400302909 Min: 0.0\n","ReLU Activation - Max: 0.8784269815351058 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024574825139957335 Min: -0.0024778449502914903\n","Layer 0 - Gradient Weights Max: 0.004047474649159505 Min: -0.004697480461711708\n","ReLU Activation - Max: 0.7393703776192739 Min: 0.0\n","ReLU Activation - Max: 0.878892000858053 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00245473267384867 Min: -0.0024748052942253434\n","Layer 0 - Gradient Weights Max: 0.0040586358181033035 Min: -0.004673908594782947\n","ReLU Activation - Max: 0.7396288657327593 Min: 0.0\n","ReLU Activation - Max: 0.8793494686895815 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024521697234051645 Min: -0.0024719687338741374\n","Layer 0 - Gradient Weights Max: 0.004119193885722405 Min: -0.00466175166140285\n","ReLU Activation - Max: 0.7398842341128774 Min: 0.0\n","ReLU Activation - Max: 0.8798046551125225 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024497809223732593 Min: -0.0024693162504847727\n","Layer 0 - Gradient Weights Max: 0.004153679793200931 Min: -0.00469152206142752\n","ReLU Activation - Max: 0.7401438846169007 Min: 0.0\n","ReLU Activation - Max: 0.8802548166736925 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002447750988502607 Min: -0.0024670418825715103\n","Layer 0 - Gradient Weights Max: 0.004176325077230575 Min: -0.004669621820095561\n","ReLU Activation - Max: 0.7404030303053776 Min: 0.0\n","ReLU Activation - Max: 0.8807013968008264 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002445991761940276 Min: -0.0024650492062911074\n","Layer 0 - Gradient Weights Max: 0.004220458684935366 Min: -0.004658840837972743\n","ReLU Activation - Max: 0.7406662846689821 Min: 0.0\n","ReLU Activation - Max: 0.8811435701942542 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024445679707285717 Min: -0.002463410186358368\n","Layer 0 - Gradient Weights Max: 0.004211280003759885 Min: -0.004618742669194908\n","ReLU Activation - Max: 0.7409207122823084 Min: 0.0\n","ReLU Activation - Max: 0.8815920538895669 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024434277286336384 Min: -0.0024620579974360866\n","Layer 0 - Gradient Weights Max: 0.004287097537926103 Min: -0.004613056012727914\n","ReLU Activation - Max: 0.7411757098709881 Min: 0.0\n","Epoch 500, Loss: 0.4012, Test Accuracy: 0.6961\n","ReLU Activation - Max: 0.8820369124501226 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002442502300478933 Min: -0.0024609497620248577\n","Layer 0 - Gradient Weights Max: 0.00432964191457131 Min: -0.004637898454961806\n","ReLU Activation - Max: 0.7414308917542711 Min: 0.0\n","ReLU Activation - Max: 0.8824800745318051 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002441853253720578 Min: -0.0024601265864154084\n","Layer 0 - Gradient Weights Max: 0.00437376894112983 Min: -0.004651080722407866\n","ReLU Activation - Max: 0.7416842470795743 Min: 0.0\n","ReLU Activation - Max: 0.8829173548072227 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024413798778227945 Min: -0.002459498168875782\n","Layer 0 - Gradient Weights Max: 0.004401101226476607 Min: -0.004661590518055787\n","ReLU Activation - Max: 0.7419430979498762 Min: 0.0\n","ReLU Activation - Max: 0.8833485618367807 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024411651519095933 Min: -0.0024591405639419806\n","Layer 0 - Gradient Weights Max: 0.004400128702421305 Min: -0.004681711993015641\n","ReLU Activation - Max: 0.7423666085157495 Min: 0.0\n","ReLU Activation - Max: 0.8837631701597148 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024410424438036245 Min: -0.0024588896879283607\n","Layer 0 - Gradient Weights Max: 0.004419738247182029 Min: -0.0046942649183069896\n","ReLU Activation - Max: 0.7427858194638464 Min: 0.0\n","ReLU Activation - Max: 0.884173378302656 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002441072504633497 Min: -0.002458810025103278\n","Layer 0 - Gradient Weights Max: 0.004438357172157411 Min: -0.004687143521637158\n","ReLU Activation - Max: 0.7432105513273003 Min: 0.0\n","ReLU Activation - Max: 0.884573888257073 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002441290751919158 Min: -0.002458930252377908\n","Layer 0 - Gradient Weights Max: 0.00445610335899995 Min: -0.004660058133219581\n","ReLU Activation - Max: 0.7436367879955336 Min: 0.0\n","ReLU Activation - Max: 0.8849694917481731 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002441592973920487 Min: -0.002459141944794735\n","Layer 0 - Gradient Weights Max: 0.0044838276242994985 Min: -0.004636842818014738\n","ReLU Activation - Max: 0.7440589061853076 Min: 0.0\n","ReLU Activation - Max: 0.885363041144832 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024419880642197822 Min: -0.002459459743329521\n","Layer 0 - Gradient Weights Max: 0.004463392832599885 Min: -0.004651284282050599\n","ReLU Activation - Max: 0.7444750538174167 Min: 0.0\n","ReLU Activation - Max: 0.8857521922883114 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002442509010743814 Min: -0.0024599175306902472\n","Layer 0 - Gradient Weights Max: 0.004519765910536097 Min: -0.004661325420317477\n","ReLU Activation - Max: 0.744890203714361 Min: 0.0\n","Epoch 510, Loss: 0.4008, Test Accuracy: 0.6968\n","ReLU Activation - Max: 0.8861360095046193 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024433133726412754 Min: -0.0024606770123503314\n","Layer 0 - Gradient Weights Max: 0.0045327796918202545 Min: -0.004656413982669368\n","ReLU Activation - Max: 0.7453041757413974 Min: 0.0\n","ReLU Activation - Max: 0.8865136534657491 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024442308261495346 Min: -0.0024615716927703147\n","Layer 0 - Gradient Weights Max: 0.004552531543590404 Min: -0.004677502417740977\n","ReLU Activation - Max: 0.7457131060689167 Min: 0.0\n","ReLU Activation - Max: 0.8868868598359776 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024452845356226433 Min: -0.0024626154186771975\n","Layer 0 - Gradient Weights Max: 0.004566760482725201 Min: -0.004665346107605071\n","ReLU Activation - Max: 0.7461194795087686 Min: 0.0\n","ReLU Activation - Max: 0.887259878176524 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002446492453336441 Min: -0.002463822835786887\n","Layer 0 - Gradient Weights Max: 0.0045918820826348515 Min: -0.004688608008344719\n","ReLU Activation - Max: 0.7465226108429172 Min: 0.0\n","ReLU Activation - Max: 0.8876289991473822 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024479164646053846 Min: -0.0024652666344194226\n","Layer 0 - Gradient Weights Max: 0.004668130497747699 Min: -0.004728125136669836\n","ReLU Activation - Max: 0.7469173522765982 Min: 0.0\n","ReLU Activation - Max: 0.8879915558542549 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024494990202605514 Min: -0.0024668864946233915\n","Layer 0 - Gradient Weights Max: 0.004677553636128217 Min: -0.0047244686078722386\n","ReLU Activation - Max: 0.7473056370106128 Min: 0.0\n","ReLU Activation - Max: 0.8883515060349302 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024511494433696835 Min: -0.0024685948268491453\n","Layer 0 - Gradient Weights Max: 0.004682500557895028 Min: -0.004721400300586615\n","ReLU Activation - Max: 0.7476928762621886 Min: 0.0\n","ReLU Activation - Max: 0.8887120023244344 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024528921090471558 Min: -0.002470403245187745\n","Layer 0 - Gradient Weights Max: 0.004720703317435419 Min: -0.0047178238030337145\n","ReLU Activation - Max: 0.7480826117518088 Min: 0.0\n","ReLU Activation - Max: 0.889066224500504 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002454779694665789 Min: -0.002472373310567748\n","Layer 0 - Gradient Weights Max: 0.0046916930120549614 Min: -0.004713001176944876\n","ReLU Activation - Max: 0.7484687307227411 Min: 0.0\n","ReLU Activation - Max: 0.8894205908295465 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024566552832937334 Min: -0.0024743425267729374\n","Layer 0 - Gradient Weights Max: 0.0047103174528215375 Min: -0.004765763586656674\n","ReLU Activation - Max: 0.7488516249252442 Min: 0.0\n","Epoch 520, Loss: 0.4004, Test Accuracy: 0.6971\n","ReLU Activation - Max: 0.8897733692512496 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002458582146037151 Min: -0.002476369701403884\n","Layer 0 - Gradient Weights Max: 0.00471377790578277 Min: -0.0048183079468736715\n","ReLU Activation - Max: 0.749234382088775 Min: 0.0\n","ReLU Activation - Max: 0.8901247365912599 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002460589168050829 Min: -0.002478489660621691\n","Layer 0 - Gradient Weights Max: 0.004736104297479964 Min: -0.004839018880046022\n","ReLU Activation - Max: 0.7502074390877037 Min: 0.0\n","ReLU Activation - Max: 0.8904728156103758 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024626957046373354 Min: -0.002480722693440031\n","Layer 0 - Gradient Weights Max: 0.004752207891737596 Min: -0.004836878241353554\n","ReLU Activation - Max: 0.7512203012595028 Min: 0.0\n","ReLU Activation - Max: 0.8908207766061208 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002464983755893235 Min: -0.0024831511574386796\n","Layer 0 - Gradient Weights Max: 0.00480934094779541 Min: -0.004839537108679719\n","ReLU Activation - Max: 0.7522339499400398 Min: 0.0\n","ReLU Activation - Max: 0.8911661759691024 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002467540007561512 Min: -0.0024858655508541963\n","Layer 0 - Gradient Weights Max: 0.004829559472016022 Min: -0.004776054381106657\n","ReLU Activation - Max: 0.753238234404347 Min: 0.0\n","ReLU Activation - Max: 0.8915059712013697 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024701705323100028 Min: -0.0024886628281606004\n","Layer 0 - Gradient Weights Max: 0.004864977789895758 Min: -0.004760220337686494\n","ReLU Activation - Max: 0.7542385084716193 Min: 0.0\n","ReLU Activation - Max: 0.8918423937412798 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002472975889788964 Min: -0.002491655529843553\n","Layer 0 - Gradient Weights Max: 0.004927858668862261 Min: -0.004750481959474384\n","ReLU Activation - Max: 0.7552355254251585 Min: 0.0\n","ReLU Activation - Max: 0.8921783866068141 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024759450132079055 Min: -0.002494828512015927\n","Layer 0 - Gradient Weights Max: 0.004933471699144926 Min: -0.004748627618198609\n","ReLU Activation - Max: 0.7562314822792291 Min: 0.0\n","ReLU Activation - Max: 0.8925066824477528 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024789561969991675 Min: -0.002498044606526288\n","Layer 0 - Gradient Weights Max: 0.004939526043113876 Min: -0.004758967206522121\n","ReLU Activation - Max: 0.7572270858958037 Min: 0.0\n","ReLU Activation - Max: 0.8928315454286944 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002481993626222929 Min: -0.0025012948678315035\n","Layer 0 - Gradient Weights Max: 0.004940575573884402 Min: -0.0047628174734014265\n","ReLU Activation - Max: 0.7582204629182605 Min: 0.0\n","Epoch 530, Loss: 0.4000, Test Accuracy: 0.6971\n","ReLU Activation - Max: 0.8931525213680782 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024849814382423693 Min: -0.0025045009496626473\n","Layer 0 - Gradient Weights Max: 0.004981480880914358 Min: -0.004740748427851505\n","ReLU Activation - Max: 0.7592102189529113 Min: 0.0\n","ReLU Activation - Max: 0.8934679275939305 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024882117791555983 Min: -0.0025079694782733897\n","Layer 0 - Gradient Weights Max: 0.004986165546302395 Min: -0.004735586128534671\n","ReLU Activation - Max: 0.7601973156791009 Min: 0.0\n","ReLU Activation - Max: 0.893782579849152 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002491536982972516 Min: -0.0025115433629976495\n","Layer 0 - Gradient Weights Max: 0.005010103875174889 Min: -0.004738860376721107\n","ReLU Activation - Max: 0.7611856243478511 Min: 0.0\n","ReLU Activation - Max: 0.8940959138121252 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00249501384379661 Min: -0.0025152776093826847\n","Layer 0 - Gradient Weights Max: 0.005011367257031054 Min: -0.004747459113116881\n","ReLU Activation - Max: 0.7621746537482071 Min: 0.0\n","ReLU Activation - Max: 0.8944055281591282 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0024985932232312194 Min: -0.002519125371115123\n","Layer 0 - Gradient Weights Max: 0.005059485426408158 Min: -0.004744078375723972\n","ReLU Activation - Max: 0.7631592822896647 Min: 0.0\n","ReLU Activation - Max: 0.8947121925668582 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002502407027255423 Min: -0.0025232317620017273\n","Layer 0 - Gradient Weights Max: 0.005056616686845203 Min: -0.0047250032786264225\n","ReLU Activation - Max: 0.7641399040822465 Min: 0.0\n","ReLU Activation - Max: 0.8950179449046325 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002506259935309402 Min: -0.002527374542539251\n","Layer 0 - Gradient Weights Max: 0.0050292685170898405 Min: -0.0047347106242749265\n","ReLU Activation - Max: 0.7651260428009892 Min: 0.0\n","ReLU Activation - Max: 0.8953259799529938 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025101648945772253 Min: -0.0025315767880079625\n","Layer 0 - Gradient Weights Max: 0.005038311559318569 Min: -0.004748089198043707\n","ReLU Activation - Max: 0.7661137963344965 Min: 0.0\n","ReLU Activation - Max: 0.8956391168273645 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025141249876047085 Min: -0.0025358392164372286\n","Layer 0 - Gradient Weights Max: 0.005060006942769737 Min: -0.004712937145366674\n","ReLU Activation - Max: 0.7670958166302527 Min: 0.0\n","ReLU Activation - Max: 0.8959514021432243 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025182227675987503 Min: -0.002540260560977423\n","Layer 0 - Gradient Weights Max: 0.005037938300887778 Min: -0.004698509607339325\n","ReLU Activation - Max: 0.7680728230793136 Min: 0.0\n","Epoch 540, Loss: 0.3996, Test Accuracy: 0.6971\n","ReLU Activation - Max: 0.8962645606553606 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002522305051078249 Min: -0.002544672813691243\n","Layer 0 - Gradient Weights Max: 0.005055741904400165 Min: -0.004719322388511007\n","ReLU Activation - Max: 0.7690559543083567 Min: 0.0\n","ReLU Activation - Max: 0.8965680562997879 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002526608920709349 Min: -0.0025493113762041918\n","Layer 0 - Gradient Weights Max: 0.005096437945051506 Min: -0.004708807811650676\n","ReLU Activation - Max: 0.7700350953308457 Min: 0.0\n","ReLU Activation - Max: 0.8968709223916246 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025310562873133496 Min: -0.0025541151260495584\n","Layer 0 - Gradient Weights Max: 0.0051107413223222585 Min: -0.0047343843077244105\n","ReLU Activation - Max: 0.7710187098443473 Min: 0.0\n","ReLU Activation - Max: 0.8971692634098833 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00253564081066138 Min: -0.0025590630881914323\n","Layer 0 - Gradient Weights Max: 0.005112273591843957 Min: -0.0047393317900578874\n","ReLU Activation - Max: 0.7720001963434031 Min: 0.0\n","ReLU Activation - Max: 0.8974685468331765 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002540290154890475 Min: -0.0025640878458298186\n","Layer 0 - Gradient Weights Max: 0.005153860731989257 Min: -0.004775081438752395\n","ReLU Activation - Max: 0.772985157209926 Min: 0.0\n","ReLU Activation - Max: 0.897770746948345 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002545074284515438 Min: -0.00256926775023074\n","Layer 0 - Gradient Weights Max: 0.005158356783490097 Min: -0.004743752344120298\n","ReLU Activation - Max: 0.7739653355836571 Min: 0.0\n","ReLU Activation - Max: 0.8980617481289923 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025499079674785614 Min: -0.0025745016333588756\n","Layer 0 - Gradient Weights Max: 0.005160344368901813 Min: -0.004727459891794982\n","ReLU Activation - Max: 0.7749431325206209 Min: 0.0\n","ReLU Activation - Max: 0.8983535107330588 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025548224952157738 Min: -0.002579823950648543\n","Layer 0 - Gradient Weights Max: 0.0051436032494142315 Min: -0.004757437805009438\n","ReLU Activation - Max: 0.7759252911668042 Min: 0.0\n","ReLU Activation - Max: 0.8986386287694373 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025598300903890716 Min: -0.0025852488914879664\n","Layer 0 - Gradient Weights Max: 0.005172924006660313 Min: -0.004754376472640745\n","ReLU Activation - Max: 0.7769050131566105 Min: 0.0\n","ReLU Activation - Max: 0.8989231370208065 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025648053311793867 Min: -0.0025906508302284462\n","Layer 0 - Gradient Weights Max: 0.005129546933586571 Min: -0.004808827265713644\n","ReLU Activation - Max: 0.7778940217115622 Min: 0.0\n","Epoch 550, Loss: 0.3992, Test Accuracy: 0.6982\n","ReLU Activation - Max: 0.8992023940890902 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025696785933794855 Min: -0.002595955386993449\n","Layer 0 - Gradient Weights Max: 0.005130203777343432 Min: -0.0048011697899783224\n","ReLU Activation - Max: 0.778880301529023 Min: 0.0\n","ReLU Activation - Max: 0.8994738404610386 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025745229922866713 Min: -0.002601236508838195\n","Layer 0 - Gradient Weights Max: 0.0051684917751292254 Min: -0.004763013337146603\n","ReLU Activation - Max: 0.7798568420080284 Min: 0.0\n","ReLU Activation - Max: 0.8997429484227026 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002579559989897105 Min: -0.002606721027130544\n","Layer 0 - Gradient Weights Max: 0.005237462813692539 Min: -0.004748941919603791\n","ReLU Activation - Max: 0.7808269653007707 Min: 0.0\n","ReLU Activation - Max: 0.900009922798912 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025847716570829094 Min: -0.002612398546789229\n","Layer 0 - Gradient Weights Max: 0.005285668146343847 Min: -0.004741963479180218\n","ReLU Activation - Max: 0.7817961301415386 Min: 0.0\n","ReLU Activation - Max: 0.9002762046833362 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025900178080291397 Min: -0.002618125413766074\n","Layer 0 - Gradient Weights Max: 0.005275819646575 Min: -0.004772824978888771\n","ReLU Activation - Max: 0.7827685373929077 Min: 0.0\n","ReLU Activation - Max: 0.9005403847957869 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0025953850469323265 Min: -0.0026239749388123623\n","Layer 0 - Gradient Weights Max: 0.005309206879386823 Min: -0.004767264241331848\n","ReLU Activation - Max: 0.7837385758707343 Min: 0.0\n","ReLU Activation - Max: 0.9007989203384621 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002600810366971385 Min: -0.002629895069376588\n","Layer 0 - Gradient Weights Max: 0.005306414203610511 Min: -0.004736190677626564\n","ReLU Activation - Max: 0.7847033855712755 Min: 0.0\n","ReLU Activation - Max: 0.9010471426763678 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002606312940238063 Min: -0.002635902818807322\n","Layer 0 - Gradient Weights Max: 0.005328293817424571 Min: -0.004716984955492151\n","ReLU Activation - Max: 0.7856631798760099 Min: 0.0\n","ReLU Activation - Max: 0.9012865517669819 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026118792407480804 Min: -0.00264198602534338\n","Layer 0 - Gradient Weights Max: 0.005342299621769921 Min: -0.004716853500236158\n","ReLU Activation - Max: 0.7866216528459551 Min: 0.0\n","ReLU Activation - Max: 0.9015299414417958 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026175524183529022 Min: -0.0026481800069950196\n","Layer 0 - Gradient Weights Max: 0.005379245393616002 Min: -0.00471116155308831\n","ReLU Activation - Max: 0.7875790272486379 Min: 0.0\n","Epoch 560, Loss: 0.3988, Test Accuracy: 0.6986\n","ReLU Activation - Max: 0.901762770539074 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002623351051212028 Min: -0.002654506381675485\n","Layer 0 - Gradient Weights Max: 0.005418341894471449 Min: -0.004708355270965099\n","ReLU Activation - Max: 0.788535528240737 Min: 0.0\n","ReLU Activation - Max: 0.9019917207649437 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002629252862839275 Min: -0.002660943197508949\n","Layer 0 - Gradient Weights Max: 0.005409846175027644 Min: -0.004723818416509648\n","ReLU Activation - Max: 0.7894960780506319 Min: 0.0\n","ReLU Activation - Max: 0.902221770632133 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026352205864408336 Min: -0.002667454628973949\n","Layer 0 - Gradient Weights Max: 0.00547370594980376 Min: -0.0047417564324047255\n","ReLU Activation - Max: 0.790459405329568 Min: 0.0\n","ReLU Activation - Max: 0.9024493326109633 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002641228089809379 Min: -0.002674028871124287\n","Layer 0 - Gradient Weights Max: 0.005461102080487513 Min: -0.004745054258961457\n","ReLU Activation - Max: 0.791425340830864 Min: 0.0\n","ReLU Activation - Max: 0.9026784893603745 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002647291261799325 Min: -0.0026806697598976818\n","Layer 0 - Gradient Weights Max: 0.005510156204561631 Min: -0.004780782953896651\n","ReLU Activation - Max: 0.7923973458893313 Min: 0.0\n","ReLU Activation - Max: 0.902911071325015 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026535466060868154 Min: -0.0026875184336956743\n","Layer 0 - Gradient Weights Max: 0.005564326473993285 Min: -0.004792271232672504\n","ReLU Activation - Max: 0.7933720857831583 Min: 0.0\n","ReLU Activation - Max: 0.9031348259176409 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002659942047834959 Min: -0.0026945209289860137\n","Layer 0 - Gradient Weights Max: 0.005603568822181781 Min: -0.004783922706754643\n","ReLU Activation - Max: 0.7943448485835645 Min: 0.0\n","ReLU Activation - Max: 0.9033562679147604 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002666450490218421 Min: -0.0027016374161085255\n","Layer 0 - Gradient Weights Max: 0.005622484586336692 Min: -0.004801482627267015\n","ReLU Activation - Max: 0.7953212991123825 Min: 0.0\n","ReLU Activation - Max: 0.9035790111981752 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026730153566080182 Min: -0.002708824415632029\n","Layer 0 - Gradient Weights Max: 0.005610735204485613 Min: -0.0048249310906090005\n","ReLU Activation - Max: 0.7963011034158936 Min: 0.0\n","ReLU Activation - Max: 0.9037946758017203 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026796095498757377 Min: -0.0027160492526048996\n","Layer 0 - Gradient Weights Max: 0.005616428407023569 Min: -0.004827792226505493\n","ReLU Activation - Max: 0.7972799443841682 Min: 0.0\n","Epoch 570, Loss: 0.3983, Test Accuracy: 0.6996\n","ReLU Activation - Max: 0.904014197196476 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002686161681152478 Min: -0.002723245514646996\n","Layer 0 - Gradient Weights Max: 0.005577610980043685 Min: -0.0048311453604676655\n","ReLU Activation - Max: 0.7982568896011609 Min: 0.0\n","ReLU Activation - Max: 0.9042301268581501 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026927677698518945 Min: -0.0027305005974829744\n","Layer 0 - Gradient Weights Max: 0.005537373363664429 Min: -0.004859678139899762\n","ReLU Activation - Max: 0.7992406908220393 Min: 0.0\n","ReLU Activation - Max: 0.9044464548070011 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0026991956361024864 Min: -0.002737578079260917\n","Layer 0 - Gradient Weights Max: 0.005518854208229707 Min: -0.004873234333147333\n","ReLU Activation - Max: 0.8002246289920053 Min: 0.0\n","ReLU Activation - Max: 0.9046598059843076 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002705625248855566 Min: -0.002744664879838734\n","Layer 0 - Gradient Weights Max: 0.005532341889445732 Min: -0.004839548756861791\n","ReLU Activation - Max: 0.8012011037307549 Min: 0.0\n","ReLU Activation - Max: 0.9048760159982467 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002712211566225469 Min: -0.002751920147054438\n","Layer 0 - Gradient Weights Max: 0.00557207810008862 Min: -0.0048027521767944545\n","ReLU Activation - Max: 0.8021681389718648 Min: 0.0\n","ReLU Activation - Max: 0.9050899801040204 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00271878750908057 Min: -0.002759172591116311\n","Layer 0 - Gradient Weights Max: 0.005546705191615382 Min: -0.004823603918246253\n","ReLU Activation - Max: 0.8031357937768815 Min: 0.0\n","ReLU Activation - Max: 0.9052995875837629 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002725305676412573 Min: -0.002766360790459936\n","Layer 0 - Gradient Weights Max: 0.005591390689533743 Min: -0.004806055539371685\n","ReLU Activation - Max: 0.804103244406283 Min: 0.0\n","ReLU Activation - Max: 0.9055055559580356 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027320263704793796 Min: -0.0027737540933169087\n","Layer 0 - Gradient Weights Max: 0.005670414562177455 Min: -0.0047569322713753165\n","ReLU Activation - Max: 0.8050641583661959 Min: 0.0\n","ReLU Activation - Max: 0.9057130778849383 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027389420656578644 Min: -0.002781358971144199\n","Layer 0 - Gradient Weights Max: 0.005664028459483505 Min: -0.0047826333703249475\n","ReLU Activation - Max: 0.8060312346274261 Min: 0.0\n","ReLU Activation - Max: 0.9059217269348852 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002745920647045395 Min: -0.002789030581221348\n","Layer 0 - Gradient Weights Max: 0.00564596020411617 Min: -0.004759536936924928\n","ReLU Activation - Max: 0.8069979338420292 Min: 0.0\n","Epoch 580, Loss: 0.3979, Test Accuracy: 0.7000\n","ReLU Activation - Max: 0.906124787484013 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027529903541088233 Min: -0.002796792769242971\n","Layer 0 - Gradient Weights Max: 0.005652347135242883 Min: -0.004762623307124476\n","ReLU Activation - Max: 0.807969664859987 Min: 0.0\n","ReLU Activation - Max: 0.9063249564239415 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027601563651585537 Min: -0.0028046520688529675\n","Layer 0 - Gradient Weights Max: 0.0056291559827504455 Min: -0.004747152484252007\n","ReLU Activation - Max: 0.8089376749191665 Min: 0.0\n","ReLU Activation - Max: 0.9065173895962864 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002767292835017923 Min: -0.0028124835969325916\n","Layer 0 - Gradient Weights Max: 0.005606971238142112 Min: -0.004785152633683203\n","ReLU Activation - Max: 0.8099085954133285 Min: 0.0\n","ReLU Activation - Max: 0.9067083109706572 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027742510151485514 Min: -0.0028201290226944377\n","Layer 0 - Gradient Weights Max: 0.005547946859031599 Min: -0.004830413816028373\n","ReLU Activation - Max: 0.8108889169321055 Min: 0.0\n","ReLU Activation - Max: 0.9068968224865677 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002781045007929563 Min: -0.0028276042342128727\n","Layer 0 - Gradient Weights Max: 0.005612281464380759 Min: -0.004858142043199954\n","ReLU Activation - Max: 0.8118740362622202 Min: 0.0\n","ReLU Activation - Max: 0.9070827096158371 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027880409764217723 Min: -0.0028352954064877448\n","Layer 0 - Gradient Weights Max: 0.0056420920878695125 Min: -0.0048867865358384775\n","ReLU Activation - Max: 0.8128631624584638 Min: 0.0\n","ReLU Activation - Max: 0.9072679948714562 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0027950157808243475 Min: -0.0028429792754453382\n","Layer 0 - Gradient Weights Max: 0.005616416793634299 Min: -0.004853148675525371\n","ReLU Activation - Max: 0.813844907824014 Min: 0.0\n","ReLU Activation - Max: 0.9074519724579129 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028019492532742668 Min: -0.002850619562019847\n","Layer 0 - Gradient Weights Max: 0.005634743448843703 Min: -0.004750965998262901\n","ReLU Activation - Max: 0.8148032008543009 Min: 0.0\n","ReLU Activation - Max: 0.9076384371943523 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002808950213641393 Min: -0.002858329433330432\n","Layer 0 - Gradient Weights Max: 0.005645588206545328 Min: -0.0047526434578022865\n","ReLU Activation - Max: 0.8157584012869694 Min: 0.0\n","ReLU Activation - Max: 0.9078319734177963 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028160554760030287 Min: -0.0028661555774731375\n","Layer 0 - Gradient Weights Max: 0.005725799279436764 Min: -0.004767426398838372\n","ReLU Activation - Max: 0.8167133465237195 Min: 0.0\n","Epoch 590, Loss: 0.3975, Test Accuracy: 0.7011\n","ReLU Activation - Max: 0.9080207816993915 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028233512652499774 Min: -0.002874193180244193\n","Layer 0 - Gradient Weights Max: 0.005740734243916025 Min: -0.0047458830656074025\n","ReLU Activation - Max: 0.8176639381329354 Min: 0.0\n","ReLU Activation - Max: 0.9082032275729179 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028306974892981017 Min: -0.002882285893645185\n","Layer 0 - Gradient Weights Max: 0.005720884717171643 Min: -0.004765929364449788\n","ReLU Activation - Max: 0.8186189653365887 Min: 0.0\n","ReLU Activation - Max: 0.9083811718516616 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028379978881519444 Min: -0.0028903317499008306\n","Layer 0 - Gradient Weights Max: 0.005770823724955075 Min: -0.004753036268233816\n","ReLU Activation - Max: 0.819572789465437 Min: 0.0\n","ReLU Activation - Max: 0.9085676636919575 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002845308574832235 Min: -0.002898400701949155\n","Layer 0 - Gradient Weights Max: 0.005764822067851228 Min: -0.0047684818056492806\n","ReLU Activation - Max: 0.820530135537476 Min: 0.0\n","ReLU Activation - Max: 0.9087547995299738 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028525818834372147 Min: -0.0029064353943140196\n","Layer 0 - Gradient Weights Max: 0.005758293324421272 Min: -0.004776656876830692\n","ReLU Activation - Max: 0.8214896237958622 Min: 0.0\n","ReLU Activation - Max: 0.9089418826745383 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028597743873378695 Min: -0.002914392642060465\n","Layer 0 - Gradient Weights Max: 0.005758108238659239 Min: -0.004769411675119354\n","ReLU Activation - Max: 0.822449127001535 Min: 0.0\n","ReLU Activation - Max: 0.9091279590800478 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002867009221090937 Min: -0.0029223963671241066\n","Layer 0 - Gradient Weights Max: 0.005812381889704588 Min: -0.004780650859986508\n","ReLU Activation - Max: 0.8234090411052541 Min: 0.0\n","ReLU Activation - Max: 0.9093120234132965 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028743824710141747 Min: -0.002930551792509738\n","Layer 0 - Gradient Weights Max: 0.005821369639354373 Min: -0.00473880614708778\n","ReLU Activation - Max: 0.824359341468064 Min: 0.0\n","ReLU Activation - Max: 0.909494788197046 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028816148630466795 Min: -0.0029385777264188707\n","Layer 0 - Gradient Weights Max: 0.005860797001139766 Min: -0.0047769589471104955\n","ReLU Activation - Max: 0.8253133451685567 Min: 0.0\n","ReLU Activation - Max: 0.9096809226779651 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0028888707497092287 Min: -0.0029466412138710028\n","Layer 0 - Gradient Weights Max: 0.005820378550118865 Min: -0.004753069121215881\n","ReLU Activation - Max: 0.8262641095956716 Min: 0.0\n","Epoch 600, Loss: 0.3970, Test Accuracy: 0.7007\n","ReLU Activation - Max: 0.9098644714106789 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002896025415840558 Min: -0.0029546048393961984\n","Layer 0 - Gradient Weights Max: 0.005808634154884058 Min: -0.00475415010482445\n","ReLU Activation - Max: 0.8272147796751433 Min: 0.0\n","ReLU Activation - Max: 0.9100479287250887 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029032100246642375 Min: -0.0029625808888934222\n","Layer 0 - Gradient Weights Max: 0.0058828661627886875 Min: -0.004749855659321091\n","ReLU Activation - Max: 0.8281675362750468 Min: 0.0\n","ReLU Activation - Max: 0.9102300421527587 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029104535308629603 Min: -0.002970624317524306\n","Layer 0 - Gradient Weights Max: 0.005859461773086562 Min: -0.004715217013457463\n","ReLU Activation - Max: 0.8291192104485853 Min: 0.0\n","ReLU Activation - Max: 0.9104041383170421 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029175839367814125 Min: -0.002978557938414056\n","Layer 0 - Gradient Weights Max: 0.005871170171145203 Min: -0.004729778089741959\n","ReLU Activation - Max: 0.8300715356788821 Min: 0.0\n","ReLU Activation - Max: 0.9105742383013529 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00292472001463696 Min: -0.0029865138374393424\n","Layer 0 - Gradient Weights Max: 0.005925708650406901 Min: -0.004727938246131387\n","ReLU Activation - Max: 0.8310221805457135 Min: 0.0\n","ReLU Activation - Max: 0.9107413805017087 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002931929153594005 Min: -0.0029945556741299124\n","Layer 0 - Gradient Weights Max: 0.005945393772240955 Min: -0.004719022400120815\n","ReLU Activation - Max: 0.8319690701792771 Min: 0.0\n","ReLU Activation - Max: 0.9108998122198662 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029392021035754695 Min: -0.0030026641179466264\n","Layer 0 - Gradient Weights Max: 0.005922810298605356 Min: -0.0047342593148585535\n","ReLU Activation - Max: 0.8329176774969712 Min: 0.0\n","ReLU Activation - Max: 0.9110622004338251 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029464372721602775 Min: -0.003010736839526946\n","Layer 0 - Gradient Weights Max: 0.005941903368284942 Min: -0.004717127128380407\n","ReLU Activation - Max: 0.833861237626135 Min: 0.0\n","ReLU Activation - Max: 0.9112249344479907 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002953655513201745 Min: -0.0030187985351995397\n","Layer 0 - Gradient Weights Max: 0.005963942924074997 Min: -0.004705506756780856\n","ReLU Activation - Max: 0.8348057140816951 Min: 0.0\n","ReLU Activation - Max: 0.9113776074507272 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029609223287004266 Min: -0.0030269173136844873\n","Layer 0 - Gradient Weights Max: 0.0059886503294498995 Min: -0.004682412747156092\n","ReLU Activation - Max: 0.8357470405986398 Min: 0.0\n","Epoch 610, Loss: 0.3966, Test Accuracy: 0.7007\n","ReLU Activation - Max: 0.9115334260779827 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029681682681559143 Min: -0.003035019765782424\n","Layer 0 - Gradient Weights Max: 0.005993715007907402 Min: -0.00466523576768204\n","ReLU Activation - Max: 0.8366836940081804 Min: 0.0\n","ReLU Activation - Max: 0.9116942562866248 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029754179814522595 Min: -0.0030431210819800237\n","Layer 0 - Gradient Weights Max: 0.005999687215832402 Min: -0.004669913910380427\n","ReLU Activation - Max: 0.8376226235729909 Min: 0.0\n","ReLU Activation - Max: 0.9118446981185117 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.002982570191581128 Min: -0.0030511185520593637\n","Layer 0 - Gradient Weights Max: 0.00601721090498953 Min: -0.0046265462876844505\n","ReLU Activation - Max: 0.8385552723844332 Min: 0.0\n","ReLU Activation - Max: 0.9119924520062389 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029898119912604784 Min: -0.00305921283609145\n","Layer 0 - Gradient Weights Max: 0.006024727147243745 Min: -0.00464892004560073\n","ReLU Activation - Max: 0.8394896577610904 Min: 0.0\n","ReLU Activation - Max: 0.9121465021793885 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0029970400867188277 Min: -0.003067294775724042\n","Layer 0 - Gradient Weights Max: 0.006037518761372971 Min: -0.004669698843966922\n","ReLU Activation - Max: 0.8404246395448923 Min: 0.0\n","ReLU Activation - Max: 0.9123030404118376 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003004305517460591 Min: -0.003075420620586982\n","Layer 0 - Gradient Weights Max: 0.006060632138405144 Min: -0.004699930172252026\n","ReLU Activation - Max: 0.8413645611947892 Min: 0.0\n","ReLU Activation - Max: 0.9124607514611968 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003011649440078698 Min: -0.003083633517073039\n","Layer 0 - Gradient Weights Max: 0.006072695251849672 Min: -0.004702608356962987\n","ReLU Activation - Max: 0.8423063849127128 Min: 0.0\n","ReLU Activation - Max: 0.9126142101902754 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030190182456204407 Min: -0.003091873432141376\n","Layer 0 - Gradient Weights Max: 0.0060799602680038705 Min: -0.0047261421515577655\n","ReLU Activation - Max: 0.8432507079570228 Min: 0.0\n","ReLU Activation - Max: 0.9127652469705525 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030263041727728885 Min: -0.003100032789825699\n","Layer 0 - Gradient Weights Max: 0.006075764051769258 Min: -0.004721620306252608\n","ReLU Activation - Max: 0.8441920486725792 Min: 0.0\n","ReLU Activation - Max: 0.9129151686122066 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030336343195774148 Min: -0.0031082415248373948\n","Layer 0 - Gradient Weights Max: 0.006061636071303201 Min: -0.004723934851429601\n","ReLU Activation - Max: 0.8451325422560867 Min: 0.0\n","Epoch 620, Loss: 0.3961, Test Accuracy: 0.7025\n","ReLU Activation - Max: 0.9130599868923698 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003041034650026286 Min: -0.0031165161413086845\n","Layer 0 - Gradient Weights Max: 0.006135094825252123 Min: -0.004715965974188806\n","ReLU Activation - Max: 0.8460757243450637 Min: 0.0\n","ReLU Activation - Max: 0.9132051370591703 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003048599114225681 Min: -0.0031249743455670467\n","Layer 0 - Gradient Weights Max: 0.0061688032367128946 Min: -0.004706205705239732\n","ReLU Activation - Max: 0.8470147297578998 Min: 0.0\n","ReLU Activation - Max: 0.9133468463953889 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030562429073888587 Min: -0.0031335123623750048\n","Layer 0 - Gradient Weights Max: 0.006169280064720627 Min: -0.00470960412513217\n","ReLU Activation - Max: 0.8479532338493954 Min: 0.0\n","ReLU Activation - Max: 0.9134930082401841 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0030637211356947733 Min: -0.0031418852157867015\n","Layer 0 - Gradient Weights Max: 0.006112371568576368 Min: -0.004730263021440114\n","ReLU Activation - Max: 0.848889060891383 Min: 0.0\n","ReLU Activation - Max: 0.9136382186918579 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00307116966329962 Min: -0.0031502304320670574\n","Layer 0 - Gradient Weights Max: 0.006126585520431781 Min: -0.004709506460960834\n","ReLU Activation - Max: 0.8498263972535114 Min: 0.0\n","ReLU Activation - Max: 0.9137824929402282 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00307866849801384 Min: -0.003158631030442368\n","Layer 0 - Gradient Weights Max: 0.006148159480287365 Min: -0.004712233511053376\n","ReLU Activation - Max: 0.85076203620451 Min: 0.0\n","ReLU Activation - Max: 0.9139261089532268 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003086223809360715 Min: -0.0031670921122364825\n","Layer 0 - Gradient Weights Max: 0.00614952108739758 Min: -0.004693628739495354\n","ReLU Activation - Max: 0.8516974229989406 Min: 0.0\n","ReLU Activation - Max: 0.9140623028271697 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003093837178940594 Min: -0.003175613257124164\n","Layer 0 - Gradient Weights Max: 0.006186241218631837 Min: -0.004717738489396259\n","ReLU Activation - Max: 0.8526340704348647 Min: 0.0\n","ReLU Activation - Max: 0.9142006490010629 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003101509110851604 Min: -0.0031841988018608994\n","Layer 0 - Gradient Weights Max: 0.006217655509845603 Min: -0.004745113393832533\n","ReLU Activation - Max: 0.8535802539192504 Min: 0.0\n","ReLU Activation - Max: 0.9143399992672153 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031092445708730708 Min: -0.0031928654983500422\n","Layer 0 - Gradient Weights Max: 0.006247462617041091 Min: -0.004730398626721701\n","ReLU Activation - Max: 0.8545209370967503 Min: 0.0\n","Epoch 630, Loss: 0.3956, Test Accuracy: 0.7046\n","ReLU Activation - Max: 0.9144844024500154 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031170345038805626 Min: -0.0032015909576664195\n","Layer 0 - Gradient Weights Max: 0.0062517429344998805 Min: -0.004733916312080804\n","ReLU Activation - Max: 0.8554606359411 Min: 0.0\n","ReLU Activation - Max: 0.9146266567734382 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031248226681689097 Min: -0.003210321709542699\n","Layer 0 - Gradient Weights Max: 0.006236629401579392 Min: -0.0047448995511989\n","ReLU Activation - Max: 0.8564023338577094 Min: 0.0\n","ReLU Activation - Max: 0.9147681215706059 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031325349698704675 Min: -0.0032189729834741186\n","Layer 0 - Gradient Weights Max: 0.006224414022196315 Min: -0.004753950878244982\n","ReLU Activation - Max: 0.8573441202174313 Min: 0.0\n","ReLU Activation - Max: 0.9149051497256436 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003140336372434122 Min: -0.003227714309282673\n","Layer 0 - Gradient Weights Max: 0.0063365177346838015 Min: -0.004766741478055016\n","ReLU Activation - Max: 0.8582860695917478 Min: 0.0\n","ReLU Activation - Max: 0.9150413058687321 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031481798422417087 Min: -0.0032365036020253696\n","Layer 0 - Gradient Weights Max: 0.006338134442459081 Min: -0.0047592982327262145\n","ReLU Activation - Max: 0.859230099987277 Min: 0.0\n","ReLU Activation - Max: 0.9151678273171758 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031560977831619572 Min: -0.0032453656642512474\n","Layer 0 - Gradient Weights Max: 0.006353406364326667 Min: -0.004752285692027433\n","ReLU Activation - Max: 0.8601725586177382 Min: 0.0\n","ReLU Activation - Max: 0.9152959822487867 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031640573279689203 Min: -0.0032542734678099976\n","Layer 0 - Gradient Weights Max: 0.006327319984962936 Min: -0.0047239704683737045\n","ReLU Activation - Max: 0.8611083842385969 Min: 0.0\n","ReLU Activation - Max: 0.915425475556445 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031719369508153568 Min: -0.0032630951353275614\n","Layer 0 - Gradient Weights Max: 0.006323811803637176 Min: -0.004716483530821847\n","ReLU Activation - Max: 0.8620406337227755 Min: 0.0\n","ReLU Activation - Max: 0.9155532986587074 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003179758737410791 Min: -0.0032718605621566955\n","Layer 0 - Gradient Weights Max: 0.006363448676779235 Min: -0.00474683878306636\n","ReLU Activation - Max: 0.8629832518991447 Min: 0.0\n","ReLU Activation - Max: 0.915683723418831 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031877379463678135 Min: -0.0032807931066110886\n","Layer 0 - Gradient Weights Max: 0.0063994222878023385 Min: -0.00474200113235553\n","ReLU Activation - Max: 0.8639261516474381 Min: 0.0\n","Epoch 640, Loss: 0.3951, Test Accuracy: 0.7039\n","ReLU Activation - Max: 0.9158176543340177 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0031957885337925433 Min: -0.003289802908616992\n","Layer 0 - Gradient Weights Max: 0.006426408878593573 Min: -0.0047286811141273736\n","ReLU Activation - Max: 0.8648664827292389 Min: 0.0\n","ReLU Activation - Max: 0.9159516825827939 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032038361164454816 Min: -0.0032988146750970344\n","Layer 0 - Gradient Weights Max: 0.006406779295148439 Min: -0.004762371940525743\n","ReLU Activation - Max: 0.8658140140891024 Min: 0.0\n","ReLU Activation - Max: 0.9160830998487319 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032118312198187297 Min: -0.0033077694154762423\n","Layer 0 - Gradient Weights Max: 0.0064112707417301506 Min: -0.0047565086148042566\n","ReLU Activation - Max: 0.8667588806937108 Min: 0.0\n","ReLU Activation - Max: 0.9162129711733368 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003219758690230421 Min: -0.0033166580263218532\n","Layer 0 - Gradient Weights Max: 0.006429237523151593 Min: -0.004773480321173603\n","ReLU Activation - Max: 0.8677082073304856 Min: 0.0\n","ReLU Activation - Max: 0.9163494798714147 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003227777097646261 Min: -0.003325640511809039\n","Layer 0 - Gradient Weights Max: 0.006439252882095387 Min: -0.004794072250474417\n","ReLU Activation - Max: 0.8686593038250388 Min: 0.0\n","ReLU Activation - Max: 0.916470075311505 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032358196496566735 Min: -0.003334634009794657\n","Layer 0 - Gradient Weights Max: 0.006463078321830187 Min: -0.0047813595447445375\n","ReLU Activation - Max: 0.869610002866401 Min: 0.0\n","ReLU Activation - Max: 0.9165926083720863 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032437970204859785 Min: -0.003343560850214236\n","Layer 0 - Gradient Weights Max: 0.0064896579700880275 Min: -0.004790744721557213\n","ReLU Activation - Max: 0.8705610114335633 Min: 0.0\n","ReLU Activation - Max: 0.9167115890282195 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032518806328856063 Min: -0.0033525926315400778\n","Layer 0 - Gradient Weights Max: 0.006561153182151102 Min: -0.004801520779184766\n","ReLU Activation - Max: 0.8715132836773678 Min: 0.0\n","ReLU Activation - Max: 0.9168242835616779 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003260066349097845 Min: -0.0033617272359854657\n","Layer 0 - Gradient Weights Max: 0.006573746252436132 Min: -0.004832617384758052\n","ReLU Activation - Max: 0.8724674696569142 Min: 0.0\n","ReLU Activation - Max: 0.9169371479320485 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00326827597964698 Min: -0.0033708945700699033\n","Layer 0 - Gradient Weights Max: 0.006608367825889933 Min: -0.0047865888486368045\n","ReLU Activation - Max: 0.873412789921888 Min: 0.0\n","Epoch 650, Loss: 0.3946, Test Accuracy: 0.7043\n","ReLU Activation - Max: 0.9170386859510249 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0032765963729042566 Min: -0.0033801718408507492\n","Layer 0 - Gradient Weights Max: 0.006607283521126817 Min: -0.004831537188139742\n","ReLU Activation - Max: 0.8743627387891427 Min: 0.0\n","ReLU Activation - Max: 0.9171417832017892 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003284977997995135 Min: -0.003389513025409444\n","Layer 0 - Gradient Weights Max: 0.00658591693400142 Min: -0.004852825437724438\n","ReLU Activation - Max: 0.8753176190767835 Min: 0.0\n","ReLU Activation - Max: 0.9172452910055835 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003293291954684249 Min: -0.003398790178280385\n","Layer 0 - Gradient Weights Max: 0.006625042238458113 Min: -0.004851838923755992\n","ReLU Activation - Max: 0.8762703009041751 Min: 0.0\n","ReLU Activation - Max: 0.9173459706561965 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033016613629183443 Min: -0.0034081315605240096\n","Layer 0 - Gradient Weights Max: 0.0066386698347857655 Min: -0.00485445497516822\n","ReLU Activation - Max: 0.8772244059502987 Min: 0.0\n","ReLU Activation - Max: 0.917446082078757 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033100136249103952 Min: -0.0034174512742750044\n","Layer 0 - Gradient Weights Max: 0.006595062725551047 Min: -0.004854874656765092\n","ReLU Activation - Max: 0.8781779980947346 Min: 0.0\n","ReLU Activation - Max: 0.9175442652159164 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00331825755205657 Min: -0.0034266543701381704\n","Layer 0 - Gradient Weights Max: 0.006570734543968961 Min: -0.004863705229901116\n","ReLU Activation - Max: 0.8791281793759348 Min: 0.0\n","ReLU Activation - Max: 0.9176447431675476 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033263852784001084 Min: -0.0034357413729438247\n","Layer 0 - Gradient Weights Max: 0.006582534718173293 Min: -0.004867295134659221\n","ReLU Activation - Max: 0.8800759702833774 Min: 0.0\n","ReLU Activation - Max: 0.9177393839968635 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003334616577854881 Min: -0.0034449372193754643\n","Layer 0 - Gradient Weights Max: 0.006634429586766586 Min: -0.004901391579688731\n","ReLU Activation - Max: 0.8810269695040865 Min: 0.0\n","ReLU Activation - Max: 0.9178366625187827 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033429190635726703 Min: -0.00345420479062127\n","Layer 0 - Gradient Weights Max: 0.006631264013907304 Min: -0.004881682687903298\n","ReLU Activation - Max: 0.8819722972915423 Min: 0.0\n","ReLU Activation - Max: 0.91792998412061 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033511279647136048 Min: -0.0034633746401428215\n","Layer 0 - Gradient Weights Max: 0.006654419227061097 Min: -0.004890476304055361\n","ReLU Activation - Max: 0.8829187035105807 Min: 0.0\n","Epoch 660, Loss: 0.3941, Test Accuracy: 0.7068\n","ReLU Activation - Max: 0.9180231618081618 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033593767566201618 Min: -0.003472575054019161\n","Layer 0 - Gradient Weights Max: 0.006710559497413403 Min: -0.0048942656180654106\n","ReLU Activation - Max: 0.8838719680768826 Min: 0.0\n","ReLU Activation - Max: 0.9181112423644691 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033676577472785503 Min: -0.0034818133423631424\n","Layer 0 - Gradient Weights Max: 0.0067168640949673225 Min: -0.00489821305564164\n","ReLU Activation - Max: 0.8848269233688998 Min: 0.0\n","ReLU Activation - Max: 0.9181997218817287 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0033758584882626953 Min: -0.003490966881940645\n","Layer 0 - Gradient Weights Max: 0.0067003185058570735 Min: -0.004915822558496912\n","ReLU Activation - Max: 0.8857841303137155 Min: 0.0\n","ReLU Activation - Max: 0.9182850651536438 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003383981353311791 Min: -0.0035000348913787767\n","Layer 0 - Gradient Weights Max: 0.006765320224478222 Min: -0.004935231077649074\n","ReLU Activation - Max: 0.8867459682120588 Min: 0.0\n","ReLU Activation - Max: 0.9183697962867522 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003392133807846523 Min: -0.003509135803545844\n","Layer 0 - Gradient Weights Max: 0.006752455525783822 Min: -0.004892938480476897\n","ReLU Activation - Max: 0.8877003129787833 Min: 0.0\n","ReLU Activation - Max: 0.9184599602987364 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034002024761322124 Min: -0.0035181358119359836\n","Layer 0 - Gradient Weights Max: 0.0067669994267398744 Min: -0.004887539559234626\n","ReLU Activation - Max: 0.8886540531937073 Min: 0.0\n","ReLU Activation - Max: 0.918550971161231 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034081938974505218 Min: -0.0035270550829244397\n","Layer 0 - Gradient Weights Max: 0.006774590509791018 Min: -0.004889277228698305\n","ReLU Activation - Max: 0.8896085137935772 Min: 0.0\n","ReLU Activation - Max: 0.9186397484189195 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003416174532517505 Min: -0.0035359610832520687\n","Layer 0 - Gradient Weights Max: 0.006805950680635239 Min: -0.0048925574389770505\n","ReLU Activation - Max: 0.8905611823537175 Min: 0.0\n","ReLU Activation - Max: 0.918724860809673 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034242699463990865 Min: -0.003544974452395241\n","Layer 0 - Gradient Weights Max: 0.006831622420544867 Min: -0.004854971585600934\n","ReLU Activation - Max: 0.8915078770919922 Min: 0.0\n","ReLU Activation - Max: 0.9188074929950137 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003432522188036034 Min: -0.003554142288364867\n","Layer 0 - Gradient Weights Max: 0.0068892388402538186 Min: -0.004840599444265997\n","ReLU Activation - Max: 0.8924517930326993 Min: 0.0\n","Epoch 670, Loss: 0.3936, Test Accuracy: 0.7082\n","ReLU Activation - Max: 0.9188892183147304 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003440905410680105 Min: -0.003563453009938417\n","Layer 0 - Gradient Weights Max: 0.006920834653686888 Min: -0.004850491006813938\n","ReLU Activation - Max: 0.8933994192782724 Min: 0.0\n","ReLU Activation - Max: 0.9189707325536072 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003449349876022935 Min: -0.0035728244230956627\n","Layer 0 - Gradient Weights Max: 0.0068915432227233875 Min: -0.004884041084567042\n","ReLU Activation - Max: 0.8943500178644845 Min: 0.0\n","ReLU Activation - Max: 0.9190526192093367 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034578012154548336 Min: -0.0035822017360336864\n","Layer 0 - Gradient Weights Max: 0.006964341147543665 Min: -0.00486769814315108\n","ReLU Activation - Max: 0.8952979903031787 Min: 0.0\n","ReLU Activation - Max: 0.919130831359787 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034662999753581726 Min: -0.0035916281913969347\n","Layer 0 - Gradient Weights Max: 0.006956422009101258 Min: -0.004898870066530423\n","ReLU Activation - Max: 0.8962513023707662 Min: 0.0\n","ReLU Activation - Max: 0.9192109384487877 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003474760243662605 Min: -0.0036010224415868575\n","Layer 0 - Gradient Weights Max: 0.007015278660132219 Min: -0.004847146849573749\n","ReLU Activation - Max: 0.8971972570182266 Min: 0.0\n","ReLU Activation - Max: 0.9192884815651505 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034833202344610225 Min: -0.0036105024048506016\n","Layer 0 - Gradient Weights Max: 0.0070307888861585194 Min: -0.004815784232122781\n","ReLU Activation - Max: 0.8981375870824456 Min: 0.0\n","ReLU Activation - Max: 0.9193621480313234 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0034918744240076123 Min: -0.003619971633809297\n","Layer 0 - Gradient Weights Max: 0.007044408382268022 Min: -0.004820232382885595\n","ReLU Activation - Max: 0.8990757540948388 Min: 0.0\n","ReLU Activation - Max: 0.9194661157066587 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003500395852036175 Min: -0.0036294111383276166\n","Layer 0 - Gradient Weights Max: 0.007082519245099693 Min: -0.004804858282436844\n","ReLU Activation - Max: 0.9000045049693537 Min: 0.0\n","ReLU Activation - Max: 0.9196747893109314 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003508901992615298 Min: -0.0036388375736588494\n","Layer 0 - Gradient Weights Max: 0.007050414820552919 Min: -0.004823104822169898\n","ReLU Activation - Max: 0.9009381635250444 Min: 0.0\n","ReLU Activation - Max: 0.9198851358646758 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035173156016398634 Min: -0.0036481696524649923\n","Layer 0 - Gradient Weights Max: 0.007055374912644914 Min: -0.004797195067322238\n","ReLU Activation - Max: 0.9018633164670871 Min: 0.0\n","Epoch 680, Loss: 0.3930, Test Accuracy: 0.7086\n","ReLU Activation - Max: 0.9200947649919828 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035257580722806102 Min: -0.0036575414362802964\n","Layer 0 - Gradient Weights Max: 0.007046275814554345 Min: -0.004780149424233618\n","ReLU Activation - Max: 0.9027862493249991 Min: 0.0\n","ReLU Activation - Max: 0.9203035814992993 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035341314788540417 Min: -0.0036668377528764114\n","Layer 0 - Gradient Weights Max: 0.007043160800802976 Min: -0.004745069680718139\n","ReLU Activation - Max: 0.903702769850876 Min: 0.0\n","ReLU Activation - Max: 0.9205105016304901 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035423264297691427 Min: -0.003675948451698041\n","Layer 0 - Gradient Weights Max: 0.00693233042460743 Min: -0.004755172902353508\n","ReLU Activation - Max: 0.9046211549955138 Min: 0.0\n","ReLU Activation - Max: 0.9207183871774821 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003550310015544709 Min: -0.003684837868879099\n","Layer 0 - Gradient Weights Max: 0.006934537086905026 Min: -0.00475393787495332\n","ReLU Activation - Max: 0.9055388040898154 Min: 0.0\n","ReLU Activation - Max: 0.9209239277046749 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035582728640193894 Min: -0.003693700505850371\n","Layer 0 - Gradient Weights Max: 0.006942944017729174 Min: -0.004743548463697534\n","ReLU Activation - Max: 0.9064538057853628 Min: 0.0\n","ReLU Activation - Max: 0.9211287530615315 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035661807803691117 Min: -0.003702505231441408\n","Layer 0 - Gradient Weights Max: 0.00695982367754063 Min: -0.004749613162460779\n","ReLU Activation - Max: 0.9073740425779092 Min: 0.0\n","ReLU Activation - Max: 0.9213333303273032 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035742235758605633 Min: -0.0037114484062033487\n","Layer 0 - Gradient Weights Max: 0.007025907872428222 Min: -0.004742214398671841\n","ReLU Activation - Max: 0.9082924529947336 Min: 0.0\n","ReLU Activation - Max: 0.921534065417658 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035824470208476442 Min: -0.0037205872721149977\n","Layer 0 - Gradient Weights Max: 0.007049896895419925 Min: -0.004760387228293097\n","ReLU Activation - Max: 0.9092158910156236 Min: 0.0\n","ReLU Activation - Max: 0.9217325063504234 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035907723823123128 Min: -0.0037298282917537914\n","Layer 0 - Gradient Weights Max: 0.007045976154603523 Min: -0.004785318001104594\n","ReLU Activation - Max: 0.910141167825748 Min: 0.0\n","ReLU Activation - Max: 0.9219361835298276 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0035990882664497987 Min: -0.0037390583755260685\n","Layer 0 - Gradient Weights Max: 0.007066860566924599 Min: -0.004773722714255861\n","ReLU Activation - Max: 0.9110636720224491 Min: 0.0\n","Epoch 690, Loss: 0.3924, Test Accuracy: 0.7100\n","ReLU Activation - Max: 0.9221390753999257 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0036074978283925723 Min: -0.0037483824782959083\n","Layer 0 - Gradient Weights Max: 0.007123007037223285 Min: -0.004770145351915547\n","ReLU Activation - Max: 0.9119830533113769 Min: 0.0\n","ReLU Activation - Max: 0.9223398603534921 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003616012459228656 Min: -0.003757812947190271\n","Layer 0 - Gradient Weights Max: 0.007119104315619359 Min: -0.004777314248873405\n","ReLU Activation - Max: 0.9129016274230005 Min: 0.0\n","ReLU Activation - Max: 0.9225336478308264 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003624457438079942 Min: -0.0037671659544470043\n","Layer 0 - Gradient Weights Max: 0.0071033610193086995 Min: -0.0047579810646727685\n","ReLU Activation - Max: 0.9138147371768762 Min: 0.0\n","ReLU Activation - Max: 0.9227163921022933 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00363281135567164 Min: -0.0037764182730567327\n","Layer 0 - Gradient Weights Max: 0.006998936749613439 Min: -0.0047377709407296865\n","ReLU Activation - Max: 0.9147232788491758 Min: 0.0\n","ReLU Activation - Max: 0.9228974940115618 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003640880803055241 Min: -0.0037853738834607675\n","Layer 0 - Gradient Weights Max: 0.006957890138991658 Min: -0.004735857936040682\n","ReLU Activation - Max: 0.9156298427636188 Min: 0.0\n","ReLU Activation - Max: 0.9230768354224501 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003648853348349817 Min: -0.0037942286889375686\n","Layer 0 - Gradient Weights Max: 0.006956106544683754 Min: -0.004711452386174509\n","ReLU Activation - Max: 0.9165271657079092 Min: 0.0\n","ReLU Activation - Max: 0.9232603390524547 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0036569084537400464 Min: -0.0038031614773920484\n","Layer 0 - Gradient Weights Max: 0.006965954298686767 Min: -0.004723704575821388\n","ReLU Activation - Max: 0.9174308874255173 Min: 0.0\n","ReLU Activation - Max: 0.9234428871805749 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0036649200316058433 Min: -0.003812047963592227\n","Layer 0 - Gradient Weights Max: 0.006979394016091535 Min: -0.0047288039525730284\n","ReLU Activation - Max: 0.9183368994097284 Min: 0.0\n","ReLU Activation - Max: 0.9236239387210161 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0036728846945091225 Min: -0.0038208877215602324\n","Layer 0 - Gradient Weights Max: 0.00698011443079853 Min: -0.0047162535008691995\n","ReLU Activation - Max: 0.9192418877308162 Min: 0.0\n","ReLU Activation - Max: 0.9238046314163622 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003680737682498949 Min: -0.0038296055309693785\n","Layer 0 - Gradient Weights Max: 0.006985001939749546 Min: -0.004724179990018026\n","ReLU Activation - Max: 0.9201543271524439 Min: 0.0\n","Epoch 700, Loss: 0.3919, Test Accuracy: 0.7118\n","ReLU Activation - Max: 0.9239866314727578 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0036886049591239776 Min: -0.003838329385071775\n","Layer 0 - Gradient Weights Max: 0.006995653762828607 Min: -0.004714454610272573\n","ReLU Activation - Max: 0.9210622400932398 Min: 0.0\n","ReLU Activation - Max: 0.9241680309268344 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003696522569153211 Min: -0.0038471012061554065\n","Layer 0 - Gradient Weights Max: 0.006999989990058611 Min: -0.004711887942871831\n","ReLU Activation - Max: 0.9219688088551486 Min: 0.0\n","ReLU Activation - Max: 0.9243439889929328 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003704479588398648 Min: -0.0038559102802017257\n","Layer 0 - Gradient Weights Max: 0.0069779172618018685 Min: -0.004703035782761153\n","ReLU Activation - Max: 0.9228727253103981 Min: 0.0\n","ReLU Activation - Max: 0.9245174927035599 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003712413109377234 Min: -0.0038646925954403696\n","Layer 0 - Gradient Weights Max: 0.00698311120369248 Min: -0.004656195038598422\n","ReLU Activation - Max: 0.9237691577027576 Min: 0.0\n","ReLU Activation - Max: 0.9246904644211271 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003720276571492451 Min: -0.0038733960675209067\n","Layer 0 - Gradient Weights Max: 0.0069988279561154975 Min: -0.004672502171276379\n","ReLU Activation - Max: 0.9246672461567064 Min: 0.0\n","ReLU Activation - Max: 0.9248619309199736 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003728085769077621 Min: -0.0038820461094280284\n","Layer 0 - Gradient Weights Max: 0.0070328269338060705 Min: -0.0046885493393955735\n","ReLU Activation - Max: 0.925566437499761 Min: 0.0\n","ReLU Activation - Max: 0.9250329076872155 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0037358996569397786 Min: -0.00389069359474984\n","Layer 0 - Gradient Weights Max: 0.007016165129043815 Min: -0.004741144212120256\n","ReLU Activation - Max: 0.926465402857639 Min: 0.0\n","ReLU Activation - Max: 0.9252053939603745 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0037436987589803163 Min: -0.003899321300460088\n","Layer 0 - Gradient Weights Max: 0.007048059713457572 Min: -0.0047548413085015595\n","ReLU Activation - Max: 0.9273676638911831 Min: 0.0\n","ReLU Activation - Max: 0.9253796995051536 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003751577903352468 Min: -0.003908027109636063\n","Layer 0 - Gradient Weights Max: 0.007061038857076178 Min: -0.0047623515501150696\n","ReLU Activation - Max: 0.9282787227070495 Min: 0.0\n","ReLU Activation - Max: 0.9255516262399939 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003759504543297699 Min: -0.003916779847882697\n","Layer 0 - Gradient Weights Max: 0.007078827958842991 Min: -0.004781899300322794\n","ReLU Activation - Max: 0.9291914797133097 Min: 0.0\n","Epoch 710, Loss: 0.3913, Test Accuracy: 0.7132\n","ReLU Activation - Max: 0.9257230844197808 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003767397235065389 Min: -0.003925493499455606\n","Layer 0 - Gradient Weights Max: 0.0070603253927255145 Min: -0.004797790443707777\n","ReLU Activation - Max: 0.9301079349915892 Min: 0.0\n","ReLU Activation - Max: 0.9258959311725344 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003775315466491753 Min: -0.003934225973008381\n","Layer 0 - Gradient Weights Max: 0.007060855437945549 Min: -0.004774608868791817\n","ReLU Activation - Max: 0.9310205467684417 Min: 0.0\n","ReLU Activation - Max: 0.9260736462979675 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003783179605461846 Min: -0.003942894757228576\n","Layer 0 - Gradient Weights Max: 0.007062997322807883 Min: -0.004805040488135875\n","ReLU Activation - Max: 0.9319381018459939 Min: 0.0\n","ReLU Activation - Max: 0.9262481837973017 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003791088821762558 Min: -0.003951610643825844\n","Layer 0 - Gradient Weights Max: 0.007105465850153406 Min: -0.0048169602332266736\n","ReLU Activation - Max: 0.9328579729059716 Min: 0.0\n","ReLU Activation - Max: 0.9264186568146153 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003799064603621796 Min: -0.003960396467132608\n","Layer 0 - Gradient Weights Max: 0.007101296744529371 Min: -0.0048478678076654975\n","ReLU Activation - Max: 0.9337767833669567 Min: 0.0\n","ReLU Activation - Max: 0.9265884693108649 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038069975087104517 Min: -0.003969129157837721\n","Layer 0 - Gradient Weights Max: 0.007078028603981445 Min: -0.004823076646016124\n","ReLU Activation - Max: 0.9346961266060899 Min: 0.0\n","ReLU Activation - Max: 0.9267565133154927 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038147895734780214 Min: -0.003977714093091732\n","Layer 0 - Gradient Weights Max: 0.007109585915566849 Min: -0.004839044276690388\n","ReLU Activation - Max: 0.9356214698151724 Min: 0.0\n","ReLU Activation - Max: 0.926925093819961 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038225859292407628 Min: -0.003986305883387566\n","Layer 0 - Gradient Weights Max: 0.007120501624029164 Min: -0.004832129949030527\n","ReLU Activation - Max: 0.9365436678511999 Min: 0.0\n","ReLU Activation - Max: 0.9270932057551025 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003830313312829767 Min: -0.003994826374441987\n","Layer 0 - Gradient Weights Max: 0.007080760801419771 Min: -0.0048299467597256816\n","ReLU Activation - Max: 0.9374681377618252 Min: 0.0\n","ReLU Activation - Max: 0.9272623877630674 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038379829904505807 Min: -0.004003286593010623\n","Layer 0 - Gradient Weights Max: 0.007090823793820386 Min: -0.004816455709739292\n","ReLU Activation - Max: 0.938389672992639 Min: 0.0\n","Epoch 720, Loss: 0.3907, Test Accuracy: 0.7136\n","ReLU Activation - Max: 0.9274264130785324 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003845692696534196 Min: -0.004011783050267715\n","Layer 0 - Gradient Weights Max: 0.0070154713070588025 Min: -0.004829854857008133\n","ReLU Activation - Max: 0.9393140136607937 Min: 0.0\n","ReLU Activation - Max: 0.9275905458723434 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003853244491395311 Min: -0.004020119409197438\n","Layer 0 - Gradient Weights Max: 0.007024346094393839 Min: -0.004840209605643787\n","ReLU Activation - Max: 0.9402362301211992 Min: 0.0\n","ReLU Activation - Max: 0.927753502882668 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003860677903909672 Min: -0.00402832644684133\n","Layer 0 - Gradient Weights Max: 0.007018069820027024 Min: -0.004876073196856152\n","ReLU Activation - Max: 0.941165488093678 Min: 0.0\n","ReLU Activation - Max: 0.92791716638749 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003867956773138392 Min: -0.004036363936271564\n","Layer 0 - Gradient Weights Max: 0.007061570275166054 Min: -0.004879661255962271\n","ReLU Activation - Max: 0.9420956099572905 Min: 0.0\n","ReLU Activation - Max: 0.9280809022392706 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038753652909002197 Min: -0.004044528666742851\n","Layer 0 - Gradient Weights Max: 0.007050269729347479 Min: -0.004881649140410441\n","ReLU Activation - Max: 0.94302659663715 Min: 0.0\n","ReLU Activation - Max: 0.9282407729759977 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003882657438312612 Min: -0.00405257099838939\n","Layer 0 - Gradient Weights Max: 0.007062422467197143 Min: -0.0048681515140760875\n","ReLU Activation - Max: 0.9439522817083454 Min: 0.0\n","ReLU Activation - Max: 0.9284008219698108 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003889991870828997 Min: -0.004060657065269251\n","Layer 0 - Gradient Weights Max: 0.007085865621192947 Min: -0.0048703591918848555\n","ReLU Activation - Max: 0.9448757432504504 Min: 0.0\n","ReLU Activation - Max: 0.9285643934019346 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0038973097899946563 Min: -0.004068718904060233\n","Layer 0 - Gradient Weights Max: 0.007075244374277679 Min: -0.004846334799524482\n","ReLU Activation - Max: 0.9457939731257214 Min: 0.0\n","ReLU Activation - Max: 0.9287300879402762 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003904548289422267 Min: -0.004076688500212973\n","Layer 0 - Gradient Weights Max: 0.0070920665094460515 Min: -0.004886479268459649\n","ReLU Activation - Max: 0.9467157931717132 Min: 0.0\n","ReLU Activation - Max: 0.9288855270748567 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003911697637639723 Min: -0.0040845685957929334\n","Layer 0 - Gradient Weights Max: 0.007039398724818012 Min: -0.0048617728563482435\n","ReLU Activation - Max: 0.9476346035139513 Min: 0.0\n","Epoch 730, Loss: 0.3900, Test Accuracy: 0.7143\n","ReLU Activation - Max: 0.9290393322354218 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003918761978895193 Min: -0.004092352706484906\n","Layer 0 - Gradient Weights Max: 0.007045065698862757 Min: -0.004872878330935957\n","ReLU Activation - Max: 0.948550553808971 Min: 0.0\n","ReLU Activation - Max: 0.9291839208215491 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003925817047965259 Min: -0.004100122640736851\n","Layer 0 - Gradient Weights Max: 0.007067992242446621 Min: -0.004880138406918995\n","ReLU Activation - Max: 0.9494616736912272 Min: 0.0\n","ReLU Activation - Max: 0.9293218109209311 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003932985943109201 Min: -0.004107993850767874\n","Layer 0 - Gradient Weights Max: 0.007041759155388695 Min: -0.004880523620271307\n","ReLU Activation - Max: 0.9503702362482369 Min: 0.0\n","ReLU Activation - Max: 0.9294617037246701 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00394014127300979 Min: -0.004115838793649913\n","Layer 0 - Gradient Weights Max: 0.007024663673765855 Min: -0.0048927536713414665\n","ReLU Activation - Max: 0.9512779643290199 Min: 0.0\n","ReLU Activation - Max: 0.9296002061724211 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003947215283949781 Min: -0.004123589069826189\n","Layer 0 - Gradient Weights Max: 0.007037039403036317 Min: -0.00484791426703171\n","ReLU Activation - Max: 0.95218454247261 Min: 0.0\n","ReLU Activation - Max: 0.9297379683523921 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003954238983411143 Min: -0.004131279113609071\n","Layer 0 - Gradient Weights Max: 0.007027061012263816 Min: -0.004842743523396931\n","ReLU Activation - Max: 0.9530856037095824 Min: 0.0\n","ReLU Activation - Max: 0.929876072999351 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003961243969293814 Min: -0.004138943490308725\n","Layer 0 - Gradient Weights Max: 0.0070146558785246895 Min: -0.00485255307456767\n","ReLU Activation - Max: 0.9539880834389666 Min: 0.0\n","ReLU Activation - Max: 0.9300135811015998 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003968093500284272 Min: -0.004146438061842192\n","Layer 0 - Gradient Weights Max: 0.007038419536753895 Min: -0.004870421030577047\n","ReLU Activation - Max: 0.9548872109083777 Min: 0.0\n","ReLU Activation - Max: 0.9301430484903311 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0039749252695009596 Min: -0.004153916414783088\n","Layer 0 - Gradient Weights Max: 0.007023421992403782 Min: -0.004894999695593029\n","ReLU Activation - Max: 0.9557803971428641 Min: 0.0\n","ReLU Activation - Max: 0.9302731006670527 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003981745406971107 Min: -0.004161377301115486\n","Layer 0 - Gradient Weights Max: 0.007089747947053716 Min: -0.0048920139521905015\n","ReLU Activation - Max: 0.9566722511460861 Min: 0.0\n","Epoch 740, Loss: 0.3894, Test Accuracy: 0.7150\n","ReLU Activation - Max: 0.9304104561445827 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003988570156663842 Min: -0.004168846092732027\n","Layer 0 - Gradient Weights Max: 0.007077429296676067 Min: -0.00489973676183429\n","ReLU Activation - Max: 0.9575630758081457 Min: 0.0\n","ReLU Activation - Max: 0.9305488613216223 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.003995269270182705 Min: -0.0041761918899981985\n","Layer 0 - Gradient Weights Max: 0.0070835741208855945 Min: -0.004898186353849019\n","ReLU Activation - Max: 0.9584570686637307 Min: 0.0\n","ReLU Activation - Max: 0.930696474812343 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00400186816878067 Min: -0.004183432158540064\n","Layer 0 - Gradient Weights Max: 0.007106051057001257 Min: -0.004927538079461264\n","ReLU Activation - Max: 0.9593547859733416 Min: 0.0\n","ReLU Activation - Max: 0.9308381010107584 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004008666181568577 Min: -0.004190871747400084\n","Layer 0 - Gradient Weights Max: 0.007089744185121322 Min: -0.004929443075007434\n","ReLU Activation - Max: 0.9602527102764822 Min: 0.0\n","ReLU Activation - Max: 0.9309794073675564 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00401544127168249 Min: -0.004198283364624891\n","Layer 0 - Gradient Weights Max: 0.007103056784726424 Min: -0.00489732336253218\n","ReLU Activation - Max: 0.9611503861792416 Min: 0.0\n","ReLU Activation - Max: 0.931115987762407 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004022212402383912 Min: -0.004205684338565684\n","Layer 0 - Gradient Weights Max: 0.007108054584847708 Min: -0.004896430301154001\n","ReLU Activation - Max: 0.9620411357464564 Min: 0.0\n","ReLU Activation - Max: 0.9312492915156723 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004028953857583434 Min: -0.0042130492233024884\n","Layer 0 - Gradient Weights Max: 0.007132410098593977 Min: -0.004896240941080345\n","ReLU Activation - Max: 0.9629285090795933 Min: 0.0\n","ReLU Activation - Max: 0.9313793396234346 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004035696251581895 Min: -0.004220404970153368\n","Layer 0 - Gradient Weights Max: 0.007134796773898074 Min: -0.004898958938875692\n","ReLU Activation - Max: 0.9638154022983529 Min: 0.0\n","ReLU Activation - Max: 0.9315068885803199 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004042432709214354 Min: -0.004227741400048266\n","Layer 0 - Gradient Weights Max: 0.007141119940099671 Min: -0.00492797696071442\n","ReLU Activation - Max: 0.9647068202037928 Min: 0.0\n","ReLU Activation - Max: 0.9316370548922106 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004049190246370052 Min: -0.004235091827639293\n","Layer 0 - Gradient Weights Max: 0.0071439581716483055 Min: -0.004925825717026503\n","ReLU Activation - Max: 0.9656003254961746 Min: 0.0\n","Epoch 750, Loss: 0.3887, Test Accuracy: 0.7150\n","ReLU Activation - Max: 0.9317682636695929 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004055962338215738 Min: -0.004242458209518966\n","Layer 0 - Gradient Weights Max: 0.007134371823948781 Min: -0.004922337445073963\n","ReLU Activation - Max: 0.9664908199145579 Min: 0.0\n","ReLU Activation - Max: 0.9318997816675795 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004062782060455258 Min: -0.004249862868780544\n","Layer 0 - Gradient Weights Max: 0.007150497905847438 Min: -0.004956432245186987\n","ReLU Activation - Max: 0.9673772900373669 Min: 0.0\n","ReLU Activation - Max: 0.9320296016279438 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004069502709743083 Min: -0.004257161103675507\n","Layer 0 - Gradient Weights Max: 0.007150657354737881 Min: -0.004986854605636024\n","ReLU Activation - Max: 0.9682562452613155 Min: 0.0\n","ReLU Activation - Max: 0.9321712046483965 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004076145100517379 Min: -0.00426438483935879\n","Layer 0 - Gradient Weights Max: 0.007148629926941688 Min: -0.0049870739273124584\n","ReLU Activation - Max: 0.9691383597151679 Min: 0.0\n","ReLU Activation - Max: 0.9323090787128858 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004082656842356031 Min: -0.004271472911907102\n","Layer 0 - Gradient Weights Max: 0.00712764432726216 Min: -0.005047825879918386\n","ReLU Activation - Max: 0.9700170283329677 Min: 0.0\n","ReLU Activation - Max: 0.9324521833904715 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004089282711095554 Min: -0.004278681693067529\n","Layer 0 - Gradient Weights Max: 0.007124640345543711 Min: -0.005065915834123267\n","ReLU Activation - Max: 0.9708965322141437 Min: 0.0\n","ReLU Activation - Max: 0.9325975748073301 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00409585268118251 Min: -0.004285825512697432\n","Layer 0 - Gradient Weights Max: 0.00713503971577673 Min: -0.0050733003258710985\n","ReLU Activation - Max: 0.9717741830622121 Min: 0.0\n","ReLU Activation - Max: 0.9327384578055298 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004102534086136988 Min: -0.004293070242174501\n","Layer 0 - Gradient Weights Max: 0.0071912802001759864 Min: -0.005071519048950217\n","ReLU Activation - Max: 0.9726511457210517 Min: 0.0\n","ReLU Activation - Max: 0.9328797019953448 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004109258105468539 Min: -0.00430035184403079\n","Layer 0 - Gradient Weights Max: 0.007173279653013004 Min: -0.005086624885682673\n","ReLU Activation - Max: 0.9735304226390553 Min: 0.0\n","ReLU Activation - Max: 0.9330188354574793 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004116023242923066 Min: -0.004307678074431819\n","Layer 0 - Gradient Weights Max: 0.007179486479634803 Min: -0.005109273607787462\n","ReLU Activation - Max: 0.974419848632798 Min: 0.0\n","Epoch 760, Loss: 0.3881, Test Accuracy: 0.7161\n","ReLU Activation - Max: 0.9331596662461014 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004122737960502662 Min: -0.004314954840068703\n","Layer 0 - Gradient Weights Max: 0.0071742744532968 Min: -0.0051216745067437855\n","ReLU Activation - Max: 0.9753038619055784 Min: 0.0\n","ReLU Activation - Max: 0.9333012125442929 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004129381453135765 Min: -0.00432215392378854\n","Layer 0 - Gradient Weights Max: 0.007195258350488165 Min: -0.005089468812710347\n","ReLU Activation - Max: 0.9761861754657195 Min: 0.0\n","ReLU Activation - Max: 0.9334391531330706 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004135841582934393 Min: -0.004329154888896099\n","Layer 0 - Gradient Weights Max: 0.007162812026392381 Min: -0.005090297601969418\n","ReLU Activation - Max: 0.9770716612792927 Min: 0.0\n","ReLU Activation - Max: 0.9335896602085461 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004142191727392406 Min: -0.0043360366601948854\n","Layer 0 - Gradient Weights Max: 0.007146428784814452 Min: -0.0050798800384722394\n","ReLU Activation - Max: 0.9779573247155143 Min: 0.0\n","ReLU Activation - Max: 0.9337305767348105 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004148395215470537 Min: -0.00434276197528391\n","Layer 0 - Gradient Weights Max: 0.007137903367602252 Min: -0.005082685760272888\n","ReLU Activation - Max: 0.9788398490605777 Min: 0.0\n","ReLU Activation - Max: 0.9338683722922061 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004154469134357437 Min: -0.004349349057463969\n","Layer 0 - Gradient Weights Max: 0.007147839928645942 Min: -0.0050844204532160845\n","ReLU Activation - Max: 0.9797163910514062 Min: 0.0\n","ReLU Activation - Max: 0.9340030104794912 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004160624840065109 Min: -0.004356008105771763\n","Layer 0 - Gradient Weights Max: 0.00716850272721521 Min: -0.005083187383248838\n","ReLU Activation - Max: 0.980586338145802 Min: 0.0\n","ReLU Activation - Max: 0.9341359599314213 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004166728209590614 Min: -0.004362597818382919\n","Layer 0 - Gradient Weights Max: 0.007160485961182691 Min: -0.00507308878935163\n","ReLU Activation - Max: 0.9814537420657008 Min: 0.0\n","ReLU Activation - Max: 0.9342560654916972 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0041727895962004775 Min: -0.004369137533338583\n","Layer 0 - Gradient Weights Max: 0.0071570351512401115 Min: -0.005110184408940677\n","ReLU Activation - Max: 0.9823191131880352 Min: 0.0\n","ReLU Activation - Max: 0.9343837652204194 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0041787210413555655 Min: -0.004375539751453015\n","Layer 0 - Gradient Weights Max: 0.0071556097029918265 Min: -0.005106796457956246\n","ReLU Activation - Max: 0.983183309373882 Min: 0.0\n","Epoch 770, Loss: 0.3874, Test Accuracy: 0.7154\n","ReLU Activation - Max: 0.9345129126814202 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0041845800351363585 Min: -0.004381865710821476\n","Layer 0 - Gradient Weights Max: 0.007121685955892462 Min: -0.005104276209182645\n","ReLU Activation - Max: 0.9840484671867755 Min: 0.0\n","ReLU Activation - Max: 0.9346436764764483 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0041903664085871485 Min: -0.004388112393341382\n","Layer 0 - Gradient Weights Max: 0.0071036255009124675 Min: -0.005111651124557598\n","ReLU Activation - Max: 0.9849087505423507 Min: 0.0\n","ReLU Activation - Max: 0.9347787744889107 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004196067271173764 Min: -0.0043942672077317835\n","Layer 0 - Gradient Weights Max: 0.0071224752488937984 Min: -0.0051153866036758675\n","ReLU Activation - Max: 0.9857653370175725 Min: 0.0\n","ReLU Activation - Max: 0.9349135887809387 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004201761891706943 Min: -0.004400410718279333\n","Layer 0 - Gradient Weights Max: 0.007103423193567047 Min: -0.005104308505430725\n","ReLU Activation - Max: 0.986618294939031 Min: 0.0\n","ReLU Activation - Max: 0.9350481426964005 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004207439149654127 Min: -0.004406528539768518\n","Layer 0 - Gradient Weights Max: 0.0070925348561239965 Min: -0.005100012340049945\n","ReLU Activation - Max: 0.9874629704483853 Min: 0.0\n","ReLU Activation - Max: 0.9351817900184702 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004213106935985217 Min: -0.004412634395278131\n","Layer 0 - Gradient Weights Max: 0.007084067118887246 Min: -0.005087885242287812\n","ReLU Activation - Max: 0.9883074318298072 Min: 0.0\n","ReLU Activation - Max: 0.9353132903782674 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004218755455213482 Min: -0.004418716097650048\n","Layer 0 - Gradient Weights Max: 0.0071027078063196965 Min: -0.005107470992611112\n","ReLU Activation - Max: 0.9891506386069627 Min: 0.0\n","ReLU Activation - Max: 0.9354420600636912 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004224427056804103 Min: -0.004424810605581059\n","Layer 0 - Gradient Weights Max: 0.00709160746409637 Min: -0.005143505017659139\n","ReLU Activation - Max: 0.9899910611970134 Min: 0.0\n","ReLU Activation - Max: 0.9355692739655181 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004230070535218956 Min: -0.004430870587260646\n","Layer 0 - Gradient Weights Max: 0.007102714030359294 Min: -0.005143281085043657\n","ReLU Activation - Max: 0.9908313421256706 Min: 0.0\n","ReLU Activation - Max: 0.9356950100268806 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004235610411897004 Min: -0.004436827801420082\n","Layer 0 - Gradient Weights Max: 0.0071244316385621126 Min: -0.0051265138054847935\n","ReLU Activation - Max: 0.9916713338754624 Min: 0.0\n","Epoch 780, Loss: 0.3867, Test Accuracy: 0.7154\n","ReLU Activation - Max: 0.9358201880510402 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00424104025976661 Min: -0.004442664589791633\n","Layer 0 - Gradient Weights Max: 0.007118940447642298 Min: -0.005124303374333779\n","ReLU Activation - Max: 0.9925139619971153 Min: 0.0\n","ReLU Activation - Max: 0.9363730337784872 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0042462028877143995 Min: -0.004448227009233562\n","Layer 0 - Gradient Weights Max: 0.007135491893098879 Min: -0.005143193038671426\n","ReLU Activation - Max: 0.9933593313183173 Min: 0.0\n","ReLU Activation - Max: 0.9382939335290464 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004251122582831885 Min: -0.0044535404786324225\n","Layer 0 - Gradient Weights Max: 0.0071084835115965165 Min: -0.005118777734635764\n","ReLU Activation - Max: 0.9941998317745596 Min: 0.0\n","ReLU Activation - Max: 0.940219429069479 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0042559667211201965 Min: -0.004458763173574063\n","Layer 0 - Gradient Weights Max: 0.007098522327871925 Min: -0.005116580880101444\n","ReLU Activation - Max: 0.9950406737549322 Min: 0.0\n","ReLU Activation - Max: 0.9421402917201144 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0042607790339710535 Min: -0.004463937786876497\n","Layer 0 - Gradient Weights Max: 0.007129671764537347 Min: -0.005099527298337767\n","ReLU Activation - Max: 0.9958822669828523 Min: 0.0\n","ReLU Activation - Max: 0.9440642580526917 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00426555262904541 Min: -0.004469065703969144\n","Layer 0 - Gradient Weights Max: 0.0071324049277705906 Min: -0.005127658518058493\n","ReLU Activation - Max: 0.9967204499359752 Min: 0.0\n","ReLU Activation - Max: 0.9459885512054979 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004270189070549434 Min: -0.004474049693917377\n","Layer 0 - Gradient Weights Max: 0.0071514650556545045 Min: -0.005074747011408704\n","ReLU Activation - Max: 0.9975576743890218 Min: 0.0\n","ReLU Activation - Max: 0.9479149398715248 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004274815827260624 Min: -0.00447899894827928\n","Layer 0 - Gradient Weights Max: 0.007135296834174056 Min: -0.005062551557981618\n","ReLU Activation - Max: 0.9983947564385655 Min: 0.0\n","ReLU Activation - Max: 0.9498361657369991 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004279387747057767 Min: -0.004483881708444919\n","Layer 0 - Gradient Weights Max: 0.007143926030513868 Min: -0.005048429830925124\n","ReLU Activation - Max: 0.9992327623507803 Min: 0.0\n","ReLU Activation - Max: 0.9517540176239789 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004283918072884304 Min: -0.00448871827064126\n","Layer 0 - Gradient Weights Max: 0.007139118033776633 Min: -0.005049627240839616\n","ReLU Activation - Max: 1.0000726785472576 Min: 0.0\n","Epoch 790, Loss: 0.3860, Test Accuracy: 0.7150\n","ReLU Activation - Max: 0.9536682594976427 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004288402153301463 Min: -0.0044934973536169655\n","Layer 0 - Gradient Weights Max: 0.007122343234631643 Min: -0.005047331159914988\n","ReLU Activation - Max: 1.0009116266939726 Min: 0.0\n","ReLU Activation - Max: 0.9555815368296529 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004292825597599261 Min: -0.0044982127567555095\n","Layer 0 - Gradient Weights Max: 0.007116177127935084 Min: -0.005060456783535629\n","ReLU Activation - Max: 1.0017487279164348 Min: 0.0\n","ReLU Activation - Max: 0.9574924693343116 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00429710387772924 Min: -0.0045027750430184895\n","Layer 0 - Gradient Weights Max: 0.007080395565187804 Min: -0.0050768082016073085\n","ReLU Activation - Max: 1.0025858007696975 Min: 0.0\n","ReLU Activation - Max: 0.9594013911511241 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004301327433161587 Min: -0.0045072675082782915\n","Layer 0 - Gradient Weights Max: 0.007076513949511777 Min: -0.005040845623678616\n","ReLU Activation - Max: 1.0034172407490354 Min: 0.0\n","ReLU Activation - Max: 0.9613103769347242 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004305550245265865 Min: -0.004511740316495985\n","Layer 0 - Gradient Weights Max: 0.007109241064258843 Min: -0.005024239573508624\n","ReLU Activation - Max: 1.0042456510551923 Min: 0.0\n","ReLU Activation - Max: 0.9632236400499732 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043097137507900115 Min: -0.0045161434426287455\n","Layer 0 - Gradient Weights Max: 0.007106828525009995 Min: -0.0050139566477108\n","ReLU Activation - Max: 1.0051757813070838 Min: 0.0\n","ReLU Activation - Max: 0.9651372483659698 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043136451065435176 Min: -0.004520304402262819\n","Layer 0 - Gradient Weights Max: 0.007120845963382886 Min: -0.0049848725102485175\n","ReLU Activation - Max: 1.006543594072057 Min: 0.0\n","ReLU Activation - Max: 0.9670469034664873 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043174651805212546 Min: -0.00452434047647775\n","Layer 0 - Gradient Weights Max: 0.007135709224448923 Min: -0.0049732762253195\n","ReLU Activation - Max: 1.0079125511094988 Min: 0.0\n","ReLU Activation - Max: 0.9689556758931166 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004321294425029285 Min: -0.004528381520785578\n","Layer 0 - Gradient Weights Max: 0.007131897687677329 Min: -0.00497463183379818\n","ReLU Activation - Max: 1.0092859615920677 Min: 0.0\n","ReLU Activation - Max: 0.9708650514181758 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004325063348708064 Min: -0.004532349631575444\n","Layer 0 - Gradient Weights Max: 0.007115156890670515 Min: -0.004965728181489118\n","ReLU Activation - Max: 1.010652865519168 Min: 0.0\n","Epoch 800, Loss: 0.3853, Test Accuracy: 0.7161\n","ReLU Activation - Max: 0.9727737118804913 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004328805729985573 Min: -0.004536285199369792\n","Layer 0 - Gradient Weights Max: 0.007117527306212152 Min: -0.004970473895869133\n","ReLU Activation - Max: 1.0120188707237399 Min: 0.0\n","ReLU Activation - Max: 0.9746838912222341 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043325217810653745 Min: -0.004540185064576235\n","Layer 0 - Gradient Weights Max: 0.007137545638496853 Min: -0.004979372994357244\n","ReLU Activation - Max: 1.0133807315382084 Min: 0.0\n","ReLU Activation - Max: 0.9765946854836135 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004336205785169727 Min: -0.004544047220035995\n","Layer 0 - Gradient Weights Max: 0.00712744495365577 Min: -0.004989219807825668\n","ReLU Activation - Max: 1.0147410284849379 Min: 0.0\n","ReLU Activation - Max: 0.9785023540944933 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00433971008705405 Min: -0.00454772995390511\n","Layer 0 - Gradient Weights Max: 0.007123823521194971 Min: -0.004988058046241286\n","ReLU Activation - Max: 1.0161009438510809 Min: 0.0\n","ReLU Activation - Max: 0.9804067958899583 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004343106961225198 Min: -0.004551306169523676\n","Layer 0 - Gradient Weights Max: 0.007108814909763388 Min: -0.005005274430552209\n","ReLU Activation - Max: 1.0174637101917687 Min: 0.0\n","ReLU Activation - Max: 0.9823102058228913 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004346500101076449 Min: -0.004554873529997162\n","Layer 0 - Gradient Weights Max: 0.007116856439213305 Min: -0.0049960967584231734\n","ReLU Activation - Max: 1.0188350770586523 Min: 0.0\n","ReLU Activation - Max: 0.984209276469332 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004349902154888765 Min: -0.004558440338554186\n","Layer 0 - Gradient Weights Max: 0.007130156698894004 Min: -0.005042562034069023\n","ReLU Activation - Max: 1.0201994579669313 Min: 0.0\n","ReLU Activation - Max: 0.9861043934446173 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004353262822594397 Min: -0.004561966479670439\n","Layer 0 - Gradient Weights Max: 0.0071160299819553255 Min: -0.005052579801530428\n","ReLU Activation - Max: 1.0215675460558238 Min: 0.0\n","ReLU Activation - Max: 0.9880022985550491 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004356631121083521 Min: -0.004565496451868631\n","Layer 0 - Gradient Weights Max: 0.007110752811215524 Min: -0.0050596276495980435\n","ReLU Activation - Max: 1.0229345880248597 Min: 0.0\n","ReLU Activation - Max: 0.9898997913745217 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004359917033454824 Min: -0.0045689404863747994\n","Layer 0 - Gradient Weights Max: 0.007128416435665952 Min: -0.0050655331491503055\n","ReLU Activation - Max: 1.0243043153007252 Min: 0.0\n","Epoch 810, Loss: 0.3846, Test Accuracy: 0.7175\n","ReLU Activation - Max: 0.991795153722709 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004363163237846808 Min: -0.004572333247096216\n","Layer 0 - Gradient Weights Max: 0.007119495637307787 Min: -0.005046445709723149\n","ReLU Activation - Max: 1.0256671560683115 Min: 0.0\n","ReLU Activation - Max: 0.993684506300982 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004366208056512932 Min: -0.0045755073362471615\n","Layer 0 - Gradient Weights Max: 0.007117821181685566 Min: -0.005038291818034399\n","ReLU Activation - Max: 1.0270296425059537 Min: 0.0\n","ReLU Activation - Max: 0.9955756197906165 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004369227154627302 Min: -0.004578647030722768\n","Layer 0 - Gradient Weights Max: 0.0071582159067837105 Min: -0.005045197644642491\n","ReLU Activation - Max: 1.0283855949730873 Min: 0.0\n","ReLU Activation - Max: 0.9974706075270714 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004372220093997109 Min: -0.004581755263355567\n","Layer 0 - Gradient Weights Max: 0.007154143996360285 Min: -0.005057902703678152\n","ReLU Activation - Max: 1.0297369412315627 Min: 0.0\n","ReLU Activation - Max: 0.9993671680415268 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004375210170522803 Min: -0.004584851889682324\n","Layer 0 - Gradient Weights Max: 0.007174134356902896 Min: -0.005049103290317708\n","ReLU Activation - Max: 1.0310838528023225 Min: 0.0\n","ReLU Activation - Max: 1.0012617729234299 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004378196263999152 Min: -0.004587939646038824\n","Layer 0 - Gradient Weights Max: 0.007167498094083284 Min: -0.00505071287119181\n","ReLU Activation - Max: 1.0324299417282756 Min: 0.0\n","ReLU Activation - Max: 1.003153565163667 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004381113359937149 Min: -0.004590949754413466\n","Layer 0 - Gradient Weights Max: 0.007165210740254992 Min: -0.005076714401324257\n","ReLU Activation - Max: 1.0337767513010563 Min: 0.0\n","ReLU Activation - Max: 1.0050520736620006 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004383990119599726 Min: -0.004593914646197192\n","Layer 0 - Gradient Weights Max: 0.007149761771226115 Min: -0.00505762871545477\n","ReLU Activation - Max: 1.0351281289219239 Min: 0.0\n","ReLU Activation - Max: 1.006956292260626 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004386701503377032 Min: -0.004596705735310092\n","Layer 0 - Gradient Weights Max: 0.0071559303588575015 Min: -0.005057232571571925\n","ReLU Activation - Max: 1.0364727509418485 Min: 0.0\n","ReLU Activation - Max: 1.008869169948193 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004389425825098956 Min: -0.004599511003929331\n","Layer 0 - Gradient Weights Max: 0.0071673131420986495 Min: -0.005075730363134346\n","ReLU Activation - Max: 1.0378164086850743 Min: 0.0\n","Epoch 820, Loss: 0.3839, Test Accuracy: 0.7175\n","ReLU Activation - Max: 1.0107823684428074 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004392133017279301 Min: -0.004602292980394261\n","Layer 0 - Gradient Weights Max: 0.007174182530460289 Min: -0.0050708637779826185\n","ReLU Activation - Max: 1.0391601688366585 Min: 0.0\n","ReLU Activation - Max: 1.0126971342012774 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004394902735007378 Min: -0.004605131304746304\n","Layer 0 - Gradient Weights Max: 0.00721735057767072 Min: -0.0051033615157478045\n","ReLU Activation - Max: 1.0405097640214265 Min: 0.0\n","ReLU Activation - Max: 1.014615934711282 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004397703448514866 Min: -0.004607999095287208\n","Layer 0 - Gradient Weights Max: 0.007202153317616569 Min: -0.005111850059302682\n","ReLU Activation - Max: 1.0418580155563786 Min: 0.0\n","ReLU Activation - Max: 1.0165324648909653 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004400408051811753 Min: -0.004610756882088164\n","Layer 0 - Gradient Weights Max: 0.007199278641388058 Min: -0.005116942945094083\n","ReLU Activation - Max: 1.0432038362673774 Min: 0.0\n","ReLU Activation - Max: 1.018445047437684 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004403064130807055 Min: -0.0046134497248079675\n","Layer 0 - Gradient Weights Max: 0.007193372113653228 Min: -0.0051520265772731924\n","ReLU Activation - Max: 1.0445486225418943 Min: 0.0\n","ReLU Activation - Max: 1.0203657160616155 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004405701222863347 Min: -0.004616114479532401\n","Layer 0 - Gradient Weights Max: 0.007188132865221343 Min: -0.005132360528020625\n","ReLU Activation - Max: 1.045887239557792 Min: 0.0\n","ReLU Activation - Max: 1.0222864158742055 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004408322463596857 Min: -0.004618758572844599\n","Layer 0 - Gradient Weights Max: 0.007178678203625391 Min: -0.005140586587771988\n","ReLU Activation - Max: 1.0472261935832503 Min: 0.0\n","ReLU Activation - Max: 1.0242068779257676 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044108155814974025 Min: -0.004621267961494045\n","Layer 0 - Gradient Weights Max: 0.007185910921735687 Min: -0.005086182577116416\n","ReLU Activation - Max: 1.04855517753474 Min: 0.0\n","ReLU Activation - Max: 1.0261240095224533 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044130394480995355 Min: -0.004623498078962323\n","Layer 0 - Gradient Weights Max: 0.0071936224664978544 Min: -0.00508978235008755\n","ReLU Activation - Max: 1.0498814361014959 Min: 0.0\n","ReLU Activation - Max: 1.0280427628533464 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004415264717617997 Min: -0.004625721717763909\n","Layer 0 - Gradient Weights Max: 0.007197027968989003 Min: -0.005080213512665852\n","ReLU Activation - Max: 1.0512042203693526 Min: 0.0\n","Epoch 830, Loss: 0.3832, Test Accuracy: 0.7182\n","ReLU Activation - Max: 1.0299605387005641 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004417425161236741 Min: -0.004627873678836642\n","Layer 0 - Gradient Weights Max: 0.007223692099125959 Min: -0.005078884462618077\n","ReLU Activation - Max: 1.0525227257618839 Min: 0.0\n","ReLU Activation - Max: 1.0318858383949046 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004419644492260975 Min: -0.004630074935311943\n","Layer 0 - Gradient Weights Max: 0.007231527445598495 Min: -0.005066348417615581\n","ReLU Activation - Max: 1.0538471967806267 Min: 0.0\n","ReLU Activation - Max: 1.0338108356165712 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004421692818377511 Min: -0.004632088567771763\n","Layer 0 - Gradient Weights Max: 0.007186157677488731 Min: -0.005061162043806087\n","ReLU Activation - Max: 1.0551725193160717 Min: 0.0\n","ReLU Activation - Max: 1.0357358615382883 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004423701709611386 Min: -0.004634055247311822\n","Layer 0 - Gradient Weights Max: 0.007167542811739944 Min: -0.005052170261060401\n","ReLU Activation - Max: 1.0564943290587536 Min: 0.0\n","ReLU Activation - Max: 1.037656982155073 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004425691084764818 Min: -0.0046359873802717996\n","Layer 0 - Gradient Weights Max: 0.007180021440946714 Min: -0.005028800748966392\n","ReLU Activation - Max: 1.057812798899451 Min: 0.0\n","ReLU Activation - Max: 1.0395787803713061 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004427599011653605 Min: -0.004637836583572063\n","Layer 0 - Gradient Weights Max: 0.007153770960065221 Min: -0.005014815809865252\n","ReLU Activation - Max: 1.0591401201472537 Min: 0.0\n","ReLU Activation - Max: 1.041493110502152 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044294521812061356 Min: -0.004639624656690708\n","Layer 0 - Gradient Weights Max: 0.007138399544044775 Min: -0.004998617830439784\n","ReLU Activation - Max: 1.060466009769845 Min: 0.0\n","ReLU Activation - Max: 1.0434037564817717 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004431216121150491 Min: -0.0046413233602180855\n","Layer 0 - Gradient Weights Max: 0.007137722594883697 Min: -0.005033920581010482\n","ReLU Activation - Max: 1.0617929192202311 Min: 0.0\n","ReLU Activation - Max: 1.0453148845813396 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004433075354207886 Min: -0.004643114762818622\n","Layer 0 - Gradient Weights Max: 0.007154051322432445 Min: -0.004990757615588453\n","ReLU Activation - Max: 1.0631160595123081 Min: 0.0\n","ReLU Activation - Max: 1.0472250551630924 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044347846088455545 Min: -0.004644742722765541\n","Layer 0 - Gradient Weights Max: 0.007137648042900035 Min: -0.0050019851726630996\n","ReLU Activation - Max: 1.064438380585352 Min: 0.0\n","Epoch 840, Loss: 0.3824, Test Accuracy: 0.7189\n","ReLU Activation - Max: 1.0491279774418758 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00443658435076993 Min: -0.004646457313772107\n","Layer 0 - Gradient Weights Max: 0.00715862922932775 Min: -0.004975284054213465\n","ReLU Activation - Max: 1.0657592816450077 Min: 0.0\n","ReLU Activation - Max: 1.051028486033566 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044382531371135354 Min: -0.004648024721773151\n","Layer 0 - Gradient Weights Max: 0.007161382774073907 Min: -0.004994622820989932\n","ReLU Activation - Max: 1.0670809741961753 Min: 0.0\n","ReLU Activation - Max: 1.0529341493890572 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004439978620724268 Min: -0.0046496486288611\n","Layer 0 - Gradient Weights Max: 0.007137277976466086 Min: -0.004974753002408557\n","ReLU Activation - Max: 1.0684028239862042 Min: 0.0\n","ReLU Activation - Max: 1.054842091912216 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004441634174251105 Min: -0.004651190201987483\n","Layer 0 - Gradient Weights Max: 0.007141474388435841 Min: -0.004976497395445874\n","ReLU Activation - Max: 1.0697204810648269 Min: 0.0\n","ReLU Activation - Max: 1.0567421988010184 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004443143352108239 Min: -0.0046525770945888565\n","Layer 0 - Gradient Weights Max: 0.007160719218269959 Min: -0.004971830010469743\n","ReLU Activation - Max: 1.071041165517848 Min: 0.0\n","ReLU Activation - Max: 1.0586418452218291 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004444602084923425 Min: -0.00465390674983384\n","Layer 0 - Gradient Weights Max: 0.007121576359347655 Min: -0.004994484621051464\n","ReLU Activation - Max: 1.0723676737200303 Min: 0.0\n","ReLU Activation - Max: 1.0605369351587848 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004446019563160326 Min: -0.004655194088354878\n","Layer 0 - Gradient Weights Max: 0.007092010303320838 Min: -0.004983166599104641\n","ReLU Activation - Max: 1.0737032786736356 Min: 0.0\n","ReLU Activation - Max: 1.0624274138067684 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004447322726161056 Min: -0.0046563504999284615\n","Layer 0 - Gradient Weights Max: 0.00712413536974539 Min: -0.004998207402578614\n","ReLU Activation - Max: 1.0750422855017048 Min: 0.0\n","ReLU Activation - Max: 1.0643148501569575 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004448616334914407 Min: -0.004657491980971139\n","Layer 0 - Gradient Weights Max: 0.007122390939282374 Min: -0.00499623744927652\n","ReLU Activation - Max: 1.0763778350491722 Min: 0.0\n","ReLU Activation - Max: 1.0662038250629278 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044498244122919695 Min: -0.004658537988059264\n","Layer 0 - Gradient Weights Max: 0.007119490190643308 Min: -0.004998468354934931\n","ReLU Activation - Max: 1.0777125150633742 Min: 0.0\n","Epoch 850, Loss: 0.3817, Test Accuracy: 0.7189\n","ReLU Activation - Max: 1.068098241010966 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004451096154801882 Min: -0.004659643049561081\n","Layer 0 - Gradient Weights Max: 0.007092647362410398 Min: -0.004997996969027567\n","ReLU Activation - Max: 1.0790447918821915 Min: 0.0\n","ReLU Activation - Max: 1.0699912683796144 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004452333466345012 Min: -0.004660692891484658\n","Layer 0 - Gradient Weights Max: 0.007070015468414785 Min: -0.0049849509466395915\n","ReLU Activation - Max: 1.0803781723618915 Min: 0.0\n","ReLU Activation - Max: 1.071883542265805 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004453563237985213 Min: -0.004661731320049665\n","Layer 0 - Gradient Weights Max: 0.007061227522835311 Min: -0.004990706772648582\n","ReLU Activation - Max: 1.081710244105839 Min: 0.0\n","ReLU Activation - Max: 1.0737700471167395 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004454701189142912 Min: -0.004662677813189229\n","Layer 0 - Gradient Weights Max: 0.0070367183779342125 Min: -0.004981875115402008\n","ReLU Activation - Max: 1.0830420771775267 Min: 0.0\n","ReLU Activation - Max: 1.0756588137223784 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004455769155549863 Min: -0.004663546439720116\n","Layer 0 - Gradient Weights Max: 0.007038878895090152 Min: -0.004978068664342459\n","ReLU Activation - Max: 1.0843720733051274 Min: 0.0\n","ReLU Activation - Max: 1.0775474009576622 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044568407158365715 Min: -0.004664412301629824\n","Layer 0 - Gradient Weights Max: 0.0070886859144934245 Min: -0.004984396397152668\n","ReLU Activation - Max: 1.085698084861553 Min: 0.0\n","ReLU Activation - Max: 1.0794440300848653 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00445778034941407 Min: -0.00466513698974509\n","Layer 0 - Gradient Weights Max: 0.007093680260898388 Min: -0.004966876236994816\n","ReLU Activation - Max: 1.087017746693617 Min: 0.0\n","ReLU Activation - Max: 1.0813419784745832 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004458633585838303 Min: -0.0046657588557302175\n","Layer 0 - Gradient Weights Max: 0.007103594805952343 Min: -0.0049599326180368684\n","ReLU Activation - Max: 1.0883340359391787 Min: 0.0\n","ReLU Activation - Max: 1.083233773903975 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004459526264798961 Min: -0.004666411860598426\n","Layer 0 - Gradient Weights Max: 0.007098384301127145 Min: -0.004957944297961631\n","ReLU Activation - Max: 1.0896509917555557 Min: 0.0\n","ReLU Activation - Max: 1.0851272123215687 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004460233352530185 Min: -0.0046668712237413895\n","Layer 0 - Gradient Weights Max: 0.00709294450393974 Min: -0.004967426902495099\n","ReLU Activation - Max: 1.090966274155673 Min: 0.0\n","Epoch 860, Loss: 0.3809, Test Accuracy: 0.7200\n","ReLU Activation - Max: 1.0870142198188155 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004460914527818864 Min: -0.004667294121631849\n","Layer 0 - Gradient Weights Max: 0.007085220805160295 Min: -0.004993532520191098\n","ReLU Activation - Max: 1.0922841880147058 Min: 0.0\n","ReLU Activation - Max: 1.0888990447389832 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004461714641003281 Min: -0.0046678348763733385\n","Layer 0 - Gradient Weights Max: 0.007082954527958495 Min: -0.004995410966647249\n","ReLU Activation - Max: 1.093594826022429 Min: 0.0\n","ReLU Activation - Max: 1.0907845488804897 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004462494222452842 Min: -0.004668357277079186\n","Layer 0 - Gradient Weights Max: 0.007094998478821667 Min: -0.004985013919776077\n","ReLU Activation - Max: 1.0949064442395287 Min: 0.0\n","ReLU Activation - Max: 1.092670297476184 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004463201967373101 Min: -0.00466879557893629\n","Layer 0 - Gradient Weights Max: 0.007103567134950841 Min: -0.004971778908275221\n","ReLU Activation - Max: 1.0962171687376936 Min: 0.0\n","ReLU Activation - Max: 1.0945477780347066 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004463908872342213 Min: -0.0046692318761265665\n","Layer 0 - Gradient Weights Max: 0.007097502362510333 Min: -0.004950116044002544\n","ReLU Activation - Max: 1.0975261187352252 Min: 0.0\n","ReLU Activation - Max: 1.0964214698025907 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004464515882000272 Min: -0.004669562564111413\n","Layer 0 - Gradient Weights Max: 0.007085922532682578 Min: -0.004927559865563099\n","ReLU Activation - Max: 1.0988344477901275 Min: 0.0\n","ReLU Activation - Max: 1.0982888885963333 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00446504721691234 Min: -0.004669809087435571\n","Layer 0 - Gradient Weights Max: 0.007074687460230406 Min: -0.004928731931624026\n","ReLU Activation - Max: 1.1001437744155986 Min: 0.0\n","ReLU Activation - Max: 1.1001536318107052 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004465590337572126 Min: -0.004670065420902144\n","Layer 0 - Gradient Weights Max: 0.007037185009580113 Min: -0.004937033218378549\n","ReLU Activation - Max: 1.1014492709544894 Min: 0.0\n","ReLU Activation - Max: 1.1020162813611294 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004466111180875968 Min: -0.004670286650553115\n","Layer 0 - Gradient Weights Max: 0.007097155206422088 Min: -0.004945503472897239\n","ReLU Activation - Max: 1.1027559759844037 Min: 0.0\n","ReLU Activation - Max: 1.103876961180684 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004466629368472907 Min: -0.0046705005844160045\n","Layer 0 - Gradient Weights Max: 0.007082412607216637 Min: -0.004937186325287423\n","ReLU Activation - Max: 1.1040613656412588 Min: 0.0\n","Epoch 870, Loss: 0.3802, Test Accuracy: 0.7207\n","ReLU Activation - Max: 1.105735050926548 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004467116164601639 Min: -0.004670672563604275\n","Layer 0 - Gradient Weights Max: 0.007060258924613484 Min: -0.004906959164307083\n","ReLU Activation - Max: 1.1053604799400492 Min: 0.0\n","ReLU Activation - Max: 1.1075893752999901 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004467540225949112 Min: -0.0046707741951413344\n","Layer 0 - Gradient Weights Max: 0.007031952998900824 Min: -0.004885628216769476\n","ReLU Activation - Max: 1.1066568004438948 Min: 0.0\n","ReLU Activation - Max: 1.1094366313404778 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004467916027454353 Min: -0.004670818260628674\n","Layer 0 - Gradient Weights Max: 0.0070484580362741855 Min: -0.004870794452984233\n","ReLU Activation - Max: 1.1079519953608254 Min: 0.0\n","ReLU Activation - Max: 1.111284022509292 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004468143997435496 Min: -0.004670703676127924\n","Layer 0 - Gradient Weights Max: 0.0070423494648791845 Min: -0.004853461035141711\n","ReLU Activation - Max: 1.1092489732746742 Min: 0.0\n","ReLU Activation - Max: 1.113130454170638 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004468401509350689 Min: -0.004670613001351839\n","Layer 0 - Gradient Weights Max: 0.007000729280836288 Min: -0.00484756245415878\n","ReLU Activation - Max: 1.1105354630142978 Min: 0.0\n","ReLU Activation - Max: 1.1149884360483056 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004468456869965656 Min: -0.004670312566723041\n","Layer 0 - Gradient Weights Max: 0.006991391085442363 Min: -0.004850615806326107\n","ReLU Activation - Max: 1.1118217740681662 Min: 0.0\n","ReLU Activation - Max: 1.1168469213815768 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044685004139685775 Min: -0.0046699940584466835\n","Layer 0 - Gradient Weights Max: 0.00698099342254024 Min: -0.004848202089821036\n","ReLU Activation - Max: 1.1131164805398666 Min: 0.0\n","ReLU Activation - Max: 1.1186980584144037 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004468437382180542 Min: -0.004669557098764309\n","Layer 0 - Gradient Weights Max: 0.006944576148944386 Min: -0.004846413603650538\n","ReLU Activation - Max: 1.1144094975167018 Min: 0.0\n","ReLU Activation - Max: 1.120540853605301 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044682960496248685 Min: -0.004669033664241255\n","Layer 0 - Gradient Weights Max: 0.006943347980527732 Min: -0.004843899856814928\n","ReLU Activation - Max: 1.1157027715887224 Min: 0.0\n","ReLU Activation - Max: 1.1223868800071835 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004467976506724442 Min: -0.004668325007724241\n","Layer 0 - Gradient Weights Max: 0.006933334784568319 Min: -0.004817360437090485\n","ReLU Activation - Max: 1.1169981309486299 Min: 0.0\n","Epoch 880, Loss: 0.3794, Test Accuracy: 0.7229\n","ReLU Activation - Max: 1.1242340682317409 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004467659477536192 Min: -0.00466760083026517\n","Layer 0 - Gradient Weights Max: 0.006927386455660743 Min: -0.004824537485990027\n","ReLU Activation - Max: 1.118280301381078 Min: 0.0\n","ReLU Activation - Max: 1.126074736252555 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004467310576077146 Min: -0.00466684453770811\n","Layer 0 - Gradient Weights Max: 0.006919540119470712 Min: -0.004828744471550765\n","ReLU Activation - Max: 1.1195659632223451 Min: 0.0\n","ReLU Activation - Max: 1.1279136560363385 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004466897268202384 Min: -0.004666018497974445\n","Layer 0 - Gradient Weights Max: 0.00687419313692773 Min: -0.004818213672213274\n","ReLU Activation - Max: 1.1208482963622746 Min: 0.0\n","ReLU Activation - Max: 1.129755612985001 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004466313306496273 Min: -0.004665011251688467\n","Layer 0 - Gradient Weights Max: 0.00690753616791365 Min: -0.004797428911325061\n","ReLU Activation - Max: 1.122129256274821 Min: 0.0\n","ReLU Activation - Max: 1.1315986087054872 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00446569432610296 Min: -0.0046639586294223824\n","Layer 0 - Gradient Weights Max: 0.006942862762700048 Min: -0.004811182029003801\n","ReLU Activation - Max: 1.1234054053847842 Min: 0.0\n","ReLU Activation - Max: 1.133446552363415 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004465169234625908 Min: -0.0046630030377959415\n","Layer 0 - Gradient Weights Max: 0.006906930212900663 Min: -0.004817981406923947\n","ReLU Activation - Max: 1.1246895335478182 Min: 0.0\n","ReLU Activation - Max: 1.1352928782666796 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004464676451903427 Min: -0.004662065195723356\n","Layer 0 - Gradient Weights Max: 0.006880157773778807 Min: -0.00481272110416111\n","ReLU Activation - Max: 1.1259702376071092 Min: 0.0\n","ReLU Activation - Max: 1.1371370564751466 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004464176394755456 Min: -0.004661120934084566\n","Layer 0 - Gradient Weights Max: 0.006864152019291633 Min: -0.004800636883525007\n","ReLU Activation - Max: 1.127249062737291 Min: 0.0\n","ReLU Activation - Max: 1.1389751484584438 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004463620718286263 Min: -0.00466011025983973\n","Layer 0 - Gradient Weights Max: 0.006862957030862568 Min: -0.004791094369038729\n","ReLU Activation - Max: 1.1285278731951862 Min: 0.0\n","ReLU Activation - Max: 1.1408137258965647 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044629936646314585 Min: -0.004659033275110097\n","Layer 0 - Gradient Weights Max: 0.006828197421079445 Min: -0.004774819593922289\n","ReLU Activation - Max: 1.12980475233779 Min: 0.0\n","Epoch 890, Loss: 0.3787, Test Accuracy: 0.7236\n","ReLU Activation - Max: 1.1426491276509376 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004462229687777098 Min: -0.004657815949870347\n","Layer 0 - Gradient Weights Max: 0.006823321866804256 Min: -0.004799759530288836\n","ReLU Activation - Max: 1.1310663043940696 Min: 0.0\n","ReLU Activation - Max: 1.144479942280059 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004461329235833126 Min: -0.004656460298381903\n","Layer 0 - Gradient Weights Max: 0.006819256462273075 Min: -0.004789795894172519\n","ReLU Activation - Max: 1.1323279797691328 Min: 0.0\n","ReLU Activation - Max: 1.1463086511062501 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004460369036973246 Min: -0.004655039228781597\n","Layer 0 - Gradient Weights Max: 0.00682461126571686 Min: -0.0047708231682989845\n","ReLU Activation - Max: 1.13358558293499 Min: 0.0\n","ReLU Activation - Max: 1.1481408965055064 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004459473359427563 Min: -0.004653677435079321\n","Layer 0 - Gradient Weights Max: 0.006823613649888645 Min: -0.004787497199556296\n","ReLU Activation - Max: 1.134845451998501 Min: 0.0\n","ReLU Activation - Max: 1.1499721907835394 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044585751193908256 Min: -0.004652307770641488\n","Layer 0 - Gradient Weights Max: 0.006795936298709686 Min: -0.004766517417109953\n","ReLU Activation - Max: 1.1361081379440463 Min: 0.0\n","ReLU Activation - Max: 1.1517983445592614 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004457565359131794 Min: -0.004650812412263598\n","Layer 0 - Gradient Weights Max: 0.006799457670149723 Min: -0.004768207036982804\n","ReLU Activation - Max: 1.137364804062286 Min: 0.0\n","ReLU Activation - Max: 1.153628005614535 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004456404467889255 Min: -0.004649154772751302\n","Layer 0 - Gradient Weights Max: 0.006794928921738441 Min: -0.004782522813767606\n","ReLU Activation - Max: 1.1386254172089443 Min: 0.0\n","ReLU Activation - Max: 1.1554530916024437 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004455156997786735 Min: -0.004647408577292177\n","Layer 0 - Gradient Weights Max: 0.0067751209210441575 Min: -0.004775573128527654\n","ReLU Activation - Max: 1.13989048207674 Min: 0.0\n","ReLU Activation - Max: 1.1572747054221906 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004453809790152393 Min: -0.004645556998963401\n","Layer 0 - Gradient Weights Max: 0.006765957279251416 Min: -0.00477603600128143\n","ReLU Activation - Max: 1.1411574134802753 Min: 0.0\n","ReLU Activation - Max: 1.1590942054010926 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004452425241462627 Min: -0.004643663648723629\n","Layer 0 - Gradient Weights Max: 0.006752032744594944 Min: -0.004766219132501876\n","ReLU Activation - Max: 1.1424209922794282 Min: 0.0\n","Epoch 900, Loss: 0.3779, Test Accuracy: 0.7246\n","ReLU Activation - Max: 1.1609166977412817 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004450877832478751 Min: -0.004641593530781258\n","Layer 0 - Gradient Weights Max: 0.0067494329030381665 Min: -0.004751182156535027\n","ReLU Activation - Max: 1.1436831821751947 Min: 0.0\n","ReLU Activation - Max: 1.162738370733562 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004449393550395544 Min: -0.004639583295721749\n","Layer 0 - Gradient Weights Max: 0.006743102483053282 Min: -0.00474498286252501\n","ReLU Activation - Max: 1.144943851606213 Min: 0.0\n","ReLU Activation - Max: 1.1645579807401623 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004447985600870548 Min: -0.0046376487233569744\n","Layer 0 - Gradient Weights Max: 0.006692801322409777 Min: -0.004740259480064112\n","ReLU Activation - Max: 1.1461994868887684 Min: 0.0\n","ReLU Activation - Max: 1.1663680204887588 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044464905125299594 Min: -0.004635616481212863\n","Layer 0 - Gradient Weights Max: 0.006685251403347607 Min: -0.0047333354059982706\n","ReLU Activation - Max: 1.1474526641405938 Min: 0.0\n","ReLU Activation - Max: 1.1681794571459447 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004444999730054827 Min: -0.004633596085957994\n","Layer 0 - Gradient Weights Max: 0.00662527786793119 Min: -0.004731650603051161\n","ReLU Activation - Max: 1.1487042968421 Min: 0.0\n","ReLU Activation - Max: 1.1699927139719808 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044435861370945495 Min: -0.004631652070132106\n","Layer 0 - Gradient Weights Max: 0.006617479178430268 Min: -0.00474940952209022\n","ReLU Activation - Max: 1.1499449446429255 Min: 0.0\n","ReLU Activation - Max: 1.1718056522173605 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004442205108889519 Min: -0.0046297413300610876\n","Layer 0 - Gradient Weights Max: 0.0066171707607168145 Min: -0.004750283222067735\n","ReLU Activation - Max: 1.1511854329230697 Min: 0.0\n","ReLU Activation - Max: 1.1736178272037192 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004440844509565726 Min: -0.004627851587310313\n","Layer 0 - Gradient Weights Max: 0.0065885604875809065 Min: -0.004727426004144105\n","ReLU Activation - Max: 1.1524284444951631 Min: 0.0\n","ReLU Activation - Max: 1.175424992462449 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004439351132336277 Min: -0.004625809385411036\n","Layer 0 - Gradient Weights Max: 0.006590253123614667 Min: -0.004731524229615406\n","ReLU Activation - Max: 1.1536691435859296 Min: 0.0\n","ReLU Activation - Max: 1.1772351473902476 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004437916023579671 Min: -0.004623816068304795\n","Layer 0 - Gradient Weights Max: 0.006565773170541847 Min: -0.004712268116140003\n","ReLU Activation - Max: 1.1549071594514968 Min: 0.0\n","Epoch 910, Loss: 0.3772, Test Accuracy: 0.7275\n","ReLU Activation - Max: 1.1790427670890107 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044365544155751105 Min: -0.004621885303416522\n","Layer 0 - Gradient Weights Max: 0.006538752609041929 Min: -0.004711368339453153\n","ReLU Activation - Max: 1.1561469546072531 Min: 0.0\n","ReLU Activation - Max: 1.1808455029959002 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004435175122420911 Min: -0.004619927447922193\n","Layer 0 - Gradient Weights Max: 0.006532216082694983 Min: -0.004708102189219033\n","ReLU Activation - Max: 1.1573885535762367 Min: 0.0\n","ReLU Activation - Max: 1.1826445168075266 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044336546581791566 Min: -0.004617823084750411\n","Layer 0 - Gradient Weights Max: 0.006513144047658264 Min: -0.0047128542875697526\n","ReLU Activation - Max: 1.1586264411308431 Min: 0.0\n","ReLU Activation - Max: 1.18443664126476 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004432158172837447 Min: -0.004615732844358471\n","Layer 0 - Gradient Weights Max: 0.006509776918533609 Min: -0.004697224948522154\n","ReLU Activation - Max: 1.1598683698757133 Min: 0.0\n","ReLU Activation - Max: 1.1862251634045584 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004430470622180155 Min: -0.004613439934973166\n","Layer 0 - Gradient Weights Max: 0.006494258936189047 Min: -0.004705970165941641\n","ReLU Activation - Max: 1.1611073531773068 Min: 0.0\n","ReLU Activation - Max: 1.1880096362036427 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004428642896959619 Min: -0.004611000416243385\n","Layer 0 - Gradient Weights Max: 0.006499144459053805 Min: -0.004682383354496616\n","ReLU Activation - Max: 1.1623471769803635 Min: 0.0\n","ReLU Activation - Max: 1.1897929549416364 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004426776268437029 Min: -0.004608520385791104\n","Layer 0 - Gradient Weights Max: 0.006484321820154688 Min: -0.004683587348502725\n","ReLU Activation - Max: 1.1635779246050877 Min: 0.0\n","ReLU Activation - Max: 1.191573214978796 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004424836568020485 Min: -0.0046059621551777594\n","Layer 0 - Gradient Weights Max: 0.006481488835963295 Min: -0.004676186725547527\n","ReLU Activation - Max: 1.164812338474091 Min: 0.0\n","ReLU Activation - Max: 1.1933491538111332 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044229234558390805 Min: -0.004603422137585406\n","Layer 0 - Gradient Weights Max: 0.006471878127585138 Min: -0.00469806626560647\n","ReLU Activation - Max: 1.1660479417683958 Min: 0.0\n","ReLU Activation - Max: 1.1951221792373756 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004420990402325583 Min: -0.004600860742572697\n","Layer 0 - Gradient Weights Max: 0.0064622012910158805 Min: -0.004699737980274486\n","ReLU Activation - Max: 1.1672764407401974 Min: 0.0\n","Epoch 920, Loss: 0.3764, Test Accuracy: 0.7268\n","ReLU Activation - Max: 1.1968975342372663 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004418971291528082 Min: -0.004598207218741518\n","Layer 0 - Gradient Weights Max: 0.006463233010621624 Min: -0.004699321005274972\n","ReLU Activation - Max: 1.168501390582541 Min: 0.0\n","ReLU Activation - Max: 1.1986729956701085 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044170538038553475 Min: -0.004595641367680206\n","Layer 0 - Gradient Weights Max: 0.0063843614255103985 Min: -0.0047010202957606315\n","ReLU Activation - Max: 1.1697315059255824 Min: 0.0\n","ReLU Activation - Max: 1.2004380388880387 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044148440590704476 Min: -0.0045927826296972\n","Layer 0 - Gradient Weights Max: 0.006351630341587685 Min: -0.004689143036752515\n","ReLU Activation - Max: 1.170960641648365 Min: 0.0\n","ReLU Activation - Max: 1.2021988180562957 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044124496400881 Min: -0.004589735184788471\n","Layer 0 - Gradient Weights Max: 0.006336848916067024 Min: -0.004680057812011271\n","ReLU Activation - Max: 1.1721920885482546 Min: 0.0\n","ReLU Activation - Max: 1.2039587057755188 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004410003177141024 Min: -0.0045866308428757635\n","Layer 0 - Gradient Weights Max: 0.006336469728054397 Min: -0.004677134793057389\n","ReLU Activation - Max: 1.1734228108332552 Min: 0.0\n","ReLU Activation - Max: 1.20571211429715 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004407582399046088 Min: -0.004583540825748224\n","Layer 0 - Gradient Weights Max: 0.006332629250757175 Min: -0.004681674122863793\n","ReLU Activation - Max: 1.1746512050791944 Min: 0.0\n","ReLU Activation - Max: 1.2074644067982974 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0044051955491848475 Min: -0.004580483743094851\n","Layer 0 - Gradient Weights Max: 0.0063767617653202715 Min: -0.004700674019869197\n","ReLU Activation - Max: 1.1758863163472717 Min: 0.0\n","ReLU Activation - Max: 1.2092283313951497 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004402799880600207 Min: -0.004577414879430725\n","Layer 0 - Gradient Weights Max: 0.006375259642895333 Min: -0.00468180534799828\n","ReLU Activation - Max: 1.1771135817734253 Min: 0.0\n","ReLU Activation - Max: 1.2109874384332493 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00440038147863518 Min: -0.004574312988613608\n","Layer 0 - Gradient Weights Max: 0.006377117115460307 Min: -0.004667612981234167\n","ReLU Activation - Max: 1.1783399624787463 Min: 0.0\n","ReLU Activation - Max: 1.212745434641129 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004397936010160717 Min: -0.004571180333621464\n","Layer 0 - Gradient Weights Max: 0.006381801295763097 Min: -0.004682410791985641\n","ReLU Activation - Max: 1.1795677618477838 Min: 0.0\n","Epoch 930, Loss: 0.3756, Test Accuracy: 0.7271\n","ReLU Activation - Max: 1.214510221322207 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004395454290422307 Min: -0.004568006739290819\n","Layer 0 - Gradient Weights Max: 0.006371345459082488 Min: -0.004675092295216385\n","ReLU Activation - Max: 1.1807938027467064 Min: 0.0\n","ReLU Activation - Max: 1.216271410682583 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00439295482667917 Min: -0.004564818954831965\n","Layer 0 - Gradient Weights Max: 0.0063714475434369635 Min: -0.004671487157695399\n","ReLU Activation - Max: 1.1820191714007497 Min: 0.0\n","ReLU Activation - Max: 1.218029139360629 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004390355633342134 Min: -0.004561542479246737\n","Layer 0 - Gradient Weights Max: 0.006378520043999857 Min: -0.0046797353479685585\n","ReLU Activation - Max: 1.1832500066654201 Min: 0.0\n","ReLU Activation - Max: 1.2197827393267124 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004387837317746954 Min: -0.00455835151766336\n","Layer 0 - Gradient Weights Max: 0.006306845933989313 Min: -0.0046919124279418255\n","ReLU Activation - Max: 1.1844791764610063 Min: 0.0\n","ReLU Activation - Max: 1.2215286559048752 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004385336200239819 Min: -0.004555176940243916\n","Layer 0 - Gradient Weights Max: 0.006291756102894697 Min: -0.004683861625077682\n","ReLU Activation - Max: 1.1857147475126213 Min: 0.0\n","ReLU Activation - Max: 1.2232675957575616 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043827619715564895 Min: -0.004551927994793537\n","Layer 0 - Gradient Weights Max: 0.006275961992823081 Min: -0.004677405075602896\n","ReLU Activation - Max: 1.186950617864597 Min: 0.0\n","ReLU Activation - Max: 1.2250036423581416 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004380338084521302 Min: -0.004548830981818949\n","Layer 0 - Gradient Weights Max: 0.006273484062035959 Min: -0.004687410230660658\n","ReLU Activation - Max: 1.1881877142953925 Min: 0.0\n","ReLU Activation - Max: 1.226739458848276 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004377953445799008 Min: -0.0045457751797275785\n","Layer 0 - Gradient Weights Max: 0.00625835630197153 Min: -0.004657712755345826\n","ReLU Activation - Max: 1.1894127843976325 Min: 0.0\n","ReLU Activation - Max: 1.2284753148937442 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004375516125331282 Min: -0.00454266319187956\n","Layer 0 - Gradient Weights Max: 0.006230406730780772 Min: -0.004667666253391031\n","ReLU Activation - Max: 1.1906411873201277 Min: 0.0\n","ReLU Activation - Max: 1.2302100231371944 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004373008809197398 Min: -0.004539477633221521\n","Layer 0 - Gradient Weights Max: 0.006187616096344665 Min: -0.004665453455307759\n","ReLU Activation - Max: 1.1918745460792433 Min: 0.0\n","Epoch 940, Loss: 0.3749, Test Accuracy: 0.7279\n","ReLU Activation - Max: 1.2319382058120718 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004370397342888861 Min: -0.0045361806211262215\n","Layer 0 - Gradient Weights Max: 0.006161436587056929 Min: -0.0046625975343799\n","ReLU Activation - Max: 1.1931070770638845 Min: 0.0\n","ReLU Activation - Max: 1.2336620061770225 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004367808581745406 Min: -0.00453290227894522\n","Layer 0 - Gradient Weights Max: 0.0061610338286312875 Min: -0.004710374708917689\n","ReLU Activation - Max: 1.1943359526431427 Min: 0.0\n","ReLU Activation - Max: 1.2353853363353233 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004365233925232575 Min: -0.004529642691836456\n","Layer 0 - Gradient Weights Max: 0.006148746834651213 Min: -0.004708996932017231\n","ReLU Activation - Max: 1.1955645686503455 Min: 0.0\n","ReLU Activation - Max: 1.2371059383179495 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004362619692197154 Min: -0.004526335938409353\n","Layer 0 - Gradient Weights Max: 0.0061422921717810336 Min: -0.004708244371814868\n","ReLU Activation - Max: 1.1967794605592053 Min: 0.0\n","ReLU Activation - Max: 1.2388235965282246 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043598660716204635 Min: -0.004522888944388857\n","Layer 0 - Gradient Weights Max: 0.006172890335025711 Min: -0.0047075878204650866\n","ReLU Activation - Max: 1.197983546311716 Min: 0.0\n","ReLU Activation - Max: 1.2405380028735702 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004356993977821349 Min: -0.004519328326361048\n","Layer 0 - Gradient Weights Max: 0.006174055749996989 Min: -0.004706817309819831\n","ReLU Activation - Max: 1.1991826625891493 Min: 0.0\n","ReLU Activation - Max: 1.242247726674345 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004354157849482087 Min: -0.004515791244922408\n","Layer 0 - Gradient Weights Max: 0.0061600132469296276 Min: -0.004686704887394493\n","ReLU Activation - Max: 1.20038326773557 Min: 0.0\n","ReLU Activation - Max: 1.2439539930916497 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004351237271062794 Min: -0.0045121590661848735\n","Layer 0 - Gradient Weights Max: 0.006185654192004132 Min: -0.004686531278409676\n","ReLU Activation - Max: 1.2015823872029678 Min: 0.0\n","ReLU Activation - Max: 1.245654058183247 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004348290846955275 Min: -0.004508498511960025\n","Layer 0 - Gradient Weights Max: 0.006156147381124233 Min: -0.004690463640596312\n","ReLU Activation - Max: 1.2027832773491014 Min: 0.0\n","ReLU Activation - Max: 1.2473493381937104 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004345228958431537 Min: -0.004504718639355112\n","Layer 0 - Gradient Weights Max: 0.006144903015379802 Min: -0.00468191559451187\n","ReLU Activation - Max: 1.2039830545032266 Min: 0.0\n","Epoch 950, Loss: 0.3741, Test Accuracy: 0.7282\n","ReLU Activation - Max: 1.24904312395256 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004342203631686688 Min: -0.00450097460964023\n","Layer 0 - Gradient Weights Max: 0.006152477771434057 Min: -0.004661036021151543\n","ReLU Activation - Max: 1.2051879562269918 Min: 0.0\n","ReLU Activation - Max: 1.2507272634951845 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004338965369178055 Min: -0.004497003724401833\n","Layer 0 - Gradient Weights Max: 0.006151739952924282 Min: -0.004650923018432872\n","ReLU Activation - Max: 1.2063866319255274 Min: 0.0\n","ReLU Activation - Max: 1.2524098574850935 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004335712000462779 Min: -0.004493021176225597\n","Layer 0 - Gradient Weights Max: 0.00615076918786142 Min: -0.004626010293370388\n","ReLU Activation - Max: 1.2075825854597666 Min: 0.0\n","ReLU Activation - Max: 1.2540912371791573 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004332371002967673 Min: -0.004488939184193298\n","Layer 0 - Gradient Weights Max: 0.006146980903735332 Min: -0.004630700704222332\n","ReLU Activation - Max: 1.2087786444948507 Min: 0.0\n","ReLU Activation - Max: 1.255769878761956 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004329040503249116 Min: -0.004484857917095489\n","Layer 0 - Gradient Weights Max: 0.0061178517493623895 Min: -0.00465563170693341\n","ReLU Activation - Max: 1.2099718762295948 Min: 0.0\n","ReLU Activation - Max: 1.2574500852609631 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004325639947425348 Min: -0.0044807021638497155\n","Layer 0 - Gradient Weights Max: 0.006109423332691778 Min: -0.004650960662531868\n","ReLU Activation - Max: 1.2111521721101794 Min: 0.0\n","ReLU Activation - Max: 1.259128426397652 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004322231369982186 Min: -0.004476528325757742\n","Layer 0 - Gradient Weights Max: 0.006079257998273727 Min: -0.00463430060851642\n","ReLU Activation - Max: 1.212321698012207 Min: 0.0\n","ReLU Activation - Max: 1.260804675280834 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00431870764700489 Min: -0.004472230529082357\n","Layer 0 - Gradient Weights Max: 0.006093324687295603 Min: -0.004629077897125915\n","ReLU Activation - Max: 1.2134846638520749 Min: 0.0\n","ReLU Activation - Max: 1.2624722247576132 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004315129875409892 Min: -0.0044678831428811554\n","Layer 0 - Gradient Weights Max: 0.006101116386807303 Min: -0.004615231091970745\n","ReLU Activation - Max: 1.2146479343836047 Min: 0.0\n","ReLU Activation - Max: 1.2641331516923517 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004311521705267815 Min: -0.004463500407094041\n","Layer 0 - Gradient Weights Max: 0.006102142592896954 Min: -0.00459221868009018\n","ReLU Activation - Max: 1.2158151947071296 Min: 0.0\n","Epoch 960, Loss: 0.3734, Test Accuracy: 0.7282\n","ReLU Activation - Max: 1.2657913377518657 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004307873959646768 Min: -0.0044590818981906375\n","Layer 0 - Gradient Weights Max: 0.006083839816888373 Min: -0.004585495994011018\n","ReLU Activation - Max: 1.216978786030521 Min: 0.0\n","ReLU Activation - Max: 1.267445296212184 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004304110745866725 Min: -0.00445454931231261\n","Layer 0 - Gradient Weights Max: 0.006104714492810552 Min: -0.004570405911314018\n","ReLU Activation - Max: 1.2181355976375325 Min: 0.0\n","ReLU Activation - Max: 1.2690917948667664 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0043002395018968565 Min: -0.004449895608740157\n","Layer 0 - Gradient Weights Max: 0.006120323317852125 Min: -0.004533878094396288\n","ReLU Activation - Max: 1.2192909615575414 Min: 0.0\n","ReLU Activation - Max: 1.2707328887049052 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004296269333456502 Min: -0.004445125582358348\n","Layer 0 - Gradient Weights Max: 0.006127031935589562 Min: -0.004552159571526072\n","ReLU Activation - Max: 1.2204428144020476 Min: 0.0\n","ReLU Activation - Max: 1.2723682704672752 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004292390753330453 Min: -0.004440453809957535\n","Layer 0 - Gradient Weights Max: 0.006151719846213036 Min: -0.004540107135041499\n","ReLU Activation - Max: 1.2215911358860236 Min: 0.0\n","ReLU Activation - Max: 1.2740060280191075 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004288620020850297 Min: -0.0044358877871989435\n","Layer 0 - Gradient Weights Max: 0.006152467476566115 Min: -0.004522582579233337\n","ReLU Activation - Max: 1.2227337271846253 Min: 0.0\n","ReLU Activation - Max: 1.2756406570966417 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004284790843038612 Min: -0.004431255847680717\n","Layer 0 - Gradient Weights Max: 0.0061275785854341324 Min: -0.004523651769590458\n","ReLU Activation - Max: 1.2238717611096102 Min: 0.0\n","ReLU Activation - Max: 1.277278083927319 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004281046041379767 Min: -0.004426711350703529\n","Layer 0 - Gradient Weights Max: 0.006116439604017972 Min: -0.0045174188703281494\n","ReLU Activation - Max: 1.2250096445040317 Min: 0.0\n","ReLU Activation - Max: 1.278913594121635 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004277436143655553 Min: -0.004422299771817071\n","Layer 0 - Gradient Weights Max: 0.006139154489661092 Min: -0.004513086975978367\n","ReLU Activation - Max: 1.2261456795441 Min: 0.0\n","ReLU Activation - Max: 1.2805429950250857 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004273857755211343 Min: -0.004417924942603156\n","Layer 0 - Gradient Weights Max: 0.006154537242429543 Min: -0.0044989060727773636\n","ReLU Activation - Max: 1.227283016881434 Min: 0.0\n","Epoch 970, Loss: 0.3727, Test Accuracy: 0.7279\n","ReLU Activation - Max: 1.2821698421067498 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00427018688959432 Min: -0.004413453471800676\n","Layer 0 - Gradient Weights Max: 0.006172971544165363 Min: -0.004500351835374033\n","ReLU Activation - Max: 1.2284190147308973 Min: 0.0\n","ReLU Activation - Max: 1.2837929947374036 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004266529401672313 Min: -0.004408994075314822\n","Layer 0 - Gradient Weights Max: 0.006153896001320092 Min: -0.004486714550472506\n","ReLU Activation - Max: 1.229553776096731 Min: 0.0\n","ReLU Activation - Max: 1.2854190330736963 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004262810790163267 Min: -0.004404465483383327\n","Layer 0 - Gradient Weights Max: 0.00615469476282406 Min: -0.004478893600905983\n","ReLU Activation - Max: 1.230695990576531 Min: 0.0\n","ReLU Activation - Max: 1.2870348859368694 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004259048511450957 Min: -0.004399895589815011\n","Layer 0 - Gradient Weights Max: 0.0061706607491567485 Min: -0.0044696755325382255\n","ReLU Activation - Max: 1.2318407772513829 Min: 0.0\n","ReLU Activation - Max: 1.2886491585574593 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004255248055706655 Min: -0.00439528945672548\n","Layer 0 - Gradient Weights Max: 0.006181492107010911 Min: -0.004463997228205057\n","ReLU Activation - Max: 1.2329883356142293 Min: 0.0\n","ReLU Activation - Max: 1.2902615022146382 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0042514337223078765 Min: -0.0043906707507072856\n","Layer 0 - Gradient Weights Max: 0.006190204532074169 Min: -0.004464500475043865\n","ReLU Activation - Max: 1.2341345432746818 Min: 0.0\n","ReLU Activation - Max: 1.29187540120511 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0042475353053374205 Min: -0.0043859679096103065\n","Layer 0 - Gradient Weights Max: 0.0061828420146505395 Min: -0.004453571648812628\n","ReLU Activation - Max: 1.2352885815056391 Min: 0.0\n","ReLU Activation - Max: 1.2934891712832992 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00424351050278627 Min: -0.0043811371881051645\n","Layer 0 - Gradient Weights Max: 0.006163441969894304 Min: -0.0044391548257901765\n","ReLU Activation - Max: 1.2364416668585916 Min: 0.0\n","ReLU Activation - Max: 1.2950993555441153 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004239352071818021 Min: -0.004376163964469688\n","Layer 0 - Gradient Weights Max: 0.006153366979522108 Min: -0.004437148154639228\n","ReLU Activation - Max: 1.2375875422940625 Min: 0.0\n","ReLU Activation - Max: 1.296718912591313 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004234983586644918 Min: -0.004370972429537134\n","Layer 0 - Gradient Weights Max: 0.006140178786132539 Min: -0.004434908729163788\n","ReLU Activation - Max: 1.2387315491805322 Min: 0.0\n","Epoch 980, Loss: 0.3719, Test Accuracy: 0.7279\n","ReLU Activation - Max: 1.2983332312755378 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004230673488380129 Min: -0.004365826588399798\n","Layer 0 - Gradient Weights Max: 0.006154548066510223 Min: -0.004425064452786091\n","ReLU Activation - Max: 1.2398791225675885 Min: 0.0\n","ReLU Activation - Max: 1.299938828643621 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004226319433027778 Min: -0.004360640018785119\n","Layer 0 - Gradient Weights Max: 0.006147909276984947 Min: -0.004410132962212914\n","ReLU Activation - Max: 1.2410253728869216 Min: 0.0\n","ReLU Activation - Max: 1.3015391413695987 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0042219693697777266 Min: -0.004355452237837362\n","Layer 0 - Gradient Weights Max: 0.006132706252732506 Min: -0.004397031024289854\n","ReLU Activation - Max: 1.2421708926489288 Min: 0.0\n","ReLU Activation - Max: 1.3031318169702824 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00421761652100469 Min: -0.004350256760197798\n","Layer 0 - Gradient Weights Max: 0.006119501130153491 Min: -0.004410390386734981\n","ReLU Activation - Max: 1.2433066847172067 Min: 0.0\n","ReLU Activation - Max: 1.3047235510071484 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.00421331775131302 Min: -0.0043451225551727635\n","Layer 0 - Gradient Weights Max: 0.006109092039653862 Min: -0.004412089584653366\n","ReLU Activation - Max: 1.2444423546370658 Min: 0.0\n","ReLU Activation - Max: 1.306311581425678 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004208954094002793 Min: -0.00433991146053863\n","Layer 0 - Gradient Weights Max: 0.006097425267735021 Min: -0.004414051727079117\n","ReLU Activation - Max: 1.2455824703050549 Min: 0.0\n","ReLU Activation - Max: 1.3078946530468676 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004204468928643695 Min: -0.004334581011539854\n","Layer 0 - Gradient Weights Max: 0.006084881653192971 Min: -0.004403719476467889\n","ReLU Activation - Max: 1.246721141929274 Min: 0.0\n","ReLU Activation - Max: 1.3094766092363552 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004199970963725687 Min: -0.004329243494480387\n","Layer 0 - Gradient Weights Max: 0.006089093231703729 Min: -0.004401931457434974\n","ReLU Activation - Max: 1.2478547090353942 Min: 0.0\n","ReLU Activation - Max: 1.3110542787824424 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004195494610400367 Min: -0.004323923912869692\n","Layer 0 - Gradient Weights Max: 0.00609992346452628 Min: -0.004389149722525259\n","ReLU Activation - Max: 1.2489868906682557 Min: 0.0\n","ReLU Activation - Max: 1.3126369673200056 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004191007543146133 Min: -0.004318597005691794\n","Layer 0 - Gradient Weights Max: 0.006108796595371795 Min: -0.004372779971501039\n","ReLU Activation - Max: 1.2501190933242012 Min: 0.0\n","Epoch 990, Loss: 0.3712, Test Accuracy: 0.7279\n","ReLU Activation - Max: 1.3142180882973196 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0041864804849921 Min: -0.004313234888540938\n","Layer 0 - Gradient Weights Max: 0.006102491621635215 Min: -0.004363011967533134\n","ReLU Activation - Max: 1.2512503069398622 Min: 0.0\n","ReLU Activation - Max: 1.3158022897736996 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004181885369896407 Min: -0.004307800879183697\n","Layer 0 - Gradient Weights Max: 0.006107365910129693 Min: -0.004329139103272509\n","ReLU Activation - Max: 1.2523883106502556 Min: 0.0\n","ReLU Activation - Max: 1.3173876770069282 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004177102818782625 Min: -0.00430218036915167\n","Layer 0 - Gradient Weights Max: 0.006104835245316668 Min: -0.0043392275662015945\n","ReLU Activation - Max: 1.2535177761408842 Min: 0.0\n","ReLU Activation - Max: 1.3189698545683124 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004172125688400462 Min: -0.004296367551437924\n","Layer 0 - Gradient Weights Max: 0.006096555689428402 Min: -0.004320702705173623\n","ReLU Activation - Max: 1.254646694806523 Min: 0.0\n","ReLU Activation - Max: 1.3205478945532885 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004167178776739896 Min: -0.0042905834963165995\n","Layer 0 - Gradient Weights Max: 0.006101164457027258 Min: -0.004321511309673162\n","ReLU Activation - Max: 1.2557743708551776 Min: 0.0\n","ReLU Activation - Max: 1.322123718961397 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.0041622855369349504 Min: -0.004284858733469168\n","Layer 0 - Gradient Weights Max: 0.006099888730254153 Min: -0.004312978997377544\n","ReLU Activation - Max: 1.256899525369776 Min: 0.0\n","ReLU Activation - Max: 1.3236982778851059 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004157466596143555 Min: -0.00427919826272009\n","Layer 0 - Gradient Weights Max: 0.0060931321592577435 Min: -0.004293773499880388\n","ReLU Activation - Max: 1.258027186964782 Min: 0.0\n","ReLU Activation - Max: 1.325268792222837 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004152625996081938 Min: -0.004273516548288312\n","Layer 0 - Gradient Weights Max: 0.006102953854274465 Min: -0.004298967124832907\n","ReLU Activation - Max: 1.259152612108594 Min: 0.0\n","ReLU Activation - Max: 1.3268324586533198 Min: 0.0\n","Layer 1 - Gradient Weights Max: 0.004147823276330832 Min: -0.004267871664707613\n","Layer 0 - Gradient Weights Max: 0.00610903272653206 Min: -0.004299236382744605\n","ReLU Activation - Max: 1.2602784028295646 Min: 0.0\n"]}],"source":["def train_mse(model, X_train, y_train, X_test, y_test, epochs=100):\n","    training_losses = []\n","    test_accuracies = []\n","\n","    for epoch in range(epochs):\n","        # Forward pass\n","        predictions, _ = model.forward_pass(X_train)\n","\n","        # Compute loss using MSE\n","        loss = model.compute_loss(predictions, y_train)\n","        training_losses.append(loss)\n","\n","        # Backpropagation to update weights\n","        model.backpropagate(X_train, y_train, _)\n","\n","        # Compute accuracy on the test set\n","        test_predictions, _ = model.forward_pass(X_test)\n","        test_predicted_classes = np.argmax(test_predictions, axis=1)\n","        y_test_classes = np.argmax(y_test, axis=1)\n","        test_accuracy = np.mean(test_predicted_classes == y_test_classes)\n","        test_accuracies.append(test_accuracy)\n","\n","        # Print progress\n","        if epoch % 10 == 0:\n","            print(f\"Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","\n","    return training_losses, test_accuracies\n","\n","training_losses_mse, test_accuracies_mse = train_mse(\n","    model=model_mse,\n","    X_train=X_train_normalized,\n","    y_train=y_train_one_hot,\n","    X_test=X_test_normalized,\n","    y_test=y_test_one_hot,\n","    epochs=1000\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":2010,"status":"ok","timestamp":1711357790409,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"PAtp3BEI32cr","outputId":"97c2667a-f122-4095-edd4-2082064b420a"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1400x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(14, 5))\n","\n","# Plot training loss\n","plt.subplot(1, 2, 1)\n","plt.plot(training_losses_mse, label='Training Loss')\n","plt.title('Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('MSE Loss')\n","plt.legend()\n","\n","# Plot test accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(test_accuracies_mse, label='Test Accuracy', color='orange')\n","plt.title('Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1711358145831,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"R6CSOxGx40_t","outputId":"b9ecb187-440d-424a-94c7-67c537c4084d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","[[1052  403]\n"," [ 372  973]]\n"]}],"source":["# Function to compute the confusion matrix\n","def compute_confusion_matrix(true, preds, num_classes):\n","    cm = np.zeros((num_classes, num_classes), dtype=int)\n","    for true_label, predicted_label in zip(true, preds):\n","        cm[true_label, predicted_label] += 1\n","    return cm\n","\n","# `y_test_one_hot` is your ground truth in one-hot format\n","y_test_classes = np.argmax(y_test_one_hot, axis=1)  # Convert from one-hot to class indices\n","\n","# Check if `test_predictions` needs conversion from scores to class indices\n","if test_predictions.ndim > 1 and test_predictions.shape[1] > 1:\n","    # If `test_predictions` is 2D with shape [num_samples, num_classes], convert scores to class indices\n","    test_predicted_classes = np.argmax(test_predictions, axis=1)\n","else:\n","    # If `test_predictions` is already class indices, use it directly\n","    test_predicted_classes = test_predictions\n","\n","# number of classes can be derived from `y_test_one_hot`\n","num_classes = y_test_one_hot.shape[1]\n","\n","# Compute the confusion matrix\n","confusion_mat = compute_confusion_matrix(y_test_classes, test_predicted_classes, num_classes)\n","\n","# Print the confusion matrix\n","print(\"Confusion Matrix:\")\n","print(confusion_mat)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1711358148440,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"1uav-rCG48lX","outputId":"ee730f8f-bb35-4b62-e880-0f0540668981"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class 0: F1 Score = 0.7308\n","Class 1: F1 Score = 0.7152\n"]}],"source":["def calculate_classwise_f1(confusion_mat):\n","    \"\"\"\n","    Calculates class-wise F1 scores from the confusion matrix.\n","\n","    Parameters:\n","    - confusion_mat: The confusion matrix as a 2D Numpy array.\n","\n","    Returns:\n","    - f1_scores: A list containing the F1 score for each class.\n","    \"\"\"\n","    num_classes = confusion_mat.shape[0]\n","    f1_scores = []\n","\n","    for i in range(num_classes):\n","        TP = confusion_mat[i, i]\n","        FP = np.sum(confusion_mat[:, i]) - TP\n","        FN = np.sum(confusion_mat[i, :]) - TP\n","        TN = np.sum(confusion_mat) - (TP + FP + FN)\n","\n","        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","        f1_scores.append(f1_score)\n","\n","    return f1_scores\n","\n","# 'confusion_mat' is previously computed confusion matrix\n","f1_scores = calculate_classwise_f1(confusion_mat)\n","\n","# Print the F1 scores for each class\n","for i, score in enumerate(f1_scores):\n","    print(f\"Class {i}: F1 Score = {score:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"tHMv5FvW5ubd"},"source":["## **3.1.2 - MLP - Multiclass classification**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4660,"status":"ok","timestamp":1711358906815,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"CUoVWS1E770g","outputId":"034cbc69-b91b-45a5-ffc7-2e89da7cf7c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Features shape: (70000, 25)\n","Labels shape: (70000,)\n"]}],"source":["import numpy as np\n","\n","# Define the path to your file\n","file_path = '/content/drive/MyDrive/PRNN Assignment 2/multi_class_classification_data_group_25_train2.txt'\n","\n","# Load the text file\n","with open(file_path, 'r') as file:\n","    # Read the lines in the file\n","    lines = file.readlines()\n","\n","# Process the lines to split them into features and labels\n","data = np.array([line.strip().split('\\t') for line in lines[1:]])  # Skip header and split each line\n","\n","# Separate features and labels\n","X = data[:, :-1].astype(float)  # Convert features to float, corrected from np.float to float\n","y = data[:, -1].astype(int)  # Convert labels to int, corrected from np.int to int\n","\n","# Check the shapes of features and labels\n","print(\"Features shape:\", X.shape)\n","print(\"Labels shape:\", y.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":575,"status":"ok","timestamp":1711358915782,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"3WbeucVr59YR","outputId":"7e1b07d0-70ee-42a0-e496-77202eecaab1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: (56000, 25) (56000,)\n","Testing set size: (14000, 25) (14000,)\n"]}],"source":["# Shuffle the data\n","np.random.seed(42)  # For reproducibility\n","shuffled_indices = np.random.permutation(len(X))\n","X_shuffled = X[shuffled_indices]\n","y_shuffled = y[shuffled_indices]\n","\n","# Define the size of the training set (e.g., 80% of the dataset)\n","train_size = int(0.8 * len(X))\n","\n","# Split the data\n","X_train, X_test = X_shuffled[:train_size], X_shuffled[train_size:]\n","y_train, y_test = y_shuffled[:train_size], y_shuffled[train_size:]\n","\n","# Verify the size of the splits\n","print(\"Training set size:\", X_train.shape, y_train.shape)\n","print(\"Testing set size:\", X_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6z6S0aBq8jrD"},"outputs":[],"source":["# Assuming y_train and y_test are your label vectors\n","\n","# One-hot encode labels\n","def one_hot_encode(labels, num_classes=10):\n","    one_hot_labels = np.zeros((labels.size, num_classes))\n","    one_hot_labels[np.arange(labels.size), labels] = 1\n","    return one_hot_labels\n","\n","y_train_encoded = one_hot_encode(y_train)\n","y_test_encoded = one_hot_encode(y_test)\n","\n","# Normalize features\n","X_train_normalized = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n","X_test_normalized = (X_test - X_test.mean(axis=0)) / X_test.std(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1711359138198,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"OKeiL7iU9Axw","outputId":"082fce5a-5548-4914-fe2e-61ade78e0747"},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU Activation - Max: 0.1795178584339259 Min: 0.0\n","ReLU Activation - Max: 0.010229069903401547 Min: 0.0\n","Softmax Output - Max: 0.10004470231803755 Min: 0.09996439532580803 Sum (first example): 1.0\n","Initial loss: 2.3025687083603747\n"]}],"source":["# Neural network architecture parameters\n","input_size = X_train_normalized.shape[1]  # Number of features\n","output_size = y_train_encoded.shape[1]  # Number of classes (10 if you have labels 0-9)\n","hidden_layers = [64, 32]  # two hidden layers with 64 and 32 neurons\n","loss_function = 'crossentropy'  # Using cross-entropy loss for classification\n","learning_rate = 0.01  # Example learning rate\n","\n","# Initialize the neural network\n","nn_model = FullyConnectedNeuralNetwork(input_size=input_size,\n","                                        output_size=output_size,\n","                                        hidden_layers=hidden_layers,\n","                                        loss_function=loss_function,\n","                                        learning_rate=learning_rate)\n","\n","# Perform a forward pass with a batch of your training data\n","output, activations = nn_model.forward_pass(X_train_normalized[:32])  # Example with a batch of 32\n","\n","# Compute the loss for this batch\n","loss = nn_model.compute_loss(output, y_train_encoded[:32])\n","\n","print(\"Initial loss:\", loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhEprqH-9McA"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def train_model(model, X_train, y_train, X_test, y_test, epochs, batch_size):\n","    m = X_train.shape[0]\n","    n_batches = m // batch_size\n","\n","    training_losses = []\n","    test_accuracies = []\n","\n","    for epoch in range(epochs):\n","        epoch_losses = []\n","        for i in range(n_batches):\n","            batch_start, batch_end = i * batch_size, (i + 1) * batch_size\n","            X_batch, y_batch = X_train[batch_start:batch_end], y_train[batch_start:batch_end]\n","\n","            # Forward pass\n","            output, activations = model.forward_pass(X_batch)\n","\n","            # Compute loss\n","            loss = model.compute_loss(output, y_batch)\n","            epoch_losses.append(loss)\n","\n","            # Backpropagation and weight update\n","            model.backpropagate(X_batch, y_batch, activations)\n","\n","        # Store the average loss for this epoch\n","        training_losses.append(np.mean(epoch_losses))\n","\n","        # Evaluate the model on the test set after each epoch\n","        test_accuracy = evaluate_model(model, X_test, y_test)\n","        test_accuracies.append(test_accuracy)\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss: {training_losses[-1]:.4f} - Test Accuracy: {test_accuracy:.4f}\")\n","\n","    return training_losses, test_accuracies\n","\n","def evaluate_model(model, X_test, y_test):\n","    # Perform a forward pass with the test set\n","    output, _ = model.forward_pass(X_test)\n","    predictions = np.argmax(output, axis=1)\n","    true_labels = np.argmax(y_test, axis=1)\n","    accuracy = np.mean(predictions == true_labels)\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":809379,"status":"ok","timestamp":1711360522432,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"KGl6muB_9Zn_","outputId":"07a349b3-de43-4fb7-c713-28ca5ed63c43"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Softmax Output - Max: 0.9888558822818466 Min: 6.658540482849204e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07671964129313807 Min: -0.11282267955538647\n","Layer 1 - Gradient Weights Max: 0.15366244472147375 Min: -0.13886610722356857\n","Layer 0 - Gradient Weights Max: 0.2402392235423777 Min: -0.17405789367661298\n","ReLU Activation - Max: 5.361928068051118 Min: 0.0\n","ReLU Activation - Max: 4.899752957162492 Min: 0.0\n","Softmax Output - Max: 0.9430128222879552 Min: 9.479463726599895e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0921457698002561 Min: -0.16123242655961995\n","Layer 1 - Gradient Weights Max: 0.1713189924425376 Min: -0.28689758434600776\n","Layer 0 - Gradient Weights Max: 0.17544320360336926 Min: -0.15875497291677873\n","ReLU Activation - Max: 5.964669539867933 Min: 0.0\n","ReLU Activation - Max: 3.6536469411827617 Min: 0.0\n","Softmax Output - Max: 0.9596894759889121 Min: 5.120234495324526e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15951088590411508 Min: -0.11005706285871772\n","Layer 1 - Gradient Weights Max: 0.18735834919403715 Min: -0.20635060649548048\n","Layer 0 - Gradient Weights Max: 0.1995566755235367 Min: -0.17293496115030874\n","ReLU Activation - Max: 4.5126616096287195 Min: 0.0\n","ReLU Activation - Max: 4.543471056048496 Min: 0.0\n","Softmax Output - Max: 0.9611400296852844 Min: 1.6897471978284053e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11273172454215814 Min: -0.17361288304322886\n","Layer 1 - Gradient Weights Max: 0.2178532606558912 Min: -0.15744332993300453\n","Layer 0 - Gradient Weights Max: 0.22176127253243777 Min: -0.2027969284343598\n","ReLU Activation - Max: 4.354927614797457 Min: 0.0\n","ReLU Activation - Max: 4.303474737844195 Min: 0.0\n","Softmax Output - Max: 0.9575269532109983 Min: 9.488981701175627e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07331296245345648 Min: -0.1415169938896972\n","Layer 1 - Gradient Weights Max: 0.12902258521453788 Min: -0.14041674200197934\n","Layer 0 - Gradient Weights Max: 0.15657443660245535 Min: -0.2143905454937416\n","ReLU Activation - Max: 4.501959308104317 Min: 0.0\n","ReLU Activation - Max: 4.0227174319054475 Min: 0.0\n","Softmax Output - Max: 0.9935347001482778 Min: 5.986337143808467e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08843301573452667 Min: -0.15556492623055856\n","Layer 1 - Gradient Weights Max: 0.20356517532479976 Min: -0.17487639623029738\n","Layer 0 - Gradient Weights Max: 0.16289435764369975 Min: -0.24584317583963478\n","ReLU Activation - Max: 5.35438270244059 Min: 0.0\n","ReLU Activation - Max: 3.8183704540081482 Min: 0.0\n","Softmax Output - Max: 0.9938173190115238 Min: 6.0133898090723075e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1390763898007478 Min: -0.08297981761195261\n","Layer 1 - Gradient Weights Max: 0.14687724323412343 Min: -0.1715882951277376\n","Layer 0 - Gradient Weights Max: 0.17011384797342072 Min: -0.18295548473572665\n","ReLU Activation - Max: 5.949261807534777 Min: 0.0\n","ReLU Activation - Max: 4.305773943866745 Min: 0.0\n","Softmax Output - Max: 0.9239539948543755 Min: 3.10906547352719e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17497241194878124 Min: -0.13679683597828776\n","Layer 1 - Gradient Weights Max: 0.17281427315299355 Min: -0.17220137770936692\n","Layer 0 - Gradient Weights Max: 0.22612695583351558 Min: -0.1828278511897177\n","ReLU Activation - Max: 4.492185998707112 Min: 0.0\n","ReLU Activation - Max: 4.1735439804092636 Min: 0.0\n","Softmax Output - Max: 0.9618080586817305 Min: 6.837111303390723e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08721228535797299 Min: -0.13617419274242537\n","Layer 1 - Gradient Weights Max: 0.18607450767790618 Min: -0.12803345899024296\n","Layer 0 - Gradient Weights Max: 0.17611468747229728 Min: -0.24708170253339515\n","ReLU Activation - Max: 6.894159276824748 Min: 0.0\n","ReLU Activation - Max: 4.565870948735257 Min: 0.0\n","Softmax Output - Max: 0.9582622889819961 Min: 0.00010070059931781011 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09042263092116423 Min: -0.12489082805765896\n","Layer 1 - Gradient Weights Max: 0.13881054896611186 Min: -0.2265914341041558\n","Layer 0 - Gradient Weights Max: 0.16809482296606656 Min: -0.2018019124752467\n","ReLU Activation - Max: 5.114650536206636 Min: 0.0\n","ReLU Activation - Max: 4.410632922228557 Min: 0.0\n","Softmax Output - Max: 0.9613792321855402 Min: 7.894291181174841e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06944323652836516 Min: -0.07441334812173224\n","Layer 1 - Gradient Weights Max: 0.13911475100729462 Min: -0.12482999349039697\n","Layer 0 - Gradient Weights Max: 0.16885352772563605 Min: -0.2231731925660607\n","ReLU Activation - Max: 4.81912738348685 Min: 0.0\n","ReLU Activation - Max: 3.8125775382721345 Min: 0.0\n","Softmax Output - Max: 0.9809610846152823 Min: 3.291669489929259e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11945907812695203 Min: -0.2401236814280023\n","Layer 1 - Gradient Weights Max: 0.1906289957589978 Min: -0.18292737598447675\n","Layer 0 - Gradient Weights Max: 0.15584037506695808 Min: -0.16366616516778182\n","ReLU Activation - Max: 5.690712226489381 Min: 0.0\n","ReLU Activation - Max: 3.643030187902064 Min: 0.0\n","Softmax Output - Max: 0.9535761907260868 Min: 6.096580081185518e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12047629320579455 Min: -0.177315571670896\n","Layer 1 - Gradient Weights Max: 0.18357848001777366 Min: -0.13984718824421893\n","Layer 0 - Gradient Weights Max: 0.2166107007756158 Min: -0.20496365550397294\n","ReLU Activation - Max: 6.618028837948737 Min: 0.0\n","ReLU Activation - Max: 5.143779087448376 Min: 0.0\n","Softmax Output - Max: 0.9825453999152367 Min: 5.1785330287605235e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.07700338543253914 Min: -0.09674583027119991\n","Layer 1 - Gradient Weights Max: 0.18724388505671624 Min: -0.15517507508839712\n","Layer 0 - Gradient Weights Max: 0.19752381104454939 Min: -0.2344091863153254\n","ReLU Activation - Max: 5.039947523891394 Min: 0.0\n","ReLU Activation - Max: 4.470441559127045 Min: 0.0\n","Softmax Output - Max: 0.9663451418080562 Min: 2.258987785566065e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0862998889502034 Min: -0.1258381640808497\n","Layer 1 - Gradient Weights Max: 0.12002628642644747 Min: -0.1491328778189774\n","Layer 0 - Gradient Weights Max: 0.2191620771597656 Min: -0.1556063571101622\n","ReLU Activation - Max: 5.612788343587706 Min: 0.0\n","ReLU Activation - Max: 3.5438809532512003 Min: 0.0\n","Softmax Output - Max: 0.9915057238496054 Min: 9.912857541109725e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10521855728462995 Min: -0.0924589582893193\n","Layer 1 - Gradient Weights Max: 0.24256204676642965 Min: -0.14029981378675718\n","Layer 0 - Gradient Weights Max: 0.19007280068461554 Min: -0.20324805603360688\n","ReLU Activation - Max: 4.223059581691158 Min: 0.0\n","ReLU Activation - Max: 4.064210488940598 Min: 0.0\n","Softmax Output - Max: 0.9589971623420938 Min: 1.807466642256153e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08808634789121143 Min: -0.11493006828005443\n","Layer 1 - Gradient Weights Max: 0.11426325542385724 Min: -0.1075708870677583\n","Layer 0 - Gradient Weights Max: 0.15817296923133908 Min: -0.20788360225246905\n","ReLU Activation - Max: 5.617007747916466 Min: 0.0\n","ReLU Activation - Max: 4.359181798547898 Min: 0.0\n","Softmax Output - Max: 0.9844870868709441 Min: 1.8675733171298744e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14728298311506524 Min: -0.08645175018842834\n","Layer 1 - Gradient Weights Max: 0.13610449155179768 Min: -0.14182843897636888\n","Layer 0 - Gradient Weights Max: 0.16371984074717705 Min: -0.16282513567482843\n","ReLU Activation - Max: 4.536871754598697 Min: 0.0\n","ReLU Activation - Max: 4.01968663263432 Min: 0.0\n","Softmax Output - Max: 0.9896488995906242 Min: 3.6807992895349808e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09284058369295607 Min: -0.11171352069283837\n","Layer 1 - Gradient Weights Max: 0.1292162538736808 Min: -0.1572979240695507\n","Layer 0 - Gradient Weights Max: 0.27661468682823287 Min: -0.18354976652652036\n","ReLU Activation - Max: 5.165070203588875 Min: 0.0\n","ReLU Activation - Max: 3.8389619097589054 Min: 0.0\n","Softmax Output - Max: 0.9779950729843526 Min: 3.558801969143972e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12429196744810518 Min: -0.12556035661645146\n","Layer 1 - Gradient Weights Max: 0.15133050040358573 Min: -0.14240590166685269\n","Layer 0 - Gradient Weights Max: 0.18778785363870995 Min: -0.20422607525420158\n","ReLU Activation - Max: 4.183743619600214 Min: 0.0\n","ReLU Activation - Max: 4.582839480991864 Min: 0.0\n","Softmax Output - Max: 0.9799373706009707 Min: 1.0749792797785792e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0919591558992852 Min: -0.1443696193327549\n","Layer 1 - Gradient Weights Max: 0.13362115669632976 Min: -0.22633549853239354\n","Layer 0 - Gradient Weights Max: 0.15191707218897552 Min: -0.18200989583112717\n","ReLU Activation - Max: 5.303859728493336 Min: 0.0\n","ReLU Activation - Max: 4.502833815545492 Min: 0.0\n","Softmax Output - Max: 0.9624501470200025 Min: 2.284443925931626e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17012293572829187 Min: -0.1070713818862354\n","Layer 1 - Gradient Weights Max: 0.22382939376483463 Min: -0.18109959302847936\n","Layer 0 - Gradient Weights Max: 0.22856210705184749 Min: -0.18168071764026675\n","ReLU Activation - Max: 5.033525041010052 Min: 0.0\n","ReLU Activation - Max: 3.7613152267880303 Min: 0.0\n","Softmax Output - Max: 0.9525267761963959 Min: 8.764747710402916e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13536438981493057 Min: -0.23148935448317934\n","Layer 1 - Gradient Weights Max: 0.21943129579239648 Min: -0.21441330395825908\n","Layer 0 - Gradient Weights Max: 0.2239212292580215 Min: -0.2416689099822679\n","ReLU Activation - Max: 4.641726852631133 Min: 0.0\n","ReLU Activation - Max: 3.777507535806975 Min: 0.0\n","Softmax Output - Max: 0.9413533573755075 Min: 5.4626709844409294e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.13855848203477744 Min: -0.16167166285840337\n","Layer 1 - Gradient Weights Max: 0.19086700163423817 Min: -0.1556411041046833\n","Layer 0 - Gradient Weights Max: 0.2698520242802405 Min: -0.23656808362308354\n","ReLU Activation - Max: 5.516741686865786 Min: 0.0\n","ReLU Activation - Max: 4.24199397153895 Min: 0.0\n","Softmax Output - Max: 0.9729454523437168 Min: 1.439060957620085e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09553878730675243 Min: -0.09210634805471857\n","Layer 1 - Gradient Weights Max: 0.1564787710402688 Min: -0.15697222953079534\n","Layer 0 - Gradient Weights Max: 0.2149686207714675 Min: -0.20149867497171456\n","ReLU Activation - Max: 4.507837908223255 Min: 0.0\n","ReLU Activation - Max: 4.181495162322853 Min: 0.0\n","Softmax Output - Max: 0.9707188972726993 Min: 7.073139107571377e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17171164978872636 Min: -0.16277624500293444\n","Layer 1 - Gradient Weights Max: 0.22280451732298273 Min: -0.20695286848673264\n","Layer 0 - Gradient Weights Max: 0.15533641132326562 Min: -0.23768518794586121\n","ReLU Activation - Max: 4.987586900982965 Min: 0.0\n","ReLU Activation - Max: 4.370571933856142 Min: 0.0\n","Softmax Output - Max: 0.9965784047951354 Min: 3.170770308361422e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.0876937068683737 Min: -0.07882797682490539\n","Layer 1 - Gradient Weights Max: 0.1251518908790997 Min: -0.13018608559230607\n","Layer 0 - Gradient Weights Max: 0.15856570742610474 Min: -0.21466066802072528\n","ReLU Activation - Max: 4.691805311951143 Min: 0.0\n","ReLU Activation - Max: 3.8492514664441693 Min: 0.0\n","Softmax Output - Max: 0.9753581811483488 Min: 3.990132812748441e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1056044517441333 Min: -0.1534514803585654\n","Layer 1 - Gradient Weights Max: 0.13169573780265376 Min: -0.13013807547379935\n","Layer 0 - Gradient Weights Max: 0.19997346063553872 Min: -0.19452408339730454\n","ReLU Activation - Max: 4.812948181589576 Min: 0.0\n","ReLU Activation - Max: 4.037408483462017 Min: 0.0\n","Softmax Output - Max: 0.9771451158731083 Min: 2.590829035183322e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1139174052652817 Min: -0.09306153324601743\n","Layer 1 - Gradient Weights Max: 0.16163028757311018 Min: -0.1926187819760975\n","Layer 0 - Gradient Weights Max: 0.19913557410664498 Min: -0.16936040410764377\n","ReLU Activation - Max: 4.688697941402179 Min: 0.0\n","ReLU Activation - Max: 4.656010068040231 Min: 0.0\n","Softmax Output - Max: 0.9920407157076945 Min: 1.3807718358028554e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1016792802300143 Min: -0.13287139262212794\n","Layer 1 - Gradient Weights Max: 0.153093374099117 Min: -0.16238964540436912\n","Layer 0 - Gradient Weights Max: 0.22825042146152177 Min: -0.24787020534928328\n","ReLU Activation - Max: 4.827543123809272 Min: 0.0\n","ReLU Activation - Max: 4.348363296582697 Min: 0.0\n","Softmax Output - Max: 0.9663146974581713 Min: 4.882820506545527e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.14384226918744786 Min: -0.14638815280057427\n","Layer 1 - Gradient Weights Max: 0.18133186108120916 Min: -0.22861068542981997\n","Layer 0 - Gradient Weights Max: 0.1775944463951714 Min: -0.17062666669009013\n","ReLU Activation - Max: 5.40572799466807 Min: 0.0\n","ReLU Activation - Max: 4.032144842180999 Min: 0.0\n","Softmax Output - Max: 0.8758485821624745 Min: 4.8580807630088455e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11746720199135734 Min: -0.12287734886020953\n","Layer 1 - Gradient Weights Max: 0.14332700545059882 Min: -0.17125740137272932\n","Layer 0 - Gradient Weights Max: 0.25608098688312414 Min: -0.1710928978970183\n","ReLU Activation - Max: 6.053773818811094 Min: 0.0\n","ReLU Activation - Max: 4.303912116752163 Min: 0.0\n","Softmax Output - Max: 0.9312294084444233 Min: 1.0512902865712832e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08512205749830129 Min: -0.10298111537586035\n","Layer 1 - Gradient Weights Max: 0.13620186929680206 Min: -0.12353814435870825\n","Layer 0 - Gradient Weights Max: 0.17093093946334562 Min: -0.23266206647991933\n","ReLU Activation - Max: 5.291291185871709 Min: 0.0\n","ReLU Activation - Max: 4.117347365394515 Min: 0.0\n","Softmax Output - Max: 0.9972377486983097 Min: 1.827446151956781e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07988362312216737 Min: -0.08351899211970124\n","Layer 1 - Gradient Weights Max: 0.12499061086482174 Min: -0.13668077208284238\n","Layer 0 - Gradient Weights Max: 0.15326452907650642 Min: -0.14356164891870013\n","ReLU Activation - Max: 4.874508638529413 Min: 0.0\n","ReLU Activation - Max: 4.525225568412244 Min: 0.0\n","Softmax Output - Max: 0.9369176896051662 Min: 2.2549195112526152e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08787675896242228 Min: -0.07739384804046386\n","Layer 1 - Gradient Weights Max: 0.12014790431475514 Min: -0.14527118265717281\n","Layer 0 - Gradient Weights Max: 0.16283152269241888 Min: -0.19220712251316277\n","ReLU Activation - Max: 4.299990702128648 Min: 0.0\n","ReLU Activation - Max: 3.887883964819097 Min: 0.0\n","Softmax Output - Max: 0.9934948898940313 Min: 3.196524468054954e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13919075730164496 Min: -0.1515073904090543\n","Layer 1 - Gradient Weights Max: 0.1596233465356778 Min: -0.1756336998172225\n","Layer 0 - Gradient Weights Max: 0.20628284495479002 Min: -0.1612085223287208\n","ReLU Activation - Max: 4.597668959484023 Min: 0.0\n","ReLU Activation - Max: 3.6366685126364673 Min: 0.0\n","Softmax Output - Max: 0.960225400092593 Min: 2.320156392152824e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11833883532033404 Min: -0.09870571265936069\n","Layer 1 - Gradient Weights Max: 0.12143558539845449 Min: -0.13666732089772915\n","Layer 0 - Gradient Weights Max: 0.24163603863430635 Min: -0.19258569217879287\n","ReLU Activation - Max: 4.896415005240609 Min: 0.0\n","ReLU Activation - Max: 3.5828185938603108 Min: 0.0\n","Softmax Output - Max: 0.9607410720751277 Min: 4.2346921707832586e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.11322061757210848 Min: -0.1845661406434161\n","Layer 1 - Gradient Weights Max: 0.1978542631821292 Min: -0.2030339441660035\n","Layer 0 - Gradient Weights Max: 0.2466894575061622 Min: -0.2258634869173608\n","ReLU Activation - Max: 4.8053702432694205 Min: 0.0\n","ReLU Activation - Max: 5.888166776845749 Min: 0.0\n","Softmax Output - Max: 0.9418737785525304 Min: 3.858798938017788e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07623867746919444 Min: -0.1633367058306358\n","Layer 1 - Gradient Weights Max: 0.16310155713160676 Min: -0.15646867304511802\n","Layer 0 - Gradient Weights Max: 0.18782786212831443 Min: -0.13791480310508705\n","ReLU Activation - Max: 4.8945427364674545 Min: 0.0\n","ReLU Activation - Max: 4.815031745792929 Min: 0.0\n","Softmax Output - Max: 0.9824850665613518 Min: 3.409249883972995e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.133900093213825 Min: -0.14225609228651817\n","Layer 1 - Gradient Weights Max: 0.19534280473895377 Min: -0.2024851563255421\n","Layer 0 - Gradient Weights Max: 0.17955623190535505 Min: -0.14869194959154608\n","ReLU Activation - Max: 4.450873110270459 Min: 0.0\n","ReLU Activation - Max: 3.551152665201785 Min: 0.0\n","Softmax Output - Max: 0.9792062798867088 Min: 1.4720490253256909e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08907103481456827 Min: -0.07716211476803601\n","Layer 1 - Gradient Weights Max: 0.14097165376535864 Min: -0.1089416813006144\n","Layer 0 - Gradient Weights Max: 0.1801197530550276 Min: -0.1916630955379472\n","ReLU Activation - Max: 4.866945278123392 Min: 0.0\n","ReLU Activation - Max: 3.3682653643588654 Min: 0.0\n","Softmax Output - Max: 0.9602221274453633 Min: 0.00013202860219416104 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09715226018082254 Min: -0.1054548932960266\n","Layer 1 - Gradient Weights Max: 0.16651501257089674 Min: -0.1498730395050177\n","Layer 0 - Gradient Weights Max: 0.22155005756777724 Min: -0.17861665230237223\n","ReLU Activation - Max: 5.145617578669777 Min: 0.0\n","ReLU Activation - Max: 5.447387508048419 Min: 0.0\n","Softmax Output - Max: 0.9585206071409379 Min: 5.355611001783179e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07002476670190616 Min: -0.10686651299594278\n","Layer 1 - Gradient Weights Max: 0.15738913823844075 Min: -0.13589375264093426\n","Layer 0 - Gradient Weights Max: 0.24421207706415138 Min: -0.14965389304190246\n","ReLU Activation - Max: 4.955662627099307 Min: 0.0\n","ReLU Activation - Max: 3.8968043599923763 Min: 0.0\n","Softmax Output - Max: 0.9612916187609117 Min: 0.00018163970321089995 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09980422738215677 Min: -0.11735181726350077\n","Layer 1 - Gradient Weights Max: 0.21183892938611834 Min: -0.18847897457987603\n","Layer 0 - Gradient Weights Max: 0.19004913857776987 Min: -0.1993316209920736\n","ReLU Activation - Max: 5.75836362150545 Min: 0.0\n","ReLU Activation - Max: 3.97736760271296 Min: 0.0\n","Softmax Output - Max: 0.9063962160425969 Min: 3.039986376888257e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07833750103535193 Min: -0.11950939456006701\n","Layer 1 - Gradient Weights Max: 0.15202575106607988 Min: -0.14372815318449944\n","Layer 0 - Gradient Weights Max: 0.17476941261675172 Min: -0.1880046601960013\n","ReLU Activation - Max: 5.546670863965968 Min: 0.0\n","ReLU Activation - Max: 4.372575561386133 Min: 0.0\n","Softmax Output - Max: 0.9608646046807006 Min: 5.3123797925112205e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10371808836848401 Min: -0.15127141983000936\n","Layer 1 - Gradient Weights Max: 0.13479155971249715 Min: -0.20648010923019652\n","Layer 0 - Gradient Weights Max: 0.15324372679082957 Min: -0.16266939233424893\n","ReLU Activation - Max: 4.681697876947895 Min: 0.0\n","ReLU Activation - Max: 4.0069009935195705 Min: 0.0\n","Softmax Output - Max: 0.9610399172614008 Min: 1.2860506773422745e-05 Sum (first example): 1.0000000000000004\n","Layer 2 - Gradient Weights Max: 0.1030873148183579 Min: -0.13022953961866246\n","Layer 1 - Gradient Weights Max: 0.1126213960680482 Min: -0.1767656484150994\n","Layer 0 - Gradient Weights Max: 0.24061902037656457 Min: -0.22112469581937041\n","ReLU Activation - Max: 5.653237830956314 Min: 0.0\n","ReLU Activation - Max: 3.758151412799021 Min: 0.0\n","Softmax Output - Max: 0.9924519407709653 Min: 2.5839687635076637e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.11512801632448062 Min: -0.16280801502148923\n","Layer 1 - Gradient Weights Max: 0.2429299678599811 Min: -0.15397892299896312\n","Layer 0 - Gradient Weights Max: 0.1772238550818353 Min: -0.17974971052788605\n","ReLU Activation - Max: 5.417931242085558 Min: 0.0\n","ReLU Activation - Max: 4.179671647824955 Min: 0.0\n","Softmax Output - Max: 0.9701239717967165 Min: 8.831229706373959e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11038966567119389 Min: -0.0911365942063457\n","Layer 1 - Gradient Weights Max: 0.16191616353953472 Min: -0.13552154086377685\n","Layer 0 - Gradient Weights Max: 0.1539361627301743 Min: -0.15960291864640644\n","ReLU Activation - Max: 5.097154468208136 Min: 0.0\n","ReLU Activation - Max: 4.149652342238504 Min: 0.0\n","Softmax Output - Max: 0.9923076278906343 Min: 1.2288675522908557e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1109237320356445 Min: -0.13525600493495585\n","Layer 1 - Gradient Weights Max: 0.1642245460747603 Min: -0.21006130252485605\n","Layer 0 - Gradient Weights Max: 0.21067315246364163 Min: -0.19385562513519014\n","ReLU Activation - Max: 4.879120118044584 Min: 0.0\n","ReLU Activation - Max: 4.601841928802249 Min: 0.0\n","Softmax Output - Max: 0.9951978386964925 Min: 7.313209228969698e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10273941099995873 Min: -0.13317475212489663\n","Layer 1 - Gradient Weights Max: 0.1585077080356344 Min: -0.15956076563776272\n","Layer 0 - Gradient Weights Max: 0.2470750623775334 Min: -0.20917338384916867\n","ReLU Activation - Max: 4.680444168396335 Min: 0.0\n","ReLU Activation - Max: 4.549074426092936 Min: 0.0\n","Softmax Output - Max: 0.9658697964375578 Min: 6.161503153788902e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1288437290504784 Min: -0.1960081664515257\n","Layer 1 - Gradient Weights Max: 0.29710645985738404 Min: -0.21371614351894924\n","Layer 0 - Gradient Weights Max: 0.29003143268668863 Min: -0.24184573109052138\n","ReLU Activation - Max: 5.138624227324855 Min: 0.0\n","ReLU Activation - Max: 4.177171236052531 Min: 0.0\n","Softmax Output - Max: 0.9734638311705065 Min: 2.5453710930924128e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11905915535921258 Min: -0.1235597376924301\n","Layer 1 - Gradient Weights Max: 0.18352040143088375 Min: -0.18868961031514453\n","Layer 0 - Gradient Weights Max: 0.2879873548821717 Min: -0.23489528082862202\n","ReLU Activation - Max: 4.5883890435615715 Min: 0.0\n","ReLU Activation - Max: 3.5746722180782022 Min: 0.0\n","Softmax Output - Max: 0.9979138940075986 Min: 1.578150016530638e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11397681140660762 Min: -0.15288815060617295\n","Layer 1 - Gradient Weights Max: 0.15357355454961194 Min: -0.18920710833816792\n","Layer 0 - Gradient Weights Max: 0.163756042543808 Min: -0.1917735732007511\n","ReLU Activation - Max: 5.892693967433688 Min: 0.0\n","ReLU Activation - Max: 4.479561696083663 Min: 0.0\n","Softmax Output - Max: 0.9293018117498925 Min: 1.209937539799616e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14669924051100472 Min: -0.20574000952299645\n","Layer 1 - Gradient Weights Max: 0.24654967493427815 Min: -0.1774596237929464\n","Layer 0 - Gradient Weights Max: 0.23001744945689467 Min: -0.17854704722441664\n","ReLU Activation - Max: 5.250040451920101 Min: 0.0\n","ReLU Activation - Max: 4.149999856475447 Min: 0.0\n","Softmax Output - Max: 0.9323088040004758 Min: 2.952280438959463e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08925779768315452 Min: -0.15629458540391067\n","Layer 1 - Gradient Weights Max: 0.14702170862510142 Min: -0.15598067522732809\n","Layer 0 - Gradient Weights Max: 0.15075443885058512 Min: -0.13607138649659115\n","ReLU Activation - Max: 5.612202724352463 Min: 0.0\n","ReLU Activation - Max: 4.226350592412467 Min: 0.0\n","Softmax Output - Max: 0.9838641913756819 Min: 2.7080953293452503e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1019342275889635 Min: -0.12892287719179146\n","Layer 1 - Gradient Weights Max: 0.19116836318229452 Min: -0.2059205365215632\n","Layer 0 - Gradient Weights Max: 0.20931495854550808 Min: -0.18613596153216028\n","ReLU Activation - Max: 4.72646096384501 Min: 0.0\n","ReLU Activation - Max: 4.098112812888059 Min: 0.0\n","Softmax Output - Max: 0.9548410407001189 Min: 3.386939721631409e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0711545025953513 Min: -0.0749390763703779\n","Layer 1 - Gradient Weights Max: 0.15570589016545916 Min: -0.12786810826319567\n","Layer 0 - Gradient Weights Max: 0.25607869943067146 Min: -0.22776875083346335\n","ReLU Activation - Max: 4.54854511351122 Min: 0.0\n","ReLU Activation - Max: 4.413291998514846 Min: 0.0\n","Softmax Output - Max: 0.9371729252358965 Min: 2.7396562315324126e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09167454293708646 Min: -0.08356100141405699\n","Layer 1 - Gradient Weights Max: 0.11816959701556087 Min: -0.11402021791775736\n","Layer 0 - Gradient Weights Max: 0.16013577985358687 Min: -0.23687416746503445\n","ReLU Activation - Max: 5.458574629408703 Min: 0.0\n","ReLU Activation - Max: 4.0885391604718 Min: 0.0\n","Softmax Output - Max: 0.9437518056580045 Min: 5.994595279722186e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07087460564255414 Min: -0.16605683734656934\n","Layer 1 - Gradient Weights Max: 0.17330412458481215 Min: -0.21444069674867577\n","Layer 0 - Gradient Weights Max: 0.1648355846705694 Min: -0.2132936510697191\n","ReLU Activation - Max: 4.467712059806352 Min: 0.0\n","ReLU Activation - Max: 4.133982783298935 Min: 0.0\n","Softmax Output - Max: 0.9905810568053272 Min: 3.3626303624726035e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06648946707034929 Min: -0.10394244111851088\n","Layer 1 - Gradient Weights Max: 0.10521316365507416 Min: -0.1390720660617686\n","Layer 0 - Gradient Weights Max: 0.23892209874094747 Min: -0.27635797614622887\n","ReLU Activation - Max: 5.0583472906669575 Min: 0.0\n","ReLU Activation - Max: 3.9080359576804042 Min: 0.0\n","Softmax Output - Max: 0.9360032111598029 Min: 4.479937711064004e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09950139985913374 Min: -0.13304126888974432\n","Layer 1 - Gradient Weights Max: 0.18922985683356297 Min: -0.22093630888730018\n","Layer 0 - Gradient Weights Max: 0.389864386504566 Min: -0.3174210784177793\n","ReLU Activation - Max: 5.431119354376342 Min: 0.0\n","ReLU Activation - Max: 3.9254082699902053 Min: 0.0\n","Softmax Output - Max: 0.9953431727385443 Min: 1.072169741270153e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09677726553725771 Min: -0.2629704440359917\n","Layer 1 - Gradient Weights Max: 0.1770536915233761 Min: -0.27149544244279916\n","Layer 0 - Gradient Weights Max: 0.24450248367868674 Min: -0.2879956676822726\n","ReLU Activation - Max: 5.893652546296138 Min: 0.0\n","ReLU Activation - Max: 4.177756053181074 Min: 0.0\n","Softmax Output - Max: 0.9841438804399619 Min: 2.3499966867770447e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1204612409366291 Min: -0.11522366992873291\n","Layer 1 - Gradient Weights Max: 0.1430705027429036 Min: -0.1652474530620804\n","Layer 0 - Gradient Weights Max: 0.15815448217447628 Min: -0.2037984180337837\n","ReLU Activation - Max: 4.816072591668521 Min: 0.0\n","ReLU Activation - Max: 4.163864950564256 Min: 0.0\n","Softmax Output - Max: 0.9655065756324838 Min: 5.48205308077566e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09012859526594408 Min: -0.10793864956363135\n","Layer 1 - Gradient Weights Max: 0.15290515501153892 Min: -0.16166141200326437\n","Layer 0 - Gradient Weights Max: 0.16384972999948724 Min: -0.2584431284434361\n","ReLU Activation - Max: 4.565699583493418 Min: 0.0\n","ReLU Activation - Max: 3.2854319501376916 Min: 0.0\n","Softmax Output - Max: 0.9812668807317462 Min: 5.233656168548038e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0675826023371788 Min: -0.16897273259267653\n","Layer 1 - Gradient Weights Max: 0.1688138471783313 Min: -0.18247714182482255\n","Layer 0 - Gradient Weights Max: 0.19377158727088956 Min: -0.17483759071994268\n","ReLU Activation - Max: 4.502598229369037 Min: 0.0\n","ReLU Activation - Max: 4.725481344387972 Min: 0.0\n","Softmax Output - Max: 0.9571274291843963 Min: 7.3653011251742315e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.17363554739918466 Min: -0.13287482089608904\n","Layer 1 - Gradient Weights Max: 0.15019743460885687 Min: -0.1388202384496466\n","Layer 0 - Gradient Weights Max: 0.25893590458094945 Min: -0.13832015239175313\n","ReLU Activation - Max: 4.802751040529674 Min: 0.0\n","ReLU Activation - Max: 4.131609752357302 Min: 0.0\n","Softmax Output - Max: 0.9728217807151869 Min: 4.605224093956865e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06635741116147684 Min: -0.0751438385848818\n","Layer 1 - Gradient Weights Max: 0.14232190819330182 Min: -0.2164673328282488\n","Layer 0 - Gradient Weights Max: 0.3204884664739685 Min: -0.20070965513933497\n","ReLU Activation - Max: 6.3695699205850795 Min: 0.0\n","ReLU Activation - Max: 3.5983347166960007 Min: 0.0\n","Softmax Output - Max: 0.9691222338596278 Min: 4.007478658383927e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09044411384884388 Min: -0.10482500252425524\n","Layer 1 - Gradient Weights Max: 0.21249269211688654 Min: -0.17861081742641025\n","Layer 0 - Gradient Weights Max: 0.17554671048164255 Min: -0.2807909017826978\n","ReLU Activation - Max: 4.716679953140465 Min: 0.0\n","ReLU Activation - Max: 3.8412667338129447 Min: 0.0\n","Softmax Output - Max: 0.9480329320599867 Min: 5.208855788107211e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07979549851965612 Min: -0.14310125029207116\n","Layer 1 - Gradient Weights Max: 0.14588013994213586 Min: -0.19817393777989237\n","Layer 0 - Gradient Weights Max: 0.2096296079405963 Min: -0.2445722921970058\n","ReLU Activation - Max: 4.5027730457546005 Min: 0.0\n","ReLU Activation - Max: 3.878078718121536 Min: 0.0\n","Softmax Output - Max: 0.9713519546824384 Min: 3.828546996231806e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09584171772217011 Min: -0.12581532471344165\n","Layer 1 - Gradient Weights Max: 0.13666630812008942 Min: -0.16809250945485452\n","Layer 0 - Gradient Weights Max: 0.15955694612753957 Min: -0.15712996032199664\n","ReLU Activation - Max: 4.536749303272765 Min: 0.0\n","ReLU Activation - Max: 3.8647300057417207 Min: 0.0\n","Softmax Output - Max: 0.9872985865632263 Min: 3.3037325409994905e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07827001085655384 Min: -0.07605205810560764\n","Layer 1 - Gradient Weights Max: 0.11797859174305206 Min: -0.10716086124955859\n","Layer 0 - Gradient Weights Max: 0.14944279608875957 Min: -0.1597549673146965\n","ReLU Activation - Max: 3.682661097402162 Min: 0.0\n","ReLU Activation - Max: 3.8560057884055214 Min: 0.0\n","Softmax Output - Max: 0.9569345060106663 Min: 5.506271248718436e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10956211124603069 Min: -0.14670997372192734\n","Layer 1 - Gradient Weights Max: 0.13357238738345992 Min: -0.09937819578222659\n","Layer 0 - Gradient Weights Max: 0.21810564237968877 Min: -0.17934616790007513\n","ReLU Activation - Max: 5.117850354406836 Min: 0.0\n","ReLU Activation - Max: 3.7429406639955936 Min: 0.0\n","Softmax Output - Max: 0.9448351243788324 Min: 7.565327976623975e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09678708632360228 Min: -0.1029859752089741\n","Layer 1 - Gradient Weights Max: 0.16567928986233219 Min: -0.1187931025128435\n","Layer 0 - Gradient Weights Max: 0.2510522714262796 Min: -0.2681332658757664\n","ReLU Activation - Max: 4.63757525888637 Min: 0.0\n","ReLU Activation - Max: 3.9078488515934953 Min: 0.0\n","Softmax Output - Max: 0.9516175785649686 Min: 4.088439504909987e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10929851472237229 Min: -0.10566994736237613\n","Layer 1 - Gradient Weights Max: 0.17354409212973396 Min: -0.1688093339430094\n","Layer 0 - Gradient Weights Max: 0.31905599928733375 Min: -0.23899550776019832\n","ReLU Activation - Max: 5.47217980819481 Min: 0.0\n","ReLU Activation - Max: 4.091811358076408 Min: 0.0\n","Softmax Output - Max: 0.9982220939545223 Min: 3.004431908920951e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.14416866835253359 Min: -0.1225210794110752\n","Layer 1 - Gradient Weights Max: 0.16719813620161983 Min: -0.1443408370756764\n","Layer 0 - Gradient Weights Max: 0.2769246351494946 Min: -0.19168806419493845\n","ReLU Activation - Max: 6.168033510212407 Min: 0.0\n","ReLU Activation - Max: 5.211459660034035 Min: 0.0\n","Softmax Output - Max: 0.9343822094860911 Min: 2.0634897894309034e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.20314673926545862 Min: -0.12396969563295306\n","Layer 1 - Gradient Weights Max: 0.20996335384464007 Min: -0.1602294761266048\n","Layer 0 - Gradient Weights Max: 0.1732432401572598 Min: -0.1592690261885529\n","ReLU Activation - Max: 5.891381522154405 Min: 0.0\n","ReLU Activation - Max: 4.018004021717121 Min: 0.0\n","Softmax Output - Max: 0.9488877172635504 Min: 0.00013791129173963468 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06434442367703312 Min: -0.09810241272841232\n","Layer 1 - Gradient Weights Max: 0.122626239589271 Min: -0.11621259586504826\n","Layer 0 - Gradient Weights Max: 0.15763394391930027 Min: -0.23402967688126053\n","ReLU Activation - Max: 4.486707776816703 Min: 0.0\n","ReLU Activation - Max: 3.4342942916763244 Min: 0.0\n","Softmax Output - Max: 0.978395336677288 Min: 1.0347224102439806e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11244729083696259 Min: -0.08360849916753638\n","Layer 1 - Gradient Weights Max: 0.1609017680483426 Min: -0.13327053738510442\n","Layer 0 - Gradient Weights Max: 0.1758908268960465 Min: -0.34445234340725017\n","ReLU Activation - Max: 4.993397551637367 Min: 0.0\n","ReLU Activation - Max: 4.154455982652651 Min: 0.0\n","Softmax Output - Max: 0.980877955620682 Min: 2.4049848416065254e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09488452234145818 Min: -0.11215018666951915\n","Layer 1 - Gradient Weights Max: 0.1574416503662681 Min: -0.1538783541144882\n","Layer 0 - Gradient Weights Max: 0.26103714527528193 Min: -0.16397677145120756\n","ReLU Activation - Max: 5.769290754295273 Min: 0.0\n","ReLU Activation - Max: 4.17519002367346 Min: 0.0\n","Softmax Output - Max: 0.9869148455219682 Min: 5.0360203296561096e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.06739600552393891 Min: -0.18556602888866341\n","Layer 1 - Gradient Weights Max: 0.24927530016564622 Min: -0.15889099665653322\n","Layer 0 - Gradient Weights Max: 0.28640231012398354 Min: -0.3231976529830816\n","ReLU Activation - Max: 5.598125670808662 Min: 0.0\n","ReLU Activation - Max: 4.048488391426526 Min: 0.0\n","Softmax Output - Max: 0.9893368633366346 Min: 3.0187060629681694e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.14952764684315256 Min: -0.1065294088270402\n","Layer 1 - Gradient Weights Max: 0.13991230247281528 Min: -0.12694628721013135\n","Layer 0 - Gradient Weights Max: 0.2346500055053142 Min: -0.12389081411935152\n","ReLU Activation - Max: 5.171709213697007 Min: 0.0\n","ReLU Activation - Max: 4.323389227367317 Min: 0.0\n","Softmax Output - Max: 0.9440894712877853 Min: 2.909276852728962e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11821400969070732 Min: -0.08981634495104081\n","Layer 1 - Gradient Weights Max: 0.12426403162098783 Min: -0.14035238651121512\n","Layer 0 - Gradient Weights Max: 0.16444280280782195 Min: -0.2087504414632474\n","ReLU Activation - Max: 5.410007068272632 Min: 0.0\n","ReLU Activation - Max: 4.069998382743405 Min: 0.0\n","Softmax Output - Max: 0.9457297037762432 Min: 1.0932773041985183e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08359296421115106 Min: -0.12066361648584395\n","Layer 1 - Gradient Weights Max: 0.15320637148624588 Min: -0.12640643589079656\n","Layer 0 - Gradient Weights Max: 0.1630636513438867 Min: -0.2001113197789779\n","ReLU Activation - Max: 5.013543238838248 Min: 0.0\n","ReLU Activation - Max: 4.190191742524649 Min: 0.0\n","Softmax Output - Max: 0.9824719645362573 Min: 4.4172489130997604e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12867674553974434 Min: -0.10759472247418604\n","Layer 1 - Gradient Weights Max: 0.19499217346691575 Min: -0.1516135300320767\n","Layer 0 - Gradient Weights Max: 0.135745335401609 Min: -0.16959091952281025\n","ReLU Activation - Max: 4.777064392729573 Min: 0.0\n","ReLU Activation - Max: 3.757117999946641 Min: 0.0\n","Softmax Output - Max: 0.9137485894243734 Min: 2.860991627657146e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10366636966078423 Min: -0.16289530668702154\n","Layer 1 - Gradient Weights Max: 0.15928474553441987 Min: -0.15880455522280204\n","Layer 0 - Gradient Weights Max: 0.2635210202536898 Min: -0.1937462962520459\n","ReLU Activation - Max: 6.134737989410115 Min: 0.0\n","ReLU Activation - Max: 4.516247214796057 Min: 0.0\n","Softmax Output - Max: 0.9809960926122739 Min: 4.749917771268007e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11351206785353116 Min: -0.10608740373513356\n","Layer 1 - Gradient Weights Max: 0.13533539573031175 Min: -0.17996044496465988\n","Layer 0 - Gradient Weights Max: 0.29882795315960614 Min: -0.1708891844994844\n","ReLU Activation - Max: 4.641420781009971 Min: 0.0\n","ReLU Activation - Max: 3.842733205147812 Min: 0.0\n","Softmax Output - Max: 0.9938150341247729 Min: 3.915709942851424e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14922786643671285 Min: -0.10088817261416413\n","Layer 1 - Gradient Weights Max: 0.13019761848235173 Min: -0.18182701127410458\n","Layer 0 - Gradient Weights Max: 0.2204083421367213 Min: -0.18138566924595154\n","ReLU Activation - Max: 5.038910470704694 Min: 0.0\n","ReLU Activation - Max: 4.173882549587604 Min: 0.0\n","Softmax Output - Max: 0.9447681906747671 Min: 2.6184643343039017e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09584305630943692 Min: -0.15104405713862615\n","Layer 1 - Gradient Weights Max: 0.17055382626840782 Min: -0.20602152469316007\n","Layer 0 - Gradient Weights Max: 0.19912667030124168 Min: -0.18858931027055068\n","ReLU Activation - Max: 6.0695109406119805 Min: 0.0\n","ReLU Activation - Max: 3.817978112455969 Min: 0.0\n","Softmax Output - Max: 0.9925591541872889 Min: 1.8601781269680823e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08808345280891286 Min: -0.18511052234136066\n","Layer 1 - Gradient Weights Max: 0.15920031972115986 Min: -0.2111435624466014\n","Layer 0 - Gradient Weights Max: 0.18584297246605486 Min: -0.16870631374624848\n","ReLU Activation - Max: 5.214916765154805 Min: 0.0\n","ReLU Activation - Max: 3.780304672908182 Min: 0.0\n","Softmax Output - Max: 0.9582932150308179 Min: 3.595037751894374e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08113249085043583 Min: -0.08587982424454665\n","Layer 1 - Gradient Weights Max: 0.11432874625158113 Min: -0.13605740563109997\n","Layer 0 - Gradient Weights Max: 0.1783346707637006 Min: -0.15505597278477257\n","ReLU Activation - Max: 5.118351042458121 Min: 0.0\n","ReLU Activation - Max: 4.360842917896978 Min: 0.0\n","Softmax Output - Max: 0.986277406653149 Min: 1.0284636616338789e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09896828081674415 Min: -0.156989363050478\n","Layer 1 - Gradient Weights Max: 0.12205543800770959 Min: -0.1955587513919348\n","Layer 0 - Gradient Weights Max: 0.13431015435056368 Min: -0.1937397908243832\n","ReLU Activation - Max: 5.879177122677722 Min: 0.0\n","ReLU Activation - Max: 5.426851174899596 Min: 0.0\n","Softmax Output - Max: 0.9850161902603463 Min: 7.63313736648832e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0797281725914477 Min: -0.10141707986346707\n","Layer 1 - Gradient Weights Max: 0.16457904874651927 Min: -0.13993065941638957\n","Layer 0 - Gradient Weights Max: 0.22602429224194603 Min: -0.22878023059543523\n","ReLU Activation - Max: 4.463985547731994 Min: 0.0\n","ReLU Activation - Max: 4.614482286215691 Min: 0.0\n","Softmax Output - Max: 0.9658098204302468 Min: 5.002933362501703e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10437220328761897 Min: -0.10460433636882101\n","Layer 1 - Gradient Weights Max: 0.16271742686836238 Min: -0.19427116187850027\n","Layer 0 - Gradient Weights Max: 0.21586461390721381 Min: -0.23861452359349047\n","ReLU Activation - Max: 4.29721117747452 Min: 0.0\n","ReLU Activation - Max: 4.241931974862171 Min: 0.0\n","Softmax Output - Max: 0.9747904060497404 Min: 3.139598997052388e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1240160298285195 Min: -0.15590470027903716\n","Layer 1 - Gradient Weights Max: 0.13714986439409518 Min: -0.19887059929296674\n","Layer 0 - Gradient Weights Max: 0.18947978508461868 Min: -0.1671881407544761\n","ReLU Activation - Max: 5.338032362418923 Min: 0.0\n","ReLU Activation - Max: 4.07856622455433 Min: 0.0\n","Softmax Output - Max: 0.9829929729099794 Min: 9.469619997490297e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.05900503767007436 Min: -0.10842189835790793\n","Layer 1 - Gradient Weights Max: 0.16990094214250967 Min: -0.11823613473701658\n","Layer 0 - Gradient Weights Max: 0.21342359886968604 Min: -0.1841392424603424\n","ReLU Activation - Max: 4.592561049834817 Min: 0.0\n","ReLU Activation - Max: 3.601223381744482 Min: 0.0\n","Softmax Output - Max: 0.8827536638603038 Min: 3.793631310843788e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11458358090533514 Min: -0.10842960991887828\n","Layer 1 - Gradient Weights Max: 0.1732636937798444 Min: -0.1912596431359959\n","Layer 0 - Gradient Weights Max: 0.16865658171506462 Min: -0.22542442817876698\n","ReLU Activation - Max: 4.442507251248197 Min: 0.0\n","ReLU Activation - Max: 4.008092723721151 Min: 0.0\n","Softmax Output - Max: 0.9835775544857429 Min: 6.840004231457005e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.1022358740003106 Min: -0.10107701835539548\n","Layer 1 - Gradient Weights Max: 0.18298002484785772 Min: -0.16135850527639287\n","Layer 0 - Gradient Weights Max: 0.2514617965463883 Min: -0.17449951077962383\n","ReLU Activation - Max: 4.291510961067923 Min: 0.0\n","ReLU Activation - Max: 3.7216971284531186 Min: 0.0\n","Softmax Output - Max: 0.9504198050267757 Min: 4.3439671429190955e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.13666167734836135 Min: -0.13584571555019795\n","Layer 1 - Gradient Weights Max: 0.17076912607292297 Min: -0.20699265525620278\n","Layer 0 - Gradient Weights Max: 0.218226926223537 Min: -0.20976050094849025\n","ReLU Activation - Max: 5.123636050680356 Min: 0.0\n","ReLU Activation - Max: 4.354382331353252 Min: 0.0\n","Softmax Output - Max: 0.9968843294073658 Min: 1.639008265765047e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09723434291281619 Min: -0.10754066313419365\n","Layer 1 - Gradient Weights Max: 0.11963621602206977 Min: -0.12712222248956098\n","Layer 0 - Gradient Weights Max: 0.1763183220600263 Min: -0.11800252347203766\n","ReLU Activation - Max: 6.532937149161864 Min: 0.0\n","ReLU Activation - Max: 4.210007111252056 Min: 0.0\n","Softmax Output - Max: 0.9308698671144867 Min: 2.3201010782785446e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09487689537169049 Min: -0.1297290518750059\n","Layer 1 - Gradient Weights Max: 0.2496446876358284 Min: -0.177060931198401\n","Layer 0 - Gradient Weights Max: 0.22517022209763668 Min: -0.15189663752658503\n","ReLU Activation - Max: 5.3578588903277335 Min: 0.0\n","ReLU Activation - Max: 3.863399343559054 Min: 0.0\n","Softmax Output - Max: 0.9757839938149433 Min: 1.3904483294716505e-05 Sum (first example): 0.9999999999999997\n","Layer 2 - Gradient Weights Max: 0.1115841148101065 Min: -0.11484237328327888\n","Layer 1 - Gradient Weights Max: 0.17827283695102575 Min: -0.14120143162898818\n","Layer 0 - Gradient Weights Max: 0.24378293226547496 Min: -0.17309671360525142\n","ReLU Activation - Max: 5.352136071024302 Min: 0.0\n","ReLU Activation - Max: 4.5500931025661755 Min: 0.0\n","Softmax Output - Max: 0.9874431732724328 Min: 2.3473771808667995e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11837906565090119 Min: -0.10990982894912578\n","Layer 1 - Gradient Weights Max: 0.11005035198939582 Min: -0.15623449920224847\n","Layer 0 - Gradient Weights Max: 0.19063167207208576 Min: -0.2635573581251725\n","ReLU Activation - Max: 4.650899281660549 Min: 0.0\n","ReLU Activation - Max: 4.2909116948028 Min: 0.0\n","Softmax Output - Max: 0.9505155132831534 Min: 8.007229086657367e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.06096729592938628 Min: -0.09942112910638196\n","Layer 1 - Gradient Weights Max: 0.10633071520040696 Min: -0.14167936453438748\n","Layer 0 - Gradient Weights Max: 0.2470164902112515 Min: -0.20759477515801417\n","ReLU Activation - Max: 4.606929380770171 Min: 0.0\n","ReLU Activation - Max: 3.879431704330805 Min: 0.0\n","Softmax Output - Max: 0.945097149748533 Min: 5.227993371879911e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1311936921549089 Min: -0.19234877779652979\n","Layer 1 - Gradient Weights Max: 0.20242568499080607 Min: -0.16306447370599206\n","Layer 0 - Gradient Weights Max: 0.19335023342337765 Min: -0.1599165259402928\n","ReLU Activation - Max: 5.74789750503811 Min: 0.0\n","ReLU Activation - Max: 4.571804788381703 Min: 0.0\n","Softmax Output - Max: 0.9950274191807492 Min: 5.648205398033952e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10917936909160389 Min: -0.09991803193151831\n","Layer 1 - Gradient Weights Max: 0.13974728551961513 Min: -0.28122945167108027\n","Layer 0 - Gradient Weights Max: 0.21141300394854828 Min: -0.19728182939646383\n","ReLU Activation - Max: 4.922055371783065 Min: 0.0\n","ReLU Activation - Max: 4.552854614868129 Min: 0.0\n","Softmax Output - Max: 0.9782829913324566 Min: 1.5561093122317077e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08185766617014797 Min: -0.09215618493460298\n","Layer 1 - Gradient Weights Max: 0.1151958130062838 Min: -0.1334890439837361\n","Layer 0 - Gradient Weights Max: 0.19728992077469198 Min: -0.1819703502259369\n","ReLU Activation - Max: 5.395091905656271 Min: 0.0\n","ReLU Activation - Max: 4.4643522936710545 Min: 0.0\n","Softmax Output - Max: 0.8900034302321023 Min: 1.4005701482567928e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1385346346497477 Min: -0.13788172410137642\n","Layer 1 - Gradient Weights Max: 0.13752311742049653 Min: -0.16421856882747893\n","Layer 0 - Gradient Weights Max: 0.14398946346497193 Min: -0.17727780669556606\n","ReLU Activation - Max: 6.5403031587788725 Min: 0.0\n","ReLU Activation - Max: 4.468766789584167 Min: 0.0\n","Softmax Output - Max: 0.9930885031581514 Min: 4.4238368115880515e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11493816371178686 Min: -0.11124149439137859\n","Layer 1 - Gradient Weights Max: 0.13823712774413371 Min: -0.12047060220958829\n","Layer 0 - Gradient Weights Max: 0.19598835520461252 Min: -0.16157389159519694\n","ReLU Activation - Max: 5.559252434203712 Min: 0.0\n","ReLU Activation - Max: 4.584117438106673 Min: 0.0\n","Softmax Output - Max: 0.9830233302585106 Min: 1.0529967019245648e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08942214664376213 Min: -0.08445282092577829\n","Layer 1 - Gradient Weights Max: 0.1582900535852757 Min: -0.13550184815092367\n","Layer 0 - Gradient Weights Max: 0.17239023982841362 Min: -0.16624460608883315\n","ReLU Activation - Max: 5.850950800419699 Min: 0.0\n","ReLU Activation - Max: 3.579564593864914 Min: 0.0\n","Softmax Output - Max: 0.9392184134291555 Min: 1.7820908309628445e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.09025166914485515 Min: -0.10798890974683434\n","Layer 1 - Gradient Weights Max: 0.12041735029317151 Min: -0.1373454936092486\n","Layer 0 - Gradient Weights Max: 0.16603435135241695 Min: -0.2718414079124522\n","ReLU Activation - Max: 4.590289381943499 Min: 0.0\n","ReLU Activation - Max: 4.410698155928325 Min: 0.0\n","Softmax Output - Max: 0.8861008208776442 Min: 4.01479253987365e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11279677268855226 Min: -0.1286369951712603\n","Layer 1 - Gradient Weights Max: 0.15151147594072933 Min: -0.13732664050889304\n","Layer 0 - Gradient Weights Max: 0.22917049140734966 Min: -0.21781959518163452\n","ReLU Activation - Max: 5.414102306409554 Min: 0.0\n","ReLU Activation - Max: 4.016071632374814 Min: 0.0\n","Softmax Output - Max: 0.9806439472218051 Min: 3.6760734971764785e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10469626714028586 Min: -0.0893512697061685\n","Layer 1 - Gradient Weights Max: 0.14118454665426552 Min: -0.23426565254079526\n","Layer 0 - Gradient Weights Max: 0.1636074285662547 Min: -0.1757028065610881\n","ReLU Activation - Max: 4.13538545720807 Min: 0.0\n","ReLU Activation - Max: 3.7638300230486785 Min: 0.0\n","Softmax Output - Max: 0.9820001863867962 Min: 5.981156478704368e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09938839360675442 Min: -0.11175356546820191\n","Layer 1 - Gradient Weights Max: 0.17543162506505555 Min: -0.1762093654592344\n","Layer 0 - Gradient Weights Max: 0.23960860211099344 Min: -0.16066337966993866\n","ReLU Activation - Max: 7.158839739583087 Min: 0.0\n","ReLU Activation - Max: 4.328860408143147 Min: 0.0\n","Softmax Output - Max: 0.9844309054600142 Min: 1.1997389473666307e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.06132476935581868 Min: -0.08926090909700446\n","Layer 1 - Gradient Weights Max: 0.12613477718763974 Min: -0.13292872578745685\n","Layer 0 - Gradient Weights Max: 0.25565508044190066 Min: -0.1982823435607172\n","ReLU Activation - Max: 5.340507537831187 Min: 0.0\n","ReLU Activation - Max: 3.7999322113443474 Min: 0.0\n","Softmax Output - Max: 0.9544839069682496 Min: 5.4083253969035415e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09663325967947016 Min: -0.13205719729663187\n","Layer 1 - Gradient Weights Max: 0.14642310470561676 Min: -0.17322216855077963\n","Layer 0 - Gradient Weights Max: 0.2025730417917059 Min: -0.18198846691846426\n","ReLU Activation - Max: 5.150651144626643 Min: 0.0\n","ReLU Activation - Max: 4.089192640752924 Min: 0.0\n","Softmax Output - Max: 0.9765751113371248 Min: 0.00012862891084220322 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.0799779711202643 Min: -0.07673864032572333\n","Layer 1 - Gradient Weights Max: 0.15501865252096425 Min: -0.18512538926516145\n","Layer 0 - Gradient Weights Max: 0.2305174166796036 Min: -0.18885023892325922\n","ReLU Activation - Max: 4.67332126768717 Min: 0.0\n","ReLU Activation - Max: 4.142569931084948 Min: 0.0\n","Softmax Output - Max: 0.9527330094617619 Min: 4.782974410863066e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08751860694495721 Min: -0.11330532484575463\n","Layer 1 - Gradient Weights Max: 0.14847368222756205 Min: -0.13468788143217975\n","Layer 0 - Gradient Weights Max: 0.20877216821504357 Min: -0.17939281036782856\n","ReLU Activation - Max: 6.143411938084936 Min: 0.0\n","ReLU Activation - Max: 4.366964997742479 Min: 0.0\n","Softmax Output - Max: 0.9672443568370791 Min: 6.044743584995865e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07016703511993182 Min: -0.11046096030668826\n","Layer 1 - Gradient Weights Max: 0.16603650412768844 Min: -0.11673071967944927\n","Layer 0 - Gradient Weights Max: 0.17060929287040397 Min: -0.21342692557793042\n","ReLU Activation - Max: 6.112569768941276 Min: 0.0\n","ReLU Activation - Max: 4.758051684008257 Min: 0.0\n","Softmax Output - Max: 0.9400788563631481 Min: 5.5331441805755845e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.16401819151879207 Min: -0.16670541112416407\n","Layer 1 - Gradient Weights Max: 0.19275756380340636 Min: -0.21254252247204003\n","Layer 0 - Gradient Weights Max: 0.15865726401107555 Min: -0.18114499810208953\n","ReLU Activation - Max: 5.201471876077077 Min: 0.0\n","ReLU Activation - Max: 4.489521455771332 Min: 0.0\n","Softmax Output - Max: 0.9885723752491643 Min: 6.587845390456545e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09759647221405113 Min: -0.09287297725625226\n","Layer 1 - Gradient Weights Max: 0.13343153873095168 Min: -0.13416495438134307\n","Layer 0 - Gradient Weights Max: 0.16119527101723652 Min: -0.15367374408856854\n","ReLU Activation - Max: 4.455780533275906 Min: 0.0\n","ReLU Activation - Max: 3.688053083484831 Min: 0.0\n","Softmax Output - Max: 0.9675332928774868 Min: 4.8289746298349916e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0594624487917642 Min: -0.11189136800866266\n","Layer 1 - Gradient Weights Max: 0.13837025247535018 Min: -0.15988690457013371\n","Layer 0 - Gradient Weights Max: 0.24445256215489541 Min: -0.18433670922976614\n","ReLU Activation - Max: 5.528518839023487 Min: 0.0\n","ReLU Activation - Max: 4.171166684821895 Min: 0.0\n","Softmax Output - Max: 0.929960587527436 Min: 5.5159311513067916e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07101240001506627 Min: -0.08726624372346992\n","Layer 1 - Gradient Weights Max: 0.16647531697427448 Min: -0.14392435940161136\n","Layer 0 - Gradient Weights Max: 0.14182956259287482 Min: -0.2143725505296406\n","ReLU Activation - Max: 4.946356979173749 Min: 0.0\n","ReLU Activation - Max: 4.162167091475597 Min: 0.0\n","Softmax Output - Max: 0.9540838325645019 Min: 1.558067438502537e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07286985450430304 Min: -0.07711924244622743\n","Layer 1 - Gradient Weights Max: 0.18861355492952525 Min: -0.11976126992217392\n","Layer 0 - Gradient Weights Max: 0.17993473233978532 Min: -0.1692109743812424\n","ReLU Activation - Max: 5.509524414243997 Min: 0.0\n","ReLU Activation - Max: 4.250491324579302 Min: 0.0\n","Softmax Output - Max: 0.997230101887154 Min: 6.620636907740377e-07 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08273497750108831 Min: -0.08471752907794106\n","Layer 1 - Gradient Weights Max: 0.11618201920999376 Min: -0.12064120547610113\n","Layer 0 - Gradient Weights Max: 0.194168069013374 Min: -0.19882862341768204\n","ReLU Activation - Max: 5.187554800640537 Min: 0.0\n","ReLU Activation - Max: 3.663700382090159 Min: 0.0\n","Softmax Output - Max: 0.982070925803287 Min: 4.6748254678230226e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10816124628758253 Min: -0.17189696192278212\n","Layer 1 - Gradient Weights Max: 0.18522130852579832 Min: -0.19571529601352033\n","Layer 0 - Gradient Weights Max: 0.1786222231739411 Min: -0.21020262923299293\n","ReLU Activation - Max: 4.597198262901238 Min: 0.0\n","ReLU Activation - Max: 3.8386528281447894 Min: 0.0\n","Softmax Output - Max: 0.980822878617608 Min: 9.505389019520044e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09136133513864884 Min: -0.13869750125872873\n","Layer 1 - Gradient Weights Max: 0.19581601844173954 Min: -0.15481019553446265\n","Layer 0 - Gradient Weights Max: 0.17263571940472794 Min: -0.20446285078011878\n","ReLU Activation - Max: 5.048383569703059 Min: 0.0\n","ReLU Activation - Max: 3.5543189805188695 Min: 0.0\n","Softmax Output - Max: 0.8807102162333672 Min: 2.6047982156023353e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07601416121509072 Min: -0.11476511094738678\n","Layer 1 - Gradient Weights Max: 0.11597410217639627 Min: -0.13975586530027373\n","Layer 0 - Gradient Weights Max: 0.19237274939056848 Min: -0.26858968223142715\n","ReLU Activation - Max: 4.90259945046633 Min: 0.0\n","ReLU Activation - Max: 4.617947419958324 Min: 0.0\n","Softmax Output - Max: 0.9791007919789896 Min: 2.3343007251637845e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11684273969012984 Min: -0.17754859754933658\n","Layer 1 - Gradient Weights Max: 0.14956792934803792 Min: -0.22865299097164957\n","Layer 0 - Gradient Weights Max: 0.2243490931737518 Min: -0.2748006132391401\n","ReLU Activation - Max: 6.21792273322127 Min: 0.0\n","ReLU Activation - Max: 4.312963883772173 Min: 0.0\n","Softmax Output - Max: 0.9833550703903692 Min: 4.609062851333196e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07425634454524396 Min: -0.14109526924291385\n","Layer 1 - Gradient Weights Max: 0.2749674872281541 Min: -0.26546533093306174\n","Layer 0 - Gradient Weights Max: 0.18602239319548197 Min: -0.2536973080113315\n","ReLU Activation - Max: 4.706901386993668 Min: 0.0\n","ReLU Activation - Max: 4.512589037637615 Min: 0.0\n","Softmax Output - Max: 0.9423653503022996 Min: 7.535701663229715e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.073666580966427 Min: -0.1271814147646172\n","Layer 1 - Gradient Weights Max: 0.10998334306444107 Min: -0.15417154276257922\n","Layer 0 - Gradient Weights Max: 0.17587606674047654 Min: -0.18083069263249701\n","ReLU Activation - Max: 4.3181478958599415 Min: 0.0\n","ReLU Activation - Max: 4.1167065671579035 Min: 0.0\n","Softmax Output - Max: 0.964109239901229 Min: 3.495733362086537e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08262185359081134 Min: -0.09292769983040994\n","Layer 1 - Gradient Weights Max: 0.1655170746550719 Min: -0.14014625634195943\n","Layer 0 - Gradient Weights Max: 0.15362944644347504 Min: -0.19669164470718667\n","ReLU Activation - Max: 4.902025870266534 Min: 0.0\n","ReLU Activation - Max: 4.784232429850855 Min: 0.0\n","Softmax Output - Max: 0.9919788355135697 Min: 1.2552773286819295e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11502482174895033 Min: -0.1424938733657972\n","Layer 1 - Gradient Weights Max: 0.17142752524423552 Min: -0.13568429134206525\n","Layer 0 - Gradient Weights Max: 0.2243992563386488 Min: -0.17008560557207897\n","ReLU Activation - Max: 5.80234581352154 Min: 0.0\n","ReLU Activation - Max: 4.635671329451265 Min: 0.0\n","Softmax Output - Max: 0.9824035443880644 Min: 5.80848316367477e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.18045163668034572 Min: -0.08826367830089951\n","Layer 1 - Gradient Weights Max: 0.20179378480873308 Min: -0.15263168083126208\n","Layer 0 - Gradient Weights Max: 0.25491337447498824 Min: -0.19860318028674662\n","ReLU Activation - Max: 4.73178716388393 Min: 0.0\n","ReLU Activation - Max: 4.876915510681116 Min: 0.0\n","Softmax Output - Max: 0.9856050498593738 Min: 5.212769364909786e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07766616585926654 Min: -0.1206613038769225\n","Layer 1 - Gradient Weights Max: 0.15974375647221595 Min: -0.15155727110102263\n","Layer 0 - Gradient Weights Max: 0.19238980297390304 Min: -0.16620505846752537\n","ReLU Activation - Max: 5.73894901621324 Min: 0.0\n","ReLU Activation - Max: 4.324481954771408 Min: 0.0\n","Softmax Output - Max: 0.9874967718986802 Min: 1.5423646621404897e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07019800227947694 Min: -0.12759059735508863\n","Layer 1 - Gradient Weights Max: 0.11582920731115577 Min: -0.1276130011848592\n","Layer 0 - Gradient Weights Max: 0.19384541539623207 Min: -0.19910778663591092\n","ReLU Activation - Max: 4.901330269631133 Min: 0.0\n","ReLU Activation - Max: 3.7977498150639075 Min: 0.0\n","Softmax Output - Max: 0.9591961555391503 Min: 0.00012273400242892538 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12819978681958608 Min: -0.07150673975107727\n","Layer 1 - Gradient Weights Max: 0.15490999744020714 Min: -0.11876700776926503\n","Layer 0 - Gradient Weights Max: 0.20665428680452913 Min: -0.20153055758165234\n","ReLU Activation - Max: 4.8836180597998 Min: 0.0\n","ReLU Activation - Max: 4.6195437643904835 Min: 0.0\n","Softmax Output - Max: 0.9866968894291634 Min: 1.689983442193883e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09284467875167554 Min: -0.08920736186794082\n","Layer 1 - Gradient Weights Max: 0.15602607362474535 Min: -0.13999429018415185\n","Layer 0 - Gradient Weights Max: 0.1588499024887314 Min: -0.1425586113800112\n","ReLU Activation - Max: 5.513600940234665 Min: 0.0\n","ReLU Activation - Max: 4.228606510919507 Min: 0.0\n","Softmax Output - Max: 0.9724035172459738 Min: 3.978644070704333e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07670812258709515 Min: -0.12310582162287179\n","Layer 1 - Gradient Weights Max: 0.1675292957706453 Min: -0.13300642633328977\n","Layer 0 - Gradient Weights Max: 0.16321066344407711 Min: -0.15817536857371953\n","ReLU Activation - Max: 5.7465421619499235 Min: 0.0\n","ReLU Activation - Max: 4.156017616701832 Min: 0.0\n","Softmax Output - Max: 0.9045357557302216 Min: 0.0001097827766498702 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06638412058984428 Min: -0.11592052333280245\n","Layer 1 - Gradient Weights Max: 0.13022664476720736 Min: -0.19079275502915718\n","Layer 0 - Gradient Weights Max: 0.1849113797234331 Min: -0.19836736510457306\n","ReLU Activation - Max: 6.071882485778616 Min: 0.0\n","ReLU Activation - Max: 3.77113418815746 Min: 0.0\n","Softmax Output - Max: 0.9316351716639524 Min: 1.5237218096888722e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11027172372757688 Min: -0.08991160337689802\n","Layer 1 - Gradient Weights Max: 0.13285984380684165 Min: -0.18851083448495437\n","Layer 0 - Gradient Weights Max: 0.21682091585809596 Min: -0.16543818941652036\n","ReLU Activation - Max: 5.523017821773272 Min: 0.0\n","ReLU Activation - Max: 5.118955388398719 Min: 0.0\n","Softmax Output - Max: 0.9954932904457616 Min: 3.089130990642566e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07258435882516165 Min: -0.10546903780364852\n","Layer 1 - Gradient Weights Max: 0.1323391223890701 Min: -0.10870213790787245\n","Layer 0 - Gradient Weights Max: 0.18110799497015434 Min: -0.19264368942753787\n","ReLU Activation - Max: 4.952974871428142 Min: 0.0\n","ReLU Activation - Max: 4.499933666486839 Min: 0.0\n","Softmax Output - Max: 0.9598394725731086 Min: 3.980100989338303e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13544725286512763 Min: -0.1520486455527533\n","Layer 1 - Gradient Weights Max: 0.20507294068411264 Min: -0.1957241829359267\n","Layer 0 - Gradient Weights Max: 0.1978104548638777 Min: -0.20288530192329376\n","ReLU Activation - Max: 5.385696584539071 Min: 0.0\n","ReLU Activation - Max: 3.8604790535542524 Min: 0.0\n","Softmax Output - Max: 0.9698286121523607 Min: 2.0927525655989805e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08375825005447914 Min: -0.09604404070837237\n","Layer 1 - Gradient Weights Max: 0.17228380549319375 Min: -0.16749956105408617\n","Layer 0 - Gradient Weights Max: 0.24913250710954174 Min: -0.199684593486626\n","ReLU Activation - Max: 4.736861910360848 Min: 0.0\n","ReLU Activation - Max: 3.7817533396676373 Min: 0.0\n","Softmax Output - Max: 0.9451405304710352 Min: 0.00011338709945852837 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08666963044987973 Min: -0.12693649105922528\n","Layer 1 - Gradient Weights Max: 0.17562189073404086 Min: -0.14442788441006382\n","Layer 0 - Gradient Weights Max: 0.1458679772882294 Min: -0.14803694400573225\n","ReLU Activation - Max: 5.936095366981094 Min: 0.0\n","ReLU Activation - Max: 5.371058455365671 Min: 0.0\n","Softmax Output - Max: 0.9607159209423263 Min: 9.116831638320874e-08 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14488451333651123 Min: -0.09013638491104732\n","Layer 1 - Gradient Weights Max: 0.13927386093163122 Min: -0.16832298043292476\n","Layer 0 - Gradient Weights Max: 0.16551413632035997 Min: -0.16148727473093316\n","ReLU Activation - Max: 3.99394416829583 Min: 0.0\n","ReLU Activation - Max: 3.960844605855382 Min: 0.0\n","Softmax Output - Max: 0.982602785122398 Min: 7.095845301536165e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06561977939522237 Min: -0.13882758344069854\n","Layer 1 - Gradient Weights Max: 0.14857390825501704 Min: -0.16160239770856386\n","Layer 0 - Gradient Weights Max: 0.1388578773824147 Min: -0.1867824850245153\n","ReLU Activation - Max: 4.125510843854602 Min: 0.0\n","ReLU Activation - Max: 3.8376633575249555 Min: 0.0\n","Softmax Output - Max: 0.9280428772885742 Min: 0.00014968493240499517 Sum (first example): 0.9999999999999997\n","Layer 2 - Gradient Weights Max: 0.10067692547071701 Min: -0.12259051998437485\n","Layer 1 - Gradient Weights Max: 0.18369734895200204 Min: -0.16963202571169395\n","Layer 0 - Gradient Weights Max: 0.21687407080251692 Min: -0.16933340516633616\n","ReLU Activation - Max: 4.361269715141154 Min: 0.0\n","ReLU Activation - Max: 3.780585585582906 Min: 0.0\n","Softmax Output - Max: 0.9348505559193757 Min: 6.468054028465054e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09170064263475855 Min: -0.16069213288266007\n","Layer 1 - Gradient Weights Max: 0.17622631369214978 Min: -0.23409531963783925\n","Layer 0 - Gradient Weights Max: 0.22054158338955765 Min: -0.17147219886799075\n","ReLU Activation - Max: 4.710850161223551 Min: 0.0\n","ReLU Activation - Max: 3.7269880361087813 Min: 0.0\n","Softmax Output - Max: 0.9643694089601295 Min: 2.5033789429054423e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12732745037856402 Min: -0.08872806362748427\n","Layer 1 - Gradient Weights Max: 0.14301017972029018 Min: -0.16470188133391483\n","Layer 0 - Gradient Weights Max: 0.22915300276711034 Min: -0.19758681935454928\n","ReLU Activation - Max: 6.09278468767106 Min: 0.0\n","ReLU Activation - Max: 4.671891145527862 Min: 0.0\n","Softmax Output - Max: 0.9808491558881146 Min: 3.643564269525393e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11248739820880807 Min: -0.09940267615593174\n","Layer 1 - Gradient Weights Max: 0.11975314949329886 Min: -0.15373941639668795\n","Layer 0 - Gradient Weights Max: 0.1805088659538743 Min: -0.15320782029029062\n","ReLU Activation - Max: 5.3674646134672415 Min: 0.0\n","ReLU Activation - Max: 4.110809190674881 Min: 0.0\n","Softmax Output - Max: 0.9813159942249456 Min: 0.00010783671424331093 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10453799927730215 Min: -0.12627883553443195\n","Layer 1 - Gradient Weights Max: 0.14290510114280378 Min: -0.16017326921760375\n","Layer 0 - Gradient Weights Max: 0.15672057807075257 Min: -0.18872845198717822\n","ReLU Activation - Max: 4.370405908189545 Min: 0.0\n","ReLU Activation - Max: 3.567970949361378 Min: 0.0\n","Softmax Output - Max: 0.9686120638211153 Min: 2.0751030791244952e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09732681378029825 Min: -0.10129484468480439\n","Layer 1 - Gradient Weights Max: 0.17801433326273877 Min: -0.1633849489290856\n","Layer 0 - Gradient Weights Max: 0.24562660363847608 Min: -0.2879526548838008\n","ReLU Activation - Max: 6.31336551279809 Min: 0.0\n","ReLU Activation - Max: 4.464847229124217 Min: 0.0\n","Softmax Output - Max: 0.9601590381675162 Min: 1.537751592817078e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06762600072636434 Min: -0.08340308961898293\n","Layer 1 - Gradient Weights Max: 0.1470287117084829 Min: -0.13209248992972977\n","Layer 0 - Gradient Weights Max: 0.2387583466823256 Min: -0.17647891582547626\n","ReLU Activation - Max: 5.611484691335108 Min: 0.0\n","ReLU Activation - Max: 3.7548157285952426 Min: 0.0\n","Softmax Output - Max: 0.9725351810824305 Min: 2.3102360831621337e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09903251982780004 Min: -0.1417598430623787\n","Layer 1 - Gradient Weights Max: 0.13738613338298886 Min: -0.1869741414316466\n","Layer 0 - Gradient Weights Max: 0.17146409335248733 Min: -0.13993476201026211\n","ReLU Activation - Max: 5.424105641585615 Min: 0.0\n","ReLU Activation - Max: 4.14303661268635 Min: 0.0\n","Softmax Output - Max: 0.952253315345419 Min: 3.9600087138948076e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09695166572158885 Min: -0.09169941026700928\n","Layer 1 - Gradient Weights Max: 0.14376955112190543 Min: -0.24645742354937974\n","Layer 0 - Gradient Weights Max: 0.22046825821212085 Min: -0.19315018091971903\n","ReLU Activation - Max: 5.334633161151342 Min: 0.0\n","ReLU Activation - Max: 4.278134815396716 Min: 0.0\n","Softmax Output - Max: 0.9198897775521298 Min: 5.761301079930163e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0775667350069189 Min: -0.13404508081745958\n","Layer 1 - Gradient Weights Max: 0.13253843866862344 Min: -0.17733340147715265\n","Layer 0 - Gradient Weights Max: 0.23711247438132513 Min: -0.1988899105338916\n","ReLU Activation - Max: 4.463107862313811 Min: 0.0\n","ReLU Activation - Max: 4.301013353893799 Min: 0.0\n","Softmax Output - Max: 0.9805043382473277 Min: 3.369264739377192e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09383945616964785 Min: -0.1333420332093917\n","Layer 1 - Gradient Weights Max: 0.16333215171312052 Min: -0.1603628782405006\n","Layer 0 - Gradient Weights Max: 0.24742331721554658 Min: -0.2262978309233156\n","ReLU Activation - Max: 4.929834796899622 Min: 0.0\n","ReLU Activation - Max: 3.825193088990451 Min: 0.0\n","Softmax Output - Max: 0.9840311820667993 Min: 7.599209669642561e-07 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.09037628166834312 Min: -0.11263192876836356\n","Layer 1 - Gradient Weights Max: 0.18727224295787676 Min: -0.1586946918719026\n","Layer 0 - Gradient Weights Max: 0.22269303626395864 Min: -0.1596497950721163\n","ReLU Activation - Max: 5.582627665270873 Min: 0.0\n","ReLU Activation - Max: 4.650361680302395 Min: 0.0\n","Softmax Output - Max: 0.9741065591707412 Min: 2.4577561921321054e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10832290760429746 Min: -0.1291466493536887\n","Layer 1 - Gradient Weights Max: 0.1518259538232978 Min: -0.16448945593516673\n","Layer 0 - Gradient Weights Max: 0.24089635363191841 Min: -0.20139898650881047\n","ReLU Activation - Max: 5.8930642519126595 Min: 0.0\n","ReLU Activation - Max: 4.237349664723218 Min: 0.0\n","Softmax Output - Max: 0.948625345844927 Min: 6.0013177322064526e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12884970393466152 Min: -0.23060593113006797\n","Layer 1 - Gradient Weights Max: 0.1603070016650535 Min: -0.22790228750015473\n","Layer 0 - Gradient Weights Max: 0.14329240174165644 Min: -0.1753654145197961\n","ReLU Activation - Max: 4.2075814599256 Min: 0.0\n","ReLU Activation - Max: 4.30322334908698 Min: 0.0\n","Softmax Output - Max: 0.978726619485173 Min: 1.2717507250672254e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08686147600760445 Min: -0.09067886066237156\n","Layer 1 - Gradient Weights Max: 0.13947911836508253 Min: -0.16346016554105222\n","Layer 0 - Gradient Weights Max: 0.1901751688005522 Min: -0.1570556136673239\n","ReLU Activation - Max: 4.83669447430541 Min: 0.0\n","ReLU Activation - Max: 3.685416694491141 Min: 0.0\n","Softmax Output - Max: 0.9827673423188341 Min: 1.617353056949635e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1235169505171324 Min: -0.0771429936578032\n","Layer 1 - Gradient Weights Max: 0.11685405766157918 Min: -0.1656970232373434\n","Layer 0 - Gradient Weights Max: 0.1798189184859956 Min: -0.1727389773715506\n","ReLU Activation - Max: 4.502282135015506 Min: 0.0\n","ReLU Activation - Max: 4.587111129883915 Min: 0.0\n","Softmax Output - Max: 0.8840392587310437 Min: 3.9844054311011695e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07731402054656407 Min: -0.07935398345799946\n","Layer 1 - Gradient Weights Max: 0.14448461196787765 Min: -0.15628369530290448\n","Layer 0 - Gradient Weights Max: 0.22337051968678762 Min: -0.20792261113467964\n","ReLU Activation - Max: 5.053169765064958 Min: 0.0\n","ReLU Activation - Max: 3.4452921014705384 Min: 0.0\n","Softmax Output - Max: 0.9365848864083124 Min: 7.70046426644743e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.11351884851209473 Min: -0.12411574576315035\n","Layer 1 - Gradient Weights Max: 0.13960699956185282 Min: -0.22643828350566467\n","Layer 0 - Gradient Weights Max: 0.18261424788290243 Min: -0.2312740660774672\n","ReLU Activation - Max: 5.0447526818228 Min: 0.0\n","ReLU Activation - Max: 5.067373723050972 Min: 0.0\n","Softmax Output - Max: 0.9905685163419491 Min: 1.6578943592624337e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.1321232445219617 Min: -0.09756819267963113\n","Layer 1 - Gradient Weights Max: 0.13437591114437675 Min: -0.16755438481786347\n","Layer 0 - Gradient Weights Max: 0.16595629373924864 Min: -0.1596202898843018\n","ReLU Activation - Max: 5.417714905493844 Min: 0.0\n","ReLU Activation - Max: 4.512481979863687 Min: 0.0\n","Softmax Output - Max: 0.9153807781705987 Min: 4.414224079427396e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09850401028910125 Min: -0.18974432023250304\n","Layer 1 - Gradient Weights Max: 0.2236649116645867 Min: -0.20980404684570733\n","Layer 0 - Gradient Weights Max: 0.22879740564643913 Min: -0.21783153205592348\n","ReLU Activation - Max: 4.192759186673629 Min: 0.0\n","ReLU Activation - Max: 3.5854129070511385 Min: 0.0\n","Softmax Output - Max: 0.979840174233705 Min: 7.246523358871852e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10089782854531397 Min: -0.11319057758021381\n","Layer 1 - Gradient Weights Max: 0.18700340181851893 Min: -0.19749848480556706\n","Layer 0 - Gradient Weights Max: 0.1760061878777249 Min: -0.20552244286066526\n","ReLU Activation - Max: 5.046560299299 Min: 0.0\n","ReLU Activation - Max: 4.041948489704054 Min: 0.0\n","Softmax Output - Max: 0.9398435853815481 Min: 1.4960047472885056e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09107987840958778 Min: -0.13655934198859804\n","Layer 1 - Gradient Weights Max: 0.14658822570187713 Min: -0.18961615033039983\n","Layer 0 - Gradient Weights Max: 0.3255862522152614 Min: -0.3167710065182939\n","ReLU Activation - Max: 4.6302779527443905 Min: 0.0\n","ReLU Activation - Max: 4.0722794458455 Min: 0.0\n","Softmax Output - Max: 0.9798469598263496 Min: 9.265347074725465e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08011382491947393 Min: -0.11906636083429703\n","Layer 1 - Gradient Weights Max: 0.15547990784814392 Min: -0.2291298159376428\n","Layer 0 - Gradient Weights Max: 0.2761089728947332 Min: -0.21209197622064105\n","ReLU Activation - Max: 4.475390975570055 Min: 0.0\n","ReLU Activation - Max: 4.3086078022638175 Min: 0.0\n","Softmax Output - Max: 0.8763150227048201 Min: 1.6504055917598758e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10503479801678198 Min: -0.09852189364553567\n","Layer 1 - Gradient Weights Max: 0.18000761083094388 Min: -0.14185423204790426\n","Layer 0 - Gradient Weights Max: 0.22251448076419242 Min: -0.19968670729464638\n","ReLU Activation - Max: 5.47602115273635 Min: 0.0\n","ReLU Activation - Max: 3.9472377520850706 Min: 0.0\n","Softmax Output - Max: 0.932135544809673 Min: 5.382169739382452e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11259076730368127 Min: -0.06516346332117222\n","Layer 1 - Gradient Weights Max: 0.13622209826419013 Min: -0.15500280504912525\n","Layer 0 - Gradient Weights Max: 0.18326395121981437 Min: -0.23939330265432773\n","ReLU Activation - Max: 5.7428037320070064 Min: 0.0\n","ReLU Activation - Max: 4.908540926686888 Min: 0.0\n","Softmax Output - Max: 0.9666786599120086 Min: 3.5435027270639376e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08762889962648968 Min: -0.11485934876772487\n","Layer 1 - Gradient Weights Max: 0.15391986807379554 Min: -0.13101794131610256\n","Layer 0 - Gradient Weights Max: 0.22869188590903777 Min: -0.261420762131943\n","ReLU Activation - Max: 5.665684619215879 Min: 0.0\n","ReLU Activation - Max: 4.810382409832458 Min: 0.0\n","Softmax Output - Max: 0.9324167458397608 Min: 2.1426900478297167e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10357355227867832 Min: -0.12348697957082659\n","Layer 1 - Gradient Weights Max: 0.13151615896183336 Min: -0.13977975952877508\n","Layer 0 - Gradient Weights Max: 0.20558274077209032 Min: -0.2160308726082232\n","ReLU Activation - Max: 4.501898835566569 Min: 0.0\n","ReLU Activation - Max: 4.755898728358384 Min: 0.0\n","Softmax Output - Max: 0.9132764817181028 Min: 3.691500826640855e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1563703861670985 Min: -0.13027187894706294\n","Layer 1 - Gradient Weights Max: 0.11289299318992704 Min: -0.1828038292421536\n","Layer 0 - Gradient Weights Max: 0.14087934576395372 Min: -0.16852419525737208\n","ReLU Activation - Max: 4.816503388047216 Min: 0.0\n","ReLU Activation - Max: 5.03743299093287 Min: 0.0\n","Softmax Output - Max: 0.9947726636294808 Min: 3.007150650392661e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07732074017602607 Min: -0.11562530362499165\n","Layer 1 - Gradient Weights Max: 0.17220375764019955 Min: -0.13052818362663332\n","Layer 0 - Gradient Weights Max: 0.1570671528309126 Min: -0.1827515726815726\n","ReLU Activation - Max: 4.795039031414301 Min: 0.0\n","ReLU Activation - Max: 4.548530929647352 Min: 0.0\n","Softmax Output - Max: 0.9784396779161801 Min: 3.782802354376314e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08731157109984201 Min: -0.1255195536060739\n","Layer 1 - Gradient Weights Max: 0.13511116389997174 Min: -0.14633880822336623\n","Layer 0 - Gradient Weights Max: 0.19134853783940411 Min: -0.13586938066519996\n","ReLU Activation - Max: 4.218821864703834 Min: 0.0\n","ReLU Activation - Max: 4.0475575904945735 Min: 0.0\n","Softmax Output - Max: 0.9752313212623375 Min: 1.0241990169516732e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08682812983672493 Min: -0.14491980952309177\n","Layer 1 - Gradient Weights Max: 0.11960819639202817 Min: -0.16922739308176513\n","Layer 0 - Gradient Weights Max: 0.1768587108842084 Min: -0.17715649146401127\n","ReLU Activation - Max: 5.105890462669285 Min: 0.0\n","ReLU Activation - Max: 3.874987626606437 Min: 0.0\n","Softmax Output - Max: 0.9925188638864307 Min: 8.691774807159613e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13250285607975387 Min: -0.11492203726198001\n","Layer 1 - Gradient Weights Max: 0.1879841464800405 Min: -0.1748097319588879\n","Layer 0 - Gradient Weights Max: 0.1439411468197008 Min: -0.18108540054679043\n","ReLU Activation - Max: 5.143535434774259 Min: 0.0\n","ReLU Activation - Max: 4.001609379270424 Min: 0.0\n","Softmax Output - Max: 0.9519092269699754 Min: 7.077027220111098e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06882145596085455 Min: -0.06708478577217798\n","Layer 1 - Gradient Weights Max: 0.17140990658808447 Min: -0.17920604039454244\n","Layer 0 - Gradient Weights Max: 0.15710591709553123 Min: -0.17411851881051355\n","ReLU Activation - Max: 5.055389291225088 Min: 0.0\n","ReLU Activation - Max: 4.259495788324074 Min: 0.0\n","Softmax Output - Max: 0.9859746060585419 Min: 5.578900769611276e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1350104537532862 Min: -0.11513107924913582\n","Layer 1 - Gradient Weights Max: 0.11637149112487849 Min: -0.2164519583485212\n","Layer 0 - Gradient Weights Max: 0.16057733170778074 Min: -0.1950269664835879\n","ReLU Activation - Max: 5.506474332827567 Min: 0.0\n","ReLU Activation - Max: 4.16287289973431 Min: 0.0\n","Softmax Output - Max: 0.9508503516561143 Min: 2.3605016678331854e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12712195867755383 Min: -0.1622049161596391\n","Layer 1 - Gradient Weights Max: 0.1413521130714277 Min: -0.1891094772758837\n","Layer 0 - Gradient Weights Max: 0.18203683318524008 Min: -0.17417092653154984\n","ReLU Activation - Max: 5.755949945525626 Min: 0.0\n","ReLU Activation - Max: 3.796380562182576 Min: 0.0\n","Softmax Output - Max: 0.9907494603076025 Min: 4.4072261110549666e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.07458853828183996 Min: -0.1446267356442011\n","Layer 1 - Gradient Weights Max: 0.1583977023826126 Min: -0.16548517136477414\n","Layer 0 - Gradient Weights Max: 0.17741073503414595 Min: -0.23283873420871867\n","ReLU Activation - Max: 4.186516341838046 Min: 0.0\n","ReLU Activation - Max: 4.0713497493872985 Min: 0.0\n","Softmax Output - Max: 0.968050125730342 Min: 1.6994077326337757e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.06172600104981973 Min: -0.15801774810754116\n","Layer 1 - Gradient Weights Max: 0.14417966538269422 Min: -0.16917837113805445\n","Layer 0 - Gradient Weights Max: 0.197895754536756 Min: -0.17095831277318455\n","ReLU Activation - Max: 5.303059741642711 Min: 0.0\n","ReLU Activation - Max: 4.5588987603895355 Min: 0.0\n","Softmax Output - Max: 0.9458338831795241 Min: 2.1833367523007334e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0854222337151713 Min: -0.13389208485621873\n","Layer 1 - Gradient Weights Max: 0.1329872406357413 Min: -0.16751126492642912\n","Layer 0 - Gradient Weights Max: 0.1595801653624549 Min: -0.1882473295786398\n","ReLU Activation - Max: 5.305571210232485 Min: 0.0\n","ReLU Activation - Max: 4.291914832664625 Min: 0.0\n","Softmax Output - Max: 0.9916408537369743 Min: 2.965637511851421e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1124955932786606 Min: -0.13793126361415303\n","Layer 1 - Gradient Weights Max: 0.17324544072330303 Min: -0.14693773745805327\n","Layer 0 - Gradient Weights Max: 0.2541215929097007 Min: -0.13681001293729733\n","ReLU Activation - Max: 4.1881883173786445 Min: 0.0\n","ReLU Activation - Max: 4.261037448284198 Min: 0.0\n","Softmax Output - Max: 0.9463276987233917 Min: 8.663394019561655e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0896777398683201 Min: -0.15163670473511087\n","Layer 1 - Gradient Weights Max: 0.14523537364038622 Min: -0.15537311740253948\n","Layer 0 - Gradient Weights Max: 0.16781119976218217 Min: -0.21320021034568676\n","ReLU Activation - Max: 5.928658535117145 Min: 0.0\n","ReLU Activation - Max: 4.567617857433063 Min: 0.0\n","Softmax Output - Max: 0.9814724454090027 Min: 1.1235935781705863e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07970031538822844 Min: -0.07445135153283516\n","Layer 1 - Gradient Weights Max: 0.18306976770790126 Min: -0.13179927512189418\n","Layer 0 - Gradient Weights Max: 0.16020801327564627 Min: -0.23569527252984673\n","ReLU Activation - Max: 5.477665923280603 Min: 0.0\n","ReLU Activation - Max: 4.22487867139258 Min: 0.0\n","Softmax Output - Max: 0.9921414180826434 Min: 1.2998975337438562e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08885578844834637 Min: -0.08880880133505256\n","Layer 1 - Gradient Weights Max: 0.15938504973660308 Min: -0.14691936672817033\n","Layer 0 - Gradient Weights Max: 0.2021847612058776 Min: -0.15152436400310282\n","ReLU Activation - Max: 5.051931743198907 Min: 0.0\n","ReLU Activation - Max: 3.943621851548398 Min: 0.0\n","Softmax Output - Max: 0.967341882788822 Min: 2.5660029063566056e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09555687625580651 Min: -0.13212636394067437\n","Layer 1 - Gradient Weights Max: 0.15490916007976385 Min: -0.1479505613205056\n","Layer 0 - Gradient Weights Max: 0.20259182286461566 Min: -0.18640725175464853\n","ReLU Activation - Max: 4.565667126508688 Min: 0.0\n","ReLU Activation - Max: 3.9306834113531437 Min: 0.0\n","Softmax Output - Max: 0.9754388767406065 Min: 7.349959170912706e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07669854282858328 Min: -0.15636139959234116\n","Layer 1 - Gradient Weights Max: 0.12062688439343974 Min: -0.1886811388014474\n","Layer 0 - Gradient Weights Max: 0.158238264708589 Min: -0.15572109245748\n","ReLU Activation - Max: 5.301610718757868 Min: 0.0\n","ReLU Activation - Max: 4.726347752947961 Min: 0.0\n","Softmax Output - Max: 0.9828387522696057 Min: 3.4970977416646614e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08317532041865319 Min: -0.1259973533100585\n","Layer 1 - Gradient Weights Max: 0.12102524865151144 Min: -0.13953998702474182\n","Layer 0 - Gradient Weights Max: 0.17094455493256377 Min: -0.19991102641417108\n","ReLU Activation - Max: 5.097992478097627 Min: 0.0\n","ReLU Activation - Max: 4.454857029562717 Min: 0.0\n","Softmax Output - Max: 0.9490435675964985 Min: 9.763194808948672e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09109248537004204 Min: -0.12796020108186776\n","Layer 1 - Gradient Weights Max: 0.17262810689825697 Min: -0.21874395713262024\n","Layer 0 - Gradient Weights Max: 0.18397762211059412 Min: -0.2579587157389135\n","ReLU Activation - Max: 5.185791500647495 Min: 0.0\n","ReLU Activation - Max: 3.9838653804389366 Min: 0.0\n","Softmax Output - Max: 0.9532793267168356 Min: 2.982738366441069e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09045259033810886 Min: -0.098280373856071\n","Layer 1 - Gradient Weights Max: 0.197784828450806 Min: -0.15115647807670607\n","Layer 0 - Gradient Weights Max: 0.22302254003510252 Min: -0.24366872997719943\n","ReLU Activation - Max: 5.529979802468097 Min: 0.0\n","ReLU Activation - Max: 4.017280347023215 Min: 0.0\n","Softmax Output - Max: 0.9806946515721608 Min: 4.7868750791388336e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1038201106239754 Min: -0.12106995020241819\n","Layer 1 - Gradient Weights Max: 0.1610608400571215 Min: -0.14952995422376986\n","Layer 0 - Gradient Weights Max: 0.18598106247122717 Min: -0.16194974859446565\n","ReLU Activation - Max: 4.885329042257676 Min: 0.0\n","ReLU Activation - Max: 4.099983018062475 Min: 0.0\n","Softmax Output - Max: 0.9830627633732831 Min: 3.304138067168932e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.137060280989625 Min: -0.14294723470808854\n","Layer 1 - Gradient Weights Max: 0.18167451193620326 Min: -0.23173138973818708\n","Layer 0 - Gradient Weights Max: 0.19892884649836265 Min: -0.21630144643393392\n","ReLU Activation - Max: 5.2211830514444 Min: 0.0\n","ReLU Activation - Max: 4.199728397158241 Min: 0.0\n","Softmax Output - Max: 0.9844990092569963 Min: 8.407257591810364e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.16988403121119702 Min: -0.1213521907187176\n","Layer 1 - Gradient Weights Max: 0.17082712374073414 Min: -0.21596227861591422\n","Layer 0 - Gradient Weights Max: 0.16627657669880638 Min: -0.1715796675456636\n","ReLU Activation - Max: 6.297590000676028 Min: 0.0\n","ReLU Activation - Max: 4.532347072937138 Min: 0.0\n","Softmax Output - Max: 0.9545068575281701 Min: 5.393172695707651e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11104392696828119 Min: -0.09455815551082061\n","Layer 1 - Gradient Weights Max: 0.14569886741182903 Min: -0.17040363714351323\n","Layer 0 - Gradient Weights Max: 0.1969387161688025 Min: -0.1958450190618733\n","ReLU Activation - Max: 4.792509834083843 Min: 0.0\n","ReLU Activation - Max: 4.019782520032169 Min: 0.0\n","Softmax Output - Max: 0.9708074915985299 Min: 5.7345873151614e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10117197057866521 Min: -0.18991581589474574\n","Layer 1 - Gradient Weights Max: 0.14888123195200817 Min: -0.17180713190673616\n","Layer 0 - Gradient Weights Max: 0.22579259808621893 Min: -0.17881584727867436\n","ReLU Activation - Max: 4.791322932210734 Min: 0.0\n","ReLU Activation - Max: 4.112501447029941 Min: 0.0\n","Softmax Output - Max: 0.9693285642729657 Min: 4.8174935569235815e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09906803103175839 Min: -0.10712661967111387\n","Layer 1 - Gradient Weights Max: 0.1305988788144588 Min: -0.17212754654543322\n","Layer 0 - Gradient Weights Max: 0.17252977451431623 Min: -0.1928967117087347\n","ReLU Activation - Max: 4.393446811232282 Min: 0.0\n","ReLU Activation - Max: 4.159201146392189 Min: 0.0\n","Softmax Output - Max: 0.9837564974550967 Min: 1.4795678048670535e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12689710501173151 Min: -0.07904737812519609\n","Layer 1 - Gradient Weights Max: 0.1455607174662326 Min: -0.1560221719482225\n","Layer 0 - Gradient Weights Max: 0.16031595760534625 Min: -0.17464508562390596\n","ReLU Activation - Max: 6.468068236829113 Min: 0.0\n","ReLU Activation - Max: 4.502860361308988 Min: 0.0\n","Softmax Output - Max: 0.9824766214223185 Min: 1.4048479762068137e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.06190420236856515 Min: -0.12357337211039598\n","Layer 1 - Gradient Weights Max: 0.13999110252624075 Min: -0.21196413471724834\n","Layer 0 - Gradient Weights Max: 0.15212496018760685 Min: -0.17322423305437684\n","ReLU Activation - Max: 4.7318933965266945 Min: 0.0\n","ReLU Activation - Max: 3.563499126604859 Min: 0.0\n","Softmax Output - Max: 0.9375276671575123 Min: 8.559039046578304e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.076501256197805 Min: -0.10502984496547126\n","Layer 1 - Gradient Weights Max: 0.13716772320716306 Min: -0.1699914913567046\n","Layer 0 - Gradient Weights Max: 0.19331087047657117 Min: -0.2813209054247726\n","ReLU Activation - Max: 5.291841458072054 Min: 0.0\n","ReLU Activation - Max: 4.517838993472209 Min: 0.0\n","Softmax Output - Max: 0.9710678266230263 Min: 4.3848025100634126e-07 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09582462730865296 Min: -0.11663209680866167\n","Layer 1 - Gradient Weights Max: 0.14392396086644796 Min: -0.16843817676345033\n","Layer 0 - Gradient Weights Max: 0.2022805306792235 Min: -0.167025872391664\n","ReLU Activation - Max: 4.857813783840223 Min: 0.0\n","ReLU Activation - Max: 4.3974680307211 Min: 0.0\n","Softmax Output - Max: 0.9716039570695462 Min: 2.7565440882580317e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08979147909139558 Min: -0.11256472078855352\n","Layer 1 - Gradient Weights Max: 0.11786589807664688 Min: -0.14723528450932763\n","Layer 0 - Gradient Weights Max: 0.215066543371612 Min: -0.238544188753562\n","ReLU Activation - Max: 4.428308185263068 Min: 0.0\n","ReLU Activation - Max: 4.09023564044367 Min: 0.0\n","Softmax Output - Max: 0.9690615898137384 Min: 5.79286515219206e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17588637704161736 Min: -0.10260201556315802\n","Layer 1 - Gradient Weights Max: 0.2018654770115832 Min: -0.21851294739137725\n","Layer 0 - Gradient Weights Max: 0.2339167470663707 Min: -0.231887841706775\n","ReLU Activation - Max: 6.637097793667839 Min: 0.0\n","ReLU Activation - Max: 5.057765110840699 Min: 0.0\n","Softmax Output - Max: 0.9564046426152655 Min: 2.6899373853508697e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.18627338303880175 Min: -0.2913029495458172\n","Layer 1 - Gradient Weights Max: 0.25408376786178405 Min: -0.21689410316103283\n","Layer 0 - Gradient Weights Max: 0.18658945000517851 Min: -0.19266240578376825\n","ReLU Activation - Max: 6.189054544697111 Min: 0.0\n","ReLU Activation - Max: 3.707250360675408 Min: 0.0\n","Softmax Output - Max: 0.9645318962334065 Min: 2.7507953700735588e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10745733020043986 Min: -0.09088075865738614\n","Layer 1 - Gradient Weights Max: 0.14660239431246883 Min: -0.18348535649704384\n","Layer 0 - Gradient Weights Max: 0.2330553753694991 Min: -0.2166620817647187\n","ReLU Activation - Max: 4.602618536114973 Min: 0.0\n","ReLU Activation - Max: 3.817832199035497 Min: 0.0\n","Softmax Output - Max: 0.9449651269178314 Min: 5.5765879804472506e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07201595500583945 Min: -0.1007154726123884\n","Layer 1 - Gradient Weights Max: 0.1288278311262136 Min: -0.1493017562658351\n","Layer 0 - Gradient Weights Max: 0.17626532312001614 Min: -0.19945747649420703\n","ReLU Activation - Max: 4.6596877708835995 Min: 0.0\n","ReLU Activation - Max: 3.6312975854338116 Min: 0.0\n","Softmax Output - Max: 0.953033421023821 Min: 6.257014731976827e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08274411736775057 Min: -0.10237909311012335\n","Layer 1 - Gradient Weights Max: 0.10809219892364516 Min: -0.14722599052568397\n","Layer 0 - Gradient Weights Max: 0.19566485476184073 Min: -0.19832946478500746\n","ReLU Activation - Max: 6.30952056106664 Min: 0.0\n","ReLU Activation - Max: 4.397035481690278 Min: 0.0\n","Softmax Output - Max: 0.9053777342445224 Min: 1.0448692387469059e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.2131923413390836 Min: -0.10083663960916292\n","Layer 1 - Gradient Weights Max: 0.19646271773943044 Min: -0.13578987753683486\n","Layer 0 - Gradient Weights Max: 0.16115650534870163 Min: -0.1700790425855658\n","ReLU Activation - Max: 5.212946462629815 Min: 0.0\n","ReLU Activation - Max: 4.059484562005557 Min: 0.0\n","Softmax Output - Max: 0.9510494311566825 Min: 4.36402902528095e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08852809630670348 Min: -0.08657863571917607\n","Layer 1 - Gradient Weights Max: 0.18343321451995673 Min: -0.16638200246077323\n","Layer 0 - Gradient Weights Max: 0.18630176781313806 Min: -0.19200096994905672\n","ReLU Activation - Max: 4.4238814247469795 Min: 0.0\n","ReLU Activation - Max: 3.7060052738085143 Min: 0.0\n","Softmax Output - Max: 0.9859852407307333 Min: 6.0518247683390994e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10637255186973389 Min: -0.14633445120142968\n","Layer 1 - Gradient Weights Max: 0.15726621913626992 Min: -0.1973713900286457\n","Layer 0 - Gradient Weights Max: 0.2540478285349152 Min: -0.16548286742224647\n","ReLU Activation - Max: 3.784279985225839 Min: 0.0\n","ReLU Activation - Max: 3.715531098071591 Min: 0.0\n","Softmax Output - Max: 0.975493512535738 Min: 2.0365059584538742e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08093204152150785 Min: -0.09736163423475566\n","Layer 1 - Gradient Weights Max: 0.13161002200053723 Min: -0.15090372069664715\n","Layer 0 - Gradient Weights Max: 0.19704812377230757 Min: -0.20870140298157058\n","ReLU Activation - Max: 4.569612353032088 Min: 0.0\n","ReLU Activation - Max: 4.44076045404365 Min: 0.0\n","Softmax Output - Max: 0.9893208759233397 Min: 1.2954192530077018e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08362366560356124 Min: -0.10367218158548142\n","Layer 1 - Gradient Weights Max: 0.13330989761211198 Min: -0.19318221726631507\n","Layer 0 - Gradient Weights Max: 0.17851585211208656 Min: -0.19526919997464348\n","ReLU Activation - Max: 4.523561105171703 Min: 0.0\n","ReLU Activation - Max: 4.838024663012134 Min: 0.0\n","Softmax Output - Max: 0.991438350586679 Min: 1.441827940036623e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15192788227670512 Min: -0.12898714497151897\n","Layer 1 - Gradient Weights Max: 0.17011830633146674 Min: -0.20519619305678605\n","Layer 0 - Gradient Weights Max: 0.18770290490878788 Min: -0.21972436563890058\n","ReLU Activation - Max: 5.032616122387753 Min: 0.0\n","ReLU Activation - Max: 4.78213898669995 Min: 0.0\n","Softmax Output - Max: 0.9576195788623472 Min: 7.243868054764874e-07 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.0576151349918592 Min: -0.06832135022971186\n","Layer 1 - Gradient Weights Max: 0.106779653914985 Min: -0.11260475757758275\n","Layer 0 - Gradient Weights Max: 0.16032752524005245 Min: -0.1828921465184516\n","ReLU Activation - Max: 6.548670904042265 Min: 0.0\n","ReLU Activation - Max: 3.8549025676022324 Min: 0.0\n","Softmax Output - Max: 0.8790391805500287 Min: 6.251941104466784e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09798679597798611 Min: -0.07616899297671324\n","Layer 1 - Gradient Weights Max: 0.16505436880843447 Min: -0.13459994509497183\n","Layer 0 - Gradient Weights Max: 0.18529367715613754 Min: -0.18854434768037523\n","ReLU Activation - Max: 6.000199779603072 Min: 0.0\n","ReLU Activation - Max: 3.8442939807035845 Min: 0.0\n","Softmax Output - Max: 0.9439780062645972 Min: 4.687151716002151e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.07829442765507287 Min: -0.11264447674016771\n","Layer 1 - Gradient Weights Max: 0.14193657550421263 Min: -0.18132424524687796\n","Layer 0 - Gradient Weights Max: 0.1535819145923914 Min: -0.1393782022098313\n","ReLU Activation - Max: 5.856805325485507 Min: 0.0\n","ReLU Activation - Max: 4.572731492897954 Min: 0.0\n","Softmax Output - Max: 0.9592595763185364 Min: 2.3583628486919675e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13641493108713476 Min: -0.14330061955171908\n","Layer 1 - Gradient Weights Max: 0.13371468891712143 Min: -0.15199538715974042\n","Layer 0 - Gradient Weights Max: 0.19013964588154306 Min: -0.22455075004457703\n","ReLU Activation - Max: 4.204668789905561 Min: 0.0\n","ReLU Activation - Max: 4.229298018651909 Min: 0.0\n","Softmax Output - Max: 0.8832030321782062 Min: 8.705257320525266e-07 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11986082551332129 Min: -0.10737247579791166\n","Layer 1 - Gradient Weights Max: 0.16619759857154529 Min: -0.1571263490751299\n","Layer 0 - Gradient Weights Max: 0.15083424334177606 Min: -0.1592607756948229\n","ReLU Activation - Max: 4.486910629469249 Min: 0.0\n","ReLU Activation - Max: 4.0632695804834995 Min: 0.0\n","Softmax Output - Max: 0.9619593825287444 Min: 6.863772122520651e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06593353390672022 Min: -0.09610595492684114\n","Layer 1 - Gradient Weights Max: 0.12866129092000395 Min: -0.16788318501723207\n","Layer 0 - Gradient Weights Max: 0.1808663076818719 Min: -0.15264169462210217\n","ReLU Activation - Max: 4.9429345847672845 Min: 0.0\n","ReLU Activation - Max: 3.9983727761088512 Min: 0.0\n","Softmax Output - Max: 0.9544334885606148 Min: 7.387177903226768e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09177879926619295 Min: -0.09615671296884377\n","Layer 1 - Gradient Weights Max: 0.1919522038432108 Min: -0.13055324955355552\n","Layer 0 - Gradient Weights Max: 0.16684096930504322 Min: -0.16591187652290035\n","ReLU Activation - Max: 4.966876595090112 Min: 0.0\n","ReLU Activation - Max: 4.21765697433047 Min: 0.0\n","Softmax Output - Max: 0.946871566294456 Min: 7.533793744561076e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13252143061988644 Min: -0.15136377834715617\n","Layer 1 - Gradient Weights Max: 0.12695266331857838 Min: -0.1526007521055629\n","Layer 0 - Gradient Weights Max: 0.1720731289260086 Min: -0.22820436144230158\n","ReLU Activation - Max: 4.659168041798716 Min: 0.0\n","ReLU Activation - Max: 3.756978263836514 Min: 0.0\n","Softmax Output - Max: 0.9704233418037647 Min: 3.977504916064804e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1157334193734488 Min: -0.15367053388258353\n","Layer 1 - Gradient Weights Max: 0.1533048269692257 Min: -0.17711461510335197\n","Layer 0 - Gradient Weights Max: 0.18306224587186995 Min: -0.1670657404480764\n","ReLU Activation - Max: 5.272742533952049 Min: 0.0\n","ReLU Activation - Max: 3.7094372045555057 Min: 0.0\n","Softmax Output - Max: 0.9630958291333 Min: 3.4958623200200945e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.06762492673264776 Min: -0.08578645087505632\n","Layer 1 - Gradient Weights Max: 0.12807365595316386 Min: -0.16142712086004762\n","Layer 0 - Gradient Weights Max: 0.19021151745164652 Min: -0.18484019524340747\n","ReLU Activation - Max: 5.295488165887447 Min: 0.0\n","ReLU Activation - Max: 4.2185434921715865 Min: 0.0\n","Softmax Output - Max: 0.9804951142576465 Min: 9.283164401314825e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09575057975977583 Min: -0.07763338363468547\n","Layer 1 - Gradient Weights Max: 0.15034269132277886 Min: -0.14859551829218046\n","Layer 0 - Gradient Weights Max: 0.24076696256434602 Min: -0.2554575863983593\n","ReLU Activation - Max: 5.6719615578700475 Min: 0.0\n","ReLU Activation - Max: 4.231014826253153 Min: 0.0\n","Softmax Output - Max: 0.9854420146137639 Min: 1.003125337247135e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.0965878858244238 Min: -0.08922015456833472\n","Layer 1 - Gradient Weights Max: 0.16378706110325192 Min: -0.16889921087050583\n","Layer 0 - Gradient Weights Max: 0.26952522106925514 Min: -0.20738539088340727\n","ReLU Activation - Max: 4.899681225168152 Min: 0.0\n","ReLU Activation - Max: 4.9483763072210145 Min: 0.0\n","Softmax Output - Max: 0.9656740546013197 Min: 6.019216815825375e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08549667613067229 Min: -0.09881069475764745\n","Layer 1 - Gradient Weights Max: 0.18917359678444845 Min: -0.17640915472360433\n","Layer 0 - Gradient Weights Max: 0.2399277951834574 Min: -0.2201861830816061\n","ReLU Activation - Max: 5.4971694842125585 Min: 0.0\n","ReLU Activation - Max: 4.618529345429626 Min: 0.0\n","Softmax Output - Max: 0.971247139007486 Min: 5.790237632376915e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07742461426391414 Min: -0.0743737057558217\n","Layer 1 - Gradient Weights Max: 0.14392544696717355 Min: -0.16429205967124316\n","Layer 0 - Gradient Weights Max: 0.19738511299490988 Min: -0.22776947497644545\n","ReLU Activation - Max: 5.036904937238304 Min: 0.0\n","ReLU Activation - Max: 4.106204102225252 Min: 0.0\n","Softmax Output - Max: 0.9249457244523109 Min: 6.896729250817817e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07747007685402062 Min: -0.19583015689083247\n","Layer 1 - Gradient Weights Max: 0.163476659485911 Min: -0.215988388020813\n","Layer 0 - Gradient Weights Max: 0.14968123932328173 Min: -0.20144439202497175\n","ReLU Activation - Max: 4.908660744789282 Min: 0.0\n","ReLU Activation - Max: 4.44055464052789 Min: 0.0\n","Softmax Output - Max: 0.9398440114941132 Min: 6.0270231986180385e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11842064522396793 Min: -0.0989470254095473\n","Layer 1 - Gradient Weights Max: 0.14923357993132527 Min: -0.1619377114732434\n","Layer 0 - Gradient Weights Max: 0.1772407330343045 Min: -0.2133130552207116\n","ReLU Activation - Max: 4.909740951912582 Min: 0.0\n","ReLU Activation - Max: 4.28489059440071 Min: 0.0\n","Softmax Output - Max: 0.9399223517567343 Min: 8.185140217280839e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09203882635549719 Min: -0.13308295080561564\n","Layer 1 - Gradient Weights Max: 0.14680808025961478 Min: -0.1650491120268049\n","Layer 0 - Gradient Weights Max: 0.22039651780317265 Min: -0.16172434896457144\n","ReLU Activation - Max: 4.286033708692105 Min: 0.0\n","ReLU Activation - Max: 3.992520717110844 Min: 0.0\n","Softmax Output - Max: 0.9068237855418823 Min: 8.200127072645624e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.0956126060331673 Min: -0.15147947351603552\n","Layer 1 - Gradient Weights Max: 0.18158988255998937 Min: -0.17331933911897515\n","Layer 0 - Gradient Weights Max: 0.204464706120117 Min: -0.1807683501516203\n","ReLU Activation - Max: 4.2500450169538375 Min: 0.0\n","ReLU Activation - Max: 3.7181046727352016 Min: 0.0\n","Softmax Output - Max: 0.9838062741913971 Min: 3.319811306581221e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10338054910452689 Min: -0.09842699167137806\n","Layer 1 - Gradient Weights Max: 0.13581736534383346 Min: -0.1461703538917555\n","Layer 0 - Gradient Weights Max: 0.14502966813923138 Min: -0.2013284207905102\n","ReLU Activation - Max: 6.265550717838362 Min: 0.0\n","ReLU Activation - Max: 4.478792643259599 Min: 0.0\n","Softmax Output - Max: 0.9788076827266992 Min: 2.190613797309478e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12317428243292428 Min: -0.1207065832076285\n","Layer 1 - Gradient Weights Max: 0.17683229206027412 Min: -0.22042520091455536\n","Layer 0 - Gradient Weights Max: 0.194226766102795 Min: -0.1949689965794449\n","ReLU Activation - Max: 5.5438248445362 Min: 0.0\n","ReLU Activation - Max: 4.201934868127118 Min: 0.0\n","Softmax Output - Max: 0.9275690724188866 Min: 5.223077331966352e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14211462572529207 Min: -0.10990333347359783\n","Layer 1 - Gradient Weights Max: 0.21262577329089977 Min: -0.24368741338595853\n","Layer 0 - Gradient Weights Max: 0.15386718950171038 Min: -0.15185128540712456\n","ReLU Activation - Max: 4.1546640965496096 Min: 0.0\n","ReLU Activation - Max: 3.606375233797463 Min: 0.0\n","Softmax Output - Max: 0.9565436018875869 Min: 6.6904399186508064e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10802857218961168 Min: -0.12080414596593443\n","Layer 1 - Gradient Weights Max: 0.1613457181600626 Min: -0.13991380445113158\n","Layer 0 - Gradient Weights Max: 0.15832458369873154 Min: -0.1628644704896937\n","ReLU Activation - Max: 5.393859229014941 Min: 0.0\n","ReLU Activation - Max: 4.408633597058365 Min: 0.0\n","Softmax Output - Max: 0.9400439055043536 Min: 1.0218576199780791e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.07957404491633571 Min: -0.08345938463076567\n","Layer 1 - Gradient Weights Max: 0.16506267459255455 Min: -0.15138961178361438\n","Layer 0 - Gradient Weights Max: 0.15272223726134698 Min: -0.16004720465070765\n","ReLU Activation - Max: 4.339634965373184 Min: 0.0\n","ReLU Activation - Max: 4.37618930495858 Min: 0.0\n","Softmax Output - Max: 0.9862145996617795 Min: 9.161435903317132e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.05686005253633513 Min: -0.06435975133911404\n","Layer 1 - Gradient Weights Max: 0.10137271636415243 Min: -0.13635608804637483\n","Layer 0 - Gradient Weights Max: 0.21572956622754774 Min: -0.18698968626431403\n","ReLU Activation - Max: 4.570530530867978 Min: 0.0\n","ReLU Activation - Max: 3.9530769771897227 Min: 0.0\n","Softmax Output - Max: 0.9741396241324907 Min: 3.6491248940882096e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08100060611784375 Min: -0.10062787402625854\n","Layer 1 - Gradient Weights Max: 0.13726310681006806 Min: -0.09789274000680845\n","Layer 0 - Gradient Weights Max: 0.14078645452057015 Min: -0.13739764102415664\n","ReLU Activation - Max: 4.174195717120122 Min: 0.0\n","ReLU Activation - Max: 4.26407139880144 Min: 0.0\n","Softmax Output - Max: 0.979377597934599 Min: 3.734754036576828e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1475965935714236 Min: -0.13854010751187096\n","Layer 1 - Gradient Weights Max: 0.1820327696123929 Min: -0.30118143664716474\n","Layer 0 - Gradient Weights Max: 0.17254691195590854 Min: -0.3280563524284613\n","ReLU Activation - Max: 4.43760027420959 Min: 0.0\n","ReLU Activation - Max: 3.800523400908611 Min: 0.0\n","Softmax Output - Max: 0.9559082389679856 Min: 4.174230568935415e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1128781897155137 Min: -0.09734449553046585\n","Layer 1 - Gradient Weights Max: 0.2344515536576898 Min: -0.16608883140468272\n","Layer 0 - Gradient Weights Max: 0.22656754034392232 Min: -0.149048742559339\n","ReLU Activation - Max: 6.521985170927451 Min: 0.0\n","ReLU Activation - Max: 4.662985502224466 Min: 0.0\n","Softmax Output - Max: 0.9635679475615014 Min: 1.07918214140363e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0908486081318723 Min: -0.1183052361563441\n","Layer 1 - Gradient Weights Max: 0.11652098966508485 Min: -0.133347880810861\n","Layer 0 - Gradient Weights Max: 0.16502604133145402 Min: -0.26465493064689527\n","ReLU Activation - Max: 5.136167223868105 Min: 0.0\n","ReLU Activation - Max: 4.251445317297698 Min: 0.0\n","Softmax Output - Max: 0.9779524922745986 Min: 3.171497992777259e-05 Sum (first example): 1.0000000000000004\n","Layer 2 - Gradient Weights Max: 0.08291674325342814 Min: -0.10303223913439787\n","Layer 1 - Gradient Weights Max: 0.18266894864580124 Min: -0.15074242012793435\n","Layer 0 - Gradient Weights Max: 0.19703143010550223 Min: -0.22816776716919726\n","ReLU Activation - Max: 4.6305910366127065 Min: 0.0\n","ReLU Activation - Max: 3.5921336194545255 Min: 0.0\n","Softmax Output - Max: 0.9324029182664962 Min: 1.234715991063103e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.13834114892383803 Min: -0.15970937668435678\n","Layer 1 - Gradient Weights Max: 0.1622374794889769 Min: -0.2505539623076798\n","Layer 0 - Gradient Weights Max: 0.18376919790506233 Min: -0.2383614686487769\n","ReLU Activation - Max: 5.026959658057555 Min: 0.0\n","ReLU Activation - Max: 4.444223667326875 Min: 0.0\n","Softmax Output - Max: 0.9917778710392108 Min: 7.013235603880125e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0843800316647067 Min: -0.12236383234634036\n","Layer 1 - Gradient Weights Max: 0.20220976108302238 Min: -0.17785187937466337\n","Layer 0 - Gradient Weights Max: 0.13532451794574388 Min: -0.18688104645374135\n","ReLU Activation - Max: 4.5190559243152935 Min: 0.0\n","ReLU Activation - Max: 4.017865886432571 Min: 0.0\n","Softmax Output - Max: 0.9637826194865607 Min: 4.674618337600679e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07979071025990908 Min: -0.08277755619729849\n","Layer 1 - Gradient Weights Max: 0.14983756602258286 Min: -0.13554354634027702\n","Layer 0 - Gradient Weights Max: 0.19564138878671727 Min: -0.21438137571679536\n","ReLU Activation - Max: 7.0313753976034565 Min: 0.0\n","ReLU Activation - Max: 4.498971927816574 Min: 0.0\n","Softmax Output - Max: 0.9886522278462979 Min: 4.391574208904703e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.169664504841633 Min: -0.21064463410163028\n","Layer 1 - Gradient Weights Max: 0.17336914853798333 Min: -0.19205995264679088\n","Layer 0 - Gradient Weights Max: 0.22534729738363454 Min: -0.243465971425935\n","ReLU Activation - Max: 4.288838090270327 Min: 0.0\n","ReLU Activation - Max: 4.310263897569932 Min: 0.0\n","Softmax Output - Max: 0.9910803519113643 Min: 8.328550507751386e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.0911853926078605 Min: -0.1110359172839373\n","Layer 1 - Gradient Weights Max: 0.14272168999723622 Min: -0.15596685573680866\n","Layer 0 - Gradient Weights Max: 0.16631789079489823 Min: -0.18369540002484067\n","ReLU Activation - Max: 6.497486608695522 Min: 0.0\n","ReLU Activation - Max: 4.8433959508347595 Min: 0.0\n","Softmax Output - Max: 0.9797066279446869 Min: 2.0753100808397504e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14615779382307845 Min: -0.08945063770365565\n","Layer 1 - Gradient Weights Max: 0.12877569822279047 Min: -0.13724060764851254\n","Layer 0 - Gradient Weights Max: 0.15130914341185286 Min: -0.194996004500156\n","ReLU Activation - Max: 4.213723849870163 Min: 0.0\n","ReLU Activation - Max: 4.38373505986074 Min: 0.0\n","Softmax Output - Max: 0.9657000152516731 Min: 4.344512293991821e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.18702687876119206 Min: -0.08902734684552317\n","Layer 1 - Gradient Weights Max: 0.1871622481220846 Min: -0.16845041742969644\n","Layer 0 - Gradient Weights Max: 0.1263047941517339 Min: -0.1944928954810263\n","ReLU Activation - Max: 4.640333578070272 Min: 0.0\n","ReLU Activation - Max: 4.6980273889136415 Min: 0.0\n","Softmax Output - Max: 0.9781189818644396 Min: 1.8674552097002028e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11693348868235137 Min: -0.13154925442188137\n","Layer 1 - Gradient Weights Max: 0.15399950854424455 Min: -0.1943848325663121\n","Layer 0 - Gradient Weights Max: 0.15813331506060993 Min: -0.1649254827616269\n","ReLU Activation - Max: 4.130996231305391 Min: 0.0\n","ReLU Activation - Max: 3.875400080940021 Min: 0.0\n","Softmax Output - Max: 0.9328822182159312 Min: 9.569041969927047e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08630505682644656 Min: -0.13212789738971045\n","Layer 1 - Gradient Weights Max: 0.20595999357008707 Min: -0.15029761040943615\n","Layer 0 - Gradient Weights Max: 0.22131697131840897 Min: -0.23241978382270861\n","ReLU Activation - Max: 5.687163256110464 Min: 0.0\n","ReLU Activation - Max: 3.6822904539460644 Min: 0.0\n","Softmax Output - Max: 0.9868777918613284 Min: 2.9961583737902617e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09133508239109689 Min: -0.09260724690990078\n","Layer 1 - Gradient Weights Max: 0.18412399104068994 Min: -0.16796309830218178\n","Layer 0 - Gradient Weights Max: 0.23639166592692823 Min: -0.1768787758419891\n","ReLU Activation - Max: 6.435063529815836 Min: 0.0\n","ReLU Activation - Max: 5.166218700888378 Min: 0.0\n","Softmax Output - Max: 0.9481411634375959 Min: 1.880577518048633e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11980444662670715 Min: -0.09750831540858401\n","Layer 1 - Gradient Weights Max: 0.19582273110379425 Min: -0.21097449054037531\n","Layer 0 - Gradient Weights Max: 0.2507914555191323 Min: -0.2496688100934197\n","ReLU Activation - Max: 5.104128560259752 Min: 0.0\n","ReLU Activation - Max: 3.7460704327071634 Min: 0.0\n","Softmax Output - Max: 0.9725891430738384 Min: 3.3806122125838015e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09477043730013837 Min: -0.159464194461556\n","Layer 1 - Gradient Weights Max: 0.18660370958517108 Min: -0.17539858621625315\n","Layer 0 - Gradient Weights Max: 0.17786576432047924 Min: -0.15552166011705623\n","ReLU Activation - Max: 4.927630761721522 Min: 0.0\n","ReLU Activation - Max: 4.876457560950935 Min: 0.0\n","Softmax Output - Max: 0.9873015687730781 Min: 5.888126187079315e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17061377763586172 Min: -0.1313483193603645\n","Layer 1 - Gradient Weights Max: 0.17395180281246284 Min: -0.16195426984959904\n","Layer 0 - Gradient Weights Max: 0.20489599699170302 Min: -0.1899971417970526\n","ReLU Activation - Max: 5.140432404094223 Min: 0.0\n","ReLU Activation - Max: 4.2594395581033595 Min: 0.0\n","Softmax Output - Max: 0.9836756585976635 Min: 3.4841548891338333e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10732724083172954 Min: -0.21367979559881914\n","Layer 1 - Gradient Weights Max: 0.2046451969244528 Min: -0.1667880957361118\n","Layer 0 - Gradient Weights Max: 0.15304270337388967 Min: -0.174213075735577\n","ReLU Activation - Max: 4.760894603638909 Min: 0.0\n","ReLU Activation - Max: 4.1084104824971925 Min: 0.0\n","Softmax Output - Max: 0.9738174432110293 Min: 2.8078846653796724e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09769354223345253 Min: -0.12150437197131435\n","Layer 1 - Gradient Weights Max: 0.11823893997268861 Min: -0.11840469426002517\n","Layer 0 - Gradient Weights Max: 0.18861944584180915 Min: -0.17361053935009718\n","ReLU Activation - Max: 4.932799073234766 Min: 0.0\n","ReLU Activation - Max: 3.572955948968226 Min: 0.0\n","Softmax Output - Max: 0.9652931438700542 Min: 3.1526182978892715e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.15021936591609208 Min: -0.085128191574802\n","Layer 1 - Gradient Weights Max: 0.21696451577118905 Min: -0.14914643030713512\n","Layer 0 - Gradient Weights Max: 0.219071311988762 Min: -0.26683455060211375\n","ReLU Activation - Max: 4.8397835316291244 Min: 0.0\n","ReLU Activation - Max: 4.232161029266464 Min: 0.0\n","Softmax Output - Max: 0.9296991939746454 Min: 9.343254362037263e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10264311101283492 Min: -0.08325726456148137\n","Layer 1 - Gradient Weights Max: 0.21562070177428344 Min: -0.20199368213856742\n","Layer 0 - Gradient Weights Max: 0.2129826583356929 Min: -0.23354339849644362\n","ReLU Activation - Max: 4.702501773190182 Min: 0.0\n","ReLU Activation - Max: 4.339853267877718 Min: 0.0\n","Softmax Output - Max: 0.9133414440910028 Min: 4.26780501418725e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0878195205518656 Min: -0.09101080412843428\n","Layer 1 - Gradient Weights Max: 0.14074750162341595 Min: -0.16223153904711296\n","Layer 0 - Gradient Weights Max: 0.18584324233173874 Min: -0.2730800186440328\n","ReLU Activation - Max: 4.9376585172199166 Min: 0.0\n","ReLU Activation - Max: 3.994006140712966 Min: 0.0\n","Softmax Output - Max: 0.9668876296445207 Min: 3.4544284010566435e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1055554520548418 Min: -0.14859435239926358\n","Layer 1 - Gradient Weights Max: 0.1510371608216308 Min: -0.1627319954275557\n","Layer 0 - Gradient Weights Max: 0.20304780769960698 Min: -0.20299561772608207\n","ReLU Activation - Max: 5.174089836938395 Min: 0.0\n","ReLU Activation - Max: 3.6550000972378967 Min: 0.0\n","Softmax Output - Max: 0.9532106437198381 Min: 3.53533161913145e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09098330591907322 Min: -0.12194926972717608\n","Layer 1 - Gradient Weights Max: 0.18370113420682077 Min: -0.2096225256477973\n","Layer 0 - Gradient Weights Max: 0.26050886069785917 Min: -0.2682460825397292\n","ReLU Activation - Max: 5.096214592679928 Min: 0.0\n","ReLU Activation - Max: 4.828833984311569 Min: 0.0\n","Softmax Output - Max: 0.9187938925643073 Min: 3.855880331922584e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10606371912775103 Min: -0.12632286363508935\n","Layer 1 - Gradient Weights Max: 0.17935513061256747 Min: -0.14743695889528274\n","Layer 0 - Gradient Weights Max: 0.275364801737128 Min: -0.2965298764120836\n","ReLU Activation - Max: 5.409881626765262 Min: 0.0\n","ReLU Activation - Max: 3.9867405100020665 Min: 0.0\n","Softmax Output - Max: 0.9650802015173046 Min: 3.766203150798298e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12633782049752187 Min: -0.17154504649339658\n","Layer 1 - Gradient Weights Max: 0.18261852480999477 Min: -0.24882233187197195\n","Layer 0 - Gradient Weights Max: 0.26709037698550364 Min: -0.1784757331624362\n","ReLU Activation - Max: 5.039173447835952 Min: 0.0\n","ReLU Activation - Max: 4.583231702113237 Min: 0.0\n","Softmax Output - Max: 0.9779770693339871 Min: 5.362406927947761e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10276177277334061 Min: -0.12853258186530594\n","Layer 1 - Gradient Weights Max: 0.13820409807083459 Min: -0.16246264266934682\n","Layer 0 - Gradient Weights Max: 0.1395405730808587 Min: -0.1599797618130398\n","ReLU Activation - Max: 4.3742909616332355 Min: 0.0\n","ReLU Activation - Max: 5.225276413947479 Min: 0.0\n","Softmax Output - Max: 0.9428627588463108 Min: 1.061910954507656e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08140915116777765 Min: -0.12326667287407966\n","Layer 1 - Gradient Weights Max: 0.1698405487421114 Min: -0.1868261926393962\n","Layer 0 - Gradient Weights Max: 0.16280923806957942 Min: -0.1905463073573125\n","ReLU Activation - Max: 5.700375636606846 Min: 0.0\n","ReLU Activation - Max: 3.753086629749481 Min: 0.0\n","Softmax Output - Max: 0.9532548737022719 Min: 7.721988538173182e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07125731845167729 Min: -0.10733764608190964\n","Layer 1 - Gradient Weights Max: 0.14970897262947133 Min: -0.13921297310396571\n","Layer 0 - Gradient Weights Max: 0.17424132100827444 Min: -0.20812441832252435\n","ReLU Activation - Max: 4.566976480068236 Min: 0.0\n","ReLU Activation - Max: 3.95948005203638 Min: 0.0\n","Softmax Output - Max: 0.9811456833707473 Min: 4.942488195760292e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10052200161478823 Min: -0.14306538527211304\n","Layer 1 - Gradient Weights Max: 0.16968161394180234 Min: -0.17835108080012493\n","Layer 0 - Gradient Weights Max: 0.17038692391581917 Min: -0.22545042236557944\n","ReLU Activation - Max: 4.834589850757565 Min: 0.0\n","ReLU Activation - Max: 4.602554592290686 Min: 0.0\n","Softmax Output - Max: 0.974126525887512 Min: 4.694125995994049e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0754075969586422 Min: -0.1465880288629618\n","Layer 1 - Gradient Weights Max: 0.16532249070700702 Min: -0.19672392349252257\n","Layer 0 - Gradient Weights Max: 0.18665241906020114 Min: -0.1935143791687498\n","ReLU Activation - Max: 5.945696473185539 Min: 0.0\n","ReLU Activation - Max: 4.7283629050228795 Min: 0.0\n","Softmax Output - Max: 0.9403631798762947 Min: 2.373565916419525e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12941964572906292 Min: -0.10015178030315858\n","Layer 1 - Gradient Weights Max: 0.1646429290042624 Min: -0.16783643450381844\n","Layer 0 - Gradient Weights Max: 0.16735726111276322 Min: -0.19585055995927633\n","ReLU Activation - Max: 4.627524087072992 Min: 0.0\n","ReLU Activation - Max: 4.684834223259049 Min: 0.0\n","Softmax Output - Max: 0.963658211711683 Min: 6.9757079392896125e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11051151516450475 Min: -0.14982064369872725\n","Layer 1 - Gradient Weights Max: 0.17826760148239318 Min: -0.1696927959058257\n","Layer 0 - Gradient Weights Max: 0.2009547101784643 Min: -0.22651890840056027\n","ReLU Activation - Max: 4.638941696038236 Min: 0.0\n","ReLU Activation - Max: 3.8639277823719707 Min: 0.0\n","Softmax Output - Max: 0.9610298267331507 Min: 0.00023344896791038255 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0949463625999255 Min: -0.15938821441004575\n","Layer 1 - Gradient Weights Max: 0.17170021884597864 Min: -0.24194241670398065\n","Layer 0 - Gradient Weights Max: 0.1998884257056419 Min: -0.18570769594344985\n","ReLU Activation - Max: 4.250312691214709 Min: 0.0\n","ReLU Activation - Max: 3.7708231753644053 Min: 0.0\n","Softmax Output - Max: 0.9761585492844502 Min: 6.980772171418525e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09747808983666273 Min: -0.07427770697311002\n","Layer 1 - Gradient Weights Max: 0.11231071861540853 Min: -0.141599775280023\n","Layer 0 - Gradient Weights Max: 0.1388933457757945 Min: -0.19051625847687004\n","ReLU Activation - Max: 4.43897305909196 Min: 0.0\n","ReLU Activation - Max: 3.9982409125234923 Min: 0.0\n","Softmax Output - Max: 0.9271192723602407 Min: 5.856986679547947e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10241039443906537 Min: -0.0721943927488953\n","Layer 1 - Gradient Weights Max: 0.16693892080272238 Min: -0.1755918706775145\n","Layer 0 - Gradient Weights Max: 0.18816733050718898 Min: -0.18961810309743865\n","ReLU Activation - Max: 5.73816919184541 Min: 0.0\n","ReLU Activation - Max: 3.733482710186541 Min: 0.0\n","Softmax Output - Max: 0.9713877867156085 Min: 2.0010853517939383e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.06891891761731037 Min: -0.0794197494912795\n","Layer 1 - Gradient Weights Max: 0.1356617427449971 Min: -0.11644227713413308\n","Layer 0 - Gradient Weights Max: 0.14544917833066887 Min: -0.17034626994866162\n","ReLU Activation - Max: 5.02499693701496 Min: 0.0\n","ReLU Activation - Max: 4.04908908753971 Min: 0.0\n","Softmax Output - Max: 0.9809794782223515 Min: 9.296214654672894e-07 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0933615772697469 Min: -0.08458442890169894\n","Layer 1 - Gradient Weights Max: 0.1282151163549434 Min: -0.12102618498132002\n","Layer 0 - Gradient Weights Max: 0.16732008836634504 Min: -0.1546400819207241\n","ReLU Activation - Max: 4.655332964055275 Min: 0.0\n","ReLU Activation - Max: 3.9212357602697976 Min: 0.0\n","Softmax Output - Max: 0.9494818639526014 Min: 2.5016997995312285e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.082909774377994 Min: -0.11429195929409754\n","Layer 1 - Gradient Weights Max: 0.1503874379558466 Min: -0.20525683123569874\n","Layer 0 - Gradient Weights Max: 0.16972788667033267 Min: -0.18820231495166415\n","ReLU Activation - Max: 5.071526531516438 Min: 0.0\n","ReLU Activation - Max: 4.063991469185912 Min: 0.0\n","Softmax Output - Max: 0.9353730295783504 Min: 0.0002777465065602193 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08468608076697537 Min: -0.12143475926285179\n","Layer 1 - Gradient Weights Max: 0.11905456838212553 Min: -0.15287670862281702\n","Layer 0 - Gradient Weights Max: 0.16004867443248746 Min: -0.19343946183211974\n","ReLU Activation - Max: 4.554527303533531 Min: 0.0\n","ReLU Activation - Max: 3.556705566336536 Min: 0.0\n","Softmax Output - Max: 0.9349716260869697 Min: 4.00311346815844e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08005420181398144 Min: -0.09035996802628123\n","Layer 1 - Gradient Weights Max: 0.19376906396990495 Min: -0.14683536637245878\n","Layer 0 - Gradient Weights Max: 0.18472609039840854 Min: -0.17541255257468366\n","ReLU Activation - Max: 5.527331484144364 Min: 0.0\n","ReLU Activation - Max: 3.813259229520259 Min: 0.0\n","Softmax Output - Max: 0.9837126337858156 Min: 5.089719088746026e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09971675035241818 Min: -0.10006297538955194\n","Layer 1 - Gradient Weights Max: 0.15788872835156523 Min: -0.13321557225025746\n","Layer 0 - Gradient Weights Max: 0.197223959572511 Min: -0.18151450617250003\n","ReLU Activation - Max: 4.071224913447892 Min: 0.0\n","ReLU Activation - Max: 4.353960382302283 Min: 0.0\n","Softmax Output - Max: 0.906976083216555 Min: 1.3067817014921536e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08258132722760257 Min: -0.12066667998421059\n","Layer 1 - Gradient Weights Max: 0.17721230503724325 Min: -0.12052877887012808\n","Layer 0 - Gradient Weights Max: 0.3163685369672615 Min: -0.2510222306162862\n","ReLU Activation - Max: 6.381863946920299 Min: 0.0\n","ReLU Activation - Max: 4.122439912184859 Min: 0.0\n","Softmax Output - Max: 0.920594292071004 Min: 3.2629030460231954e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09477960789831662 Min: -0.11140444346420592\n","Layer 1 - Gradient Weights Max: 0.18785224654557764 Min: -0.16579986500906785\n","Layer 0 - Gradient Weights Max: 0.18582990169668714 Min: -0.16994230445167247\n","ReLU Activation - Max: 7.219964941081967 Min: 0.0\n","ReLU Activation - Max: 3.9583312515424813 Min: 0.0\n","Softmax Output - Max: 0.9701696966116782 Min: 1.9352417764621342e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10196838629508843 Min: -0.13475802232577333\n","Layer 1 - Gradient Weights Max: 0.14792795011999496 Min: -0.18998052999343418\n","Layer 0 - Gradient Weights Max: 0.18991370536161553 Min: -0.2163487887936121\n","ReLU Activation - Max: 4.418513512177748 Min: 0.0\n","ReLU Activation - Max: 4.693377269873592 Min: 0.0\n","Softmax Output - Max: 0.9848682878695451 Min: 1.4044629138379438e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.15167895308995785 Min: -0.19041948619907473\n","Layer 1 - Gradient Weights Max: 0.16294591393862343 Min: -0.19774016734021563\n","Layer 0 - Gradient Weights Max: 0.21606493644473923 Min: -0.22998184119146456\n","ReLU Activation - Max: 5.231564446148549 Min: 0.0\n","ReLU Activation - Max: 4.442499803921928 Min: 0.0\n","Softmax Output - Max: 0.9293145611539149 Min: 3.634111493064992e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10259338059253666 Min: -0.08894110128351888\n","Layer 1 - Gradient Weights Max: 0.1314673193953436 Min: -0.14412847619052002\n","Layer 0 - Gradient Weights Max: 0.18132825781464929 Min: -0.221681907934536\n","ReLU Activation - Max: 5.885783070390896 Min: 0.0\n","ReLU Activation - Max: 4.540615657068116 Min: 0.0\n","Softmax Output - Max: 0.9380030800401287 Min: 2.8104626117234225e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09809135704634737 Min: -0.11436712685270209\n","Layer 1 - Gradient Weights Max: 0.15284926206287666 Min: -0.15529794788461887\n","Layer 0 - Gradient Weights Max: 0.15840196475980758 Min: -0.18483260611545949\n","ReLU Activation - Max: 4.645942161076003 Min: 0.0\n","ReLU Activation - Max: 4.17786894052465 Min: 0.0\n","Softmax Output - Max: 0.9361676927695417 Min: 8.795293294057404e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07565441106455102 Min: -0.11905485089111478\n","Layer 1 - Gradient Weights Max: 0.1465668667675345 Min: -0.18166554463896514\n","Layer 0 - Gradient Weights Max: 0.3256062714485107 Min: -0.17190713652764733\n","ReLU Activation - Max: 4.495828832720488 Min: 0.0\n","ReLU Activation - Max: 3.8942256250337683 Min: 0.0\n","Softmax Output - Max: 0.9259333992945062 Min: 8.866698135740249e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1307086499242586 Min: -0.11879870079228327\n","Layer 1 - Gradient Weights Max: 0.17714067536639266 Min: -0.1551119157179046\n","Layer 0 - Gradient Weights Max: 0.3167003540661067 Min: -0.18277625014488935\n","ReLU Activation - Max: 5.066469745262533 Min: 0.0\n","ReLU Activation - Max: 4.668670055845529 Min: 0.0\n","Softmax Output - Max: 0.9933902540481869 Min: 2.714636962584283e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08203352897278719 Min: -0.06677572957008417\n","Layer 1 - Gradient Weights Max: 0.1501147747557985 Min: -0.12793959423324905\n","Layer 0 - Gradient Weights Max: 0.19452460201216185 Min: -0.23006070752023655\n","ReLU Activation - Max: 4.181412604897118 Min: 0.0\n","ReLU Activation - Max: 3.6587092638139103 Min: 0.0\n","Softmax Output - Max: 0.9289765024239883 Min: 5.299197701503848e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06704713112026674 Min: -0.06397442275296822\n","Layer 1 - Gradient Weights Max: 0.16212350624004698 Min: -0.11757780852075421\n","Layer 0 - Gradient Weights Max: 0.17973607874399106 Min: -0.270892415610797\n","ReLU Activation - Max: 4.74793599623277 Min: 0.0\n","ReLU Activation - Max: 4.218020478423105 Min: 0.0\n","Softmax Output - Max: 0.9864689509278475 Min: 9.312942515910846e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11717281001799716 Min: -0.07640228892078249\n","Layer 1 - Gradient Weights Max: 0.12386527436701128 Min: -0.16392913737447912\n","Layer 0 - Gradient Weights Max: 0.1973300137091319 Min: -0.16647819654198737\n","ReLU Activation - Max: 4.495504228221574 Min: 0.0\n","ReLU Activation - Max: 3.6586290117469877 Min: 0.0\n","Softmax Output - Max: 0.9380325046837924 Min: 3.585058709336448e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11035870477838645 Min: -0.10778631116254908\n","Layer 1 - Gradient Weights Max: 0.1039753859266042 Min: -0.10317557935232315\n","Layer 0 - Gradient Weights Max: 0.14587746966747153 Min: -0.17631314092839873\n","ReLU Activation - Max: 3.7979822419504323 Min: 0.0\n","ReLU Activation - Max: 3.7714963053017083 Min: 0.0\n","Softmax Output - Max: 0.9306291592091007 Min: 4.320166033786721e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09325325776555231 Min: -0.20521969372205154\n","Layer 1 - Gradient Weights Max: 0.19120035370701607 Min: -0.12817112161348185\n","Layer 0 - Gradient Weights Max: 0.16996210320712857 Min: -0.1938020154551772\n","ReLU Activation - Max: 5.24145852064235 Min: 0.0\n","ReLU Activation - Max: 3.922629580500952 Min: 0.0\n","Softmax Output - Max: 0.9416603756250557 Min: 4.367302701522679e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10631567296445744 Min: -0.1380792038217095\n","Layer 1 - Gradient Weights Max: 0.1751081223658535 Min: -0.17754096068104908\n","Layer 0 - Gradient Weights Max: 0.19301962097028327 Min: -0.20123350656247083\n","ReLU Activation - Max: 4.644973288442233 Min: 0.0\n","ReLU Activation - Max: 4.301196562805772 Min: 0.0\n","Softmax Output - Max: 0.9487397352262673 Min: 4.120972261363888e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08117123540167119 Min: -0.13801356054999314\n","Layer 1 - Gradient Weights Max: 0.18708760008179964 Min: -0.14507712469165102\n","Layer 0 - Gradient Weights Max: 0.14831084064392897 Min: -0.16401144508206583\n","ReLU Activation - Max: 5.015301235217959 Min: 0.0\n","ReLU Activation - Max: 4.863232604472201 Min: 0.0\n","Softmax Output - Max: 0.9738710717751032 Min: 1.7382914154341574e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1341117678648202 Min: -0.15927137664148452\n","Layer 1 - Gradient Weights Max: 0.13758705191702766 Min: -0.16961768358945764\n","Layer 0 - Gradient Weights Max: 0.25981852277013867 Min: -0.25646387006641214\n","ReLU Activation - Max: 4.427553306497542 Min: 0.0\n","ReLU Activation - Max: 3.9979911765016407 Min: 0.0\n","Softmax Output - Max: 0.9462065928045189 Min: 3.877152668400576e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0939814146115134 Min: -0.09106284076739546\n","Layer 1 - Gradient Weights Max: 0.13939890039301506 Min: -0.13133483729414921\n","Layer 0 - Gradient Weights Max: 0.23031955457990208 Min: -0.26295102730741426\n","ReLU Activation - Max: 4.801281528306827 Min: 0.0\n","ReLU Activation - Max: 3.6582764853468888 Min: 0.0\n","Softmax Output - Max: 0.9698624451714734 Min: 1.260302502949767e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08859601155067372 Min: -0.07344035134092218\n","Layer 1 - Gradient Weights Max: 0.12288662561675312 Min: -0.1387280193687917\n","Layer 0 - Gradient Weights Max: 0.18444661017325745 Min: -0.2004604474944251\n","ReLU Activation - Max: 5.49584950905254 Min: 0.0\n","ReLU Activation - Max: 4.285386585424423 Min: 0.0\n","Softmax Output - Max: 0.9539780111293555 Min: 4.6603310583046894e-07 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1836791615743698 Min: -0.15948645280178433\n","Layer 1 - Gradient Weights Max: 0.19954876589376544 Min: -0.15912663239729852\n","Layer 0 - Gradient Weights Max: 0.1735383147886409 Min: -0.24107705693033335\n","ReLU Activation - Max: 4.713516721159098 Min: 0.0\n","ReLU Activation - Max: 4.436502996015051 Min: 0.0\n","Softmax Output - Max: 0.9533396490511589 Min: 5.185706325087601e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07013673236183894 Min: -0.1662905853374212\n","Layer 1 - Gradient Weights Max: 0.14403226977268968 Min: -0.18512346333222082\n","Layer 0 - Gradient Weights Max: 0.21144533199397478 Min: -0.2938252457621896\n","ReLU Activation - Max: 5.317106741604775 Min: 0.0\n","ReLU Activation - Max: 4.059840670006081 Min: 0.0\n","Softmax Output - Max: 0.9552358623828027 Min: 5.629058541780563e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08571158955296271 Min: -0.17138525184475484\n","Layer 1 - Gradient Weights Max: 0.18578470759398194 Min: -0.15797546465684958\n","Layer 0 - Gradient Weights Max: 0.15053145739657656 Min: -0.19723086774100615\n","ReLU Activation - Max: 4.772023430832841 Min: 0.0\n","ReLU Activation - Max: 4.312779471287611 Min: 0.0\n","Softmax Output - Max: 0.9658293312904638 Min: 7.106975608581913e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07236720275666389 Min: -0.11939935373961094\n","Layer 1 - Gradient Weights Max: 0.16195707893242958 Min: -0.14405098768687433\n","Layer 0 - Gradient Weights Max: 0.17310289038071844 Min: -0.1831684315245899\n","ReLU Activation - Max: 4.688991143382217 Min: 0.0\n","ReLU Activation - Max: 4.87217947549305 Min: 0.0\n","Softmax Output - Max: 0.959998301071671 Min: 3.657802856038271e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11952000541167396 Min: -0.14230966768162398\n","Layer 1 - Gradient Weights Max: 0.1680962094415543 Min: -0.1572266483731532\n","Layer 0 - Gradient Weights Max: 0.2987447471401162 Min: -0.3208873609245823\n","ReLU Activation - Max: 5.944843708959498 Min: 0.0\n","ReLU Activation - Max: 4.481860432334107 Min: 0.0\n","Softmax Output - Max: 0.9767276103631238 Min: 1.168739725799137e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0971552723506552 Min: -0.1082880467042008\n","Layer 1 - Gradient Weights Max: 0.13654091527920983 Min: -0.12499780259291503\n","Layer 0 - Gradient Weights Max: 0.17198266037140802 Min: -0.18220376879417202\n","ReLU Activation - Max: 4.4050708033387815 Min: 0.0\n","ReLU Activation - Max: 4.079448248026175 Min: 0.0\n","Softmax Output - Max: 0.977192222071985 Min: 3.04688179761092e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11580108470372265 Min: -0.12728717447891438\n","Layer 1 - Gradient Weights Max: 0.18166885820672912 Min: -0.22150192234116808\n","Layer 0 - Gradient Weights Max: 0.19219494865544143 Min: -0.17504264142975898\n","ReLU Activation - Max: 4.590046297094797 Min: 0.0\n","ReLU Activation - Max: 4.2393448887021234 Min: 0.0\n","Softmax Output - Max: 0.9795673034374072 Min: 9.584655830669333e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12329907705879146 Min: -0.16851957279649538\n","Layer 1 - Gradient Weights Max: 0.15256731396807877 Min: -0.23323201596614762\n","Layer 0 - Gradient Weights Max: 0.22736144707870332 Min: -0.1941260251623317\n","ReLU Activation - Max: 6.140459874505387 Min: 0.0\n","ReLU Activation - Max: 4.1527172636990795 Min: 0.0\n","Softmax Output - Max: 0.9510138674785431 Min: 3.9602805593655645e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11590538577092974 Min: -0.14295317066613614\n","Layer 1 - Gradient Weights Max: 0.1686863309794872 Min: -0.18679376015126162\n","Layer 0 - Gradient Weights Max: 0.1736875173079109 Min: -0.16290990291802185\n","ReLU Activation - Max: 3.9084632294245436 Min: 0.0\n","ReLU Activation - Max: 4.627773451726116 Min: 0.0\n","Softmax Output - Max: 0.9860867458301329 Min: 6.140353138078826e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09987045294500453 Min: -0.09112961657751469\n","Layer 1 - Gradient Weights Max: 0.15427522894785198 Min: -0.1396726943894292\n","Layer 0 - Gradient Weights Max: 0.1516786634563814 Min: -0.16000756925646595\n","ReLU Activation - Max: 4.489633201368593 Min: 0.0\n","ReLU Activation - Max: 3.917904634695145 Min: 0.0\n","Softmax Output - Max: 0.995471435610817 Min: 2.2510558004955288e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1034940263107082 Min: -0.1436456520505034\n","Layer 1 - Gradient Weights Max: 0.16767480865853202 Min: -0.1350905363505925\n","Layer 0 - Gradient Weights Max: 0.1601109198680613 Min: -0.1800273886127484\n","ReLU Activation - Max: 4.48432637668667 Min: 0.0\n","ReLU Activation - Max: 3.8581179546641016 Min: 0.0\n","Softmax Output - Max: 0.9165257098457512 Min: 7.309838802950589e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1348445428014364 Min: -0.11708513503129017\n","Layer 1 - Gradient Weights Max: 0.24467340161764264 Min: -0.15442088981929414\n","Layer 0 - Gradient Weights Max: 0.1798175445013991 Min: -0.18144189637565417\n","ReLU Activation - Max: 4.950886042094345 Min: 0.0\n","ReLU Activation - Max: 3.4406122208459697 Min: 0.0\n","Softmax Output - Max: 0.9498531406319125 Min: 1.2652540095688869e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10486219862116983 Min: -0.10137705809765098\n","Layer 1 - Gradient Weights Max: 0.15181428462995064 Min: -0.13348610997428423\n","Layer 0 - Gradient Weights Max: 0.21159908086682955 Min: -0.17529676834057345\n","ReLU Activation - Max: 5.877345297391555 Min: 0.0\n","ReLU Activation - Max: 4.142178437624397 Min: 0.0\n","Softmax Output - Max: 0.9396456610603606 Min: 4.181864023614782e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1655503258492058 Min: -0.12081291445982881\n","Layer 1 - Gradient Weights Max: 0.22885379040959936 Min: -0.14945944040505818\n","Layer 0 - Gradient Weights Max: 0.14627694869808142 Min: -0.13927113073634442\n","ReLU Activation - Max: 5.411883857061432 Min: 0.0\n","ReLU Activation - Max: 3.7322176838254326 Min: 0.0\n","Softmax Output - Max: 0.961443693108703 Min: 1.3814346139629312e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09388716402558243 Min: -0.14676069082884477\n","Layer 1 - Gradient Weights Max: 0.1395422398252394 Min: -0.17067462254797378\n","Layer 0 - Gradient Weights Max: 0.1830566916954154 Min: -0.2572783612817883\n","ReLU Activation - Max: 5.828103537681225 Min: 0.0\n","ReLU Activation - Max: 4.382396158312247 Min: 0.0\n","Softmax Output - Max: 0.9549641984974458 Min: 4.689594389476114e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.13242121272451055 Min: -0.12964120584757838\n","Layer 1 - Gradient Weights Max: 0.17452497326154656 Min: -0.17091637686037106\n","Layer 0 - Gradient Weights Max: 0.1907605457526165 Min: -0.18758488385046507\n","ReLU Activation - Max: 4.816989670132305 Min: 0.0\n","ReLU Activation - Max: 4.406819215220159 Min: 0.0\n","Softmax Output - Max: 0.9544599191362327 Min: 2.7013597835208662e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.117125769354508 Min: -0.12254296833765763\n","Layer 1 - Gradient Weights Max: 0.1639465323304936 Min: -0.23964521181050638\n","Layer 0 - Gradient Weights Max: 0.2661474331195313 Min: -0.25546572651673904\n","ReLU Activation - Max: 5.659441557216345 Min: 0.0\n","ReLU Activation - Max: 3.8648284741240295 Min: 0.0\n","Softmax Output - Max: 0.9709791097281241 Min: 1.9650117521726258e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09728020330883516 Min: -0.09510459691432034\n","Layer 1 - Gradient Weights Max: 0.1614771136390027 Min: -0.15925999318149348\n","Layer 0 - Gradient Weights Max: 0.1973051972626641 Min: -0.18257620935516858\n","ReLU Activation - Max: 4.751423308722792 Min: 0.0\n","ReLU Activation - Max: 3.592223435138447 Min: 0.0\n","Softmax Output - Max: 0.9605540192247415 Min: 5.279041826325056e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09309841609539277 Min: -0.14372424982974327\n","Layer 1 - Gradient Weights Max: 0.16459549088386666 Min: -0.19801035351973828\n","Layer 0 - Gradient Weights Max: 0.19109658309160493 Min: -0.21421089933091414\n","ReLU Activation - Max: 3.8263989369025295 Min: 0.0\n","ReLU Activation - Max: 3.9661238526933036 Min: 0.0\n","Softmax Output - Max: 0.9621188317743996 Min: 0.0001356474461163368 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1144986449607014 Min: -0.12661485853343674\n","Layer 1 - Gradient Weights Max: 0.17482893052796614 Min: -0.19988486801007546\n","Layer 0 - Gradient Weights Max: 0.23049158177635293 Min: -0.25220758851261293\n","ReLU Activation - Max: 5.2088198596610695 Min: 0.0\n","ReLU Activation - Max: 3.785637551403368 Min: 0.0\n","Softmax Output - Max: 0.9185432886471209 Min: 6.889660939031056e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0804164218071867 Min: -0.08227732391347095\n","Layer 1 - Gradient Weights Max: 0.14908033428238768 Min: -0.13118939202917856\n","Layer 0 - Gradient Weights Max: 0.19280568026351733 Min: -0.16500420105585858\n","ReLU Activation - Max: 5.563439604032599 Min: 0.0\n","ReLU Activation - Max: 4.187740563838914 Min: 0.0\n","Softmax Output - Max: 0.979826336066868 Min: 1.3173728735146746e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11309112315055163 Min: -0.15872560572437372\n","Layer 1 - Gradient Weights Max: 0.12051806924018556 Min: -0.17472257527830481\n","Layer 0 - Gradient Weights Max: 0.1728823780268705 Min: -0.15725877958704818\n","ReLU Activation - Max: 4.737949075562334 Min: 0.0\n","ReLU Activation - Max: 4.221579145292081 Min: 0.0\n","Softmax Output - Max: 0.9739056575589261 Min: 4.3451726248158726e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10025057653886908 Min: -0.11385614719992251\n","Layer 1 - Gradient Weights Max: 0.1500253708529243 Min: -0.14187599253898855\n","Layer 0 - Gradient Weights Max: 0.15508729689180822 Min: -0.15112341604613377\n","ReLU Activation - Max: 6.2089947067724784 Min: 0.0\n","ReLU Activation - Max: 4.279940944251015 Min: 0.0\n","Softmax Output - Max: 0.9484993601989321 Min: 2.4545630036609577e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07769262181944488 Min: -0.09360599963615113\n","Layer 1 - Gradient Weights Max: 0.1889602778837998 Min: -0.14476777574670688\n","Layer 0 - Gradient Weights Max: 0.2884679410773096 Min: -0.21393398967923571\n","ReLU Activation - Max: 4.5292709598908285 Min: 0.0\n","ReLU Activation - Max: 3.3686127641234638 Min: 0.0\n","Softmax Output - Max: 0.9933956688203425 Min: 1.2546548235033354e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14177874147942343 Min: -0.12692993987566462\n","Layer 1 - Gradient Weights Max: 0.2114743483028944 Min: -0.1523562929021468\n","Layer 0 - Gradient Weights Max: 0.2080932148223114 Min: -0.23639065188195293\n","ReLU Activation - Max: 5.48262266140854 Min: 0.0\n","ReLU Activation - Max: 4.674339725110014 Min: 0.0\n","Softmax Output - Max: 0.9362813873487621 Min: 2.5103208642062483e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09847100426728277 Min: -0.1453376429586202\n","Layer 1 - Gradient Weights Max: 0.13798995513249326 Min: -0.1661351595331068\n","Layer 0 - Gradient Weights Max: 0.16284031188876608 Min: -0.15555236866057703\n","ReLU Activation - Max: 5.368871125810579 Min: 0.0\n","ReLU Activation - Max: 4.145982630909132 Min: 0.0\n","Softmax Output - Max: 0.982690708873471 Min: 1.1078040161395644e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07522924225295162 Min: -0.09712476017876652\n","Layer 1 - Gradient Weights Max: 0.16528416312034522 Min: -0.19062684302071084\n","Layer 0 - Gradient Weights Max: 0.16467923012269967 Min: -0.17974693556977764\n","ReLU Activation - Max: 5.157241960171858 Min: 0.0\n","ReLU Activation - Max: 3.90936770557414 Min: 0.0\n","Softmax Output - Max: 0.9737680762003261 Min: 4.069501732676315e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1058445601813112 Min: -0.12125840274612446\n","Layer 1 - Gradient Weights Max: 0.1744992430164388 Min: -0.19259573081115772\n","Layer 0 - Gradient Weights Max: 0.16368246804460376 Min: -0.18272121987545864\n","ReLU Activation - Max: 5.308372440090424 Min: 0.0\n","ReLU Activation - Max: 3.9818587051690097 Min: 0.0\n","Softmax Output - Max: 0.9558763589514184 Min: 1.4430230710811306e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11169238105478409 Min: -0.12308310152557504\n","Layer 1 - Gradient Weights Max: 0.13490616313589268 Min: -0.15994630508548582\n","Layer 0 - Gradient Weights Max: 0.19072810852574584 Min: -0.2308919916301778\n","ReLU Activation - Max: 5.110203306131862 Min: 0.0\n","ReLU Activation - Max: 4.374964191252263 Min: 0.0\n","Softmax Output - Max: 0.9829143113877227 Min: 6.685891215529633e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11240921489365288 Min: -0.07779886496380471\n","Layer 1 - Gradient Weights Max: 0.14084761129195414 Min: -0.15265976867695266\n","Layer 0 - Gradient Weights Max: 0.2651330345751894 Min: -0.22422644016522747\n","ReLU Activation - Max: 4.843173839685917 Min: 0.0\n","ReLU Activation - Max: 3.786079734354915 Min: 0.0\n","Softmax Output - Max: 0.9122983870647612 Min: 5.818542477100535e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08169188387327338 Min: -0.10870150210408999\n","Layer 1 - Gradient Weights Max: 0.15855423359823503 Min: -0.18570907024130306\n","Layer 0 - Gradient Weights Max: 0.19772202579273626 Min: -0.22038085989021744\n","ReLU Activation - Max: 4.74975693404908 Min: 0.0\n","ReLU Activation - Max: 4.32706309869143 Min: 0.0\n","Softmax Output - Max: 0.9786346071827895 Min: 1.838802186368596e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1357150206407288 Min: -0.15664563150966257\n","Layer 1 - Gradient Weights Max: 0.12297146795423844 Min: -0.15080628245884722\n","Layer 0 - Gradient Weights Max: 0.16175025104996185 Min: -0.1634288212134125\n","ReLU Activation - Max: 5.607223294768858 Min: 0.0\n","ReLU Activation - Max: 4.120741564833033 Min: 0.0\n","Softmax Output - Max: 0.9440357853967501 Min: 7.352997496131398e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07990773872456798 Min: -0.131148856718517\n","Layer 1 - Gradient Weights Max: 0.150649182596377 Min: -0.15048386144172982\n","Layer 0 - Gradient Weights Max: 0.17501387781547875 Min: -0.18107352270368018\n","ReLU Activation - Max: 5.681281803614655 Min: 0.0\n","ReLU Activation - Max: 4.326030875385742 Min: 0.0\n","Softmax Output - Max: 0.978713228282264 Min: 8.971451499973223e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09007635403984561 Min: -0.15751441455310777\n","Layer 1 - Gradient Weights Max: 0.13725320737703234 Min: -0.1514587597478936\n","Layer 0 - Gradient Weights Max: 0.18207124849128975 Min: -0.15104238156174882\n","ReLU Activation - Max: 4.924329417039722 Min: 0.0\n","ReLU Activation - Max: 4.750876546733763 Min: 0.0\n","Softmax Output - Max: 0.9509383911789909 Min: 9.831770989513008e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07849317501592576 Min: -0.07340784189578067\n","Layer 1 - Gradient Weights Max: 0.10649987641154078 Min: -0.14124441881619812\n","Layer 0 - Gradient Weights Max: 0.20586432114117484 Min: -0.19725197562928426\n","ReLU Activation - Max: 5.135441474652245 Min: 0.0\n","ReLU Activation - Max: 5.1647253005791205 Min: 0.0\n","Softmax Output - Max: 0.9655327665239178 Min: 5.774307406298877e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07952479271679186 Min: -0.13039325816577782\n","Layer 1 - Gradient Weights Max: 0.153006130232467 Min: -0.19765862437395942\n","Layer 0 - Gradient Weights Max: 0.21212792436703412 Min: -0.19493271834029166\n","ReLU Activation - Max: 4.832024833229558 Min: 0.0\n","ReLU Activation - Max: 4.181573776637458 Min: 0.0\n","Softmax Output - Max: 0.9712355306298214 Min: 0.00013304196546823948 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.099423115919567 Min: -0.09747249394859528\n","Layer 1 - Gradient Weights Max: 0.1341317123443608 Min: -0.12822360075278566\n","Layer 0 - Gradient Weights Max: 0.23100613721942564 Min: -0.23209750852169944\n","ReLU Activation - Max: 5.828903660385022 Min: 0.0\n","ReLU Activation - Max: 3.5975958782894653 Min: 0.0\n","Softmax Output - Max: 0.9859687672729271 Min: 1.0554507292729516e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09024827857720197 Min: -0.12946402015542158\n","Layer 1 - Gradient Weights Max: 0.16080409781324773 Min: -0.24712148363367245\n","Layer 0 - Gradient Weights Max: 0.3157276259383154 Min: -0.17085512870526726\n","ReLU Activation - Max: 5.4291169909181445 Min: 0.0\n","ReLU Activation - Max: 4.319973918800457 Min: 0.0\n","Softmax Output - Max: 0.9933559970824829 Min: 3.3207137303257145e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10044052769150205 Min: -0.10833382130287257\n","Layer 1 - Gradient Weights Max: 0.1830982581373569 Min: -0.25537391162260004\n","Layer 0 - Gradient Weights Max: 0.2671417555188085 Min: -0.16232666264180512\n","ReLU Activation - Max: 5.532186047197106 Min: 0.0\n","ReLU Activation - Max: 4.505550991078798 Min: 0.0\n","Softmax Output - Max: 0.9784560180715641 Min: 2.2528483409067053e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07983638245636514 Min: -0.10530551069858883\n","Layer 1 - Gradient Weights Max: 0.1373223061612208 Min: -0.11406774132532978\n","Layer 0 - Gradient Weights Max: 0.16315435568941386 Min: -0.19134093570544955\n","ReLU Activation - Max: 4.950472610845619 Min: 0.0\n","ReLU Activation - Max: 4.869548619966558 Min: 0.0\n","Softmax Output - Max: 0.9867033224542777 Min: 2.7182986996649928e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06486948785486832 Min: -0.09899273686235346\n","Layer 1 - Gradient Weights Max: 0.12479647444536786 Min: -0.17891914282747826\n","Layer 0 - Gradient Weights Max: 0.22071956402886286 Min: -0.2459035692180375\n","ReLU Activation - Max: 6.950586076685817 Min: 0.0\n","ReLU Activation - Max: 4.247657654030366 Min: 0.0\n","Softmax Output - Max: 0.9753930508220939 Min: 4.210355812133944e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1400671608179258 Min: -0.10389004372533003\n","Layer 1 - Gradient Weights Max: 0.15321199329843635 Min: -0.15784710156530085\n","Layer 0 - Gradient Weights Max: 0.1648945342443366 Min: -0.20575010681257158\n","ReLU Activation - Max: 5.890074014395699 Min: 0.0\n","ReLU Activation - Max: 4.314321478032388 Min: 0.0\n","Softmax Output - Max: 0.9696811433244579 Min: 7.086432466978881e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09988439026415029 Min: -0.15158159897570184\n","Layer 1 - Gradient Weights Max: 0.1594002722220405 Min: -0.13988544395891722\n","Layer 0 - Gradient Weights Max: 0.15335121311427785 Min: -0.15442234455141968\n","ReLU Activation - Max: 4.832196923015363 Min: 0.0\n","ReLU Activation - Max: 3.778051186312851 Min: 0.0\n","Softmax Output - Max: 0.9867418660117825 Min: 4.019960060421732e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1319628124605831 Min: -0.17331899409426774\n","Layer 1 - Gradient Weights Max: 0.199393201839136 Min: -0.1334212311861401\n","Layer 0 - Gradient Weights Max: 0.25128701538478204 Min: -0.1922983187438696\n","ReLU Activation - Max: 5.355236434299238 Min: 0.0\n","ReLU Activation - Max: 3.937637653718868 Min: 0.0\n","Softmax Output - Max: 0.975785743193964 Min: 4.378457390415407e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07469492198342784 Min: -0.12016350747932346\n","Layer 1 - Gradient Weights Max: 0.1278832896855601 Min: -0.1445664585914507\n","Layer 0 - Gradient Weights Max: 0.18067344029279522 Min: -0.16310172184121027\n","ReLU Activation - Max: 5.424088631024622 Min: 0.0\n","ReLU Activation - Max: 4.51576210828667 Min: 0.0\n","Softmax Output - Max: 0.9773839439613881 Min: 1.5907446675983543e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09360876689276308 Min: -0.11130146841719919\n","Layer 1 - Gradient Weights Max: 0.1433767255162288 Min: -0.2165383361149554\n","Layer 0 - Gradient Weights Max: 0.220023970861771 Min: -0.18640603407061954\n","ReLU Activation - Max: 4.321342994090078 Min: 0.0\n","ReLU Activation - Max: 4.782594119672184 Min: 0.0\n","Softmax Output - Max: 0.9615682358584534 Min: 1.5494918076765196e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.15933507105372638 Min: -0.14701127014664497\n","Layer 1 - Gradient Weights Max: 0.126093576458649 Min: -0.16120031987316571\n","Layer 0 - Gradient Weights Max: 0.2509578833631156 Min: -0.1694673625726338\n","ReLU Activation - Max: 5.860932191598881 Min: 0.0\n","ReLU Activation - Max: 4.031998167381387 Min: 0.0\n","Softmax Output - Max: 0.9586231383682422 Min: 6.420726054098056e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07030731622017986 Min: -0.10807993160332169\n","Layer 1 - Gradient Weights Max: 0.1414178676945948 Min: -0.14614844164162125\n","Layer 0 - Gradient Weights Max: 0.14377640727910615 Min: -0.23191051652910502\n","ReLU Activation - Max: 4.811453451194698 Min: 0.0\n","ReLU Activation - Max: 3.953711772593297 Min: 0.0\n","Softmax Output - Max: 0.9681953772435791 Min: 5.009283274354529e-06 Sum (first example): 1.0000000000000004\n","Layer 2 - Gradient Weights Max: 0.10232276612927554 Min: -0.12390904130124886\n","Layer 1 - Gradient Weights Max: 0.18020586955384352 Min: -0.1914844462458384\n","Layer 0 - Gradient Weights Max: 0.293092748263532 Min: -0.21437847508305588\n","ReLU Activation - Max: 6.170523131236515 Min: 0.0\n","ReLU Activation - Max: 4.178635819938726 Min: 0.0\n","Softmax Output - Max: 0.9405797037361077 Min: 2.939336636873438e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09282200712399746 Min: -0.1014084259533962\n","Layer 1 - Gradient Weights Max: 0.17139443990383127 Min: -0.16327168188295305\n","Layer 0 - Gradient Weights Max: 0.29551024699652323 Min: -0.18114610424329725\n","ReLU Activation - Max: 4.908624493084831 Min: 0.0\n","ReLU Activation - Max: 4.0094901449165725 Min: 0.0\n","Softmax Output - Max: 0.9434996863709064 Min: 1.1193127080991434e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0832278270362946 Min: -0.12092471234344317\n","Layer 1 - Gradient Weights Max: 0.20081386750797714 Min: -0.18875887245969009\n","Layer 0 - Gradient Weights Max: 0.24516918035996077 Min: -0.17835948751986266\n","ReLU Activation - Max: 4.246886185919831 Min: 0.0\n","ReLU Activation - Max: 3.962363254492597 Min: 0.0\n","Softmax Output - Max: 0.9610972860053086 Min: 1.5111259154027116e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07267739309105786 Min: -0.0993274669079053\n","Layer 1 - Gradient Weights Max: 0.11268378358536357 Min: -0.1447688311715342\n","Layer 0 - Gradient Weights Max: 0.1635018292418471 Min: -0.14814550477475\n","ReLU Activation - Max: 5.036548155464677 Min: 0.0\n","ReLU Activation - Max: 3.94276039475614 Min: 0.0\n","Softmax Output - Max: 0.9390303126036165 Min: 4.8765167788928135e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10713694788394731 Min: -0.1988256756881786\n","Layer 1 - Gradient Weights Max: 0.15269618848678887 Min: -0.1284403044834192\n","Layer 0 - Gradient Weights Max: 0.18605283037716433 Min: -0.21123481398917107\n","ReLU Activation - Max: 5.072831435451884 Min: 0.0\n","ReLU Activation - Max: 4.2333176054024975 Min: 0.0\n","Softmax Output - Max: 0.9890849847643522 Min: 0.0001302087796091823 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07466368925989057 Min: -0.06559488989990192\n","Layer 1 - Gradient Weights Max: 0.19008132049168153 Min: -0.11928175912777875\n","Layer 0 - Gradient Weights Max: 0.17767375517018136 Min: -0.22442265670998757\n","ReLU Activation - Max: 4.922930035464006 Min: 0.0\n","ReLU Activation - Max: 4.451589098568281 Min: 0.0\n","Softmax Output - Max: 0.9809077648267139 Min: 5.300749100177441e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11756054815447449 Min: -0.1642573573794358\n","Layer 1 - Gradient Weights Max: 0.11460492702298711 Min: -0.23423418864909165\n","Layer 0 - Gradient Weights Max: 0.1773904613979743 Min: -0.24349376927173155\n","ReLU Activation - Max: 5.224844092258319 Min: 0.0\n","ReLU Activation - Max: 4.1669855054339315 Min: 0.0\n","Softmax Output - Max: 0.9775664122021802 Min: 3.5254139293783245e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07868766964379234 Min: -0.06326191825278087\n","Layer 1 - Gradient Weights Max: 0.12806096585445512 Min: -0.14275765746777921\n","Layer 0 - Gradient Weights Max: 0.19112760112320554 Min: -0.17352033413638576\n","ReLU Activation - Max: 4.745737586022113 Min: 0.0\n","ReLU Activation - Max: 4.5522526571148125 Min: 0.0\n","Softmax Output - Max: 0.9770309073586573 Min: 4.8879358523530346e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12018944774891557 Min: -0.14142983333971715\n","Layer 1 - Gradient Weights Max: 0.1559366323616343 Min: -0.20521831699985268\n","Layer 0 - Gradient Weights Max: 0.24052633459794298 Min: -0.20233751874106573\n","ReLU Activation - Max: 4.541567701922549 Min: 0.0\n","ReLU Activation - Max: 3.889426500596639 Min: 0.0\n","Softmax Output - Max: 0.9657222175687136 Min: 1.0251106216088465e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09810065503775249 Min: -0.14589404060369293\n","Layer 1 - Gradient Weights Max: 0.18544806147627593 Min: -0.163603160905303\n","Layer 0 - Gradient Weights Max: 0.17442947896680672 Min: -0.18879618133618528\n","ReLU Activation - Max: 4.47659832877275 Min: 0.0\n","ReLU Activation - Max: 3.6741021788481127 Min: 0.0\n","Softmax Output - Max: 0.9105041566861435 Min: 0.00019701802972043548 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11967289551788582 Min: -0.13532436836582445\n","Layer 1 - Gradient Weights Max: 0.15816880429107674 Min: -0.21462491025456673\n","Layer 0 - Gradient Weights Max: 0.28024051311559467 Min: -0.2078105702311779\n","ReLU Activation - Max: 4.7634123764114245 Min: 0.0\n","ReLU Activation - Max: 4.0655919605154685 Min: 0.0\n","Softmax Output - Max: 0.9648247243840782 Min: 3.539267439069269e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06713574766597628 Min: -0.08794598854370626\n","Layer 1 - Gradient Weights Max: 0.15417632312864227 Min: -0.15573488504606078\n","Layer 0 - Gradient Weights Max: 0.1316419908678252 Min: -0.16116134725311773\n","ReLU Activation - Max: 4.476659057295209 Min: 0.0\n","ReLU Activation - Max: 3.593878647088345 Min: 0.0\n","Softmax Output - Max: 0.963872088216765 Min: 1.9466495551070777e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0628609102232218 Min: -0.09125527380786673\n","Layer 1 - Gradient Weights Max: 0.14801501134521322 Min: -0.0983301685714375\n","Layer 0 - Gradient Weights Max: 0.17415811083985538 Min: -0.20233339666641398\n","ReLU Activation - Max: 4.887323804776807 Min: 0.0\n","ReLU Activation - Max: 3.7740487853228313 Min: 0.0\n","Softmax Output - Max: 0.9653602203061624 Min: 7.553099386611825e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.05833587433884083 Min: -0.11537650611795919\n","Layer 1 - Gradient Weights Max: 0.10691315555975239 Min: -0.11824209993818655\n","Layer 0 - Gradient Weights Max: 0.16707375355021392 Min: -0.13414552828632834\n","ReLU Activation - Max: 4.497009783794287 Min: 0.0\n","ReLU Activation - Max: 3.9607551253496434 Min: 0.0\n","Softmax Output - Max: 0.9759595088169278 Min: 1.3565672941093716e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09936528665998383 Min: -0.09997903068008365\n","Layer 1 - Gradient Weights Max: 0.1857307537155891 Min: -0.13522206868712006\n","Layer 0 - Gradient Weights Max: 0.1558839382481632 Min: -0.18727142218908985\n","ReLU Activation - Max: 5.791224019927696 Min: 0.0\n","ReLU Activation - Max: 3.7143430220748748 Min: 0.0\n","Softmax Output - Max: 0.9330737507759035 Min: 4.66125482999993e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08364329326930559 Min: -0.10080494214107229\n","Layer 1 - Gradient Weights Max: 0.15763215550224008 Min: -0.14413914665655853\n","Layer 0 - Gradient Weights Max: 0.15684226098833562 Min: -0.17736758555179138\n","ReLU Activation - Max: 4.190577255165644 Min: 0.0\n","ReLU Activation - Max: 4.561132465385301 Min: 0.0\n","Softmax Output - Max: 0.9515124529171055 Min: 2.0370441500768053e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11844657632569454 Min: -0.12207955102264774\n","Layer 1 - Gradient Weights Max: 0.17115839145777567 Min: -0.16678789117799\n","Layer 0 - Gradient Weights Max: 0.17009556478486346 Min: -0.17510463451913638\n","ReLU Activation - Max: 3.9262367103807394 Min: 0.0\n","ReLU Activation - Max: 3.735480696987671 Min: 0.0\n","Softmax Output - Max: 0.9717685187510552 Min: 3.589467173891947e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06410410303987914 Min: -0.1163233986543283\n","Layer 1 - Gradient Weights Max: 0.21295441908625565 Min: -0.12177695797958217\n","Layer 0 - Gradient Weights Max: 0.1334043839871917 Min: -0.19901719847758736\n","ReLU Activation - Max: 4.65985877752569 Min: 0.0\n","ReLU Activation - Max: 3.8941014239892553 Min: 0.0\n","Softmax Output - Max: 0.9420752770953137 Min: 3.668131221814455e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11460788934649951 Min: -0.11883198057839256\n","Layer 1 - Gradient Weights Max: 0.12330237290862278 Min: -0.18773298571066135\n","Layer 0 - Gradient Weights Max: 0.24190809914383035 Min: -0.15462011188048794\n","ReLU Activation - Max: 5.640139444719277 Min: 0.0\n","ReLU Activation - Max: 3.8915534844733823 Min: 0.0\n","Softmax Output - Max: 0.9714281235562495 Min: 3.713259170698541e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09738505290152022 Min: -0.13106145351252643\n","Layer 1 - Gradient Weights Max: 0.1560995371995515 Min: -0.16796986968518685\n","Layer 0 - Gradient Weights Max: 0.20271654968013722 Min: -0.25533322365096317\n","ReLU Activation - Max: 5.638735725824404 Min: 0.0\n","ReLU Activation - Max: 4.47987137146343 Min: 0.0\n","Softmax Output - Max: 0.9950923180323272 Min: 1.0723875861350802e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.0985849060063316 Min: -0.12041090087118062\n","Layer 1 - Gradient Weights Max: 0.16679984667584616 Min: -0.1877798049728642\n","Layer 0 - Gradient Weights Max: 0.2377534891080369 Min: -0.1859818102195533\n","ReLU Activation - Max: 5.496932413319842 Min: 0.0\n","ReLU Activation - Max: 4.3386689953708215 Min: 0.0\n","Softmax Output - Max: 0.9856714499868989 Min: 7.479552256623092e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07202711006924287 Min: -0.11704936269405143\n","Layer 1 - Gradient Weights Max: 0.11094127862418281 Min: -0.13237064421605815\n","Layer 0 - Gradient Weights Max: 0.14694701928510137 Min: -0.14083020599372573\n","ReLU Activation - Max: 4.577114287318912 Min: 0.0\n","ReLU Activation - Max: 4.42964587078323 Min: 0.0\n","Softmax Output - Max: 0.9682480072659815 Min: 8.971310769780131e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07369098712647795 Min: -0.11109729288712844\n","Layer 1 - Gradient Weights Max: 0.13025043884839033 Min: -0.15290266238087016\n","Layer 0 - Gradient Weights Max: 0.1680376621351311 Min: -0.26163275428028443\n","ReLU Activation - Max: 4.736775073983818 Min: 0.0\n","ReLU Activation - Max: 5.210401800036767 Min: 0.0\n","Softmax Output - Max: 0.9954816409293016 Min: 2.686032508882309e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09874670035447376 Min: -0.13522467721386058\n","Layer 1 - Gradient Weights Max: 0.1639798348940491 Min: -0.1938311492277539\n","Layer 0 - Gradient Weights Max: 0.2186403955036471 Min: -0.23099001066474809\n","ReLU Activation - Max: 4.765307513074637 Min: 0.0\n","ReLU Activation - Max: 5.676120802532418 Min: 0.0\n","Softmax Output - Max: 0.973271411851869 Min: 4.87121513634862e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.05920735307883373 Min: -0.16159719741368564\n","Layer 1 - Gradient Weights Max: 0.11117228296343344 Min: -0.14480928311727118\n","Layer 0 - Gradient Weights Max: 0.18278967916008643 Min: -0.1332071214285164\n","ReLU Activation - Max: 4.057028539051348 Min: 0.0\n","ReLU Activation - Max: 4.135862301422109 Min: 0.0\n","Softmax Output - Max: 0.9703437989636252 Min: 1.4289326317572974e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14696651489911985 Min: -0.18826000713417956\n","Layer 1 - Gradient Weights Max: 0.16995635316293264 Min: -0.19787324832838848\n","Layer 0 - Gradient Weights Max: 0.22523653385992667 Min: -0.2504196478510803\n","ReLU Activation - Max: 5.485090435574342 Min: 0.0\n","ReLU Activation - Max: 4.337981097480849 Min: 0.0\n","Softmax Output - Max: 0.9873232768564334 Min: 3.875827825555735e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08865014570524635 Min: -0.06533662615404197\n","Layer 1 - Gradient Weights Max: 0.12343299431261553 Min: -0.1296402904120836\n","Layer 0 - Gradient Weights Max: 0.175271036443147 Min: -0.1596619707533647\n","ReLU Activation - Max: 4.736205722056237 Min: 0.0\n","ReLU Activation - Max: 3.607858394581391 Min: 0.0\n","Softmax Output - Max: 0.9036827470029563 Min: 5.2949293213052414e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08732859095982791 Min: -0.09505719032225346\n","Layer 1 - Gradient Weights Max: 0.16996589516188404 Min: -0.20658254193828673\n","Layer 0 - Gradient Weights Max: 0.1772998855403361 Min: -0.17346832529218129\n","ReLU Activation - Max: 4.270574536026713 Min: 0.0\n","ReLU Activation - Max: 4.801948330254013 Min: 0.0\n","Softmax Output - Max: 0.9826408118594656 Min: 5.610321588710534e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07957130133584618 Min: -0.09038183504354598\n","Layer 1 - Gradient Weights Max: 0.1534538113718064 Min: -0.17534610417516702\n","Layer 0 - Gradient Weights Max: 0.1766076865268656 Min: -0.30137679834821396\n","ReLU Activation - Max: 6.592558154801958 Min: 0.0\n","ReLU Activation - Max: 4.885393219805204 Min: 0.0\n","Softmax Output - Max: 0.9928950580008102 Min: 1.4717504702478804e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08240293927182014 Min: -0.08183463653108816\n","Layer 1 - Gradient Weights Max: 0.12436264550393299 Min: -0.1418520278787525\n","Layer 0 - Gradient Weights Max: 0.21970497915366757 Min: -0.3896796780018977\n","ReLU Activation - Max: 4.31095131046796 Min: 0.0\n","ReLU Activation - Max: 3.775732341548931 Min: 0.0\n","Softmax Output - Max: 0.9938539589363753 Min: 2.7997283219070073e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11587321256274986 Min: -0.1429554600398893\n","Layer 1 - Gradient Weights Max: 0.13108546140661437 Min: -0.14859766429473886\n","Layer 0 - Gradient Weights Max: 0.1571101070773513 Min: -0.2114645044412371\n","ReLU Activation - Max: 4.377354611221661 Min: 0.0\n","ReLU Activation - Max: 3.880436614583583 Min: 0.0\n","Softmax Output - Max: 0.9759643923040303 Min: 3.4798205084158546e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.049840538046422356 Min: -0.05789947012853981\n","Layer 1 - Gradient Weights Max: 0.10795972201287304 Min: -0.11803886265024903\n","Layer 0 - Gradient Weights Max: 0.16414270805458916 Min: -0.21195712171120198\n","ReLU Activation - Max: 4.8585698114932265 Min: 0.0\n","ReLU Activation - Max: 4.185394259308204 Min: 0.0\n","Softmax Output - Max: 0.9292916475424029 Min: 6.203947126173398e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12714161235282828 Min: -0.11636618563013171\n","Layer 1 - Gradient Weights Max: 0.1840186448804792 Min: -0.14668490099503584\n","Layer 0 - Gradient Weights Max: 0.16070828596222164 Min: -0.20565229502777974\n","ReLU Activation - Max: 5.89682954925699 Min: 0.0\n","ReLU Activation - Max: 3.60338687578171 Min: 0.0\n","Softmax Output - Max: 0.9209765424716219 Min: 6.501066841160188e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10370695708797414 Min: -0.13611039719900603\n","Layer 1 - Gradient Weights Max: 0.14290464973082595 Min: -0.13829453851351364\n","Layer 0 - Gradient Weights Max: 0.16868866437910748 Min: -0.18091679946605152\n","ReLU Activation - Max: 5.6731427655761255 Min: 0.0\n","ReLU Activation - Max: 3.8528057745191577 Min: 0.0\n","Softmax Output - Max: 0.9524348592259454 Min: 1.3000839951934973e-05 Sum (first example): 1.0000000000000004\n","Layer 2 - Gradient Weights Max: 0.12071857455844649 Min: -0.09756709844740445\n","Layer 1 - Gradient Weights Max: 0.16035507138311494 Min: -0.1781387174072078\n","Layer 0 - Gradient Weights Max: 0.2073600125618213 Min: -0.22923939004947003\n","ReLU Activation - Max: 4.678868090846386 Min: 0.0\n","ReLU Activation - Max: 4.194402653638795 Min: 0.0\n","Softmax Output - Max: 0.982499492792346 Min: 2.5822187887798803e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10784995826756415 Min: -0.11489410350208115\n","Layer 1 - Gradient Weights Max: 0.1200080130890324 Min: -0.1348794574717876\n","Layer 0 - Gradient Weights Max: 0.16553896849423705 Min: -0.14552010033405877\n","ReLU Activation - Max: 4.9495987177575484 Min: 0.0\n","ReLU Activation - Max: 4.329028836081761 Min: 0.0\n","Softmax Output - Max: 0.9833862060269571 Min: 1.1516721884717204e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10785745059724693 Min: -0.12008812648313519\n","Layer 1 - Gradient Weights Max: 0.1805668798114595 Min: -0.18582590328050225\n","Layer 0 - Gradient Weights Max: 0.20261822650652547 Min: -0.1811641160184158\n","ReLU Activation - Max: 5.264552662164556 Min: 0.0\n","ReLU Activation - Max: 4.295654538253597 Min: 0.0\n","Softmax Output - Max: 0.9718831838384483 Min: 3.5188159573720294e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08952788417252935 Min: -0.11805709288268044\n","Layer 1 - Gradient Weights Max: 0.10559912041681986 Min: -0.16341391906946146\n","Layer 0 - Gradient Weights Max: 0.17153994216243648 Min: -0.25788225336454235\n","ReLU Activation - Max: 4.8320746634275755 Min: 0.0\n","ReLU Activation - Max: 3.852915757789657 Min: 0.0\n","Softmax Output - Max: 0.9282295412140478 Min: 2.4171236874659454e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07175407296926925 Min: -0.11848050546535872\n","Layer 1 - Gradient Weights Max: 0.15570722004466864 Min: -0.18005201226073927\n","Layer 0 - Gradient Weights Max: 0.2628706813583393 Min: -0.2265158965688605\n","ReLU Activation - Max: 4.71393643997799 Min: 0.0\n","ReLU Activation - Max: 4.158204048314042 Min: 0.0\n","Softmax Output - Max: 0.9872525030260766 Min: 1.1643208847087658e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10179135205356818 Min: -0.07694337970459945\n","Layer 1 - Gradient Weights Max: 0.1503777205699047 Min: -0.13050485600131204\n","Layer 0 - Gradient Weights Max: 0.2906258333245268 Min: -0.20847980427587548\n","ReLU Activation - Max: 4.70474998560228 Min: 0.0\n","ReLU Activation - Max: 3.7563346079892295 Min: 0.0\n","Softmax Output - Max: 0.9888317865653592 Min: 3.331874769252617e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08728285017178064 Min: -0.16258805245587216\n","Layer 1 - Gradient Weights Max: 0.13188423247278172 Min: -0.1421404120201417\n","Layer 0 - Gradient Weights Max: 0.14944894511657192 Min: -0.20132195623103447\n","ReLU Activation - Max: 4.828408941794413 Min: 0.0\n","ReLU Activation - Max: 4.006474902891125 Min: 0.0\n","Softmax Output - Max: 0.9237737018684937 Min: 8.322824229799425e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10743163283702477 Min: -0.08523734444888338\n","Layer 1 - Gradient Weights Max: 0.1745053801689466 Min: -0.13912262579162593\n","Layer 0 - Gradient Weights Max: 0.1816219850539802 Min: -0.17210811972801604\n","ReLU Activation - Max: 4.742955557937224 Min: 0.0\n","ReLU Activation - Max: 4.134365680545901 Min: 0.0\n","Softmax Output - Max: 0.9890246814256449 Min: 3.862815512090491e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11781568500897166 Min: -0.10994819454220364\n","Layer 1 - Gradient Weights Max: 0.18962600921640363 Min: -0.17075965196198076\n","Layer 0 - Gradient Weights Max: 0.27513261291785573 Min: -0.2504592881856272\n","ReLU Activation - Max: 5.154446665415982 Min: 0.0\n","ReLU Activation - Max: 3.767595685496563 Min: 0.0\n","Softmax Output - Max: 0.9817404139911545 Min: 3.265117999095055e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07935032627343307 Min: -0.09245942424700979\n","Layer 1 - Gradient Weights Max: 0.12737870087043845 Min: -0.12856078451217826\n","Layer 0 - Gradient Weights Max: 0.17880712415689165 Min: -0.15549355381038307\n","ReLU Activation - Max: 5.765055928991285 Min: 0.0\n","ReLU Activation - Max: 4.194918505284772 Min: 0.0\n","Softmax Output - Max: 0.9939842405580088 Min: 7.176268547912395e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1005510592185582 Min: -0.07912746941302862\n","Layer 1 - Gradient Weights Max: 0.09124917779643862 Min: -0.18324552600969335\n","Layer 0 - Gradient Weights Max: 0.15886999654112086 Min: -0.16176811316869202\n","ReLU Activation - Max: 4.848640172228818 Min: 0.0\n","ReLU Activation - Max: 4.2968173893807755 Min: 0.0\n","Softmax Output - Max: 0.953081642982491 Min: 8.895540753882883e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07953954426438943 Min: -0.08046777026317323\n","Layer 1 - Gradient Weights Max: 0.15678049153805393 Min: -0.1470860415275029\n","Layer 0 - Gradient Weights Max: 0.18441817379874262 Min: -0.2060365805352123\n","ReLU Activation - Max: 4.864071407133148 Min: 0.0\n","ReLU Activation - Max: 3.6148884982420073 Min: 0.0\n","Softmax Output - Max: 0.9904164859177195 Min: 3.2107911355270095e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14894743104260477 Min: -0.17120519730903314\n","Layer 1 - Gradient Weights Max: 0.2341589790826064 Min: -0.22888550923078108\n","Layer 0 - Gradient Weights Max: 0.20569596384913366 Min: -0.2542994386607249\n","ReLU Activation - Max: 4.966464468834391 Min: 0.0\n","ReLU Activation - Max: 3.966542926532667 Min: 0.0\n","Softmax Output - Max: 0.9895603965265801 Min: 5.632451606025691e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10831940258641407 Min: -0.11216503447350254\n","Layer 1 - Gradient Weights Max: 0.1865029151335878 Min: -0.14507541349367598\n","Layer 0 - Gradient Weights Max: 0.19410987658217968 Min: -0.20095318422302108\n","ReLU Activation - Max: 5.0417931486253815 Min: 0.0\n","ReLU Activation - Max: 4.462483928940049 Min: 0.0\n","Softmax Output - Max: 0.9707844053979049 Min: 1.7107479203854963e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06936825880153297 Min: -0.07368761675223938\n","Layer 1 - Gradient Weights Max: 0.123403112330535 Min: -0.13611169210146012\n","Layer 0 - Gradient Weights Max: 0.17596274345026783 Min: -0.16882323944797606\n","ReLU Activation - Max: 5.055739939185538 Min: 0.0\n","ReLU Activation - Max: 4.9420828247299635 Min: 0.0\n","Softmax Output - Max: 0.9702391320763297 Min: 1.2496975677438769e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10691303793866129 Min: -0.10383773218376678\n","Layer 1 - Gradient Weights Max: 0.16408273607966753 Min: -0.13727509249713757\n","Layer 0 - Gradient Weights Max: 0.15607974223730087 Min: -0.1417677429602904\n","ReLU Activation - Max: 5.173638784061973 Min: 0.0\n","ReLU Activation - Max: 3.9466918009010405 Min: 0.0\n","Softmax Output - Max: 0.9289444958257826 Min: 0.00015789128222103176 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1272307394043205 Min: -0.14256361041722648\n","Layer 1 - Gradient Weights Max: 0.19431386516043248 Min: -0.12573055894546556\n","Layer 0 - Gradient Weights Max: 0.258771620829427 Min: -0.16469532754722785\n","ReLU Activation - Max: 5.816736885752885 Min: 0.0\n","ReLU Activation - Max: 5.324591240993711 Min: 0.0\n","Softmax Output - Max: 0.9764477942591623 Min: 5.1870715788757615e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12941362250481228 Min: -0.16413429868500706\n","Layer 1 - Gradient Weights Max: 0.21978743549718815 Min: -0.21729113671765682\n","Layer 0 - Gradient Weights Max: 0.24702586033734006 Min: -0.27611190386135576\n","ReLU Activation - Max: 4.861712171528538 Min: 0.0\n","ReLU Activation - Max: 4.243087153893649 Min: 0.0\n","Softmax Output - Max: 0.9664333960683391 Min: 1.7107139856936508e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08047569519069936 Min: -0.1288645979409239\n","Layer 1 - Gradient Weights Max: 0.1466940635843174 Min: -0.19663665217639764\n","Layer 0 - Gradient Weights Max: 0.16528916270174224 Min: -0.19639532947148242\n","ReLU Activation - Max: 5.262764710531728 Min: 0.0\n","ReLU Activation - Max: 4.097197401772183 Min: 0.0\n","Softmax Output - Max: 0.9587209718117526 Min: 4.15623009215902e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07693678233655185 Min: -0.08559421708906913\n","Layer 1 - Gradient Weights Max: 0.10973029361416814 Min: -0.13462696825814494\n","Layer 0 - Gradient Weights Max: 0.23606239730861103 Min: -0.2502370822073827\n","ReLU Activation - Max: 5.060957280744308 Min: 0.0\n","ReLU Activation - Max: 4.183147346811451 Min: 0.0\n","Softmax Output - Max: 0.956656100593272 Min: 5.0790981132045115e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12983771314412887 Min: -0.09374851626280836\n","Layer 1 - Gradient Weights Max: 0.13428954595663894 Min: -0.13464087775605613\n","Layer 0 - Gradient Weights Max: 0.2761451383996851 Min: -0.19450632684050445\n","ReLU Activation - Max: 4.549931481711345 Min: 0.0\n","ReLU Activation - Max: 3.8919092084846785 Min: 0.0\n","Softmax Output - Max: 0.9649944619651142 Min: 3.7640976108247865e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07820233350091974 Min: -0.11806906604910818\n","Layer 1 - Gradient Weights Max: 0.13136571861147683 Min: -0.1726938933077189\n","Layer 0 - Gradient Weights Max: 0.17859069786176535 Min: -0.21740198622401774\n","ReLU Activation - Max: 5.766329648289885 Min: 0.0\n","ReLU Activation - Max: 3.7268247144104145 Min: 0.0\n","Softmax Output - Max: 0.9498947020882389 Min: 3.300805119948944e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09415853480091545 Min: -0.1377980102843815\n","Layer 1 - Gradient Weights Max: 0.12050905913625917 Min: -0.1959354099210975\n","Layer 0 - Gradient Weights Max: 0.2123088727257056 Min: -0.23470168351653423\n","ReLU Activation - Max: 4.766778934227293 Min: 0.0\n","ReLU Activation - Max: 4.555454810124497 Min: 0.0\n","Softmax Output - Max: 0.9777160354445036 Min: 3.939931106897311e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07749263096603681 Min: -0.15074404127962426\n","Layer 1 - Gradient Weights Max: 0.1638517189579473 Min: -0.17075259450203467\n","Layer 0 - Gradient Weights Max: 0.16653752986065856 Min: -0.12481225866487941\n","ReLU Activation - Max: 5.150183941302495 Min: 0.0\n","ReLU Activation - Max: 4.117298461886933 Min: 0.0\n","Softmax Output - Max: 0.9310236804299739 Min: 2.736886856613692e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1141023197384577 Min: -0.08895914869057886\n","Layer 1 - Gradient Weights Max: 0.1443283100884914 Min: -0.1600196738229011\n","Layer 0 - Gradient Weights Max: 0.19800016985715363 Min: -0.14656054477812094\n","ReLU Activation - Max: 5.4469522852946435 Min: 0.0\n","ReLU Activation - Max: 4.314103220420086 Min: 0.0\n","Softmax Output - Max: 0.9739905617447734 Min: 8.264004568700172e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13486349845302373 Min: -0.12473872407963141\n","Layer 1 - Gradient Weights Max: 0.14513217379473373 Min: -0.14151215009801377\n","Layer 0 - Gradient Weights Max: 0.26252887789397017 Min: -0.14444429289251942\n","ReLU Activation - Max: 5.293883383995489 Min: 0.0\n","ReLU Activation - Max: 4.185330505674036 Min: 0.0\n","Softmax Output - Max: 0.982101253199884 Min: 1.1535330236447634e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09108381837138205 Min: -0.16004230394847718\n","Layer 1 - Gradient Weights Max: 0.22496191684115716 Min: -0.19346158221399934\n","Layer 0 - Gradient Weights Max: 0.13993398604565563 Min: -0.1913209174099715\n","ReLU Activation - Max: 5.57906001050175 Min: 0.0\n","ReLU Activation - Max: 4.9066408413548155 Min: 0.0\n","Softmax Output - Max: 0.9678945472256616 Min: 3.409198608514047e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08881896914490466 Min: -0.11060617820569409\n","Layer 1 - Gradient Weights Max: 0.14777195580702152 Min: -0.16977583876302252\n","Layer 0 - Gradient Weights Max: 0.29516805636118487 Min: -0.18437543447225643\n","ReLU Activation - Max: 4.653533456205145 Min: 0.0\n","ReLU Activation - Max: 4.411518288426739 Min: 0.0\n","Softmax Output - Max: 0.972714243849956 Min: 3.449944558976514e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09650563421154244 Min: -0.12716729634212703\n","Layer 1 - Gradient Weights Max: 0.15156100696804065 Min: -0.1715857273728017\n","Layer 0 - Gradient Weights Max: 0.22729116785819942 Min: -0.1540111416743054\n","ReLU Activation - Max: 4.29233808575391 Min: 0.0\n","ReLU Activation - Max: 4.1145055410448474 Min: 0.0\n","Softmax Output - Max: 0.9855802717101881 Min: 2.2457944652988447e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12147019744905699 Min: -0.10838782791334431\n","Layer 1 - Gradient Weights Max: 0.13453206890518926 Min: -0.24308485521194864\n","Layer 0 - Gradient Weights Max: 0.23491276171283476 Min: -0.1875907129530204\n","ReLU Activation - Max: 6.081797045982514 Min: 0.0\n","ReLU Activation - Max: 5.240726528629441 Min: 0.0\n","Softmax Output - Max: 0.9782158285923336 Min: 4.970601397810724e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12080921920314781 Min: -0.09081324872409836\n","Layer 1 - Gradient Weights Max: 0.16991842302798404 Min: -0.11590834039242828\n","Layer 0 - Gradient Weights Max: 0.21432131737377288 Min: -0.26982239040818556\n","ReLU Activation - Max: 6.013576818395687 Min: 0.0\n","ReLU Activation - Max: 4.563245314692508 Min: 0.0\n","Softmax Output - Max: 0.9351324603365739 Min: 4.448415258750874e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.14236193665379382 Min: -0.12877260207182478\n","Layer 1 - Gradient Weights Max: 0.17749898217791885 Min: -0.1533685015553045\n","Layer 0 - Gradient Weights Max: 0.192119436065947 Min: -0.1959059212322307\n","ReLU Activation - Max: 4.687800693963923 Min: 0.0\n","ReLU Activation - Max: 3.9775916445142783 Min: 0.0\n","Softmax Output - Max: 0.9797252541281931 Min: 2.174863676675043e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07866687906809158 Min: -0.10764035079646352\n","Layer 1 - Gradient Weights Max: 0.1620445530761831 Min: -0.2043771492682522\n","Layer 0 - Gradient Weights Max: 0.23286613862088393 Min: -0.23562166289806818\n","ReLU Activation - Max: 4.9605871269359305 Min: 0.0\n","ReLU Activation - Max: 3.799084502735761 Min: 0.0\n","Softmax Output - Max: 0.9413488628506398 Min: 0.0001568178347529573 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11004524155683525 Min: -0.13325981096804917\n","Layer 1 - Gradient Weights Max: 0.11830158847636218 Min: -0.1481513714894435\n","Layer 0 - Gradient Weights Max: 0.18664223774902053 Min: -0.16056049505352546\n","ReLU Activation - Max: 4.8940397365599635 Min: 0.0\n","ReLU Activation - Max: 3.861655604569502 Min: 0.0\n","Softmax Output - Max: 0.9257422522325589 Min: 5.123855232776682e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0883252402016007 Min: -0.10200000742562829\n","Layer 1 - Gradient Weights Max: 0.152481948624468 Min: -0.2069863561813897\n","Layer 0 - Gradient Weights Max: 0.2049914663768718 Min: -0.1997321360385037\n","ReLU Activation - Max: 5.010176936795132 Min: 0.0\n","ReLU Activation - Max: 4.330059166520224 Min: 0.0\n","Softmax Output - Max: 0.9571994571127352 Min: 1.3773878085423513e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11199331733663968 Min: -0.09371853843086882\n","Layer 1 - Gradient Weights Max: 0.19223884386067255 Min: -0.20873958114227187\n","Layer 0 - Gradient Weights Max: 0.24362198186012357 Min: -0.28596371603124865\n","ReLU Activation - Max: 5.67511687299582 Min: 0.0\n","ReLU Activation - Max: 3.774465216168827 Min: 0.0\n","Softmax Output - Max: 0.9779779904580096 Min: 4.986691117035618e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0845540098011387 Min: -0.0929458213586281\n","Layer 1 - Gradient Weights Max: 0.17659471728834322 Min: -0.15887198494435997\n","Layer 0 - Gradient Weights Max: 0.2750554647476115 Min: -0.26648564785899365\n","ReLU Activation - Max: 4.521577441529493 Min: 0.0\n","ReLU Activation - Max: 3.353262146632614 Min: 0.0\n","Softmax Output - Max: 0.8839666686459913 Min: 0.00017181003716188267 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1042483814715401 Min: -0.10184130603227372\n","Layer 1 - Gradient Weights Max: 0.1538233346103187 Min: -0.14978966348514194\n","Layer 0 - Gradient Weights Max: 0.21170231671748746 Min: -0.20993471228337873\n","ReLU Activation - Max: 4.980797451157882 Min: 0.0\n","ReLU Activation - Max: 4.824835874546018 Min: 0.0\n","Softmax Output - Max: 0.9731753760775823 Min: 4.7628513705877124e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11022582160418602 Min: -0.15255160364309492\n","Layer 1 - Gradient Weights Max: 0.1555247440591786 Min: -0.1711525495361204\n","Layer 0 - Gradient Weights Max: 0.14512514681475963 Min: -0.14293380041362472\n","ReLU Activation - Max: 5.646943847283333 Min: 0.0\n","ReLU Activation - Max: 3.937695098767098 Min: 0.0\n","Softmax Output - Max: 0.9851599755266888 Min: 5.59242770310731e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1537783756478848 Min: -0.09277602380049893\n","Layer 1 - Gradient Weights Max: 0.17014615806670744 Min: -0.13418183996061986\n","Layer 0 - Gradient Weights Max: 0.23679304066225637 Min: -0.1712080502465411\n","ReLU Activation - Max: 4.349824241994547 Min: 0.0\n","ReLU Activation - Max: 3.439427629644663 Min: 0.0\n","Softmax Output - Max: 0.9745178108561164 Min: 3.992584285890138e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09612266295583413 Min: -0.18065185747911205\n","Layer 1 - Gradient Weights Max: 0.15281720106897237 Min: -0.13537013964660302\n","Layer 0 - Gradient Weights Max: 0.1971622174184097 Min: -0.13916169699493797\n","ReLU Activation - Max: 5.479413165078749 Min: 0.0\n","ReLU Activation - Max: 3.6981740167108414 Min: 0.0\n","Softmax Output - Max: 0.995685038442681 Min: 4.213377776483338e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08305747086198367 Min: -0.11355933883412037\n","Layer 1 - Gradient Weights Max: 0.13180460504442681 Min: -0.13302327965961147\n","Layer 0 - Gradient Weights Max: 0.18582174032836749 Min: -0.1486509209672234\n","ReLU Activation - Max: 5.499798955772775 Min: 0.0\n","ReLU Activation - Max: 4.888525879378802 Min: 0.0\n","Softmax Output - Max: 0.9934672545349357 Min: 2.7084131425865155e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.16543342922954843 Min: -0.1310870573753612\n","Layer 1 - Gradient Weights Max: 0.22677095233187455 Min: -0.18165021052942032\n","Layer 0 - Gradient Weights Max: 0.22051164633311274 Min: -0.2820854627630351\n","ReLU Activation - Max: 5.503628003140998 Min: 0.0\n","ReLU Activation - Max: 3.4305376181622553 Min: 0.0\n","Softmax Output - Max: 0.9016446546215651 Min: 1.318653029734406e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08035945362429087 Min: -0.10340266310012784\n","Layer 1 - Gradient Weights Max: 0.16138952643404458 Min: -0.20115211916599895\n","Layer 0 - Gradient Weights Max: 0.1679099892564759 Min: -0.1829799705911134\n","ReLU Activation - Max: 4.8122678082361094 Min: 0.0\n","ReLU Activation - Max: 3.8333991766881166 Min: 0.0\n","Softmax Output - Max: 0.9562219972141237 Min: 4.4878709556814325e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07273205261718917 Min: -0.11715475562382062\n","Layer 1 - Gradient Weights Max: 0.14193323677936873 Min: -0.16138777376423669\n","Layer 0 - Gradient Weights Max: 0.1675148683563382 Min: -0.2095874860384341\n","ReLU Activation - Max: 4.138313701773765 Min: 0.0\n","ReLU Activation - Max: 4.620724225132082 Min: 0.0\n","Softmax Output - Max: 0.9675579181879341 Min: 6.650181527499513e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.14650468204505687 Min: -0.09100913789232774\n","Layer 1 - Gradient Weights Max: 0.25396168982648426 Min: -0.16932335998506293\n","Layer 0 - Gradient Weights Max: 0.18814709881233607 Min: -0.2226091354529528\n","ReLU Activation - Max: 5.304099009662215 Min: 0.0\n","ReLU Activation - Max: 4.4663119384511685 Min: 0.0\n","Softmax Output - Max: 0.9873500258763761 Min: 1.3118623987109775e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08341747050363693 Min: -0.11250368629370064\n","Layer 1 - Gradient Weights Max: 0.19375860016011096 Min: -0.14322409270123226\n","Layer 0 - Gradient Weights Max: 0.1718153710918888 Min: -0.20150528460081513\n","ReLU Activation - Max: 5.023933248971939 Min: 0.0\n","ReLU Activation - Max: 3.9505977955592066 Min: 0.0\n","Softmax Output - Max: 0.907900423797867 Min: 9.973288311673832e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.11928432204143528 Min: -0.14752063790418132\n","Layer 1 - Gradient Weights Max: 0.228332936882406 Min: -0.18149506969298307\n","Layer 0 - Gradient Weights Max: 0.14355855497368747 Min: -0.20610886145721194\n","ReLU Activation - Max: 5.013869341655925 Min: 0.0\n","ReLU Activation - Max: 3.5073783987859883 Min: 0.0\n","Softmax Output - Max: 0.9627480998355512 Min: 4.069511052949603e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07612094721958479 Min: -0.09571547722261639\n","Layer 1 - Gradient Weights Max: 0.13858657445075556 Min: -0.13024531609562848\n","Layer 0 - Gradient Weights Max: 0.21799450280650728 Min: -0.19776788329942527\n","ReLU Activation - Max: 4.830882197980715 Min: 0.0\n","ReLU Activation - Max: 4.269070840127797 Min: 0.0\n","Softmax Output - Max: 0.9950819230604515 Min: 3.208389621561991e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11111467184936276 Min: -0.13339513877963033\n","Layer 1 - Gradient Weights Max: 0.14092197178708626 Min: -0.21731642752621905\n","Layer 0 - Gradient Weights Max: 0.21247548157247179 Min: -0.1986167833644534\n","ReLU Activation - Max: 4.853580690993126 Min: 0.0\n","ReLU Activation - Max: 4.38383970748986 Min: 0.0\n","Softmax Output - Max: 0.9936147725732418 Min: 1.6793988126958258e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09461908917289488 Min: -0.15298612013175988\n","Layer 1 - Gradient Weights Max: 0.16381035114190423 Min: -0.1510857169770639\n","Layer 0 - Gradient Weights Max: 0.11932314493612681 Min: -0.16958763813976732\n","ReLU Activation - Max: 4.694007702315743 Min: 0.0\n","ReLU Activation - Max: 4.93148881217476 Min: 0.0\n","Softmax Output - Max: 0.9839181419841533 Min: 8.893491034989663e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.14288264358241556 Min: -0.09578424255955839\n","Layer 1 - Gradient Weights Max: 0.130998073289886 Min: -0.12128165629687941\n","Layer 0 - Gradient Weights Max: 0.14992158115227822 Min: -0.12354835373649964\n","ReLU Activation - Max: 4.8957060474617355 Min: 0.0\n","ReLU Activation - Max: 4.513675273621099 Min: 0.0\n","Softmax Output - Max: 0.963968089702197 Min: 1.015601071257864e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09980763869615099 Min: -0.11658942176916486\n","Layer 1 - Gradient Weights Max: 0.1725019233658867 Min: -0.13736690060641246\n","Layer 0 - Gradient Weights Max: 0.16695618750847757 Min: -0.1602011353944115\n","ReLU Activation - Max: 4.594188570884345 Min: 0.0\n","ReLU Activation - Max: 4.290752907290373 Min: 0.0\n","Softmax Output - Max: 0.9328432607673617 Min: 7.534679031932822e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10064075302690795 Min: -0.09536974168775371\n","Layer 1 - Gradient Weights Max: 0.14614230702401496 Min: -0.1287296712755742\n","Layer 0 - Gradient Weights Max: 0.18555811254513746 Min: -0.24933144711410932\n","ReLU Activation - Max: 4.704181291872295 Min: 0.0\n","ReLU Activation - Max: 4.548200669652918 Min: 0.0\n","Softmax Output - Max: 0.9763376945579187 Min: 1.2535414985400157e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07011792689332036 Min: -0.10589680639233041\n","Layer 1 - Gradient Weights Max: 0.16063692798435275 Min: -0.15442280519147566\n","Layer 0 - Gradient Weights Max: 0.18732975298305019 Min: -0.17002541980086755\n","ReLU Activation - Max: 5.091731384540222 Min: 0.0\n","ReLU Activation - Max: 4.016229563526394 Min: 0.0\n","Softmax Output - Max: 0.9784708805016691 Min: 4.291663849096684e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09254028973496212 Min: -0.1306185728123646\n","Layer 1 - Gradient Weights Max: 0.19668051551065446 Min: -0.21645957739353783\n","Layer 0 - Gradient Weights Max: 0.3108808350384305 Min: -0.262557467739722\n","ReLU Activation - Max: 6.035283625994147 Min: 0.0\n","ReLU Activation - Max: 4.8106090158848325 Min: 0.0\n","Softmax Output - Max: 0.9377014571801998 Min: 4.222064351230975e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10281476394097011 Min: -0.06759646700868986\n","Layer 1 - Gradient Weights Max: 0.1470984855337249 Min: -0.11592927998781358\n","Layer 0 - Gradient Weights Max: 0.1642982978312179 Min: -0.18382481308992454\n","ReLU Activation - Max: 4.5580710307952055 Min: 0.0\n","ReLU Activation - Max: 3.5786825087968155 Min: 0.0\n","Softmax Output - Max: 0.9767837130008542 Min: 9.277638864174382e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09093549026997269 Min: -0.10768153989785266\n","Layer 1 - Gradient Weights Max: 0.13986590912160174 Min: -0.1554650981989138\n","Layer 0 - Gradient Weights Max: 0.20434715092387137 Min: -0.24246794964227475\n","ReLU Activation - Max: 4.115503179803293 Min: 0.0\n","ReLU Activation - Max: 3.634033817524052 Min: 0.0\n","Softmax Output - Max: 0.9567980711719669 Min: 0.0001057897764154614 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11248319185198508 Min: -0.14674412408634488\n","Layer 1 - Gradient Weights Max: 0.2030414421167818 Min: -0.15613355671021387\n","Layer 0 - Gradient Weights Max: 0.3011891587492409 Min: -0.22011989280510907\n","ReLU Activation - Max: 4.694481241270711 Min: 0.0\n","ReLU Activation - Max: 4.6100354190629735 Min: 0.0\n","Softmax Output - Max: 0.9879611974080192 Min: 1.6563911158818134e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11193615712794266 Min: -0.19105437600010503\n","Layer 1 - Gradient Weights Max: 0.16580806332789957 Min: -0.1554198474510905\n","Layer 0 - Gradient Weights Max: 0.16790742270961592 Min: -0.244839414372339\n","ReLU Activation - Max: 4.5094494072546185 Min: 0.0\n","ReLU Activation - Max: 4.588149727835836 Min: 0.0\n","Softmax Output - Max: 0.9353748919359579 Min: 4.601581817352946e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.108633773642463 Min: -0.11903819455131008\n","Layer 1 - Gradient Weights Max: 0.18367462985758423 Min: -0.2085223581059047\n","Layer 0 - Gradient Weights Max: 0.20135098271835458 Min: -0.23478223027180156\n","ReLU Activation - Max: 5.652086698581454 Min: 0.0\n","ReLU Activation - Max: 3.5978385529041077 Min: 0.0\n","Softmax Output - Max: 0.8953017536789295 Min: 2.497176350470897e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09669820974864979 Min: -0.1143794464325188\n","Layer 1 - Gradient Weights Max: 0.14393795940458284 Min: -0.13460482026180745\n","Layer 0 - Gradient Weights Max: 0.1542592618858628 Min: -0.18676104484178835\n","ReLU Activation - Max: 5.3684867069329 Min: 0.0\n","ReLU Activation - Max: 4.470918031693238 Min: 0.0\n","Softmax Output - Max: 0.920918684903572 Min: 7.827301531685567e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.06714219638204545 Min: -0.06258642857737147\n","Layer 1 - Gradient Weights Max: 0.13115240235816272 Min: -0.12259888040700459\n","Layer 0 - Gradient Weights Max: 0.2387048414746505 Min: -0.2540753683998698\n","ReLU Activation - Max: 4.093839487966644 Min: 0.0\n","ReLU Activation - Max: 3.738242068732665 Min: 0.0\n","Softmax Output - Max: 0.913676816263282 Min: 6.77772155349047e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11377202240278107 Min: -0.1429495560149131\n","Layer 1 - Gradient Weights Max: 0.15134588342530697 Min: -0.18464860546005044\n","Layer 0 - Gradient Weights Max: 0.2017233405368701 Min: -0.19943129060132528\n","ReLU Activation - Max: 5.3442437757893275 Min: 0.0\n","ReLU Activation - Max: 4.641307305793907 Min: 0.0\n","Softmax Output - Max: 0.9496268050563369 Min: 9.774272025567534e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08809911456902621 Min: -0.0890517504154618\n","Layer 1 - Gradient Weights Max: 0.15201714144747785 Min: -0.1789170234205214\n","Layer 0 - Gradient Weights Max: 0.1672783236870731 Min: -0.24518351913919342\n","ReLU Activation - Max: 4.954866536834208 Min: 0.0\n","ReLU Activation - Max: 4.7997588002878135 Min: 0.0\n","Softmax Output - Max: 0.975847348092497 Min: 1.6089374486715975e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12389638093070764 Min: -0.16478945076967502\n","Layer 1 - Gradient Weights Max: 0.20948906800626857 Min: -0.1962917390135126\n","Layer 0 - Gradient Weights Max: 0.19291144713292518 Min: -0.262778593706255\n","ReLU Activation - Max: 5.550751542401031 Min: 0.0\n","ReLU Activation - Max: 3.9059089938155447 Min: 0.0\n","Softmax Output - Max: 0.9828581000139885 Min: 6.5951432523871546e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09146193173260934 Min: -0.0786187495712316\n","Layer 1 - Gradient Weights Max: 0.12630541111145493 Min: -0.15093524427224966\n","Layer 0 - Gradient Weights Max: 0.29093113303204854 Min: -0.2689097204953213\n","ReLU Activation - Max: 4.9882620945169736 Min: 0.0\n","ReLU Activation - Max: 3.854358458351885 Min: 0.0\n","Softmax Output - Max: 0.9641284500445428 Min: 7.316268644169794e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10738792938773561 Min: -0.1410088180938568\n","Layer 1 - Gradient Weights Max: 0.1731097992447919 Min: -0.19585921934200048\n","Layer 0 - Gradient Weights Max: 0.19553490897101833 Min: -0.1558074067053807\n","ReLU Activation - Max: 4.617409553008468 Min: 0.0\n","ReLU Activation - Max: 4.880827042610787 Min: 0.0\n","Softmax Output - Max: 0.9602135067888753 Min: 3.165977399051593e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06841660435451727 Min: -0.10769816211860236\n","Layer 1 - Gradient Weights Max: 0.14372141974839564 Min: -0.15693132989147332\n","Layer 0 - Gradient Weights Max: 0.16517327011871233 Min: -0.2218607870749267\n","ReLU Activation - Max: 4.617930808650024 Min: 0.0\n","ReLU Activation - Max: 3.851964270732548 Min: 0.0\n","Softmax Output - Max: 0.9322056463736395 Min: 0.00015278240257924773 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13604917026294894 Min: -0.12799493732863979\n","Layer 1 - Gradient Weights Max: 0.17162343926558027 Min: -0.16123241968256713\n","Layer 0 - Gradient Weights Max: 0.17236480726595496 Min: -0.17954808595487\n","ReLU Activation - Max: 5.367720046055508 Min: 0.0\n","ReLU Activation - Max: 3.929217504353042 Min: 0.0\n","Softmax Output - Max: 0.9241729556866439 Min: 1.9156575152415383e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08948279831416583 Min: -0.15153825821404104\n","Layer 1 - Gradient Weights Max: 0.1640791550802449 Min: -0.26428113026859457\n","Layer 0 - Gradient Weights Max: 0.23155521047537897 Min: -0.1774767599886793\n","ReLU Activation - Max: 4.782129888774238 Min: 0.0\n","ReLU Activation - Max: 4.08849577604599 Min: 0.0\n","Softmax Output - Max: 0.9788663391128118 Min: 4.572431986813405e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10752148139934048 Min: -0.1070363336066911\n","Layer 1 - Gradient Weights Max: 0.1453479329012687 Min: -0.14155203613333436\n","Layer 0 - Gradient Weights Max: 0.18626001793246685 Min: -0.15280335829310207\n","ReLU Activation - Max: 5.686298720144019 Min: 0.0\n","ReLU Activation - Max: 4.226730822322821 Min: 0.0\n","Softmax Output - Max: 0.9572710650122644 Min: 8.613986967216085e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08578572677689601 Min: -0.07472745789689772\n","Layer 1 - Gradient Weights Max: 0.12288134294070839 Min: -0.19488197237921792\n","Layer 0 - Gradient Weights Max: 0.1642430608645906 Min: -0.16027014101579362\n","ReLU Activation - Max: 4.71986879667229 Min: 0.0\n","ReLU Activation - Max: 4.385380665529157 Min: 0.0\n","Softmax Output - Max: 0.8902941578189538 Min: 2.1302030682376772e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1530580977199223 Min: -0.19425950849394905\n","Layer 1 - Gradient Weights Max: 0.1632003456335363 Min: -0.18558096680119482\n","Layer 0 - Gradient Weights Max: 0.20587467050389077 Min: -0.20106283337256134\n","ReLU Activation - Max: 5.60185769186945 Min: 0.0\n","ReLU Activation - Max: 4.116320093922419 Min: 0.0\n","Softmax Output - Max: 0.9781901043154851 Min: 3.194787625273924e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.07980276775075304 Min: -0.09397676962047709\n","Layer 1 - Gradient Weights Max: 0.17077261175738273 Min: -0.13309693936070752\n","Layer 0 - Gradient Weights Max: 0.18096954450388947 Min: -0.17993554538461493\n","ReLU Activation - Max: 5.166973191133439 Min: 0.0\n","ReLU Activation - Max: 3.9330211675436857 Min: 0.0\n","Softmax Output - Max: 0.9492824848091065 Min: 0.00010886647962347545 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15696118857413346 Min: -0.14540089203848636\n","Layer 1 - Gradient Weights Max: 0.15123677555982415 Min: -0.1335786442666903\n","Layer 0 - Gradient Weights Max: 0.19348455205218731 Min: -0.1784466987394535\n","ReLU Activation - Max: 6.415952398500517 Min: 0.0\n","ReLU Activation - Max: 4.523438360300596 Min: 0.0\n","Softmax Output - Max: 0.9676648922233723 Min: 4.2816162797421016e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11226030618265635 Min: -0.10677221504100777\n","Layer 1 - Gradient Weights Max: 0.1237778130879864 Min: -0.14148545590464803\n","Layer 0 - Gradient Weights Max: 0.20926545206510008 Min: -0.15757970218680448\n","ReLU Activation - Max: 5.417563339030461 Min: 0.0\n","ReLU Activation - Max: 3.932729441319914 Min: 0.0\n","Softmax Output - Max: 0.9986336056943008 Min: 5.957065703984853e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09925295701025715 Min: -0.1359890607766385\n","Layer 1 - Gradient Weights Max: 0.15603072176791613 Min: -0.1281085341099337\n","Layer 0 - Gradient Weights Max: 0.17038885656125033 Min: -0.16418462850710613\n","ReLU Activation - Max: 4.703763930179622 Min: 0.0\n","ReLU Activation - Max: 3.9436276769787426 Min: 0.0\n","Softmax Output - Max: 0.9821149941674016 Min: 2.891680331225511e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09013664814730973 Min: -0.07565898366914335\n","Layer 1 - Gradient Weights Max: 0.14397162127535737 Min: -0.17300265093158446\n","Layer 0 - Gradient Weights Max: 0.17334330366478812 Min: -0.1828615980332954\n","ReLU Activation - Max: 4.791797422515526 Min: 0.0\n","ReLU Activation - Max: 3.918670222960197 Min: 0.0\n","Softmax Output - Max: 0.9641822679127177 Min: 1.0037935882323955e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09591991539487801 Min: -0.0953681097053061\n","Layer 1 - Gradient Weights Max: 0.12629443038218244 Min: -0.14347396870805107\n","Layer 0 - Gradient Weights Max: 0.18654895761494328 Min: -0.27708691015341796\n","ReLU Activation - Max: 5.061890900372516 Min: 0.0\n","ReLU Activation - Max: 3.8162529279356945 Min: 0.0\n","Softmax Output - Max: 0.9939819548105515 Min: 2.8631969001898342e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08528917588106452 Min: -0.14952682404187437\n","Layer 1 - Gradient Weights Max: 0.1287965836013047 Min: -0.20304448935599814\n","Layer 0 - Gradient Weights Max: 0.17633524991484037 Min: -0.24111413695321143\n","ReLU Activation - Max: 5.480937563112408 Min: 0.0\n","ReLU Activation - Max: 4.172734961732167 Min: 0.0\n","Softmax Output - Max: 0.995875717236117 Min: 5.572341429297292e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09428936318686147 Min: -0.07601113444484398\n","Layer 1 - Gradient Weights Max: 0.14469561931453045 Min: -0.16767568183863593\n","Layer 0 - Gradient Weights Max: 0.17782131789239178 Min: -0.2510487499451234\n","ReLU Activation - Max: 4.300519855932486 Min: 0.0\n","ReLU Activation - Max: 3.8079133903372044 Min: 0.0\n","Softmax Output - Max: 0.9487693473468839 Min: 2.429119450397049e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10875002661573356 Min: -0.07592850609174925\n","Layer 1 - Gradient Weights Max: 0.168299978544045 Min: -0.1265638002302387\n","Layer 0 - Gradient Weights Max: 0.20116079054875124 Min: -0.18607841970302522\n","ReLU Activation - Max: 5.053996277373688 Min: 0.0\n","ReLU Activation - Max: 4.0831451201662725 Min: 0.0\n","Softmax Output - Max: 0.9293695056075881 Min: 1.698715809488802e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07087812982757037 Min: -0.0938948299360249\n","Layer 1 - Gradient Weights Max: 0.12908772684799888 Min: -0.17817043329281212\n","Layer 0 - Gradient Weights Max: 0.1459577957880986 Min: -0.1676734691515669\n","ReLU Activation - Max: 7.136504715137075 Min: 0.0\n","ReLU Activation - Max: 4.228199993588488 Min: 0.0\n","Softmax Output - Max: 0.979774940398936 Min: 4.021196856469732e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.15835394649370002 Min: -0.1433482236091552\n","Layer 1 - Gradient Weights Max: 0.25836511270180984 Min: -0.20853755267056132\n","Layer 0 - Gradient Weights Max: 0.267930875324272 Min: -0.26790847461773004\n","ReLU Activation - Max: 4.717524567846342 Min: 0.0\n","ReLU Activation - Max: 3.722749065563078 Min: 0.0\n","Softmax Output - Max: 0.8930029457348796 Min: 4.197868225611033e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10304863487539438 Min: -0.19727477591165202\n","Layer 1 - Gradient Weights Max: 0.22409050974280628 Min: -0.27560206496424944\n","Layer 0 - Gradient Weights Max: 0.1870345481821621 Min: -0.1870142306747563\n","ReLU Activation - Max: 5.198969177907555 Min: 0.0\n","ReLU Activation - Max: 3.781007968126828 Min: 0.0\n","Softmax Output - Max: 0.9820315504544936 Min: 4.020751309202907e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0926247469433201 Min: -0.1920550963385898\n","Layer 1 - Gradient Weights Max: 0.17856824472609426 Min: -0.18084467112242622\n","Layer 0 - Gradient Weights Max: 0.17379686660950705 Min: -0.18683655725675308\n","ReLU Activation - Max: 7.636524365614824 Min: 0.0\n","ReLU Activation - Max: 3.8841956179397616 Min: 0.0\n","Softmax Output - Max: 0.8369343813299527 Min: 4.628112218849919e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12140995406937143 Min: -0.16412200519685663\n","Layer 1 - Gradient Weights Max: 0.17020543777035865 Min: -0.2118175089447922\n","Layer 0 - Gradient Weights Max: 0.25132316077829164 Min: -0.24788636500878888\n","ReLU Activation - Max: 4.5054953182818975 Min: 0.0\n","ReLU Activation - Max: 3.8080733882112754 Min: 0.0\n","Softmax Output - Max: 0.993424406632982 Min: 1.3092419247747422e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09433979057953691 Min: -0.1258067501149055\n","Layer 1 - Gradient Weights Max: 0.1733927470161374 Min: -0.14133509550183393\n","Layer 0 - Gradient Weights Max: 0.1517344806548242 Min: -0.15161012566948162\n","ReLU Activation - Max: 5.17007651936384 Min: 0.0\n","ReLU Activation - Max: 4.4388378181736154 Min: 0.0\n","Softmax Output - Max: 0.9801256375392495 Min: 1.2911933401973684e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07067341326467819 Min: -0.07686662106340249\n","Layer 1 - Gradient Weights Max: 0.2560882899998198 Min: -0.18678735635331817\n","Layer 0 - Gradient Weights Max: 0.2904331370107935 Min: -0.2095677282126963\n","ReLU Activation - Max: 4.976930635667638 Min: 0.0\n","ReLU Activation - Max: 4.1980709349561955 Min: 0.0\n","Softmax Output - Max: 0.9331192578735967 Min: 4.222598793671056e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09463749517664603 Min: -0.11392643249637123\n","Layer 1 - Gradient Weights Max: 0.1526984993092872 Min: -0.16666521666717068\n","Layer 0 - Gradient Weights Max: 0.18686520291066205 Min: -0.3169265522010693\n","ReLU Activation - Max: 5.9183936875164385 Min: 0.0\n","ReLU Activation - Max: 5.1411251175457675 Min: 0.0\n","Softmax Output - Max: 0.9960174270137298 Min: 1.338853152960717e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10059970685278943 Min: -0.16413491170475067\n","Layer 1 - Gradient Weights Max: 0.18519988347700392 Min: -0.12315641887497221\n","Layer 0 - Gradient Weights Max: 0.21768553074462635 Min: -0.18234488060025675\n","ReLU Activation - Max: 4.249215666551267 Min: 0.0\n","ReLU Activation - Max: 4.3834749522098555 Min: 0.0\n","Softmax Output - Max: 0.9985018083489104 Min: 2.0056278271698968e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12655541395772676 Min: -0.16478469863665213\n","Layer 1 - Gradient Weights Max: 0.20707542686086314 Min: -0.200721957980755\n","Layer 0 - Gradient Weights Max: 0.20771306288106395 Min: -0.36790660898602706\n","ReLU Activation - Max: 4.685466967315242 Min: 0.0\n","ReLU Activation - Max: 6.133698300469453 Min: 0.0\n","Softmax Output - Max: 0.9540302208405712 Min: 5.697390822587122e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0899981031166709 Min: -0.11808687095377812\n","Layer 1 - Gradient Weights Max: 0.12663824413017 Min: -0.15476947033745322\n","Layer 0 - Gradient Weights Max: 0.3341135236360846 Min: -0.1980968606654474\n","ReLU Activation - Max: 4.549289360714977 Min: 0.0\n","ReLU Activation - Max: 3.884017855281075 Min: 0.0\n","Softmax Output - Max: 0.9755224161696308 Min: 4.477103689316198e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0761775005055271 Min: -0.10521490697335698\n","Layer 1 - Gradient Weights Max: 0.23985305666995738 Min: -0.226894077653398\n","Layer 0 - Gradient Weights Max: 0.18125568562365266 Min: -0.2020564174568035\n","ReLU Activation - Max: 5.918912913091229 Min: 0.0\n","ReLU Activation - Max: 4.3071506443122125 Min: 0.0\n","Softmax Output - Max: 0.9859467922041717 Min: 4.947678008131581e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12966924067770638 Min: -0.15413691068368732\n","Layer 1 - Gradient Weights Max: 0.1672544778768144 Min: -0.1596715668249367\n","Layer 0 - Gradient Weights Max: 0.22492187715351436 Min: -0.21634760567916123\n","ReLU Activation - Max: 4.894066107160308 Min: 0.0\n","ReLU Activation - Max: 3.7248555102879157 Min: 0.0\n","Softmax Output - Max: 0.9618661231078949 Min: 7.553675248734e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08629966248471183 Min: -0.10233212691837422\n","Layer 1 - Gradient Weights Max: 0.14461612812123947 Min: -0.11252742835238365\n","Layer 0 - Gradient Weights Max: 0.1618460340775613 Min: -0.18037967530932322\n","ReLU Activation - Max: 4.529690211749159 Min: 0.0\n","ReLU Activation - Max: 4.365520497172649 Min: 0.0\n","Softmax Output - Max: 0.9219800568052158 Min: 5.858461855686335e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10217648205893201 Min: -0.07481951148600009\n","Layer 1 - Gradient Weights Max: 0.18545114419121886 Min: -0.1367242609836772\n","Layer 0 - Gradient Weights Max: 0.15331722959406374 Min: -0.17504899123712114\n","ReLU Activation - Max: 4.904486818563645 Min: 0.0\n","ReLU Activation - Max: 4.246848249010263 Min: 0.0\n","Softmax Output - Max: 0.9418257676512442 Min: 7.975597402181249e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09212335207723323 Min: -0.13283081975728545\n","Layer 1 - Gradient Weights Max: 0.1598936866459338 Min: -0.19146875005442615\n","Layer 0 - Gradient Weights Max: 0.228425064678633 Min: -0.19765616879670642\n","ReLU Activation - Max: 5.513741550302848 Min: 0.0\n","ReLU Activation - Max: 4.222588550250714 Min: 0.0\n","Softmax Output - Max: 0.9857917266785727 Min: 2.4498015655277938e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08070477370076692 Min: -0.13475974534485263\n","Layer 1 - Gradient Weights Max: 0.15479907823027636 Min: -0.22446833621996437\n","Layer 0 - Gradient Weights Max: 0.17428485670947416 Min: -0.1891400859961287\n","ReLU Activation - Max: 4.661941131403762 Min: 0.0\n","ReLU Activation - Max: 4.6199675716993 Min: 0.0\n","Softmax Output - Max: 0.9336950304936685 Min: 1.3136252737236227e-08 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10260707045245926 Min: -0.12871444966993714\n","Layer 1 - Gradient Weights Max: 0.22292427126229722 Min: -0.19064559009141993\n","Layer 0 - Gradient Weights Max: 0.23902167488901055 Min: -0.16943245262166398\n","ReLU Activation - Max: 4.0131628926952745 Min: 0.0\n","ReLU Activation - Max: 3.8536608174543554 Min: 0.0\n","Softmax Output - Max: 0.9168110052031825 Min: 6.684090029484233e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1091483058628665 Min: -0.17545931507349477\n","Layer 1 - Gradient Weights Max: 0.22533199357868677 Min: -0.18265236846465646\n","Layer 0 - Gradient Weights Max: 0.1588824110157371 Min: -0.17490171085999084\n","ReLU Activation - Max: 4.336812199194144 Min: 0.0\n","ReLU Activation - Max: 4.221807814621406 Min: 0.0\n","Softmax Output - Max: 0.9115566545208419 Min: 5.375632057424559e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11982599786714246 Min: -0.0986919905948638\n","Layer 1 - Gradient Weights Max: 0.13166715613687363 Min: -0.15146752128856966\n","Layer 0 - Gradient Weights Max: 0.16240732639538738 Min: -0.14794868383813287\n","ReLU Activation - Max: 5.912101521247085 Min: 0.0\n","ReLU Activation - Max: 4.694253934876779 Min: 0.0\n","Softmax Output - Max: 0.9814493109749932 Min: 1.4263504857664457e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13822057335869678 Min: -0.18320087301793986\n","Layer 1 - Gradient Weights Max: 0.1402253080915889 Min: -0.21275052156737523\n","Layer 0 - Gradient Weights Max: 0.16629437642861958 Min: -0.1625503849508174\n","ReLU Activation - Max: 4.404089154604213 Min: 0.0\n","ReLU Activation - Max: 3.7345085719672766 Min: 0.0\n","Softmax Output - Max: 0.9418660257591863 Min: 6.906648200566439e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.11955945943626971 Min: -0.13594483761211665\n","Layer 1 - Gradient Weights Max: 0.15271645139305118 Min: -0.2054010493129412\n","Layer 0 - Gradient Weights Max: 0.17248941695166317 Min: -0.1449724482243634\n","ReLU Activation - Max: 5.0714735672064055 Min: 0.0\n","ReLU Activation - Max: 3.6840736767033744 Min: 0.0\n","Softmax Output - Max: 0.9799585425645334 Min: 0.0001257295368641458 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11781616464668124 Min: -0.08487998019869886\n","Layer 1 - Gradient Weights Max: 0.13277057403653958 Min: -0.12188604564789321\n","Layer 0 - Gradient Weights Max: 0.2647291440058101 Min: -0.18045252792209046\n","ReLU Activation - Max: 5.862153504984331 Min: 0.0\n","ReLU Activation - Max: 4.163404549305797 Min: 0.0\n","Softmax Output - Max: 0.9959786398967867 Min: 6.432012930516082e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09580559178228731 Min: -0.05735761736610338\n","Layer 1 - Gradient Weights Max: 0.1442060161299988 Min: -0.1265414549629726\n","Layer 0 - Gradient Weights Max: 0.15471669487400663 Min: -0.20781715284151933\n","ReLU Activation - Max: 4.80860219221595 Min: 0.0\n","ReLU Activation - Max: 3.7608682703364273 Min: 0.0\n","Softmax Output - Max: 0.9525956458104387 Min: 3.799471459091599e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09484767204259596 Min: -0.16352463786979007\n","Layer 1 - Gradient Weights Max: 0.11545708820781202 Min: -0.13971603111496622\n","Layer 0 - Gradient Weights Max: 0.18227755467872203 Min: -0.20548678570128157\n","ReLU Activation - Max: 5.143858028025905 Min: 0.0\n","ReLU Activation - Max: 4.652726457066562 Min: 0.0\n","Softmax Output - Max: 0.9228787558419502 Min: 2.6268104909263073e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17861799695290864 Min: -0.16860610515755753\n","Layer 1 - Gradient Weights Max: 0.13357910978813065 Min: -0.27648161100279667\n","Layer 0 - Gradient Weights Max: 0.22367416552073943 Min: -0.31672565607291686\n","ReLU Activation - Max: 4.6600720077874564 Min: 0.0\n","ReLU Activation - Max: 4.222346093567846 Min: 0.0\n","Softmax Output - Max: 0.9829909097236982 Min: 0.00011910593411647497 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12573561868834976 Min: -0.20386752578808096\n","Layer 1 - Gradient Weights Max: 0.25075916205228926 Min: -0.19841516420118796\n","Layer 0 - Gradient Weights Max: 0.2558698006656246 Min: -0.19726278060701388\n","ReLU Activation - Max: 4.801049781331903 Min: 0.0\n","ReLU Activation - Max: 4.05286384608064 Min: 0.0\n","Softmax Output - Max: 0.9326323070374939 Min: 9.200983588752592e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13225511720874406 Min: -0.09556638512010487\n","Layer 1 - Gradient Weights Max: 0.13514753179523134 Min: -0.15703678765520135\n","Layer 0 - Gradient Weights Max: 0.200184772708024 Min: -0.17354199571168652\n","ReLU Activation - Max: 5.6093543266405055 Min: 0.0\n","ReLU Activation - Max: 4.060337538602482 Min: 0.0\n","Softmax Output - Max: 0.9783896851945905 Min: 8.491697393584437e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08763694780575952 Min: -0.09923655052708706\n","Layer 1 - Gradient Weights Max: 0.1250811793104942 Min: -0.13112156836211447\n","Layer 0 - Gradient Weights Max: 0.14410381083356125 Min: -0.21649772164481887\n","ReLU Activation - Max: 4.300536620043619 Min: 0.0\n","ReLU Activation - Max: 4.105657868941528 Min: 0.0\n","Softmax Output - Max: 0.981023207840345 Min: 2.7154761062026698e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08014850739441927 Min: -0.10185780624412342\n","Layer 1 - Gradient Weights Max: 0.1098913210565815 Min: -0.15011260571601995\n","Layer 0 - Gradient Weights Max: 0.23248728361407361 Min: -0.21862020738177676\n","ReLU Activation - Max: 4.824816624753972 Min: 0.0\n","ReLU Activation - Max: 4.035398834980689 Min: 0.0\n","Softmax Output - Max: 0.9802548383180536 Min: 1.1095696514943179e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17685585139976281 Min: -0.19431021801995274\n","Layer 1 - Gradient Weights Max: 0.16330298787784936 Min: -0.18731373969039428\n","Layer 0 - Gradient Weights Max: 0.14882003422975768 Min: -0.18033076583485141\n","ReLU Activation - Max: 5.159606284403694 Min: 0.0\n","ReLU Activation - Max: 4.0960925070166745 Min: 0.0\n","Softmax Output - Max: 0.9196684118796701 Min: 6.18240467345029e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09746761955344524 Min: -0.1190704171704438\n","Layer 1 - Gradient Weights Max: 0.13894526859910267 Min: -0.15013395015406644\n","Layer 0 - Gradient Weights Max: 0.1574695192222888 Min: -0.14946892908258086\n","ReLU Activation - Max: 4.442149579433998 Min: 0.0\n","ReLU Activation - Max: 3.7968318121933198 Min: 0.0\n","Softmax Output - Max: 0.9685213093830742 Min: 2.4354153591882645e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08989222399437295 Min: -0.10871119379530371\n","Layer 1 - Gradient Weights Max: 0.11434635099456171 Min: -0.13840980172404477\n","Layer 0 - Gradient Weights Max: 0.1786049574422815 Min: -0.17281959302240965\n","ReLU Activation - Max: 4.752407959922676 Min: 0.0\n","ReLU Activation - Max: 3.976331147996931 Min: 0.0\n","Softmax Output - Max: 0.9311737912430214 Min: 0.00010672579348832964 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10465740946690141 Min: -0.11151841789422157\n","Layer 1 - Gradient Weights Max: 0.151541515627669 Min: -0.19348205824929943\n","Layer 0 - Gradient Weights Max: 0.18787584001655727 Min: -0.169729451237501\n","ReLU Activation - Max: 5.314835034791318 Min: 0.0\n","ReLU Activation - Max: 4.206566243094009 Min: 0.0\n","Softmax Output - Max: 0.9793696899936802 Min: 5.7500404519468815e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10475076782044501 Min: -0.1431119385755558\n","Layer 1 - Gradient Weights Max: 0.13529982914367675 Min: -0.21557042231339932\n","Layer 0 - Gradient Weights Max: 0.15375369656497448 Min: -0.16948978425078537\n","ReLU Activation - Max: 4.930945421391181 Min: 0.0\n","ReLU Activation - Max: 4.309186329879017 Min: 0.0\n","Softmax Output - Max: 0.9631232423315583 Min: 4.041201035685602e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.13646300570389383 Min: -0.100018955658863\n","Layer 1 - Gradient Weights Max: 0.1971628122249476 Min: -0.17295898087257888\n","Layer 0 - Gradient Weights Max: 0.19094016525383814 Min: -0.1482586839579196\n","ReLU Activation - Max: 5.786304514407217 Min: 0.0\n","ReLU Activation - Max: 4.7233754401029335 Min: 0.0\n","Softmax Output - Max: 0.9804269685631487 Min: 5.555133011606053e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09259713343435248 Min: -0.10425713868946983\n","Layer 1 - Gradient Weights Max: 0.12080257532761327 Min: -0.18613986394276688\n","Layer 0 - Gradient Weights Max: 0.19666126634515382 Min: -0.30925045047510125\n","ReLU Activation - Max: 5.319773950289367 Min: 0.0\n","ReLU Activation - Max: 4.785189135539001 Min: 0.0\n","Softmax Output - Max: 0.962495165371357 Min: 4.274329085512043e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10978378929878319 Min: -0.08806739552744487\n","Layer 1 - Gradient Weights Max: 0.1727750183759438 Min: -0.2042541122535125\n","Layer 0 - Gradient Weights Max: 0.360866724639266 Min: -0.2579466470071604\n","ReLU Activation - Max: 5.467456607037165 Min: 0.0\n","ReLU Activation - Max: 4.595014748810444 Min: 0.0\n","Softmax Output - Max: 0.9662612435066842 Min: 6.498500944732076e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11740065164082833 Min: -0.11798671935783561\n","Layer 1 - Gradient Weights Max: 0.16288778124637274 Min: -0.1606432350185519\n","Layer 0 - Gradient Weights Max: 0.1998433575021915 Min: -0.18673128190912938\n","ReLU Activation - Max: 6.382982556136804 Min: 0.0\n","ReLU Activation - Max: 4.611211409862176 Min: 0.0\n","Softmax Output - Max: 0.991103912827027 Min: 1.2169501186855828e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.09321610580573068 Min: -0.150278323615315\n","Layer 1 - Gradient Weights Max: 0.14563546522683177 Min: -0.18236434714154853\n","Layer 0 - Gradient Weights Max: 0.18721644541232346 Min: -0.1527625792345974\n","ReLU Activation - Max: 5.712404962233099 Min: 0.0\n","ReLU Activation - Max: 3.9841447047816385 Min: 0.0\n","Softmax Output - Max: 0.973993140099873 Min: 3.741867033001398e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09795880339508646 Min: -0.13199128050297257\n","Layer 1 - Gradient Weights Max: 0.16236553467126807 Min: -0.17199005355502256\n","Layer 0 - Gradient Weights Max: 0.15678075743662473 Min: -0.18394315801819197\n","ReLU Activation - Max: 4.913700818390835 Min: 0.0\n","ReLU Activation - Max: 3.8820636545533262 Min: 0.0\n","Softmax Output - Max: 0.920665344734128 Min: 1.4538655378052821e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10301959990793316 Min: -0.2036310638090789\n","Layer 1 - Gradient Weights Max: 0.12083128446307441 Min: -0.15165543730009834\n","Layer 0 - Gradient Weights Max: 0.25016380710766484 Min: -0.23663661698074315\n","ReLU Activation - Max: 4.4106225483563 Min: 0.0\n","ReLU Activation - Max: 3.8194750222258342 Min: 0.0\n","Softmax Output - Max: 0.8803486977980566 Min: 5.614304568980388e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06561359314787106 Min: -0.12712816192502893\n","Layer 1 - Gradient Weights Max: 0.14706199590014477 Min: -0.13876211675499484\n","Layer 0 - Gradient Weights Max: 0.2666382839342697 Min: -0.27400085832694976\n","ReLU Activation - Max: 5.339724355549651 Min: 0.0\n","ReLU Activation - Max: 3.825312616690944 Min: 0.0\n","Softmax Output - Max: 0.9893537383643481 Min: 2.3920756376390518e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10222974947575104 Min: -0.16057304444613124\n","Layer 1 - Gradient Weights Max: 0.21913066175811474 Min: -0.19938690773488335\n","Layer 0 - Gradient Weights Max: 0.21034514548105612 Min: -0.19885491861949486\n","ReLU Activation - Max: 4.545685818832075 Min: 0.0\n","ReLU Activation - Max: 4.660095142389648 Min: 0.0\n","Softmax Output - Max: 0.9778540684379491 Min: 2.0959377807150914e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11850497165250494 Min: -0.15126343323934086\n","Layer 1 - Gradient Weights Max: 0.14088694936471727 Min: -0.1611348647648396\n","Layer 0 - Gradient Weights Max: 0.13494071585869183 Min: -0.20427976882971127\n","ReLU Activation - Max: 4.830032407369945 Min: 0.0\n","ReLU Activation - Max: 3.7765114528856163 Min: 0.0\n","Softmax Output - Max: 0.9554380266518125 Min: 4.780686106492479e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08630699515100351 Min: -0.13306702901918047\n","Layer 1 - Gradient Weights Max: 0.14305182950146023 Min: -0.1640874744127024\n","Layer 0 - Gradient Weights Max: 0.15616733662085996 Min: -0.20668916849482066\n","ReLU Activation - Max: 6.116008621758621 Min: 0.0\n","ReLU Activation - Max: 4.610875934469491 Min: 0.0\n","Softmax Output - Max: 0.9705009240535731 Min: 7.039919311627329e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10763547773022314 Min: -0.1582284739840352\n","Layer 1 - Gradient Weights Max: 0.12332709858082946 Min: -0.14426820729560896\n","Layer 0 - Gradient Weights Max: 0.24848511616494712 Min: -0.18235353325614784\n","ReLU Activation - Max: 5.856385446571838 Min: 0.0\n","ReLU Activation - Max: 4.277053367771172 Min: 0.0\n","Softmax Output - Max: 0.9234817227664295 Min: 1.1520135579356982e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.14421610421115325 Min: -0.13884099950789777\n","Layer 1 - Gradient Weights Max: 0.15821274830191645 Min: -0.12686804556593717\n","Layer 0 - Gradient Weights Max: 0.3031028716666131 Min: -0.1781222109588958\n","ReLU Activation - Max: 4.82875138262455 Min: 0.0\n","ReLU Activation - Max: 3.6837894795135027 Min: 0.0\n","Softmax Output - Max: 0.9948999335454922 Min: 3.053575694874176e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09351852292527627 Min: -0.09241962228675997\n","Layer 1 - Gradient Weights Max: 0.12411307872755661 Min: -0.12811133973061123\n","Layer 0 - Gradient Weights Max: 0.21245083886574806 Min: -0.15754986943790222\n","ReLU Activation - Max: 5.168920514442302 Min: 0.0\n","ReLU Activation - Max: 4.183145539325517 Min: 0.0\n","Softmax Output - Max: 0.991644569117049 Min: 4.164341541942106e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1286829610417137 Min: -0.13531348565959553\n","Layer 1 - Gradient Weights Max: 0.15933727503741307 Min: -0.1991062809122406\n","Layer 0 - Gradient Weights Max: 0.18045627993918073 Min: -0.15831341715632105\n","ReLU Activation - Max: 4.231893670747341 Min: 0.0\n","ReLU Activation - Max: 4.030435040970596 Min: 0.0\n","Softmax Output - Max: 0.9633064907846636 Min: 9.297309524889748e-07 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.14113098165728943 Min: -0.1258845746571083\n","Layer 1 - Gradient Weights Max: 0.26100665700749087 Min: -0.1510659478107004\n","Layer 0 - Gradient Weights Max: 0.1824357211028631 Min: -0.25502923057940086\n","ReLU Activation - Max: 4.619385579342617 Min: 0.0\n","ReLU Activation - Max: 3.7503710544289386 Min: 0.0\n","Softmax Output - Max: 0.9326323981051872 Min: 7.319046818187032e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11192407093167817 Min: -0.11190449599568876\n","Layer 1 - Gradient Weights Max: 0.17397419335874403 Min: -0.19445494296561128\n","Layer 0 - Gradient Weights Max: 0.16371121254207952 Min: -0.13263817002192385\n","ReLU Activation - Max: 4.995987431411749 Min: 0.0\n","ReLU Activation - Max: 4.0392128572584145 Min: 0.0\n","Softmax Output - Max: 0.9112094823081853 Min: 1.9467850353806638e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.06476020754624011 Min: -0.07137629785524742\n","Layer 1 - Gradient Weights Max: 0.18707833654128936 Min: -0.12086456656921186\n","Layer 0 - Gradient Weights Max: 0.16406548244943486 Min: -0.16268527365861413\n","ReLU Activation - Max: 4.856926017740635 Min: 0.0\n","ReLU Activation - Max: 4.075765924440579 Min: 0.0\n","Softmax Output - Max: 0.9876544376962099 Min: 1.7501561547414373e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10151807614446236 Min: -0.16636669256100292\n","Layer 1 - Gradient Weights Max: 0.16916280916207121 Min: -0.19317769862961384\n","Layer 0 - Gradient Weights Max: 0.19757608261525098 Min: -0.19729758991534643\n","ReLU Activation - Max: 6.529965880457904 Min: 0.0\n","ReLU Activation - Max: 5.1710996619359575 Min: 0.0\n","Softmax Output - Max: 0.9955756523980267 Min: 6.59116513206373e-07 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10236671707997601 Min: -0.10254274799091617\n","Layer 1 - Gradient Weights Max: 0.19243050483646276 Min: -0.20960130723350726\n","Layer 0 - Gradient Weights Max: 0.1650071254444159 Min: -0.16880310773193963\n","ReLU Activation - Max: 5.503141732648758 Min: 0.0\n","ReLU Activation - Max: 4.3877201532129275 Min: 0.0\n","Softmax Output - Max: 0.971472137216139 Min: 8.74210869519251e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08096098214301423 Min: -0.09704614882987465\n","Layer 1 - Gradient Weights Max: 0.15865908613921142 Min: -0.1621017722801493\n","Layer 0 - Gradient Weights Max: 0.24510179130347165 Min: -0.23100536053304097\n","ReLU Activation - Max: 4.4366418834409345 Min: 0.0\n","ReLU Activation - Max: 4.603417266792301 Min: 0.0\n","Softmax Output - Max: 0.9257883665218055 Min: 3.0517302897695403e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13024894324638814 Min: -0.14290286079217443\n","Layer 1 - Gradient Weights Max: 0.19799055720138337 Min: -0.16051880173582803\n","Layer 0 - Gradient Weights Max: 0.1797118090946216 Min: -0.26901387174124414\n","ReLU Activation - Max: 5.5974546902069715 Min: 0.0\n","ReLU Activation - Max: 3.618214086226349 Min: 0.0\n","Softmax Output - Max: 0.9756881033555097 Min: 3.466414749371035e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15303253284449422 Min: -0.1384796717122662\n","Layer 1 - Gradient Weights Max: 0.13273139017811716 Min: -0.15526541355513068\n","Layer 0 - Gradient Weights Max: 0.1764658664743224 Min: -0.14567910148372734\n","ReLU Activation - Max: 5.038979854531114 Min: 0.0\n","ReLU Activation - Max: 3.7657343089172275 Min: 0.0\n","Softmax Output - Max: 0.972677411681011 Min: 2.209795333997693e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11204534177177765 Min: -0.13631723145679606\n","Layer 1 - Gradient Weights Max: 0.19125735733689983 Min: -0.19062983205581424\n","Layer 0 - Gradient Weights Max: 0.18093780548050117 Min: -0.1474379146095079\n","ReLU Activation - Max: 4.523192515934412 Min: 0.0\n","ReLU Activation - Max: 4.276346354787318 Min: 0.0\n","Softmax Output - Max: 0.9636749926955149 Min: 1.7639073995822926e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12451909693155001 Min: -0.1246122692386862\n","Layer 1 - Gradient Weights Max: 0.1247544938102955 Min: -0.1368667472253178\n","Layer 0 - Gradient Weights Max: 0.15066806903103674 Min: -0.2586678426590221\n","ReLU Activation - Max: 5.091373534571736 Min: 0.0\n","ReLU Activation - Max: 4.002624567215312 Min: 0.0\n","Softmax Output - Max: 0.9854126233254851 Min: 2.132244862955443e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1458657651649348 Min: -0.10506350649460408\n","Layer 1 - Gradient Weights Max: 0.1945648664661809 Min: -0.20638049134727687\n","Layer 0 - Gradient Weights Max: 0.192059875167008 Min: -0.25520906708310476\n","ReLU Activation - Max: 5.769607499675056 Min: 0.0\n","ReLU Activation - Max: 3.7791861231862347 Min: 0.0\n","Softmax Output - Max: 0.9477715933014728 Min: 1.9620377069183567e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.0905082003924038 Min: -0.1143502241822845\n","Layer 1 - Gradient Weights Max: 0.22187358719258538 Min: -0.26755316973607807\n","Layer 0 - Gradient Weights Max: 0.24857331455843149 Min: -0.2190078284058166\n","ReLU Activation - Max: 7.615195981882641 Min: 0.0\n","ReLU Activation - Max: 4.792274587906417 Min: 0.0\n","Softmax Output - Max: 0.9224981491605312 Min: 3.1974624568362234e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12386061695253497 Min: -0.1257853380755133\n","Layer 1 - Gradient Weights Max: 0.13300295498094936 Min: -0.20863758281899453\n","Layer 0 - Gradient Weights Max: 0.20037355373590413 Min: -0.18859798850150414\n","ReLU Activation - Max: 4.8899546444508974 Min: 0.0\n","ReLU Activation - Max: 4.452861545073557 Min: 0.0\n","Softmax Output - Max: 0.9874961699909197 Min: 1.7554554503872708e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.15441090100873325 Min: -0.15197314010924706\n","Layer 1 - Gradient Weights Max: 0.19597827443710852 Min: -0.15270772088150825\n","Layer 0 - Gradient Weights Max: 0.3474523340253494 Min: -0.3572079188487451\n","ReLU Activation - Max: 6.186593291168457 Min: 0.0\n","ReLU Activation - Max: 4.088048993829052 Min: 0.0\n","Softmax Output - Max: 0.9811196596858507 Min: 1.505442755976342e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11727459783012363 Min: -0.14230899006628267\n","Layer 1 - Gradient Weights Max: 0.16492686263725412 Min: -0.1871766167620447\n","Layer 0 - Gradient Weights Max: 0.17686164730672543 Min: -0.23753765772994467\n","ReLU Activation - Max: 4.684216527880335 Min: 0.0\n","ReLU Activation - Max: 5.391465705005767 Min: 0.0\n","Softmax Output - Max: 0.9687397259026529 Min: 1.4074576505398645e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1023933220347759 Min: -0.11423607501086942\n","Layer 1 - Gradient Weights Max: 0.22821133556590611 Min: -0.2306902769996641\n","Layer 0 - Gradient Weights Max: 0.2135819717170756 Min: -0.1941648789428363\n","ReLU Activation - Max: 4.556533484164625 Min: 0.0\n","ReLU Activation - Max: 3.7006989945482904 Min: 0.0\n","Softmax Output - Max: 0.9806054913348211 Min: 1.3621942913623087e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1063159051692944 Min: -0.139548232199563\n","Layer 1 - Gradient Weights Max: 0.137713310117675 Min: -0.17284617059641105\n","Layer 0 - Gradient Weights Max: 0.22536713408727493 Min: -0.16820685848034347\n","ReLU Activation - Max: 4.49200487540157 Min: 0.0\n","ReLU Activation - Max: 3.4062139847361306 Min: 0.0\n","Softmax Output - Max: 0.9502331886816926 Min: 2.3559333814733802e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08943852317695912 Min: -0.07451372513218603\n","Layer 1 - Gradient Weights Max: 0.15611490823566118 Min: -0.10350595109678011\n","Layer 0 - Gradient Weights Max: 0.20427030980382369 Min: -0.14928102625039\n","ReLU Activation - Max: 4.359055144586031 Min: 0.0\n","ReLU Activation - Max: 3.8970097678155606 Min: 0.0\n","Softmax Output - Max: 0.9415009384048788 Min: 0.00010280845363735135 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0871847449525178 Min: -0.11423842470608062\n","Layer 1 - Gradient Weights Max: 0.15220144746827607 Min: -0.13917527246227362\n","Layer 0 - Gradient Weights Max: 0.18793174162199577 Min: -0.15106618816981152\n","ReLU Activation - Max: 4.389224287827573 Min: 0.0\n","ReLU Activation - Max: 4.483029931313464 Min: 0.0\n","Softmax Output - Max: 0.961487769263637 Min: 2.3643001896520584e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.09299761038485219 Min: -0.10588986929055862\n","Layer 1 - Gradient Weights Max: 0.20467171152212577 Min: -0.16419019454393746\n","Layer 0 - Gradient Weights Max: 0.20726968364294684 Min: -0.1999732353605426\n","ReLU Activation - Max: 5.728632880832661 Min: 0.0\n","ReLU Activation - Max: 3.5416219836990726 Min: 0.0\n","Softmax Output - Max: 0.9650018639385076 Min: 0.00011210439417323802 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12771101001787077 Min: -0.1631615600265961\n","Layer 1 - Gradient Weights Max: 0.20230296069295384 Min: -0.21913524333113946\n","Layer 0 - Gradient Weights Max: 0.2774239588167999 Min: -0.21441416091459506\n","ReLU Activation - Max: 4.947588119628706 Min: 0.0\n","ReLU Activation - Max: 3.827549890366124 Min: 0.0\n","Softmax Output - Max: 0.9300785802882168 Min: 4.08791809872775e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06133406680384252 Min: -0.06274676055886276\n","Layer 1 - Gradient Weights Max: 0.10922375462255984 Min: -0.15620166468065738\n","Layer 0 - Gradient Weights Max: 0.2027195267067485 Min: -0.15342879053317404\n","ReLU Activation - Max: 5.016265277676362 Min: 0.0\n","ReLU Activation - Max: 4.105743893356547 Min: 0.0\n","Softmax Output - Max: 0.9865338575723981 Min: 4.413834288849337e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08743414331203982 Min: -0.12094347181421976\n","Layer 1 - Gradient Weights Max: 0.12729046756345086 Min: -0.15715793745173387\n","Layer 0 - Gradient Weights Max: 0.21771772188261085 Min: -0.259012118157035\n","ReLU Activation - Max: 4.582908314237098 Min: 0.0\n","ReLU Activation - Max: 4.066261787624289 Min: 0.0\n","Softmax Output - Max: 0.9843047406726249 Min: 2.8448167246801206e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11598897184585583 Min: -0.08773701659209297\n","Layer 1 - Gradient Weights Max: 0.26112642850456314 Min: -0.19023217703720252\n","Layer 0 - Gradient Weights Max: 0.22338400967233685 Min: -0.18314950660556167\n","ReLU Activation - Max: 5.089298262197793 Min: 0.0\n","ReLU Activation - Max: 4.19781398768651 Min: 0.0\n","Softmax Output - Max: 0.9959966097377132 Min: 6.033807867438413e-07 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.06557240103008924 Min: -0.13580435078262623\n","Layer 1 - Gradient Weights Max: 0.13179042689828344 Min: -0.1726691083326717\n","Layer 0 - Gradient Weights Max: 0.18121042467215856 Min: -0.21598382878075595\n","ReLU Activation - Max: 5.0400229415845255 Min: 0.0\n","ReLU Activation - Max: 4.050841276199109 Min: 0.0\n","Softmax Output - Max: 0.9629204763917941 Min: 4.721982567569726e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0641126907367106 Min: -0.11284827740735619\n","Layer 1 - Gradient Weights Max: 0.14172993570616332 Min: -0.1669239922027717\n","Layer 0 - Gradient Weights Max: 0.19979515790002583 Min: -0.19602609397018136\n","ReLU Activation - Max: 4.72850008668335 Min: 0.0\n","ReLU Activation - Max: 3.637892417630192 Min: 0.0\n","Softmax Output - Max: 0.9704535778950208 Min: 3.739630371636639e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13752962411753378 Min: -0.14225417501823498\n","Layer 1 - Gradient Weights Max: 0.1458337880893489 Min: -0.20437300638025002\n","Layer 0 - Gradient Weights Max: 0.2529628430943209 Min: -0.20818198790299836\n","ReLU Activation - Max: 4.530526948752014 Min: 0.0\n","ReLU Activation - Max: 3.9738448449607198 Min: 0.0\n","Softmax Output - Max: 0.9080023047084868 Min: 4.943494150161987e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08870990277151068 Min: -0.0696066336855961\n","Layer 1 - Gradient Weights Max: 0.14726249369850145 Min: -0.12509796976022108\n","Layer 0 - Gradient Weights Max: 0.144440986135638 Min: -0.1537892359736636\n","ReLU Activation - Max: 5.00692573918575 Min: 0.0\n","ReLU Activation - Max: 4.3098563496597215 Min: 0.0\n","Softmax Output - Max: 0.9080171641676099 Min: 7.08996891164404e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08410423354751399 Min: -0.08555031102832236\n","Layer 1 - Gradient Weights Max: 0.14813784832536797 Min: -0.12601038945514575\n","Layer 0 - Gradient Weights Max: 0.21913802399539403 Min: -0.17147741265849206\n","ReLU Activation - Max: 3.9936405273592257 Min: 0.0\n","ReLU Activation - Max: 3.8286051938271894 Min: 0.0\n","Softmax Output - Max: 0.95387831588563 Min: 0.00012577335668536953 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11219706832081185 Min: -0.10293978316841687\n","Layer 1 - Gradient Weights Max: 0.23707518950606157 Min: -0.19555326860111466\n","Layer 0 - Gradient Weights Max: 0.20712307751019576 Min: -0.17623896643927453\n","ReLU Activation - Max: 4.628663581650882 Min: 0.0\n","ReLU Activation - Max: 3.889808525550652 Min: 0.0\n","Softmax Output - Max: 0.9767144213872567 Min: 0.00011035340325062958 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14810282412851658 Min: -0.1052227364030446\n","Layer 1 - Gradient Weights Max: 0.1952646911100846 Min: -0.20960839736345876\n","Layer 0 - Gradient Weights Max: 0.192905439577218 Min: -0.16423791707423846\n","ReLU Activation - Max: 5.37537928143187 Min: 0.0\n","ReLU Activation - Max: 4.015949536757817 Min: 0.0\n","Softmax Output - Max: 0.982553366214469 Min: 2.672681987156468e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12283176174063401 Min: -0.11206475555558369\n","Layer 1 - Gradient Weights Max: 0.21644024844437912 Min: -0.10435814773212183\n","Layer 0 - Gradient Weights Max: 0.18149656057929298 Min: -0.14704866052387278\n","ReLU Activation - Max: 4.467236637560947 Min: 0.0\n","ReLU Activation - Max: 4.197239799994832 Min: 0.0\n","Softmax Output - Max: 0.9566175940194898 Min: 3.253011152683102e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10644568632523611 Min: -0.20316468867690085\n","Layer 1 - Gradient Weights Max: 0.17502796319870356 Min: -0.1870782579859044\n","Layer 0 - Gradient Weights Max: 0.18843434760248393 Min: -0.1768118625654496\n","ReLU Activation - Max: 5.409273520182864 Min: 0.0\n","ReLU Activation - Max: 3.8244095730148318 Min: 0.0\n","Softmax Output - Max: 0.9510583111570636 Min: 3.5091825761105832e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10485322969442468 Min: -0.09096921298087057\n","Layer 1 - Gradient Weights Max: 0.16368359823638717 Min: -0.153585962131375\n","Layer 0 - Gradient Weights Max: 0.1779585517662717 Min: -0.16617286353694877\n","ReLU Activation - Max: 6.305861268602711 Min: 0.0\n","ReLU Activation - Max: 4.171226886267511 Min: 0.0\n","Softmax Output - Max: 0.9401093234262116 Min: 0.00010794372795242942 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06570557952001459 Min: -0.09930160774603917\n","Layer 1 - Gradient Weights Max: 0.1292562470532449 Min: -0.1327521996917736\n","Layer 0 - Gradient Weights Max: 0.20218614995821665 Min: -0.1924044956068515\n","ReLU Activation - Max: 5.362920519004092 Min: 0.0\n","ReLU Activation - Max: 3.779336442657859 Min: 0.0\n","Softmax Output - Max: 0.9653263651769065 Min: 1.466698143290093e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10398445445700943 Min: -0.11097430472926673\n","Layer 1 - Gradient Weights Max: 0.14342169115193507 Min: -0.17089527973934784\n","Layer 0 - Gradient Weights Max: 0.19703635620227097 Min: -0.17782689035833962\n","ReLU Activation - Max: 5.090716861989919 Min: 0.0\n","ReLU Activation - Max: 3.798587385509884 Min: 0.0\n","Softmax Output - Max: 0.9491873441946165 Min: 2.762239295160885e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.06920940790176716 Min: -0.12423499721804493\n","Layer 1 - Gradient Weights Max: 0.1499987706895787 Min: -0.20795269205917263\n","Layer 0 - Gradient Weights Max: 0.21479906407531926 Min: -0.22296457950545068\n","ReLU Activation - Max: 5.51710391508591 Min: 0.0\n","ReLU Activation - Max: 4.147712296229275 Min: 0.0\n","Softmax Output - Max: 0.9847050279467173 Min: 4.694173626659107e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14551787057249968 Min: -0.09548004499973659\n","Layer 1 - Gradient Weights Max: 0.1805248630809825 Min: -0.12237844563006788\n","Layer 0 - Gradient Weights Max: 0.15490882156574284 Min: -0.1658954567940535\n","ReLU Activation - Max: 5.493466420756798 Min: 0.0\n","ReLU Activation - Max: 4.163107352202978 Min: 0.0\n","Softmax Output - Max: 0.9866140124133194 Min: 5.074290093954393e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06806871511338003 Min: -0.13119966873997863\n","Layer 1 - Gradient Weights Max: 0.14482864748959948 Min: -0.12331513687220688\n","Layer 0 - Gradient Weights Max: 0.15018844246148258 Min: -0.15248179697852657\n","ReLU Activation - Max: 4.1736675298978545 Min: 0.0\n","ReLU Activation - Max: 4.0012822106500066 Min: 0.0\n","Softmax Output - Max: 0.9991467612755962 Min: 4.92054917057173e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07430110844838453 Min: -0.07948295615969013\n","Layer 1 - Gradient Weights Max: 0.22413344804990207 Min: -0.16764979739040806\n","Layer 0 - Gradient Weights Max: 0.2091595607951245 Min: -0.23676223280335304\n","ReLU Activation - Max: 4.702803480259176 Min: 0.0\n","ReLU Activation - Max: 4.337929926557618 Min: 0.0\n","Softmax Output - Max: 0.9829417073654984 Min: 1.2209496195687192e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.0934539328026121 Min: -0.09511336383119222\n","Layer 1 - Gradient Weights Max: 0.13287779622536358 Min: -0.13937498349732133\n","Layer 0 - Gradient Weights Max: 0.3255664485382309 Min: -0.18284892125890542\n","ReLU Activation - Max: 4.711903914866 Min: 0.0\n","ReLU Activation - Max: 4.125266865264817 Min: 0.0\n","Softmax Output - Max: 0.9902933576434011 Min: 3.500881866691513e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.115864098716902 Min: -0.09743438595303285\n","Layer 1 - Gradient Weights Max: 0.20961246143300175 Min: -0.2110869314926724\n","Layer 0 - Gradient Weights Max: 0.1886935060322261 Min: -0.29545584437500844\n","ReLU Activation - Max: 6.259688800310885 Min: 0.0\n","ReLU Activation - Max: 4.3079406331509915 Min: 0.0\n","Softmax Output - Max: 0.9937213085316445 Min: 3.965125147401315e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07785816131632596 Min: -0.09355410526984315\n","Layer 1 - Gradient Weights Max: 0.14055197904378192 Min: -0.13964267131557925\n","Layer 0 - Gradient Weights Max: 0.1764187681658921 Min: -0.20651860842058364\n","ReLU Activation - Max: 4.291622889871733 Min: 0.0\n","ReLU Activation - Max: 4.775966765549519 Min: 0.0\n","Softmax Output - Max: 0.9590147480720498 Min: 3.471677976299987e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10066394340181911 Min: -0.11547510765761983\n","Layer 1 - Gradient Weights Max: 0.16539013749342 Min: -0.1616905837704043\n","Layer 0 - Gradient Weights Max: 0.25483895000907536 Min: -0.1957719936813708\n","ReLU Activation - Max: 5.190319078797249 Min: 0.0\n","ReLU Activation - Max: 4.271095681626625 Min: 0.0\n","Softmax Output - Max: 0.9554318180966621 Min: 7.007144607549103e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08726951966989045 Min: -0.08998106183246735\n","Layer 1 - Gradient Weights Max: 0.10475870908796023 Min: -0.16878248901652834\n","Layer 0 - Gradient Weights Max: 0.17413892999631794 Min: -0.22859017201355353\n","ReLU Activation - Max: 4.909485132617114 Min: 0.0\n","ReLU Activation - Max: 4.15913470089911 Min: 0.0\n","Softmax Output - Max: 0.9307590263623872 Min: 6.939729082321736e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12495337279720353 Min: -0.16975673252689577\n","Layer 1 - Gradient Weights Max: 0.1935315217735167 Min: -0.20045071422918076\n","Layer 0 - Gradient Weights Max: 0.19511065468150604 Min: -0.22785925377359825\n","ReLU Activation - Max: 7.064234766162755 Min: 0.0\n","ReLU Activation - Max: 4.073528704341776 Min: 0.0\n","Softmax Output - Max: 0.9662947833822816 Min: 2.6303099211452487e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1059595629324204 Min: -0.1672315685710401\n","Layer 1 - Gradient Weights Max: 0.12956859424047684 Min: -0.19436775139751342\n","Layer 0 - Gradient Weights Max: 0.21396580018566178 Min: -0.29237839803564547\n","ReLU Activation - Max: 5.793215048152906 Min: 0.0\n","ReLU Activation - Max: 3.7024284022864373 Min: 0.0\n","Softmax Output - Max: 0.9738617517014414 Min: 1.3506360331148043e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.09935371820723206 Min: -0.09722707494099957\n","Layer 1 - Gradient Weights Max: 0.15876683053270177 Min: -0.20446994169339064\n","Layer 0 - Gradient Weights Max: 0.1874807731996087 Min: -0.157156954913408\n","ReLU Activation - Max: 4.469225764550833 Min: 0.0\n","ReLU Activation - Max: 3.8969813313615433 Min: 0.0\n","Softmax Output - Max: 0.9912408665711966 Min: 1.9544201770785527e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08295046368549686 Min: -0.08574992740363625\n","Layer 1 - Gradient Weights Max: 0.1445214025368028 Min: -0.18508469390894086\n","Layer 0 - Gradient Weights Max: 0.14388045129356958 Min: -0.21627255696077652\n","ReLU Activation - Max: 4.059940578028856 Min: 0.0\n","ReLU Activation - Max: 4.392336435158335 Min: 0.0\n","Softmax Output - Max: 0.9399263313197259 Min: 6.501327844224754e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.06527403267569887 Min: -0.10364573924179452\n","Layer 1 - Gradient Weights Max: 0.12883015921499663 Min: -0.14761887538819346\n","Layer 0 - Gradient Weights Max: 0.2011353548831265 Min: -0.2136609402864505\n","ReLU Activation - Max: 4.8154578467524916 Min: 0.0\n","ReLU Activation - Max: 3.9887398141192274 Min: 0.0\n","Softmax Output - Max: 0.9928836292006739 Min: 1.330050907685411e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08569839114706224 Min: -0.1663374532107565\n","Layer 1 - Gradient Weights Max: 0.15286102192793177 Min: -0.1493271500561948\n","Layer 0 - Gradient Weights Max: 0.23636497655444152 Min: -0.17394163957431458\n","ReLU Activation - Max: 4.791169036654924 Min: 0.0\n","ReLU Activation - Max: 4.76038312721086 Min: 0.0\n","Softmax Output - Max: 0.9769222731458461 Min: 3.8401835479691165e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10079253132044812 Min: -0.11519567866977422\n","Layer 1 - Gradient Weights Max: 0.13196677652295186 Min: -0.16846295341432033\n","Layer 0 - Gradient Weights Max: 0.18652841827572164 Min: -0.20651591066027458\n","ReLU Activation - Max: 4.564371478664579 Min: 0.0\n","ReLU Activation - Max: 3.9674559813332655 Min: 0.0\n","Softmax Output - Max: 0.9708063943953649 Min: 0.00010710585083733734 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1088342173093491 Min: -0.1430203362698145\n","Layer 1 - Gradient Weights Max: 0.1658907816137682 Min: -0.13503543979922764\n","Layer 0 - Gradient Weights Max: 0.18694068838174752 Min: -0.20066949139500698\n","ReLU Activation - Max: 4.311627119997763 Min: 0.0\n","ReLU Activation - Max: 4.001037315368981 Min: 0.0\n","Softmax Output - Max: 0.9778003477779159 Min: 2.3768684808116636e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10195422645276458 Min: -0.1061426284041387\n","Layer 1 - Gradient Weights Max: 0.1291321348147636 Min: -0.16033328758378626\n","Layer 0 - Gradient Weights Max: 0.2358531561774177 Min: -0.20485241986363326\n","ReLU Activation - Max: 4.324800209339418 Min: 0.0\n","ReLU Activation - Max: 4.402863646218651 Min: 0.0\n","Softmax Output - Max: 0.9993911891332734 Min: 4.066075362485697e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08149408026856553 Min: -0.14900766725485962\n","Layer 1 - Gradient Weights Max: 0.16932476677199731 Min: -0.1243463691373993\n","Layer 0 - Gradient Weights Max: 0.19103491112844953 Min: -0.17505330796477697\n","ReLU Activation - Max: 6.600076795784361 Min: 0.0\n","ReLU Activation - Max: 4.6272065075336775 Min: 0.0\n","Softmax Output - Max: 0.9781075457177775 Min: 5.676006823959727e-07 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.15526269963037498 Min: -0.12305757318385935\n","Layer 1 - Gradient Weights Max: 0.22259301764642184 Min: -0.16358455927876084\n","Layer 0 - Gradient Weights Max: 0.19940994321835545 Min: -0.2022751237424741\n","ReLU Activation - Max: 4.741255910625024 Min: 0.0\n","ReLU Activation - Max: 4.794911615522286 Min: 0.0\n","Softmax Output - Max: 0.9925920951325845 Min: 1.478210959847928e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10199367133333488 Min: -0.1444067780974163\n","Layer 1 - Gradient Weights Max: 0.15387223348880935 Min: -0.14830327676581995\n","Layer 0 - Gradient Weights Max: 0.18802007040873406 Min: -0.28544155461653326\n","ReLU Activation - Max: 4.736695143586232 Min: 0.0\n","ReLU Activation - Max: 4.900611978800516 Min: 0.0\n","Softmax Output - Max: 0.9865917202917681 Min: 3.036033100587861e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12163348310803684 Min: -0.17389383422114066\n","Layer 1 - Gradient Weights Max: 0.13521364853439127 Min: -0.19090153889603909\n","Layer 0 - Gradient Weights Max: 0.24613334116276536 Min: -0.19240388979548442\n","ReLU Activation - Max: 4.615795682592998 Min: 0.0\n","ReLU Activation - Max: 4.70903389376967 Min: 0.0\n","Softmax Output - Max: 0.9356238634687671 Min: 0.00012148070496772146 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10123516829437691 Min: -0.1083747614978721\n","Layer 1 - Gradient Weights Max: 0.13822538188517036 Min: -0.1975303692702105\n","Layer 0 - Gradient Weights Max: 0.2498696423208208 Min: -0.1560513791711369\n","ReLU Activation - Max: 4.0494158291449445 Min: 0.0\n","ReLU Activation - Max: 3.7424077188285922 Min: 0.0\n","Softmax Output - Max: 0.9216480761701622 Min: 1.238101059492373e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08361166704278009 Min: -0.1628952543120497\n","Layer 1 - Gradient Weights Max: 0.19694804635714294 Min: -0.2707363345372115\n","Layer 0 - Gradient Weights Max: 0.2477858261816124 Min: -0.24537750614998086\n","ReLU Activation - Max: 5.3710262121434145 Min: 0.0\n","ReLU Activation - Max: 3.876851036783507 Min: 0.0\n","Softmax Output - Max: 0.9962504162740087 Min: 6.532638445294918e-06 Sum (first example): 1.0000000000000004\n","Layer 2 - Gradient Weights Max: 0.10327753017895186 Min: -0.12131788133776354\n","Layer 1 - Gradient Weights Max: 0.17861068254969886 Min: -0.21460540533195913\n","Layer 0 - Gradient Weights Max: 0.261868001720714 Min: -0.2267546715872739\n","ReLU Activation - Max: 3.9282897059633957 Min: 0.0\n","ReLU Activation - Max: 4.771543996072463 Min: 0.0\n","Softmax Output - Max: 0.9807040237382222 Min: 1.848495412562545e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10695450515204727 Min: -0.15695596343158508\n","Layer 1 - Gradient Weights Max: 0.1332809254138939 Min: -0.15721454082605207\n","Layer 0 - Gradient Weights Max: 0.16752258937352527 Min: -0.17862617026754712\n","ReLU Activation - Max: 4.717561383749728 Min: 0.0\n","ReLU Activation - Max: 3.7740428900926686 Min: 0.0\n","Softmax Output - Max: 0.9664035767978001 Min: 1.0202130796659095e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09711333807766462 Min: -0.15153177480479657\n","Layer 1 - Gradient Weights Max: 0.18165564074398036 Min: -0.1930758859089701\n","Layer 0 - Gradient Weights Max: 0.2138890132588268 Min: -0.21689432520432664\n","ReLU Activation - Max: 4.286461053771057 Min: 0.0\n","ReLU Activation - Max: 3.7332027900880886 Min: 0.0\n","Softmax Output - Max: 0.9514195735141099 Min: 0.0001394615719868914 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11837760757431162 Min: -0.15738959935297986\n","Layer 1 - Gradient Weights Max: 0.1539112310542773 Min: -0.1825843845205091\n","Layer 0 - Gradient Weights Max: 0.21090392580108014 Min: -0.15020952477949057\n","ReLU Activation - Max: 5.360224357716193 Min: 0.0\n","ReLU Activation - Max: 4.132584217086371 Min: 0.0\n","Softmax Output - Max: 0.9974384743490983 Min: 7.128893881200719e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12177159709358872 Min: -0.10859981367043152\n","Layer 1 - Gradient Weights Max: 0.16043795728972082 Min: -0.1436533305305274\n","Layer 0 - Gradient Weights Max: 0.23797198416242035 Min: -0.17309797033304572\n","ReLU Activation - Max: 4.984011854387783 Min: 0.0\n","ReLU Activation - Max: 4.007588764564825 Min: 0.0\n","Softmax Output - Max: 0.9744251602396885 Min: 3.796710074508939e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08856905019324116 Min: -0.12366512218539387\n","Layer 1 - Gradient Weights Max: 0.12902672709267218 Min: -0.11614358631105345\n","Layer 0 - Gradient Weights Max: 0.1443842423365857 Min: -0.14874282471532066\n","ReLU Activation - Max: 5.589822383314741 Min: 0.0\n","ReLU Activation - Max: 4.256975445618204 Min: 0.0\n","Softmax Output - Max: 0.974517793501142 Min: 7.873453979928203e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1281698971613933 Min: -0.20969034423969313\n","Layer 1 - Gradient Weights Max: 0.21997464453533364 Min: -0.2272722722912752\n","Layer 0 - Gradient Weights Max: 0.17219464865508574 Min: -0.23447549701347484\n","ReLU Activation - Max: 4.684139092179063 Min: 0.0\n","ReLU Activation - Max: 4.169542650891155 Min: 0.0\n","Softmax Output - Max: 0.9429059658947493 Min: 1.4062203153558978e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08216611138903321 Min: -0.07976688446812118\n","Layer 1 - Gradient Weights Max: 0.08645574008254531 Min: -0.17762070886323217\n","Layer 0 - Gradient Weights Max: 0.15109723669673394 Min: -0.1223878126990739\n","ReLU Activation - Max: 6.035888411257746 Min: 0.0\n","ReLU Activation - Max: 4.068851332921531 Min: 0.0\n","Softmax Output - Max: 0.9724785906130322 Min: 3.0761471901877207e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09320784752205201 Min: -0.20433457266543184\n","Layer 1 - Gradient Weights Max: 0.1435115599565613 Min: -0.1248860684639677\n","Layer 0 - Gradient Weights Max: 0.16469442951566943 Min: -0.15528163672379808\n","ReLU Activation - Max: 4.7466649135198935 Min: 0.0\n","ReLU Activation - Max: 4.7027439575382175 Min: 0.0\n","Softmax Output - Max: 0.9300587122680023 Min: 1.6709297027025113e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14596399795714746 Min: -0.1124555230945022\n","Layer 1 - Gradient Weights Max: 0.16316597948738315 Min: -0.16556131260740464\n","Layer 0 - Gradient Weights Max: 0.24364310419684923 Min: -0.16730952762843562\n","ReLU Activation - Max: 3.9588253759446266 Min: 0.0\n","ReLU Activation - Max: 3.6106607968896856 Min: 0.0\n","Softmax Output - Max: 0.9512266350276706 Min: 6.588504097483843e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.0675203676576518 Min: -0.09818566924443008\n","Layer 1 - Gradient Weights Max: 0.13031528630512335 Min: -0.15972125167413553\n","Layer 0 - Gradient Weights Max: 0.24070359999583515 Min: -0.16301845945183996\n","ReLU Activation - Max: 4.437575631767356 Min: 0.0\n","ReLU Activation - Max: 4.183060863751567 Min: 0.0\n","Softmax Output - Max: 0.9674725757867946 Min: 1.55146566145045e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09705347729942042 Min: -0.09403043670394418\n","Layer 1 - Gradient Weights Max: 0.14056329955016622 Min: -0.21760589813523015\n","Layer 0 - Gradient Weights Max: 0.1544850620392215 Min: -0.18971889605829328\n","ReLU Activation - Max: 6.390752770675931 Min: 0.0\n","ReLU Activation - Max: 4.198470828737502 Min: 0.0\n","Softmax Output - Max: 0.9942592102079072 Min: 3.3628552156019715e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11838564939368318 Min: -0.1302685725610753\n","Layer 1 - Gradient Weights Max: 0.19785065730705198 Min: -0.15184697564322122\n","Layer 0 - Gradient Weights Max: 0.171987398404934 Min: -0.19215685078289002\n","ReLU Activation - Max: 5.499447222805176 Min: 0.0\n","ReLU Activation - Max: 4.335837114020848 Min: 0.0\n","Softmax Output - Max: 0.9741584231717154 Min: 7.354561571683301e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10226334154246643 Min: -0.11794786438869742\n","Layer 1 - Gradient Weights Max: 0.21012790499378955 Min: -0.15106399590523045\n","Layer 0 - Gradient Weights Max: 0.19847323217226306 Min: -0.1606803923958238\n","ReLU Activation - Max: 4.68789245106608 Min: 0.0\n","ReLU Activation - Max: 3.8613666169767007 Min: 0.0\n","Softmax Output - Max: 0.968301336141601 Min: 9.694152713356487e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11870131444195378 Min: -0.11138150332951066\n","Layer 1 - Gradient Weights Max: 0.16349668856268615 Min: -0.15643770280588523\n","Layer 0 - Gradient Weights Max: 0.1953349801592294 Min: -0.18770070254828894\n","ReLU Activation - Max: 4.633553602166169 Min: 0.0\n","ReLU Activation - Max: 4.220726848802537 Min: 0.0\n","Softmax Output - Max: 0.9877995327350089 Min: 9.853065292248556e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.11077178802742425 Min: -0.09859959735212885\n","Layer 1 - Gradient Weights Max: 0.17160696252745078 Min: -0.1775716913839374\n","Layer 0 - Gradient Weights Max: 0.16875730914157663 Min: -0.20315469727151017\n","ReLU Activation - Max: 4.941047512410552 Min: 0.0\n","ReLU Activation - Max: 3.8644920619489866 Min: 0.0\n","Softmax Output - Max: 0.9475478166471403 Min: 5.123582611199697e-06 Sum (first example): 0.9999999999999997\n","Layer 2 - Gradient Weights Max: 0.10114005306894132 Min: -0.14693322657772695\n","Layer 1 - Gradient Weights Max: 0.13652554660000524 Min: -0.16543470574736605\n","Layer 0 - Gradient Weights Max: 0.2246314733748463 Min: -0.1667118797896948\n","ReLU Activation - Max: 5.377940994288565 Min: 0.0\n","ReLU Activation - Max: 4.576033332898507 Min: 0.0\n","Softmax Output - Max: 0.8916313490515736 Min: 1.721626482566831e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1514514874480394 Min: -0.12422186083234985\n","Layer 1 - Gradient Weights Max: 0.18542820060435655 Min: -0.1542165905126399\n","Layer 0 - Gradient Weights Max: 0.18654647548240133 Min: -0.20278565073847069\n","ReLU Activation - Max: 5.399331267243491 Min: 0.0\n","ReLU Activation - Max: 4.145874039479426 Min: 0.0\n","Softmax Output - Max: 0.9624270893168562 Min: 4.082591581648184e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09684744154842835 Min: -0.11530642908137063\n","Layer 1 - Gradient Weights Max: 0.13260009924126204 Min: -0.14269233336658532\n","Layer 0 - Gradient Weights Max: 0.17603375444833894 Min: -0.18014561473756444\n","ReLU Activation - Max: 5.350256711443967 Min: 0.0\n","ReLU Activation - Max: 4.285179476008494 Min: 0.0\n","Softmax Output - Max: 0.9582320966338426 Min: 0.00013186663819274782 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0986444996046618 Min: -0.09289755956860433\n","Layer 1 - Gradient Weights Max: 0.13456390051142975 Min: -0.16596290921505952\n","Layer 0 - Gradient Weights Max: 0.16967506595161644 Min: -0.20204301485825038\n","ReLU Activation - Max: 4.796873852421853 Min: 0.0\n","ReLU Activation - Max: 4.866751235687913 Min: 0.0\n","Softmax Output - Max: 0.9668859456307471 Min: 2.7944146049316777e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09481484711344357 Min: -0.19764816074067051\n","Layer 1 - Gradient Weights Max: 0.2854233622759885 Min: -0.17997151408678255\n","Layer 0 - Gradient Weights Max: 0.1492197192316656 Min: -0.13894384097049728\n","ReLU Activation - Max: 5.3200375793145485 Min: 0.0\n","ReLU Activation - Max: 4.16766473939146 Min: 0.0\n","Softmax Output - Max: 0.8811684347601557 Min: 1.063784963893678e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11560966608849108 Min: -0.12908757244049143\n","Layer 1 - Gradient Weights Max: 0.1453848817544746 Min: -0.22321937515547588\n","Layer 0 - Gradient Weights Max: 0.2463148083080686 Min: -0.22473556401939224\n","ReLU Activation - Max: 5.2377127701693205 Min: 0.0\n","ReLU Activation - Max: 3.8257719144912583 Min: 0.0\n","Softmax Output - Max: 0.9683965286623925 Min: 3.0916854955043156e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07921450443346022 Min: -0.08093588459630516\n","Layer 1 - Gradient Weights Max: 0.10178952041941673 Min: -0.13307399404424675\n","Layer 0 - Gradient Weights Max: 0.201205610707796 Min: -0.22845184264533877\n","ReLU Activation - Max: 7.052693574289726 Min: 0.0\n","ReLU Activation - Max: 5.560118320852378 Min: 0.0\n","Softmax Output - Max: 0.959157562212915 Min: 4.000704668366481e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11566410667469548 Min: -0.11322556023011474\n","Layer 1 - Gradient Weights Max: 0.10872898565012212 Min: -0.15982162459778138\n","Layer 0 - Gradient Weights Max: 0.12415788638814904 Min: -0.2017307530310321\n","ReLU Activation - Max: 4.2336820222631015 Min: 0.0\n","ReLU Activation - Max: 3.7581559800803324 Min: 0.0\n","Softmax Output - Max: 0.9457700551428038 Min: 1.1803233048776175e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08297703048203502 Min: -0.09343524636261008\n","Layer 1 - Gradient Weights Max: 0.14246793010491982 Min: -0.12856640907536976\n","Layer 0 - Gradient Weights Max: 0.17283756087287885 Min: -0.17136913947714422\n","ReLU Activation - Max: 5.135020681191176 Min: 0.0\n","ReLU Activation - Max: 3.7992007734521414 Min: 0.0\n","Softmax Output - Max: 0.9648698952164589 Min: 2.077330425525747e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12735430227424396 Min: -0.1380114129390431\n","Layer 1 - Gradient Weights Max: 0.15313042204026694 Min: -0.15423374334400264\n","Layer 0 - Gradient Weights Max: 0.1638045371359043 Min: -0.19328922836311307\n","ReLU Activation - Max: 5.6640039472441295 Min: 0.0\n","ReLU Activation - Max: 5.249537374780212 Min: 0.0\n","Softmax Output - Max: 0.9469177629283844 Min: 2.9102353004703805e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1543797547706036 Min: -0.11172811334473182\n","Layer 1 - Gradient Weights Max: 0.1309681029031107 Min: -0.16260978529605458\n","Layer 0 - Gradient Weights Max: 0.1855531810059048 Min: -0.2262773948056406\n","ReLU Activation - Max: 4.259648852780844 Min: 0.0\n","ReLU Activation - Max: 4.2355948154204395 Min: 0.0\n","Softmax Output - Max: 0.8745488013362359 Min: 7.459592123423213e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09267864972009669 Min: -0.14884950703077435\n","Layer 1 - Gradient Weights Max: 0.1436580575784806 Min: -0.18144983592096262\n","Layer 0 - Gradient Weights Max: 0.18497085266762092 Min: -0.21218234831225613\n","ReLU Activation - Max: 4.369986473943293 Min: 0.0\n","ReLU Activation - Max: 4.170516049568999 Min: 0.0\n","Softmax Output - Max: 0.9802704581733075 Min: 2.6892558701302178e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.17297253298889453 Min: -0.14047122900744294\n","Layer 1 - Gradient Weights Max: 0.18754546679751624 Min: -0.14964499036400197\n","Layer 0 - Gradient Weights Max: 0.16307631172299453 Min: -0.16856206241421848\n","ReLU Activation - Max: 6.348714739915479 Min: 0.0\n","ReLU Activation - Max: 4.391317471687702 Min: 0.0\n","Softmax Output - Max: 0.9909271916658066 Min: 9.989071697853117e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08046046285929496 Min: -0.1427539894949214\n","Layer 1 - Gradient Weights Max: 0.14847419049146937 Min: -0.25234376044509654\n","Layer 0 - Gradient Weights Max: 0.12756369185976812 Min: -0.14926849162889114\n","ReLU Activation - Max: 6.040000388961443 Min: 0.0\n","ReLU Activation - Max: 4.2764589101972055 Min: 0.0\n","Softmax Output - Max: 0.9840816199135838 Min: 1.2820725672496842e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07546399629424942 Min: -0.09827629253469362\n","Layer 1 - Gradient Weights Max: 0.14021144574695077 Min: -0.13487619366934503\n","Layer 0 - Gradient Weights Max: 0.16173478094330793 Min: -0.20076791701815366\n","ReLU Activation - Max: 5.132309251288799 Min: 0.0\n","ReLU Activation - Max: 3.6738311288674366 Min: 0.0\n","Softmax Output - Max: 0.9660932019788069 Min: 4.423964383724961e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07906093678064871 Min: -0.13366431182824678\n","Layer 1 - Gradient Weights Max: 0.12816956933595675 Min: -0.17854692885451903\n","Layer 0 - Gradient Weights Max: 0.17294733316552804 Min: -0.2510062535120424\n","ReLU Activation - Max: 5.649843584663631 Min: 0.0\n","ReLU Activation - Max: 3.533061099656653 Min: 0.0\n","Softmax Output - Max: 0.9551523435072248 Min: 5.120588781272223e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13122904670984345 Min: -0.16210025135243547\n","Layer 1 - Gradient Weights Max: 0.14251604432230003 Min: -0.15279037362418496\n","Layer 0 - Gradient Weights Max: 0.1415534040090769 Min: -0.13145520470314861\n","ReLU Activation - Max: 4.539421099257939 Min: 0.0\n","ReLU Activation - Max: 4.173805678300655 Min: 0.0\n","Softmax Output - Max: 0.9660390570707551 Min: 3.092231240284947e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12310463136297035 Min: -0.11736470496199718\n","Layer 1 - Gradient Weights Max: 0.15668426651268935 Min: -0.19365324385910063\n","Layer 0 - Gradient Weights Max: 0.22056293559728535 Min: -0.19343920193206937\n","ReLU Activation - Max: 5.305435504434059 Min: 0.0\n","ReLU Activation - Max: 3.636762803412647 Min: 0.0\n","Softmax Output - Max: 0.9126102971294643 Min: 8.69151041964643e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1268795120319429 Min: -0.16808930936093114\n","Layer 1 - Gradient Weights Max: 0.17652621271675883 Min: -0.19569426868425716\n","Layer 0 - Gradient Weights Max: 0.19330276866765658 Min: -0.1860792342323907\n","ReLU Activation - Max: 5.6211633647914265 Min: 0.0\n","ReLU Activation - Max: 3.75462599997226 Min: 0.0\n","Softmax Output - Max: 0.9217097183739442 Min: 7.445572621956053e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08358995061515699 Min: -0.08986347433428382\n","Layer 1 - Gradient Weights Max: 0.15427219880005402 Min: -0.1705682446803796\n","Layer 0 - Gradient Weights Max: 0.16320857608892866 Min: -0.18478066714542815\n","ReLU Activation - Max: 4.624183042578534 Min: 0.0\n","ReLU Activation - Max: 4.298863617913112 Min: 0.0\n","Softmax Output - Max: 0.9974304329825047 Min: 4.8477008422293906e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09793077165321928 Min: -0.11533584287840437\n","Layer 1 - Gradient Weights Max: 0.15143044349846535 Min: -0.1614039299104141\n","Layer 0 - Gradient Weights Max: 0.20801973380075156 Min: -0.22889701300486442\n","ReLU Activation - Max: 4.51233092306498 Min: 0.0\n","ReLU Activation - Max: 4.401406899417651 Min: 0.0\n","Softmax Output - Max: 0.9881336786582077 Min: 1.709032963786693e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08901594022842883 Min: -0.177513751280772\n","Layer 1 - Gradient Weights Max: 0.13719262128402326 Min: -0.17801892296904315\n","Layer 0 - Gradient Weights Max: 0.2308211276855931 Min: -0.17920396511915887\n","ReLU Activation - Max: 5.465432146060474 Min: 0.0\n","ReLU Activation - Max: 3.5607326860095556 Min: 0.0\n","Softmax Output - Max: 0.9641442284452353 Min: 2.979077485135735e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10397977916193138 Min: -0.12341314812338328\n","Layer 1 - Gradient Weights Max: 0.12888823564164856 Min: -0.14104082598071405\n","Layer 0 - Gradient Weights Max: 0.16792007999887837 Min: -0.16819530455933987\n","ReLU Activation - Max: 4.432693508097894 Min: 0.0\n","ReLU Activation - Max: 3.9921632958732465 Min: 0.0\n","Softmax Output - Max: 0.9744341230644268 Min: 4.575694334392283e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10409160016002494 Min: -0.10886086604301151\n","Layer 1 - Gradient Weights Max: 0.14833622481415146 Min: -0.19497305812308668\n","Layer 0 - Gradient Weights Max: 0.16297881201373807 Min: -0.15347572306236895\n","ReLU Activation - Max: 5.116217325921495 Min: 0.0\n","ReLU Activation - Max: 4.268518152492607 Min: 0.0\n","Softmax Output - Max: 0.9405390669079334 Min: 5.467011559755967e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07393167748970739 Min: -0.11398757009113773\n","Layer 1 - Gradient Weights Max: 0.14153919216424835 Min: -0.14151334314907107\n","Layer 0 - Gradient Weights Max: 0.17385141046925406 Min: -0.20207908465529903\n","ReLU Activation - Max: 5.642963255776446 Min: 0.0\n","ReLU Activation - Max: 4.329150825909219 Min: 0.0\n","Softmax Output - Max: 0.942122241439029 Min: 6.782874119326989e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11666705035769301 Min: -0.13226395838820396\n","Layer 1 - Gradient Weights Max: 0.23920212501421226 Min: -0.18170317182764587\n","Layer 0 - Gradient Weights Max: 0.17915389343151988 Min: -0.19088634065928986\n","ReLU Activation - Max: 4.998070751472256 Min: 0.0\n","ReLU Activation - Max: 5.1836445790504735 Min: 0.0\n","Softmax Output - Max: 0.9902051428917548 Min: 6.602972109697874e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06980573025042673 Min: -0.06894933584824708\n","Layer 1 - Gradient Weights Max: 0.14813164438798482 Min: -0.13405445846268604\n","Layer 0 - Gradient Weights Max: 0.23516838315682687 Min: -0.20081982873731966\n","ReLU Activation - Max: 5.679258325225291 Min: 0.0\n","ReLU Activation - Max: 4.216668986746879 Min: 0.0\n","Softmax Output - Max: 0.9557510399347031 Min: 6.403065729317283e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13422931724404702 Min: -0.1427926132840555\n","Layer 1 - Gradient Weights Max: 0.16039845016743443 Min: -0.16276120465995095\n","Layer 0 - Gradient Weights Max: 0.18537488759729295 Min: -0.164397053133873\n","ReLU Activation - Max: 5.972287537683702 Min: 0.0\n","ReLU Activation - Max: 4.137717985531735 Min: 0.0\n","Softmax Output - Max: 0.9884435232842527 Min: 2.1925644193343987e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09390993180567318 Min: -0.12618509813599854\n","Layer 1 - Gradient Weights Max: 0.13973745189478043 Min: -0.16776066088294905\n","Layer 0 - Gradient Weights Max: 0.2859290039639303 Min: -0.26363038676369427\n","ReLU Activation - Max: 5.0927486010785765 Min: 0.0\n","ReLU Activation - Max: 4.05287041628982 Min: 0.0\n","Softmax Output - Max: 0.9622844963669639 Min: 3.2082087190084914e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09862821670913516 Min: -0.11292283192748757\n","Layer 1 - Gradient Weights Max: 0.14794659433923682 Min: -0.16247261585632677\n","Layer 0 - Gradient Weights Max: 0.2528624363657548 Min: -0.22906375266245316\n","ReLU Activation - Max: 4.882612273387288 Min: 0.0\n","ReLU Activation - Max: 4.141438718109983 Min: 0.0\n","Softmax Output - Max: 0.956736667018967 Min: 5.4120746500621065e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08208332339167675 Min: -0.14111285518344666\n","Layer 1 - Gradient Weights Max: 0.14959491114700854 Min: -0.14019230738753258\n","Layer 0 - Gradient Weights Max: 0.14772584770119143 Min: -0.1807600354195206\n","ReLU Activation - Max: 5.362800359479707 Min: 0.0\n","ReLU Activation - Max: 3.8925133853631957 Min: 0.0\n","Softmax Output - Max: 0.990469886119725 Min: 5.592344440139319e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09384332833477324 Min: -0.09600841199014264\n","Layer 1 - Gradient Weights Max: 0.16486098350144623 Min: -0.16424337445155432\n","Layer 0 - Gradient Weights Max: 0.21767978777034555 Min: -0.20293540403766966\n","ReLU Activation - Max: 4.730317898048171 Min: 0.0\n","ReLU Activation - Max: 3.9126060090740107 Min: 0.0\n","Softmax Output - Max: 0.9810808270278313 Min: 7.214835185756278e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08314440088028952 Min: -0.12894963325082093\n","Layer 1 - Gradient Weights Max: 0.1235723597154376 Min: -0.13223496494805115\n","Layer 0 - Gradient Weights Max: 0.14559195820760423 Min: -0.16685525652428948\n","ReLU Activation - Max: 5.97452053236759 Min: 0.0\n","ReLU Activation - Max: 3.9378153094824935 Min: 0.0\n","Softmax Output - Max: 0.9531157608810613 Min: 0.000165848347085422 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06605023054954025 Min: -0.1380516830599231\n","Layer 1 - Gradient Weights Max: 0.1590111672791956 Min: -0.12856582188398905\n","Layer 0 - Gradient Weights Max: 0.183001795854616 Min: -0.14949118027105765\n","ReLU Activation - Max: 5.243124220379642 Min: 0.0\n","ReLU Activation - Max: 3.770022816641121 Min: 0.0\n","Softmax Output - Max: 0.981435531326684 Min: 4.861896535733688e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07410321490852924 Min: -0.1177383032950854\n","Layer 1 - Gradient Weights Max: 0.1163677422263717 Min: -0.12051397752696938\n","Layer 0 - Gradient Weights Max: 0.17722609454053512 Min: -0.2043513451483827\n","ReLU Activation - Max: 4.925300127311384 Min: 0.0\n","ReLU Activation - Max: 3.9112189946844786 Min: 0.0\n","Softmax Output - Max: 0.9567111012784715 Min: 7.233371995587167e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10462883649273576 Min: -0.11338586030532316\n","Layer 1 - Gradient Weights Max: 0.14175187721433408 Min: -0.1479434386961545\n","Layer 0 - Gradient Weights Max: 0.1645299828246493 Min: -0.21589173831507125\n","ReLU Activation - Max: 5.57728002836911 Min: 0.0\n","ReLU Activation - Max: 3.932807348893137 Min: 0.0\n","Softmax Output - Max: 0.9367773085113851 Min: 2.8606616761805622e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12420446328079984 Min: -0.15171648321795972\n","Layer 1 - Gradient Weights Max: 0.18024781720853775 Min: -0.22395308604041836\n","Layer 0 - Gradient Weights Max: 0.20738884399718865 Min: -0.18152718812323207\n","ReLU Activation - Max: 5.355410627801961 Min: 0.0\n","ReLU Activation - Max: 3.6542754774841826 Min: 0.0\n","Softmax Output - Max: 0.9896719672830174 Min: 1.89482915438004e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07491709682541198 Min: -0.09968333763965036\n","Layer 1 - Gradient Weights Max: 0.1441489728425534 Min: -0.16305153651868376\n","Layer 0 - Gradient Weights Max: 0.16639536407745703 Min: -0.15412199171952667\n","ReLU Activation - Max: 5.272970048279916 Min: 0.0\n","ReLU Activation - Max: 3.8344940433891477 Min: 0.0\n","Softmax Output - Max: 0.9267126338594533 Min: 5.378049056438912e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11166249397177859 Min: -0.16013456419288805\n","Layer 1 - Gradient Weights Max: 0.19023749408126214 Min: -0.29303609575386796\n","Layer 0 - Gradient Weights Max: 0.17027458657599262 Min: -0.1872895018783197\n","ReLU Activation - Max: 7.045532242250717 Min: 0.0\n","ReLU Activation - Max: 4.1642267814510525 Min: 0.0\n","Softmax Output - Max: 0.9314188603521393 Min: 0.00010513229728429398 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07225224875215155 Min: -0.1412036899726248\n","Layer 1 - Gradient Weights Max: 0.14250859954504314 Min: -0.14039714150028498\n","Layer 0 - Gradient Weights Max: 0.308206568531651 Min: -0.19746741235948542\n","ReLU Activation - Max: 4.793349023011849 Min: 0.0\n","ReLU Activation - Max: 4.092601358849348 Min: 0.0\n","Softmax Output - Max: 0.9477257820046339 Min: 2.937910739232021e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09990752530667574 Min: -0.0877513929144992\n","Layer 1 - Gradient Weights Max: 0.19024775611273675 Min: -0.1779789444319015\n","Layer 0 - Gradient Weights Max: 0.2229873230652793 Min: -0.28614664179573435\n","ReLU Activation - Max: 4.895172247273175 Min: 0.0\n","ReLU Activation - Max: 3.7464557762643804 Min: 0.0\n","Softmax Output - Max: 0.9838179336592653 Min: 4.1422673264497136e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07099476551486428 Min: -0.09934417627677464\n","Layer 1 - Gradient Weights Max: 0.14358038841636725 Min: -0.14575115959574975\n","Layer 0 - Gradient Weights Max: 0.16539674891239728 Min: -0.2173327278794672\n","ReLU Activation - Max: 4.6554753652757 Min: 0.0\n","ReLU Activation - Max: 3.642583999616772 Min: 0.0\n","Softmax Output - Max: 0.9675067244458545 Min: 3.7622773004069343e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1120963232520841 Min: -0.0986739715850865\n","Layer 1 - Gradient Weights Max: 0.14143840237326336 Min: -0.13952812278729207\n","Layer 0 - Gradient Weights Max: 0.18067966605029007 Min: -0.15943981339647523\n","ReLU Activation - Max: 4.3426877739885885 Min: 0.0\n","ReLU Activation - Max: 3.9534732392623226 Min: 0.0\n","Softmax Output - Max: 0.9697766099423395 Min: 1.4371296897527442e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10062204458869858 Min: -0.08980964336057089\n","Layer 1 - Gradient Weights Max: 0.14985188853073247 Min: -0.19866394030485707\n","Layer 0 - Gradient Weights Max: 0.19848395271788533 Min: -0.23179431148964977\n","ReLU Activation - Max: 6.07712844841728 Min: 0.0\n","ReLU Activation - Max: 4.542984655575394 Min: 0.0\n","Softmax Output - Max: 0.9752346436046547 Min: 1.1940315119037878e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07813739934022633 Min: -0.07016200554535255\n","Layer 1 - Gradient Weights Max: 0.12937810410016204 Min: -0.11756820301741575\n","Layer 0 - Gradient Weights Max: 0.15636710857982505 Min: -0.1775587233703387\n","ReLU Activation - Max: 4.289711190931545 Min: 0.0\n","ReLU Activation - Max: 3.906294281887956 Min: 0.0\n","Softmax Output - Max: 0.989213016032462 Min: 1.0837723726790936e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1143804855882098 Min: -0.1108772066018761\n","Layer 1 - Gradient Weights Max: 0.1797445921098744 Min: -0.11818724472446561\n","Layer 0 - Gradient Weights Max: 0.1912570652886044 Min: -0.21113046418785875\n","ReLU Activation - Max: 4.320656220851044 Min: 0.0\n","ReLU Activation - Max: 4.5383464296387155 Min: 0.0\n","Softmax Output - Max: 0.9888526060429254 Min: 3.472212441676966e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.06117101353403107 Min: -0.09095371075684161\n","Layer 1 - Gradient Weights Max: 0.14100659268003515 Min: -0.1644646274044546\n","Layer 0 - Gradient Weights Max: 0.177506652070288 Min: -0.19118423482671815\n","ReLU Activation - Max: 4.5639283032450475 Min: 0.0\n","ReLU Activation - Max: 3.9559208449647185 Min: 0.0\n","Softmax Output - Max: 0.9525995289245359 Min: 6.94052899483064e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09865915441640276 Min: -0.13679419522767042\n","Layer 1 - Gradient Weights Max: 0.1276488302466427 Min: -0.13370769645815778\n","Layer 0 - Gradient Weights Max: 0.2219471058829591 Min: -0.17802533163988857\n","ReLU Activation - Max: 4.96523937562668 Min: 0.0\n","ReLU Activation - Max: 3.8906265053971545 Min: 0.0\n","Softmax Output - Max: 0.9592896190333108 Min: 4.048973731100815e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.12360234659224434 Min: -0.15448300531605172\n","Layer 1 - Gradient Weights Max: 0.22036542494208683 Min: -0.21359639834096034\n","Layer 0 - Gradient Weights Max: 0.16296210198327507 Min: -0.17114924560692585\n","ReLU Activation - Max: 5.774518883796132 Min: 0.0\n","ReLU Activation - Max: 4.781183065446214 Min: 0.0\n","Softmax Output - Max: 0.9544318222661011 Min: 2.923315915782854e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10704138000515652 Min: -0.12640533748681965\n","Layer 1 - Gradient Weights Max: 0.14581353495506394 Min: -0.18206096198159502\n","Layer 0 - Gradient Weights Max: 0.19230839323897905 Min: -0.14002349304180384\n","ReLU Activation - Max: 4.633713252049598 Min: 0.0\n","ReLU Activation - Max: 4.17726196014853 Min: 0.0\n","Softmax Output - Max: 0.9898929843439973 Min: 2.1344960507238688e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12601054872598408 Min: -0.12796628483806602\n","Layer 1 - Gradient Weights Max: 0.18620692747611312 Min: -0.12643839810347465\n","Layer 0 - Gradient Weights Max: 0.20465045571263143 Min: -0.17727085968655898\n","ReLU Activation - Max: 5.135470276852194 Min: 0.0\n","ReLU Activation - Max: 4.31189524422229 Min: 0.0\n","Softmax Output - Max: 0.9712304538959953 Min: 1.4395295442676507e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10337982880782094 Min: -0.09894938031398737\n","Layer 1 - Gradient Weights Max: 0.15852374688634263 Min: -0.174717116619972\n","Layer 0 - Gradient Weights Max: 0.24023149069152366 Min: -0.17628806938731756\n","ReLU Activation - Max: 5.525342839810223 Min: 0.0\n","ReLU Activation - Max: 4.235697854942093 Min: 0.0\n","Softmax Output - Max: 0.9093138243617105 Min: 9.899760685656587e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11528566033253616 Min: -0.15120448837042527\n","Layer 1 - Gradient Weights Max: 0.13412662211276563 Min: -0.16981843595229712\n","Layer 0 - Gradient Weights Max: 0.14982198304669495 Min: -0.25624935084943107\n","ReLU Activation - Max: 5.015880895279805 Min: 0.0\n","ReLU Activation - Max: 3.781074022644557 Min: 0.0\n","Softmax Output - Max: 0.9803818790299274 Min: 2.974948376000923e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1498203799327445 Min: -0.10273593738333928\n","Layer 1 - Gradient Weights Max: 0.1576823915358988 Min: -0.17017941091206873\n","Layer 0 - Gradient Weights Max: 0.15664731618502825 Min: -0.20642480025343415\n","ReLU Activation - Max: 6.089292085913883 Min: 0.0\n","ReLU Activation - Max: 3.9561169252042654 Min: 0.0\n","Softmax Output - Max: 0.9748743444870592 Min: 2.176001484566688e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11029667463923506 Min: -0.12402146714456122\n","Layer 1 - Gradient Weights Max: 0.13517655999825312 Min: -0.1338784345138759\n","Layer 0 - Gradient Weights Max: 0.17967103221509576 Min: -0.276672162069459\n","ReLU Activation - Max: 6.589410671466802 Min: 0.0\n","ReLU Activation - Max: 3.9846357019926417 Min: 0.0\n","Softmax Output - Max: 0.9805598148086921 Min: 0.00011820597986033748 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1288064268687193 Min: -0.12417195141891282\n","Layer 1 - Gradient Weights Max: 0.20650030765943983 Min: -0.24498244224156204\n","Layer 0 - Gradient Weights Max: 0.1878532785472768 Min: -0.2180978475468862\n","ReLU Activation - Max: 5.003499613161127 Min: 0.0\n","ReLU Activation - Max: 4.398768503202684 Min: 0.0\n","Softmax Output - Max: 0.9254043422999246 Min: 3.8455688136695104e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.06778290057266298 Min: -0.09784611673807524\n","Layer 1 - Gradient Weights Max: 0.15722391695088037 Min: -0.12017404428044755\n","Layer 0 - Gradient Weights Max: 0.13715196442618588 Min: -0.13780356664049542\n","ReLU Activation - Max: 5.3355935665010055 Min: 0.0\n","ReLU Activation - Max: 4.3421251336815 Min: 0.0\n","Softmax Output - Max: 0.9277638913877057 Min: 5.010315756391271e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14066331772436044 Min: -0.10062948132463817\n","Layer 1 - Gradient Weights Max: 0.20226249672660526 Min: -0.16521666258678164\n","Layer 0 - Gradient Weights Max: 0.15124225831530602 Min: -0.17592082933802083\n","ReLU Activation - Max: 5.525179975616906 Min: 0.0\n","ReLU Activation - Max: 4.393133212712386 Min: 0.0\n","Softmax Output - Max: 0.9542436177130053 Min: 1.8214434804493995e-07 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10043907439877361 Min: -0.14330776442530208\n","Layer 1 - Gradient Weights Max: 0.2002881735329868 Min: -0.2006428223738629\n","Layer 0 - Gradient Weights Max: 0.2061884514320744 Min: -0.19191386328403406\n","ReLU Activation - Max: 5.424021112774864 Min: 0.0\n","ReLU Activation - Max: 5.277957330232056 Min: 0.0\n","Softmax Output - Max: 0.9931346147326979 Min: 2.973418438104842e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.15755897126249954 Min: -0.09244955894333369\n","Layer 1 - Gradient Weights Max: 0.15671467622000373 Min: -0.20423480084895954\n","Layer 0 - Gradient Weights Max: 0.16761809481103782 Min: -0.16625609687530005\n","ReLU Activation - Max: 4.624588508508056 Min: 0.0\n","ReLU Activation - Max: 4.006403147837127 Min: 0.0\n","Softmax Output - Max: 0.970977557384806 Min: 3.573747166353648e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07028737917807822 Min: -0.0901372424927281\n","Layer 1 - Gradient Weights Max: 0.15091368753317264 Min: -0.16742404025307267\n","Layer 0 - Gradient Weights Max: 0.28473483433913976 Min: -0.3100410133498968\n","ReLU Activation - Max: 4.895838472483837 Min: 0.0\n","ReLU Activation - Max: 3.971996576626967 Min: 0.0\n","Softmax Output - Max: 0.9733152233971133 Min: 0.00010967994287903414 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07142674684213267 Min: -0.0860008479731443\n","Layer 1 - Gradient Weights Max: 0.09365457100388964 Min: -0.14080757579848038\n","Layer 0 - Gradient Weights Max: 0.25070119593311485 Min: -0.2187005222535254\n","ReLU Activation - Max: 4.80312981976647 Min: 0.0\n","ReLU Activation - Max: 3.934245877159838 Min: 0.0\n","Softmax Output - Max: 0.9693411604063474 Min: 2.0083093137198404e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07655294727628777 Min: -0.08326686762092649\n","Layer 1 - Gradient Weights Max: 0.18605056713355453 Min: -0.11117628923940821\n","Layer 0 - Gradient Weights Max: 0.1997976407531526 Min: -0.17161910319593957\n","ReLU Activation - Max: 5.416686793559593 Min: 0.0\n","ReLU Activation - Max: 3.850661399081943 Min: 0.0\n","Softmax Output - Max: 0.9981366576992323 Min: 5.67611651359839e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07598950481964177 Min: -0.17024377457385403\n","Layer 1 - Gradient Weights Max: 0.1481195369678191 Min: -0.2007032634596398\n","Layer 0 - Gradient Weights Max: 0.15337319557835505 Min: -0.1254055209621882\n","ReLU Activation - Max: 5.458502347956834 Min: 0.0\n","ReLU Activation - Max: 4.242816936933266 Min: 0.0\n","Softmax Output - Max: 0.9669534543691727 Min: 3.823447359240728e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.12623828332816056 Min: -0.15504998292042682\n","Layer 1 - Gradient Weights Max: 0.16656077465748292 Min: -0.1960747259220981\n","Layer 0 - Gradient Weights Max: 0.24684638863726732 Min: -0.25106147742168167\n","ReLU Activation - Max: 4.983182017329247 Min: 0.0\n","ReLU Activation - Max: 4.133199179517025 Min: 0.0\n","Softmax Output - Max: 0.970256547407388 Min: 6.051482825031507e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0786257424639662 Min: -0.08967197222634148\n","Layer 1 - Gradient Weights Max: 0.1385365357612405 Min: -0.11638098067761432\n","Layer 0 - Gradient Weights Max: 0.24854910397411242 Min: -0.22474197440652066\n","ReLU Activation - Max: 5.509348700959204 Min: 0.0\n","ReLU Activation - Max: 4.03779209362411 Min: 0.0\n","Softmax Output - Max: 0.8925237302605373 Min: 3.4337436429108016e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0988619943993922 Min: -0.11670040635106425\n","Layer 1 - Gradient Weights Max: 0.24451824983641537 Min: -0.2468211755208043\n","Layer 0 - Gradient Weights Max: 0.19906378009734074 Min: -0.19887724911472726\n","ReLU Activation - Max: 4.854730890014875 Min: 0.0\n","ReLU Activation - Max: 4.277975245777075 Min: 0.0\n","Softmax Output - Max: 0.9680004675083383 Min: 3.7466653309336266e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12192324017676047 Min: -0.0677942863585915\n","Layer 1 - Gradient Weights Max: 0.12858599917730548 Min: -0.12493148971794675\n","Layer 0 - Gradient Weights Max: 0.17499427226126918 Min: -0.18012111241140413\n","ReLU Activation - Max: 6.5272734375345305 Min: 0.0\n","ReLU Activation - Max: 4.627859067736201 Min: 0.0\n","Softmax Output - Max: 0.9292472928476737 Min: 1.3654930109373384e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10137008840541716 Min: -0.11850688621780363\n","Layer 1 - Gradient Weights Max: 0.14982725663868535 Min: -0.19092346720218306\n","Layer 0 - Gradient Weights Max: 0.15270472010289382 Min: -0.17833965210066033\n","ReLU Activation - Max: 4.821421867172048 Min: 0.0\n","ReLU Activation - Max: 4.054398851399515 Min: 0.0\n","Softmax Output - Max: 0.9894778956675755 Min: 1.0044169063992956e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09679725884891259 Min: -0.11747114151020527\n","Layer 1 - Gradient Weights Max: 0.1267974375181534 Min: -0.13345057684308698\n","Layer 0 - Gradient Weights Max: 0.15731790787839772 Min: -0.20123507825343062\n","ReLU Activation - Max: 5.397528985452677 Min: 0.0\n","ReLU Activation - Max: 4.918983946190577 Min: 0.0\n","Softmax Output - Max: 0.9662275368020542 Min: 5.454170932892163e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08461694165470858 Min: -0.09760735035769767\n","Layer 1 - Gradient Weights Max: 0.1079332727270115 Min: -0.147753012171178\n","Layer 0 - Gradient Weights Max: 0.18950774005771898 Min: -0.18580550527955778\n","ReLU Activation - Max: 5.821064324428773 Min: 0.0\n","ReLU Activation - Max: 3.7264052701452757 Min: 0.0\n","Softmax Output - Max: 0.9690122628269755 Min: 3.455431652033748e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1061802630103363 Min: -0.14491715558694215\n","Layer 1 - Gradient Weights Max: 0.1426506449215094 Min: -0.17453581846500352\n","Layer 0 - Gradient Weights Max: 0.18954095038967297 Min: -0.1520073331972648\n","ReLU Activation - Max: 5.374540351064933 Min: 0.0\n","ReLU Activation - Max: 4.521501785601377 Min: 0.0\n","Softmax Output - Max: 0.9372392867253535 Min: 1.3420438182485832e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07699027837197475 Min: -0.0813998804892722\n","Layer 1 - Gradient Weights Max: 0.1683601831032958 Min: -0.17282332262709044\n","Layer 0 - Gradient Weights Max: 0.14991974767945182 Min: -0.1621441075579563\n","ReLU Activation - Max: 4.846436188996953 Min: 0.0\n","ReLU Activation - Max: 4.727490087272285 Min: 0.0\n","Softmax Output - Max: 0.9485607371639385 Min: 9.651591513553997e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08640947383250158 Min: -0.08284054262077199\n","Layer 1 - Gradient Weights Max: 0.1421575937768554 Min: -0.1664905423741844\n","Layer 0 - Gradient Weights Max: 0.1961981145133119 Min: -0.19203515823999381\n","ReLU Activation - Max: 4.9459893230300676 Min: 0.0\n","ReLU Activation - Max: 4.2447244852424015 Min: 0.0\n","Softmax Output - Max: 0.9410294755742494 Min: 0.00014287310864986077 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13073712145942187 Min: -0.13521131914466805\n","Layer 1 - Gradient Weights Max: 0.18745257522082423 Min: -0.18616643462173513\n","Layer 0 - Gradient Weights Max: 0.16001047029118595 Min: -0.18634950592757052\n","ReLU Activation - Max: 5.982015261303248 Min: 0.0\n","ReLU Activation - Max: 4.161830043531006 Min: 0.0\n","Softmax Output - Max: 0.9635211752894405 Min: 5.795029354183088e-05 Sum (first example): 0.9999999999999997\n","Layer 2 - Gradient Weights Max: 0.07716355938616125 Min: -0.1524155249374422\n","Layer 1 - Gradient Weights Max: 0.13989826496128518 Min: -0.14478734106992483\n","Layer 0 - Gradient Weights Max: 0.3176374174311169 Min: -0.2216515294313998\n","ReLU Activation - Max: 6.272955946830052 Min: 0.0\n","ReLU Activation - Max: 4.052119849074448 Min: 0.0\n","Softmax Output - Max: 0.9539496227709725 Min: 6.569034435661408e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09103119743248583 Min: -0.1011462900383057\n","Layer 1 - Gradient Weights Max: 0.09928883772802696 Min: -0.1473806904162793\n","Layer 0 - Gradient Weights Max: 0.1880668357068857 Min: -0.23154584507978465\n","ReLU Activation - Max: 5.014826124659742 Min: 0.0\n","ReLU Activation - Max: 4.126716885617273 Min: 0.0\n","Softmax Output - Max: 0.9954818231819121 Min: 8.378730954196591e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11154097210120166 Min: -0.15635148925332693\n","Layer 1 - Gradient Weights Max: 0.16288654779752493 Min: -0.2002685691281819\n","Layer 0 - Gradient Weights Max: 0.22357211227353807 Min: -0.16374831246204571\n","ReLU Activation - Max: 4.989596784966678 Min: 0.0\n","ReLU Activation - Max: 3.8101525051332583 Min: 0.0\n","Softmax Output - Max: 0.9656816994299653 Min: 3.5668350008064996e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09660056286132093 Min: -0.16530174939863798\n","Layer 1 - Gradient Weights Max: 0.15050472804538562 Min: -0.18554651757859006\n","Layer 0 - Gradient Weights Max: 0.24913616473763295 Min: -0.19161337385662758\n","ReLU Activation - Max: 4.865568570855321 Min: 0.0\n","ReLU Activation - Max: 4.015208089048554 Min: 0.0\n","Softmax Output - Max: 0.9894017861841538 Min: 1.160863842345718e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09567552750900636 Min: -0.1041926214283436\n","Layer 1 - Gradient Weights Max: 0.15258315781482987 Min: -0.1522489628585358\n","Layer 0 - Gradient Weights Max: 0.1503669189348466 Min: -0.2023809483117483\n","ReLU Activation - Max: 4.902974409092765 Min: 0.0\n","ReLU Activation - Max: 4.130111540149341 Min: 0.0\n","Softmax Output - Max: 0.9322227240583645 Min: 4.095926943445148e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08993141043211132 Min: -0.11383165619754054\n","Layer 1 - Gradient Weights Max: 0.12285211880418237 Min: -0.14914368179414833\n","Layer 0 - Gradient Weights Max: 0.15768422664242437 Min: -0.17035093734718612\n","ReLU Activation - Max: 4.839316607480864 Min: 0.0\n","ReLU Activation - Max: 4.575682392016928 Min: 0.0\n","Softmax Output - Max: 0.9686130628467894 Min: 5.360398242766376e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07252852399127029 Min: -0.07872489484480609\n","Layer 1 - Gradient Weights Max: 0.12255437040002758 Min: -0.139305134490132\n","Layer 0 - Gradient Weights Max: 0.15895697105730633 Min: -0.23723427181036288\n","ReLU Activation - Max: 5.96215049666998 Min: 0.0\n","ReLU Activation - Max: 4.784438527970528 Min: 0.0\n","Softmax Output - Max: 0.997633265930482 Min: 3.7629725084993465e-07 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.136550248552205 Min: -0.15674279403302763\n","Layer 1 - Gradient Weights Max: 0.1677506414581075 Min: -0.2213875379425395\n","Layer 0 - Gradient Weights Max: 0.17453853300719374 Min: -0.2051416275292851\n","ReLU Activation - Max: 5.277432035365712 Min: 0.0\n","ReLU Activation - Max: 3.9521068971520386 Min: 0.0\n","Softmax Output - Max: 0.9694510867646846 Min: 2.921513467919297e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0627992678641717 Min: -0.09188090863975697\n","Layer 1 - Gradient Weights Max: 0.1281969976684011 Min: -0.13470036365990745\n","Layer 0 - Gradient Weights Max: 0.17558339014687738 Min: -0.17009504948713436\n","ReLU Activation - Max: 5.020414204022902 Min: 0.0\n","ReLU Activation - Max: 4.888674200260577 Min: 0.0\n","Softmax Output - Max: 0.9546096329353306 Min: 6.600691059205099e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08104008520197005 Min: -0.10535278463895255\n","Layer 1 - Gradient Weights Max: 0.16290433330408863 Min: -0.15983893389985454\n","Layer 0 - Gradient Weights Max: 0.21243148478695134 Min: -0.17048150262364506\n","ReLU Activation - Max: 4.4788048491486885 Min: 0.0\n","ReLU Activation - Max: 4.177382631492823 Min: 0.0\n","Softmax Output - Max: 0.9258516248421028 Min: 9.135463434518882e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06278489274550927 Min: -0.12698276238362388\n","Layer 1 - Gradient Weights Max: 0.1364915549015336 Min: -0.17716358341290092\n","Layer 0 - Gradient Weights Max: 0.22808384571893003 Min: -0.16145188713451503\n","ReLU Activation - Max: 4.393550969174739 Min: 0.0\n","ReLU Activation - Max: 4.042461458303228 Min: 0.0\n","Softmax Output - Max: 0.9547183341400898 Min: 1.3384765220598173e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08815810139442283 Min: -0.1749543285238254\n","Layer 1 - Gradient Weights Max: 0.1458488695696554 Min: -0.18608769581571352\n","Layer 0 - Gradient Weights Max: 0.16630591092806943 Min: -0.16025847118873177\n","ReLU Activation - Max: 4.816209114660594 Min: 0.0\n","ReLU Activation - Max: 3.7630536565903983 Min: 0.0\n","Softmax Output - Max: 0.9406129399321422 Min: 1.58326749662646e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07942903859133998 Min: -0.0965729763968643\n","Layer 1 - Gradient Weights Max: 0.18455949877626002 Min: -0.14726811706157128\n","Layer 0 - Gradient Weights Max: 0.1902904160344842 Min: -0.1913889127893336\n","ReLU Activation - Max: 4.793344521938135 Min: 0.0\n","ReLU Activation - Max: 4.2026253707052215 Min: 0.0\n","Softmax Output - Max: 0.9626438225503592 Min: 5.544998090855795e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06568007345980116 Min: -0.08839362280457459\n","Layer 1 - Gradient Weights Max: 0.12371827353367779 Min: -0.11388673748071901\n","Layer 0 - Gradient Weights Max: 0.19202122886489048 Min: -0.18048615285029046\n","ReLU Activation - Max: 4.0917276160355645 Min: 0.0\n","ReLU Activation - Max: 4.460239398571844 Min: 0.0\n","Softmax Output - Max: 0.985156893726766 Min: 2.5326616261811585e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07412111679839815 Min: -0.14226439504883617\n","Layer 1 - Gradient Weights Max: 0.11269853023036949 Min: -0.15035667576794093\n","Layer 0 - Gradient Weights Max: 0.26989113057080283 Min: -0.21174402949810137\n","ReLU Activation - Max: 4.899498441161166 Min: 0.0\n","ReLU Activation - Max: 4.339686235922133 Min: 0.0\n","Softmax Output - Max: 0.9015718664942951 Min: 2.0558733849865223e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11389865917252757 Min: -0.10643900373750412\n","Layer 1 - Gradient Weights Max: 0.16269897002466008 Min: -0.17300432203985158\n","Layer 0 - Gradient Weights Max: 0.2312341743021994 Min: -0.21864863423144393\n","ReLU Activation - Max: 4.5807710445568 Min: 0.0\n","ReLU Activation - Max: 3.84674759873705 Min: 0.0\n","Softmax Output - Max: 0.963477450920361 Min: 3.820691226522091e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09192842254085375 Min: -0.12069020698569202\n","Layer 1 - Gradient Weights Max: 0.20831796120375595 Min: -0.14441721489240508\n","Layer 0 - Gradient Weights Max: 0.22389167008278596 Min: -0.20086490829330222\n","ReLU Activation - Max: 4.59626221484987 Min: 0.0\n","ReLU Activation - Max: 3.9383706552215205 Min: 0.0\n","Softmax Output - Max: 0.9527438424596316 Min: 6.671265611504785e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15743896782241706 Min: -0.1585793856868655\n","Layer 1 - Gradient Weights Max: 0.17268360537387875 Min: -0.14741027952222055\n","Layer 0 - Gradient Weights Max: 0.16342168712882307 Min: -0.17485976251345675\n","ReLU Activation - Max: 5.0124400145641435 Min: 0.0\n","ReLU Activation - Max: 4.180355440269352 Min: 0.0\n","Softmax Output - Max: 0.9856874765383797 Min: 3.326244587817875e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.10806549740017843 Min: -0.10227515771560669\n","Layer 1 - Gradient Weights Max: 0.1440084011051091 Min: -0.1921393178486157\n","Layer 0 - Gradient Weights Max: 0.1535582274654569 Min: -0.22735816291126087\n","ReLU Activation - Max: 5.6989448718690205 Min: 0.0\n","ReLU Activation - Max: 4.259998170771679 Min: 0.0\n","Softmax Output - Max: 0.9548105317477406 Min: 5.932355611573472e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09260565311895891 Min: -0.08366394621378198\n","Layer 1 - Gradient Weights Max: 0.1695199184305867 Min: -0.15207025827612583\n","Layer 0 - Gradient Weights Max: 0.26840779042684443 Min: -0.25086528119818396\n","ReLU Activation - Max: 4.854986560980902 Min: 0.0\n","ReLU Activation - Max: 4.381704069045888 Min: 0.0\n","Softmax Output - Max: 0.9424145479666511 Min: 3.291543263267969e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.0922086079028708 Min: -0.1062217790482918\n","Layer 1 - Gradient Weights Max: 0.12787863074458253 Min: -0.1546007998068888\n","Layer 0 - Gradient Weights Max: 0.22268274647576658 Min: -0.1893145997296504\n","ReLU Activation - Max: 4.749361336977443 Min: 0.0\n","ReLU Activation - Max: 4.50493606299908 Min: 0.0\n","Softmax Output - Max: 0.9780738442227788 Min: 6.573880686247512e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07776852432865151 Min: -0.09636879026491108\n","Layer 1 - Gradient Weights Max: 0.13461175201532408 Min: -0.14329959700698927\n","Layer 0 - Gradient Weights Max: 0.19347080450002893 Min: -0.15498942499257104\n","ReLU Activation - Max: 5.17837603911043 Min: 0.0\n","ReLU Activation - Max: 4.77993633856043 Min: 0.0\n","Softmax Output - Max: 0.9858509669488257 Min: 1.6386035110474923e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09009761061198043 Min: -0.14684985216696908\n","Layer 1 - Gradient Weights Max: 0.15564637747800583 Min: -0.1445620805784032\n","Layer 0 - Gradient Weights Max: 0.19947565900043288 Min: -0.21101337182172786\n","ReLU Activation - Max: 5.588191325193165 Min: 0.0\n","ReLU Activation - Max: 3.9526194708383087 Min: 0.0\n","Softmax Output - Max: 0.9722525274855428 Min: 1.838444131117581e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07280131603160657 Min: -0.13914695542463207\n","Layer 1 - Gradient Weights Max: 0.129278362606027 Min: -0.155039180028868\n","Layer 0 - Gradient Weights Max: 0.19519671287805254 Min: -0.13682221070669728\n","ReLU Activation - Max: 5.51849865298409 Min: 0.0\n","ReLU Activation - Max: 4.541249508111737 Min: 0.0\n","Softmax Output - Max: 0.9880852638045328 Min: 2.1727246090815203e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.07198212688642769 Min: -0.06699157179996847\n","Layer 1 - Gradient Weights Max: 0.11871493603882785 Min: -0.13099225487174665\n","Layer 0 - Gradient Weights Max: 0.18148699303145682 Min: -0.15394631223400365\n","ReLU Activation - Max: 4.720378431040303 Min: 0.0\n","ReLU Activation - Max: 3.917740641013085 Min: 0.0\n","Softmax Output - Max: 0.9975692524455289 Min: 2.9724433560192406e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0850519011568804 Min: -0.13649197917773714\n","Layer 1 - Gradient Weights Max: 0.12369613190604381 Min: -0.14329597059549395\n","Layer 0 - Gradient Weights Max: 0.15222072596577776 Min: -0.2164458731307717\n","ReLU Activation - Max: 4.812131559488974 Min: 0.0\n","ReLU Activation - Max: 4.151406443952279 Min: 0.0\n","Softmax Output - Max: 0.9938266588343746 Min: 1.0528069180607347e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10437034850223996 Min: -0.1287768219225156\n","Layer 1 - Gradient Weights Max: 0.11590237562686016 Min: -0.1815957357545021\n","Layer 0 - Gradient Weights Max: 0.15084664682590684 Min: -0.1739867039184358\n","ReLU Activation - Max: 4.705241476707724 Min: 0.0\n","ReLU Activation - Max: 4.205914525874149 Min: 0.0\n","Softmax Output - Max: 0.9738802112967978 Min: 4.817410914373883e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10836766626892298 Min: -0.12797441779491048\n","Layer 1 - Gradient Weights Max: 0.1315456679647285 Min: -0.13586537939312712\n","Layer 0 - Gradient Weights Max: 0.16868615070173384 Min: -0.20140684391636884\n","ReLU Activation - Max: 4.232652137208563 Min: 0.0\n","ReLU Activation - Max: 4.083884249882983 Min: 0.0\n","Softmax Output - Max: 0.9847513177180731 Min: 6.9896144855078935e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1636926069129549 Min: -0.15189443329609167\n","Layer 1 - Gradient Weights Max: 0.15028264959692003 Min: -0.17905120536681787\n","Layer 0 - Gradient Weights Max: 0.19767238908430312 Min: -0.20416957283438963\n","ReLU Activation - Max: 4.61467038708207 Min: 0.0\n","ReLU Activation - Max: 3.8433751771137175 Min: 0.0\n","Softmax Output - Max: 0.9613557602816278 Min: 7.285261402552903e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08267210209892391 Min: -0.1080463056764899\n","Layer 1 - Gradient Weights Max: 0.14535905835829546 Min: -0.22324177106646034\n","Layer 0 - Gradient Weights Max: 0.24409022060208627 Min: -0.2295092750407429\n","ReLU Activation - Max: 5.362795325094363 Min: 0.0\n","ReLU Activation - Max: 3.8186638971327023 Min: 0.0\n","Softmax Output - Max: 0.964151104051096 Min: 1.1852196420545516e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15546237388234158 Min: -0.1435008222030345\n","Layer 1 - Gradient Weights Max: 0.24493604749579864 Min: -0.14789268010711829\n","Layer 0 - Gradient Weights Max: 0.1710726256800127 Min: -0.18061836252754612\n","ReLU Activation - Max: 6.15842310010215 Min: 0.0\n","ReLU Activation - Max: 4.974077008069845 Min: 0.0\n","Softmax Output - Max: 0.967580402758379 Min: 2.7238855566159484e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.151311707612152 Min: -0.11476162877268069\n","Layer 1 - Gradient Weights Max: 0.27919860137743013 Min: -0.18534614020647971\n","Layer 0 - Gradient Weights Max: 0.22201946270736989 Min: -0.23099386306596306\n","ReLU Activation - Max: 4.995436065744065 Min: 0.0\n","ReLU Activation - Max: 4.740756719453552 Min: 0.0\n","Softmax Output - Max: 0.9606565962239475 Min: 1.1577459193793365e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.056426743153560326 Min: -0.06575811233158464\n","Layer 1 - Gradient Weights Max: 0.12109622915780062 Min: -0.14206377699848274\n","Layer 0 - Gradient Weights Max: 0.19911934888385022 Min: -0.23506382732736783\n","ReLU Activation - Max: 6.651308385555789 Min: 0.0\n","ReLU Activation - Max: 4.102264714660425 Min: 0.0\n","Softmax Output - Max: 0.9109349157068858 Min: 0.00011938075243690515 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09979048187118117 Min: -0.11070186863576573\n","Layer 1 - Gradient Weights Max: 0.15067846290578746 Min: -0.17110525719938252\n","Layer 0 - Gradient Weights Max: 0.24573077306505403 Min: -0.19620390614494282\n","ReLU Activation - Max: 5.1012641071256395 Min: 0.0\n","ReLU Activation - Max: 4.1220793602784225 Min: 0.0\n","Softmax Output - Max: 0.970148154924801 Min: 5.83456764999413e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08675763018824573 Min: -0.1068692295749054\n","Layer 1 - Gradient Weights Max: 0.12538946062297737 Min: -0.14870774280115787\n","Layer 0 - Gradient Weights Max: 0.1769801297712261 Min: -0.17056702237218585\n","ReLU Activation - Max: 5.444648830146919 Min: 0.0\n","ReLU Activation - Max: 4.292709195969568 Min: 0.0\n","Softmax Output - Max: 0.9354196419853559 Min: 2.196045225993894e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10398487363386835 Min: -0.09656972843652495\n","Layer 1 - Gradient Weights Max: 0.1446735808150283 Min: -0.15627246790732838\n","Layer 0 - Gradient Weights Max: 0.24630569375302877 Min: -0.2819210145303856\n","ReLU Activation - Max: 7.476946571554643 Min: 0.0\n","ReLU Activation - Max: 4.052925360181519 Min: 0.0\n","Softmax Output - Max: 0.950537099246913 Min: 6.410794619756941e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12099966194647599 Min: -0.13416928421248353\n","Layer 1 - Gradient Weights Max: 0.16929243039338432 Min: -0.1604957977926301\n","Layer 0 - Gradient Weights Max: 0.19152773596987405 Min: -0.15976480176989483\n","ReLU Activation - Max: 5.515169498112086 Min: 0.0\n","ReLU Activation - Max: 4.169575099860392 Min: 0.0\n","Softmax Output - Max: 0.9430084658398211 Min: 2.471740806748018e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11679285808067301 Min: -0.09631275375510238\n","Layer 1 - Gradient Weights Max: 0.16394475436752745 Min: -0.1134768127188502\n","Layer 0 - Gradient Weights Max: 0.17744798757141608 Min: -0.17768319476838898\n","ReLU Activation - Max: 5.462537989711719 Min: 0.0\n","ReLU Activation - Max: 4.022888437759574 Min: 0.0\n","Softmax Output - Max: 0.9630131213570329 Min: 5.650757133688122e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09590567030648475 Min: -0.16109497368463097\n","Layer 1 - Gradient Weights Max: 0.2031158946580542 Min: -0.20216743732704195\n","Layer 0 - Gradient Weights Max: 0.19216132075907677 Min: -0.22284509430387597\n","ReLU Activation - Max: 5.975724183165445 Min: 0.0\n","ReLU Activation - Max: 4.999650040919782 Min: 0.0\n","Softmax Output - Max: 0.9943027609911262 Min: 9.211615480449017e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09405272666639783 Min: -0.23230293605107552\n","Layer 1 - Gradient Weights Max: 0.14145675702757413 Min: -0.1752407197596265\n","Layer 0 - Gradient Weights Max: 0.26417342755299844 Min: -0.14714993458603967\n","ReLU Activation - Max: 4.488099165721118 Min: 0.0\n","ReLU Activation - Max: 3.8794779560449015 Min: 0.0\n","Softmax Output - Max: 0.9657231333707226 Min: 4.8063240384679756e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.1068721951916153 Min: -0.15098885825644304\n","Layer 1 - Gradient Weights Max: 0.15304393609586253 Min: -0.18676896985772576\n","Layer 0 - Gradient Weights Max: 0.23239393653892224 Min: -0.3072229513413334\n","ReLU Activation - Max: 5.078439708197854 Min: 0.0\n","ReLU Activation - Max: 4.282210253787969 Min: 0.0\n","Softmax Output - Max: 0.9526763780448003 Min: 1.950510695597958e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.056746348851792965 Min: -0.10872280412691329\n","Layer 1 - Gradient Weights Max: 0.14117025934778976 Min: -0.146941084368176\n","Layer 0 - Gradient Weights Max: 0.16702755525635016 Min: -0.1698697244604454\n","ReLU Activation - Max: 4.468968973165004 Min: 0.0\n","ReLU Activation - Max: 4.861723141011766 Min: 0.0\n","Softmax Output - Max: 0.9896860136092201 Min: 4.064744876880076e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08485476055864558 Min: -0.1568164045941251\n","Layer 1 - Gradient Weights Max: 0.16061164259449262 Min: -0.19834737168889363\n","Layer 0 - Gradient Weights Max: 0.1922266821331931 Min: -0.23077297040967806\n","ReLU Activation - Max: 6.223880946117032 Min: 0.0\n","ReLU Activation - Max: 4.327852167137111 Min: 0.0\n","Softmax Output - Max: 0.9936961524084799 Min: 9.13395063960333e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.11759947352782195 Min: -0.15206963524270836\n","Layer 1 - Gradient Weights Max: 0.16405879757472752 Min: -0.1631487298808579\n","Layer 0 - Gradient Weights Max: 0.2357509414729896 Min: -0.14245768335359854\n","ReLU Activation - Max: 4.407350618940574 Min: 0.0\n","ReLU Activation - Max: 4.710130024699866 Min: 0.0\n","Softmax Output - Max: 0.9622115303572341 Min: 0.0001007659704677829 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1656333953939307 Min: -0.19474551122712558\n","Layer 1 - Gradient Weights Max: 0.16647930485137338 Min: -0.2198626652746426\n","Layer 0 - Gradient Weights Max: 0.2227494468460865 Min: -0.17746949899014552\n","ReLU Activation - Max: 4.484792858208135 Min: 0.0\n","ReLU Activation - Max: 4.5837699405458245 Min: 0.0\n","Softmax Output - Max: 0.9622232705032008 Min: 1.4550536007766585e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08424430630074499 Min: -0.10294814631311658\n","Layer 1 - Gradient Weights Max: 0.13096836085861033 Min: -0.13847959364819942\n","Layer 0 - Gradient Weights Max: 0.18716345771298473 Min: -0.20005866681608797\n","ReLU Activation - Max: 4.3475542894682775 Min: 0.0\n","ReLU Activation - Max: 4.54201989539867 Min: 0.0\n","Softmax Output - Max: 0.943868732983758 Min: 4.419318049008578e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13382087246679225 Min: -0.13776438204273048\n","Layer 1 - Gradient Weights Max: 0.13794552022615741 Min: -0.16696916101602136\n","Layer 0 - Gradient Weights Max: 0.18118844577268617 Min: -0.23622180636273019\n","ReLU Activation - Max: 4.760502869563447 Min: 0.0\n","ReLU Activation - Max: 4.67869387531925 Min: 0.0\n","Softmax Output - Max: 0.973800195109998 Min: 7.239079170312823e-06 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.12542438233023193 Min: -0.19275910600972318\n","Layer 1 - Gradient Weights Max: 0.17114727514413608 Min: -0.21820855380825963\n","Layer 0 - Gradient Weights Max: 0.14078843766129195 Min: -0.1921017632148761\n","ReLU Activation - Max: 5.54765401668903 Min: 0.0\n","ReLU Activation - Max: 3.8569873250929563 Min: 0.0\n","Softmax Output - Max: 0.919100037379167 Min: 1.4754923758060316e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08124177827048551 Min: -0.11782291305263791\n","Layer 1 - Gradient Weights Max: 0.1627546343424332 Min: -0.1847833403529925\n","Layer 0 - Gradient Weights Max: 0.19084614042323444 Min: -0.2791260506406405\n","ReLU Activation - Max: 4.502100155649035 Min: 0.0\n","ReLU Activation - Max: 4.207391731219521 Min: 0.0\n","Softmax Output - Max: 0.938092998627782 Min: 5.457966202950561e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11252545887785546 Min: -0.0829919953789064\n","Layer 1 - Gradient Weights Max: 0.1173125374382636 Min: -0.12253430064418996\n","Layer 0 - Gradient Weights Max: 0.23893617955557747 Min: -0.22669861225260388\n","ReLU Activation - Max: 5.286530305358398 Min: 0.0\n","ReLU Activation - Max: 4.017888954105195 Min: 0.0\n","Softmax Output - Max: 0.9796929083156298 Min: 2.5718202843338267e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.14111364636913473 Min: -0.1248736107244888\n","Layer 1 - Gradient Weights Max: 0.17208175141234563 Min: -0.1551638973303879\n","Layer 0 - Gradient Weights Max: 0.18341416144804312 Min: -0.18901195242833416\n","ReLU Activation - Max: 4.362771558435962 Min: 0.0\n","ReLU Activation - Max: 4.636592701633591 Min: 0.0\n","Softmax Output - Max: 0.9554525309537075 Min: 0.0001295181257489094 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.059521557342025466 Min: -0.06684949151849624\n","Layer 1 - Gradient Weights Max: 0.10703263051437684 Min: -0.1503792262660671\n","Layer 0 - Gradient Weights Max: 0.21356978790624623 Min: -0.1692789685142257\n","ReLU Activation - Max: 5.765832868886449 Min: 0.0\n","ReLU Activation - Max: 3.612061180093883 Min: 0.0\n","Softmax Output - Max: 0.9557378422506467 Min: 4.304769551504655e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13372925026549262 Min: -0.14380019570340646\n","Layer 1 - Gradient Weights Max: 0.1650328290004397 Min: -0.2146185196292495\n","Layer 0 - Gradient Weights Max: 0.2324419529128381 Min: -0.22167103254939213\n","ReLU Activation - Max: 4.510259744997975 Min: 0.0\n","ReLU Activation - Max: 4.038440875965896 Min: 0.0\n","Softmax Output - Max: 0.9408460460250421 Min: 1.8535429923619767e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.14938290242308971 Min: -0.18342272294958364\n","Layer 1 - Gradient Weights Max: 0.17920230000161877 Min: -0.1810764660443076\n","Layer 0 - Gradient Weights Max: 0.1520667713021343 Min: -0.17503910384429758\n","ReLU Activation - Max: 5.335089728573254 Min: 0.0\n","ReLU Activation - Max: 4.137297742637798 Min: 0.0\n","Softmax Output - Max: 0.9357518003279142 Min: 2.1506126908736894e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.08733205154715787 Min: -0.11273177533462134\n","Layer 1 - Gradient Weights Max: 0.17029174456868743 Min: -0.17252001003031325\n","Layer 0 - Gradient Weights Max: 0.14752736679281772 Min: -0.18314936857734118\n","ReLU Activation - Max: 4.823798577614545 Min: 0.0\n","ReLU Activation - Max: 4.031289063703045 Min: 0.0\n","Softmax Output - Max: 0.992347087917858 Min: 1.6275663069122587e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08396193135019386 Min: -0.17245945907256524\n","Layer 1 - Gradient Weights Max: 0.20328760894747783 Min: -0.20838567269484584\n","Layer 0 - Gradient Weights Max: 0.16314701798922518 Min: -0.18855930176980631\n","ReLU Activation - Max: 5.053579725889153 Min: 0.0\n","ReLU Activation - Max: 4.179949311754559 Min: 0.0\n","Softmax Output - Max: 0.9674631421223099 Min: 8.565951394853223e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10238899367297875 Min: -0.12141661728412\n","Layer 1 - Gradient Weights Max: 0.1686548558330186 Min: -0.14534316597146518\n","Layer 0 - Gradient Weights Max: 0.17141311075132096 Min: -0.18791589884012763\n","ReLU Activation - Max: 4.412369529715583 Min: 0.0\n","ReLU Activation - Max: 3.8666639394913362 Min: 0.0\n","Softmax Output - Max: 0.9735626371771571 Min: 8.901531557090202e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.08532556976133536 Min: -0.10678323364469076\n","Layer 1 - Gradient Weights Max: 0.1418121761752896 Min: -0.14720802772541286\n","Layer 0 - Gradient Weights Max: 0.1836646275136091 Min: -0.14188676251773502\n","ReLU Activation - Max: 6.180079895599401 Min: 0.0\n","ReLU Activation - Max: 3.818414145700208 Min: 0.0\n","Softmax Output - Max: 0.9511094427389165 Min: 3.2434225177033434e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.17746091086701896 Min: -0.16233003356562864\n","Layer 1 - Gradient Weights Max: 0.18077585549556716 Min: -0.2848073136888916\n","Layer 0 - Gradient Weights Max: 0.2021596504377816 Min: -0.23252183320228062\n","ReLU Activation - Max: 5.007819817618406 Min: 0.0\n","ReLU Activation - Max: 3.9557345892648477 Min: 0.0\n","Softmax Output - Max: 0.9749428346985394 Min: 2.7828726145943334e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08622468470176493 Min: -0.07616238456045187\n","Layer 1 - Gradient Weights Max: 0.13139003556675893 Min: -0.18995110763931544\n","Layer 0 - Gradient Weights Max: 0.23942413400740317 Min: -0.15718132340816682\n","ReLU Activation - Max: 5.788000848071404 Min: 0.0\n","ReLU Activation - Max: 3.8671666847987214 Min: 0.0\n","Softmax Output - Max: 0.9504166324880784 Min: 3.0617948601801745e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1081051803814138 Min: -0.09952698489465389\n","Layer 1 - Gradient Weights Max: 0.1835258332607067 Min: -0.14547854831957946\n","Layer 0 - Gradient Weights Max: 0.20955190588594405 Min: -0.29025502752984417\n","ReLU Activation - Max: 5.1400036197262935 Min: 0.0\n","ReLU Activation - Max: 3.6161385387234573 Min: 0.0\n","Softmax Output - Max: 0.9935198091559155 Min: 1.0242037386662116e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13070542614477168 Min: -0.11341982289248108\n","Layer 1 - Gradient Weights Max: 0.15019363557521695 Min: -0.15905242086711133\n","Layer 0 - Gradient Weights Max: 0.15714113830632953 Min: -0.14903420410697446\n","ReLU Activation - Max: 4.710318712219522 Min: 0.0\n","ReLU Activation - Max: 4.270011813090318 Min: 0.0\n","Softmax Output - Max: 0.9942688879785564 Min: 2.1766140308419833e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06453496822914256 Min: -0.07739028903973928\n","Layer 1 - Gradient Weights Max: 0.13833650473005485 Min: -0.11043056630333704\n","Layer 0 - Gradient Weights Max: 0.17537957078591318 Min: -0.1593183920805058\n","ReLU Activation - Max: 4.397953583096021 Min: 0.0\n","ReLU Activation - Max: 3.547693280182849 Min: 0.0\n","Softmax Output - Max: 0.9735515043870233 Min: 7.352331777964646e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07162317467190102 Min: -0.09817116747168769\n","Layer 1 - Gradient Weights Max: 0.15522011256003646 Min: -0.18164078925629457\n","Layer 0 - Gradient Weights Max: 0.16688147303170175 Min: -0.18975597777096526\n","ReLU Activation - Max: 5.244288745458154 Min: 0.0\n","ReLU Activation - Max: 3.97098824554477 Min: 0.0\n","Softmax Output - Max: 0.9711121181986951 Min: 2.9225297732566502e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08997921475882133 Min: -0.1409642590470145\n","Layer 1 - Gradient Weights Max: 0.1809507499720392 Min: -0.17448753270549616\n","Layer 0 - Gradient Weights Max: 0.24353609287719238 Min: -0.35246937616385315\n","ReLU Activation - Max: 5.719352812125838 Min: 0.0\n","ReLU Activation - Max: 4.300125108959842 Min: 0.0\n","Softmax Output - Max: 0.9891493505380263 Min: 8.87308184357611e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07837289600341221 Min: -0.12796218350814612\n","Layer 1 - Gradient Weights Max: 0.15571924588146335 Min: -0.16352209439107476\n","Layer 0 - Gradient Weights Max: 0.15586225691588307 Min: -0.17000488473416556\n","ReLU Activation - Max: 4.671924062227042 Min: 0.0\n","ReLU Activation - Max: 3.625612370732414 Min: 0.0\n","Softmax Output - Max: 0.9830065234536433 Min: 2.9899108100134732e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07907246826027906 Min: -0.11642092542208306\n","Layer 1 - Gradient Weights Max: 0.22085695603381245 Min: -0.1631052419122505\n","Layer 0 - Gradient Weights Max: 0.19304131955862033 Min: -0.2266823155524961\n","ReLU Activation - Max: 4.511418072038119 Min: 0.0\n","ReLU Activation - Max: 3.5013700680493747 Min: 0.0\n","Softmax Output - Max: 0.9358531120681272 Min: 0.0001269873234844993 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08800302715069298 Min: -0.12107680194890988\n","Layer 1 - Gradient Weights Max: 0.1605064365403589 Min: -0.17389670612945235\n","Layer 0 - Gradient Weights Max: 0.28816825743558905 Min: -0.2235010163169929\n","ReLU Activation - Max: 4.998333915039447 Min: 0.0\n","ReLU Activation - Max: 5.005494207380485 Min: 0.0\n","Softmax Output - Max: 0.8806164151053185 Min: 0.00014929129867794996 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08765602790412926 Min: -0.0945261110783335\n","Layer 1 - Gradient Weights Max: 0.12419436583600413 Min: -0.15374447023242624\n","Layer 0 - Gradient Weights Max: 0.17238926445403924 Min: -0.15007118906819858\n","ReLU Activation - Max: 5.510023348899598 Min: 0.0\n","ReLU Activation - Max: 3.9665312074882606 Min: 0.0\n","Softmax Output - Max: 0.9598524190252262 Min: 4.562955723150144e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.05489158999105185 Min: -0.06825255002294119\n","Layer 1 - Gradient Weights Max: 0.12808628951363962 Min: -0.09911374221288909\n","Layer 0 - Gradient Weights Max: 0.17845415442071128 Min: -0.1782204176996268\n","ReLU Activation - Max: 4.197312244276201 Min: 0.0\n","ReLU Activation - Max: 4.665062791793863 Min: 0.0\n","Softmax Output - Max: 0.8752322666604585 Min: 3.4763410170776784e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10255880979057416 Min: -0.1586640206046775\n","Layer 1 - Gradient Weights Max: 0.17470917244421455 Min: -0.16272321047325344\n","Layer 0 - Gradient Weights Max: 0.19766535171704017 Min: -0.15650850992790563\n","ReLU Activation - Max: 4.800486916478625 Min: 0.0\n","ReLU Activation - Max: 3.8047627313316905 Min: 0.0\n","Softmax Output - Max: 0.932847372136284 Min: 5.151497056302823e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08543005669696849 Min: -0.16005091729106757\n","Layer 1 - Gradient Weights Max: 0.19557026055732232 Min: -0.17505660650430616\n","Layer 0 - Gradient Weights Max: 0.25424246159118513 Min: -0.15028799042639565\n","ReLU Activation - Max: 5.281023884049088 Min: 0.0\n","ReLU Activation - Max: 3.8042610469700917 Min: 0.0\n","Softmax Output - Max: 0.9859968437248993 Min: 2.1402250964035943e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12315582812798294 Min: -0.10736295544765066\n","Layer 1 - Gradient Weights Max: 0.20342885504494856 Min: -0.19179819753432428\n","Layer 0 - Gradient Weights Max: 0.15950922883416158 Min: -0.18583223658401818\n","ReLU Activation - Max: 5.5602482171021865 Min: 0.0\n","ReLU Activation - Max: 4.1628718134688265 Min: 0.0\n","Softmax Output - Max: 0.9628180108333021 Min: 1.2121945883078575e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.11979928644889151 Min: -0.15777460777629604\n","Layer 1 - Gradient Weights Max: 0.1907607924807769 Min: -0.16985795173528886\n","Layer 0 - Gradient Weights Max: 0.21051976487135896 Min: -0.18192138929508958\n","ReLU Activation - Max: 5.172180971013081 Min: 0.0\n","ReLU Activation - Max: 3.9345862329855374 Min: 0.0\n","Softmax Output - Max: 0.9177996662924346 Min: 9.574581910815951e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11612246476175524 Min: -0.2125815853248411\n","Layer 1 - Gradient Weights Max: 0.202377258467279 Min: -0.19348824248727373\n","Layer 0 - Gradient Weights Max: 0.29626654663519986 Min: -0.28232371451286126\n","ReLU Activation - Max: 4.369391064783156 Min: 0.0\n","ReLU Activation - Max: 4.387273777666786 Min: 0.0\n","Softmax Output - Max: 0.9916675316785108 Min: 1.3435199340637468e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.08662789606736465 Min: -0.1506779665712089\n","Layer 1 - Gradient Weights Max: 0.167464662006644 Min: -0.1121150266931592\n","Layer 0 - Gradient Weights Max: 0.19809705482749063 Min: -0.20150660480770674\n","ReLU Activation - Max: 5.481733296684579 Min: 0.0\n","ReLU Activation - Max: 4.469037592775158 Min: 0.0\n","Softmax Output - Max: 0.9910412978174914 Min: 4.9453889615806e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1430587683946759 Min: -0.1594153069625697\n","Layer 1 - Gradient Weights Max: 0.16462301308648983 Min: -0.18342956091948828\n","Layer 0 - Gradient Weights Max: 0.21140681774548964 Min: -0.21250519272619497\n","ReLU Activation - Max: 4.717542953215136 Min: 0.0\n","ReLU Activation - Max: 4.10878408618001 Min: 0.0\n","Softmax Output - Max: 0.9579005209066178 Min: 1.5564852517311508e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0848114538513157 Min: -0.11836974185989582\n","Layer 1 - Gradient Weights Max: 0.17479976706169237 Min: -0.25770860916613625\n","Layer 0 - Gradient Weights Max: 0.5133944320571995 Min: -0.24351466824380108\n","ReLU Activation - Max: 4.720075410561442 Min: 0.0\n","ReLU Activation - Max: 3.6283740938542084 Min: 0.0\n","Softmax Output - Max: 0.9414126430838424 Min: 2.2960432514027696e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07817631690644891 Min: -0.08161944150689604\n","Layer 1 - Gradient Weights Max: 0.1295301479747535 Min: -0.18767369714351162\n","Layer 0 - Gradient Weights Max: 0.15074452374702546 Min: -0.16345017652850027\n","ReLU Activation - Max: 5.342311586845491 Min: 0.0\n","ReLU Activation - Max: 4.265507155092925 Min: 0.0\n","Softmax Output - Max: 0.9396406963626045 Min: 3.7689575082174275e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08307024946073407 Min: -0.09999538122235166\n","Layer 1 - Gradient Weights Max: 0.11599291960627883 Min: -0.12621369051682418\n","Layer 0 - Gradient Weights Max: 0.14971211152102862 Min: -0.16663545025645482\n","ReLU Activation - Max: 4.986041070390291 Min: 0.0\n","ReLU Activation - Max: 3.5406051356298573 Min: 0.0\n","Softmax Output - Max: 0.9996890224330046 Min: 8.865056057515657e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09311367311298815 Min: -0.11396447004118754\n","Layer 1 - Gradient Weights Max: 0.09506427917822882 Min: -0.14260143488806326\n","Layer 0 - Gradient Weights Max: 0.17402592589203295 Min: -0.19274733291178778\n","ReLU Activation - Max: 4.286358211231982 Min: 0.0\n","ReLU Activation - Max: 3.7118497132739106 Min: 0.0\n","Softmax Output - Max: 0.9253760153566818 Min: 6.898108545925618e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10176554103104213 Min: -0.11019630801839701\n","Layer 1 - Gradient Weights Max: 0.14598332491001006 Min: -0.19649423932627655\n","Layer 0 - Gradient Weights Max: 0.19004954734036006 Min: -0.21174130421937268\n","ReLU Activation - Max: 4.5127899518276156 Min: 0.0\n","ReLU Activation - Max: 4.036088737755177 Min: 0.0\n","Softmax Output - Max: 0.9882706100665319 Min: 4.247162524496311e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06335090226860433 Min: -0.06269676847199053\n","Layer 1 - Gradient Weights Max: 0.11959318845147197 Min: -0.15925283202455767\n","Layer 0 - Gradient Weights Max: 0.19627789188513745 Min: -0.19172604179022193\n","ReLU Activation - Max: 5.5877632496330545 Min: 0.0\n","ReLU Activation - Max: 4.181857256086936 Min: 0.0\n","Softmax Output - Max: 0.97250390789968 Min: 4.864101803214368e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13640796609319333 Min: -0.12026267566570983\n","Layer 1 - Gradient Weights Max: 0.13536610019775164 Min: -0.20293840590523518\n","Layer 0 - Gradient Weights Max: 0.1966048842787622 Min: -0.22979180651502182\n","ReLU Activation - Max: 4.88316678716983 Min: 0.0\n","ReLU Activation - Max: 3.7943206332672657 Min: 0.0\n","Softmax Output - Max: 0.9894914681240573 Min: 2.52412760608273e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08404085853429702 Min: -0.11460710652351945\n","Layer 1 - Gradient Weights Max: 0.14315362331928494 Min: -0.1980284202172977\n","Layer 0 - Gradient Weights Max: 0.17618340164748364 Min: -0.16540486506158622\n","ReLU Activation - Max: 5.312600590192205 Min: 0.0\n","ReLU Activation - Max: 5.212171363563526 Min: 0.0\n","Softmax Output - Max: 0.977115168715083 Min: 3.169103873936315e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10043327619267857 Min: -0.07685778534529669\n","Layer 1 - Gradient Weights Max: 0.12258312431787623 Min: -0.11699462112078757\n","Layer 0 - Gradient Weights Max: 0.1819437778790939 Min: -0.17687601369417774\n","ReLU Activation - Max: 4.385772484504104 Min: 0.0\n","ReLU Activation - Max: 4.123686783520876 Min: 0.0\n","Softmax Output - Max: 0.9737577823789952 Min: 9.362957866282426e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09851452705030227 Min: -0.15902245982569818\n","Layer 1 - Gradient Weights Max: 0.1480552109985221 Min: -0.19796983850712377\n","Layer 0 - Gradient Weights Max: 0.1499332331767432 Min: -0.16808960679192494\n","ReLU Activation - Max: 4.946196991631527 Min: 0.0\n","ReLU Activation - Max: 3.8354951540415634 Min: 0.0\n","Softmax Output - Max: 0.9520540008602196 Min: 6.429536800448684e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13313203402917367 Min: -0.1469361865987103\n","Layer 1 - Gradient Weights Max: 0.15402264321318523 Min: -0.16509801841216906\n","Layer 0 - Gradient Weights Max: 0.22702163623192892 Min: -0.18257708228487105\n","ReLU Activation - Max: 4.699684557978665 Min: 0.0\n","ReLU Activation - Max: 4.636510414364796 Min: 0.0\n","Softmax Output - Max: 0.9426814550199994 Min: 6.468516778483952e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09966233720594811 Min: -0.10949409405176046\n","Layer 1 - Gradient Weights Max: 0.11108422853171414 Min: -0.12101651112666713\n","Layer 0 - Gradient Weights Max: 0.18612571413626522 Min: -0.13681051478342374\n","ReLU Activation - Max: 4.548889569485393 Min: 0.0\n","ReLU Activation - Max: 4.039701356011758 Min: 0.0\n","Softmax Output - Max: 0.9369944749606473 Min: 4.0881294953094044e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08836593823494003 Min: -0.10191836076863765\n","Layer 1 - Gradient Weights Max: 0.13078351365320706 Min: -0.13383877081688253\n","Layer 0 - Gradient Weights Max: 0.2013003375435653 Min: -0.18753411810056977\n","ReLU Activation - Max: 6.4790246777870975 Min: 0.0\n","ReLU Activation - Max: 4.097163522468185 Min: 0.0\n","Softmax Output - Max: 0.9980712610861366 Min: 7.107190815384998e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11988255421537956 Min: -0.15366562497571803\n","Layer 1 - Gradient Weights Max: 0.20461437121969336 Min: -0.2478651611153985\n","Layer 0 - Gradient Weights Max: 0.24296510889298487 Min: -0.18766608986163308\n","ReLU Activation - Max: 5.1558925343893725 Min: 0.0\n","ReLU Activation - Max: 4.133431214299832 Min: 0.0\n","Softmax Output - Max: 0.9681823519725927 Min: 0.00011682596079444851 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11798499102267346 Min: -0.11838445535297033\n","Layer 1 - Gradient Weights Max: 0.1741434467677228 Min: -0.20181601104624847\n","Layer 0 - Gradient Weights Max: 0.18019969741904007 Min: -0.1835125782150512\n","ReLU Activation - Max: 4.5079595633079474 Min: 0.0\n","ReLU Activation - Max: 3.9294205152788835 Min: 0.0\n","Softmax Output - Max: 0.948873136910668 Min: 0.00015514450089756842 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.09452139371532714 Min: -0.07754200850719424\n","Layer 1 - Gradient Weights Max: 0.11380769605891895 Min: -0.16591258442705692\n","Layer 0 - Gradient Weights Max: 0.2234684556337712 Min: -0.16817528698986212\n","ReLU Activation - Max: 6.889556265516733 Min: 0.0\n","ReLU Activation - Max: 4.642699637725265 Min: 0.0\n","Softmax Output - Max: 0.9814199352995054 Min: 1.335485076747906e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.19605748491486247 Min: -0.12275122009518566\n","Layer 1 - Gradient Weights Max: 0.22158884090240152 Min: -0.18847187194228746\n","Layer 0 - Gradient Weights Max: 0.19330487034068428 Min: -0.15676348863063638\n","ReLU Activation - Max: 5.459493264344232 Min: 0.0\n","ReLU Activation - Max: 4.492229656895069 Min: 0.0\n","Softmax Output - Max: 0.9774833898570195 Min: 9.207313358583398e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09550495670652129 Min: -0.07854781183803537\n","Layer 1 - Gradient Weights Max: 0.1715205853339977 Min: -0.156937397864368\n","Layer 0 - Gradient Weights Max: 0.1967146662118827 Min: -0.1748441010577035\n","ReLU Activation - Max: 6.0956542915075005 Min: 0.0\n","ReLU Activation - Max: 4.515561239838167 Min: 0.0\n","Softmax Output - Max: 0.9461545762369081 Min: 9.85973272429525e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07465476446685367 Min: -0.08706472288411783\n","Layer 1 - Gradient Weights Max: 0.130914861688795 Min: -0.1596129058918721\n","Layer 0 - Gradient Weights Max: 0.18934767046584716 Min: -0.2950133762642192\n","ReLU Activation - Max: 4.708461003958285 Min: 0.0\n","ReLU Activation - Max: 4.92173021364651 Min: 0.0\n","Softmax Output - Max: 0.9832858590040736 Min: 6.222111368111636e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.13249431227007002 Min: -0.1772237075738309\n","Layer 1 - Gradient Weights Max: 0.13300749735986933 Min: -0.1371493554939334\n","Layer 0 - Gradient Weights Max: 0.17685393759965962 Min: -0.18029903370245257\n","ReLU Activation - Max: 4.453929960468211 Min: 0.0\n","ReLU Activation - Max: 5.21905763674646 Min: 0.0\n","Softmax Output - Max: 0.9390816577447461 Min: 1.7404366847057768e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08197381223292256 Min: -0.10204985435887023\n","Layer 1 - Gradient Weights Max: 0.1905198866016989 Min: -0.2157025613236065\n","Layer 0 - Gradient Weights Max: 0.19418982438226956 Min: -0.3034663861518487\n","ReLU Activation - Max: 4.7738028137271025 Min: 0.0\n","ReLU Activation - Max: 4.3059301993858305 Min: 0.0\n","Softmax Output - Max: 0.961048162121573 Min: 1.8834462114801628e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.12595355388398286 Min: -0.14907499491188703\n","Layer 1 - Gradient Weights Max: 0.1706926439246031 Min: -0.20305943644738467\n","Layer 0 - Gradient Weights Max: 0.18771920969156886 Min: -0.11924213976327261\n","ReLU Activation - Max: 5.2019307295968344 Min: 0.0\n","ReLU Activation - Max: 4.02201691172886 Min: 0.0\n","Softmax Output - Max: 0.9785059116234488 Min: 2.405953339550091e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.14281694562567276 Min: -0.13190723283136466\n","Layer 1 - Gradient Weights Max: 0.17808427938131863 Min: -0.17062181636310864\n","Layer 0 - Gradient Weights Max: 0.24191246986901896 Min: -0.19481729627248212\n","ReLU Activation - Max: 6.315703656291914 Min: 0.0\n","ReLU Activation - Max: 3.896147108596861 Min: 0.0\n","Softmax Output - Max: 0.9469547617549526 Min: 1.680872325357382e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1135719711324227 Min: -0.10613861639481521\n","Layer 1 - Gradient Weights Max: 0.21045221842374984 Min: -0.14354232138583473\n","Layer 0 - Gradient Weights Max: 0.19296174109527375 Min: -0.1790976211599561\n","ReLU Activation - Max: 4.49814302517188 Min: 0.0\n","ReLU Activation - Max: 4.180615654978528 Min: 0.0\n","Softmax Output - Max: 0.983236844512181 Min: 3.0544805633197486e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08965991462933223 Min: -0.09287415799292355\n","Layer 1 - Gradient Weights Max: 0.133609547428255 Min: -0.14189738364930207\n","Layer 0 - Gradient Weights Max: 0.19274978306641913 Min: -0.1862618701475281\n","ReLU Activation - Max: 5.045869858298941 Min: 0.0\n","ReLU Activation - Max: 4.227484031978452 Min: 0.0\n","Softmax Output - Max: 0.928235121890622 Min: 1.0522334105391984e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.06897299088956764 Min: -0.0738582568669875\n","Layer 1 - Gradient Weights Max: 0.11608075319398185 Min: -0.12611081959060708\n","Layer 0 - Gradient Weights Max: 0.14481317589891463 Min: -0.1977782106131562\n","ReLU Activation - Max: 4.471691816835823 Min: 0.0\n","ReLU Activation - Max: 4.176297071940154 Min: 0.0\n","Softmax Output - Max: 0.966377947137556 Min: 3.6165101860604624e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11046445494031092 Min: -0.13188303858776065\n","Layer 1 - Gradient Weights Max: 0.146348207205876 Min: -0.1720118946611421\n","Layer 0 - Gradient Weights Max: 0.21956282898742574 Min: -0.21109334156758622\n","ReLU Activation - Max: 4.260219541234072 Min: 0.0\n","ReLU Activation - Max: 4.324986926198882 Min: 0.0\n","Softmax Output - Max: 0.9874168364633791 Min: 4.1902014280000474e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.09818864449089323 Min: -0.08992217507176767\n","Layer 1 - Gradient Weights Max: 0.12402444285188288 Min: -0.10229415099253028\n","Layer 0 - Gradient Weights Max: 0.17283716871487695 Min: -0.1920007503714088\n","ReLU Activation - Max: 5.3857990454933 Min: 0.0\n","ReLU Activation - Max: 4.856039852834 Min: 0.0\n","Softmax Output - Max: 0.9441298856264377 Min: 2.7862373591225716e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1353775690435734 Min: -0.10919279655708225\n","Layer 1 - Gradient Weights Max: 0.1617856729277695 Min: -0.14840900641162072\n","Layer 0 - Gradient Weights Max: 0.16909999329911066 Min: -0.2049295050900719\n","ReLU Activation - Max: 4.782992934145158 Min: 0.0\n","ReLU Activation - Max: 4.3215223242685505 Min: 0.0\n","Softmax Output - Max: 0.9419213058757299 Min: 3.7731062851828825e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10378513590683043 Min: -0.12695399194404314\n","Layer 1 - Gradient Weights Max: 0.22957308070081744 Min: -0.21935330627666816\n","Layer 0 - Gradient Weights Max: 0.196919535674464 Min: -0.2510650550321026\n","ReLU Activation - Max: 5.464917401404454 Min: 0.0\n","ReLU Activation - Max: 5.282974756375408 Min: 0.0\n","Softmax Output - Max: 0.9595117594813883 Min: 1.3859002605649053e-05 Sum (first example): 0.9999999999999998\n","Layer 2 - Gradient Weights Max: 0.08566761477796343 Min: -0.09833676565898687\n","Layer 1 - Gradient Weights Max: 0.17433291136516219 Min: -0.1325183755881166\n","Layer 0 - Gradient Weights Max: 0.1555967654571082 Min: -0.30273588161990606\n","ReLU Activation - Max: 5.651661504026674 Min: 0.0\n","ReLU Activation - Max: 4.201704650735127 Min: 0.0\n","Softmax Output - Max: 0.9469689580470367 Min: 3.274309693578696e-07 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10607815748259156 Min: -0.09959891507018298\n","Layer 1 - Gradient Weights Max: 0.11634420608730708 Min: -0.1326219576062592\n","Layer 0 - Gradient Weights Max: 0.1621357403798146 Min: -0.16893480211341957\n","ReLU Activation - Max: 5.826805349443302 Min: 0.0\n","ReLU Activation - Max: 6.259887698590469 Min: 0.0\n","Softmax Output - Max: 0.9498040586161149 Min: 2.3368011580118614e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09880251298771009 Min: -0.10361953060581516\n","Layer 1 - Gradient Weights Max: 0.14632524033156405 Min: -0.19126275992713362\n","Layer 0 - Gradient Weights Max: 0.2091119114445842 Min: -0.36047326166449767\n","ReLU Activation - Max: 5.036209055746078 Min: 0.0\n","ReLU Activation - Max: 4.30324557985308 Min: 0.0\n","Softmax Output - Max: 0.9768011673717056 Min: 1.965442205686767e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10439112637547635 Min: -0.10264351399847593\n","Layer 1 - Gradient Weights Max: 0.13142292557020702 Min: -0.1215175068963185\n","Layer 0 - Gradient Weights Max: 0.17004352988805832 Min: -0.2010132579020891\n","ReLU Activation - Max: 4.0461811875443034 Min: 0.0\n","ReLU Activation - Max: 4.370241809180324 Min: 0.0\n","Softmax Output - Max: 0.9358905581190051 Min: 1.02606949944985e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10423607246855916 Min: -0.08270580398824315\n","Layer 1 - Gradient Weights Max: 0.1420795217016904 Min: -0.1928692656959191\n","Layer 0 - Gradient Weights Max: 0.220862697059764 Min: -0.18660988672345136\n","ReLU Activation - Max: 4.771303068580111 Min: 0.0\n","ReLU Activation - Max: 4.43640548846293 Min: 0.0\n","Softmax Output - Max: 0.9798369564522719 Min: 2.9698984736112933e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11580763354894842 Min: -0.10607563750053428\n","Layer 1 - Gradient Weights Max: 0.1415088749508813 Min: -0.16435052536312394\n","Layer 0 - Gradient Weights Max: 0.1985630716109978 Min: -0.16214860185559613\n","ReLU Activation - Max: 4.815554474213742 Min: 0.0\n","ReLU Activation - Max: 4.028679631307182 Min: 0.0\n","Softmax Output - Max: 0.9514091408908009 Min: 1.3684512505768115e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0939245664981864 Min: -0.11855431923163248\n","Layer 1 - Gradient Weights Max: 0.11326991282223944 Min: -0.16438788056579884\n","Layer 0 - Gradient Weights Max: 0.1630012276397384 Min: -0.29697759904339094\n","ReLU Activation - Max: 5.5783785550548135 Min: 0.0\n","ReLU Activation - Max: 4.249350134086101 Min: 0.0\n","Softmax Output - Max: 0.9439548884726805 Min: 5.0548702563934104e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1259303695670448 Min: -0.12769147425486876\n","Layer 1 - Gradient Weights Max: 0.12210354308926998 Min: -0.17016934825867955\n","Layer 0 - Gradient Weights Max: 0.2473306988656032 Min: -0.2178993213083231\n","ReLU Activation - Max: 4.746954406294718 Min: 0.0\n","ReLU Activation - Max: 4.040174729257224 Min: 0.0\n","Softmax Output - Max: 0.9694366828609399 Min: 5.2420600257050595e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08214812851272402 Min: -0.0750155792205205\n","Layer 1 - Gradient Weights Max: 0.09812909535837182 Min: -0.1073052642137528\n","Layer 0 - Gradient Weights Max: 0.15939240377981878 Min: -0.1542292129041755\n","ReLU Activation - Max: 6.000005192478424 Min: 0.0\n","ReLU Activation - Max: 4.08280497972247 Min: 0.0\n","Softmax Output - Max: 0.9512909280454293 Min: 1.4156519699668763e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.11084505402978855 Min: -0.14189470029970794\n","Layer 1 - Gradient Weights Max: 0.1975720292684525 Min: -0.18985393035144477\n","Layer 0 - Gradient Weights Max: 0.23289023329631683 Min: -0.16891364164139105\n","ReLU Activation - Max: 5.225089160655065 Min: 0.0\n","ReLU Activation - Max: 4.161395306481583 Min: 0.0\n","Softmax Output - Max: 0.9782175146571486 Min: 0.00013906515752281403 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09314454798442989 Min: -0.18273492198668942\n","Layer 1 - Gradient Weights Max: 0.16707651106971147 Min: -0.17565297733049365\n","Layer 0 - Gradient Weights Max: 0.15750467615978087 Min: -0.16137195706443938\n","ReLU Activation - Max: 4.865847853236149 Min: 0.0\n","ReLU Activation - Max: 3.8023406455975866 Min: 0.0\n","Softmax Output - Max: 0.9922643370052554 Min: 1.7356358406013443e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10764349659570398 Min: -0.09840672629291818\n","Layer 1 - Gradient Weights Max: 0.14783554409568403 Min: -0.16778407379796348\n","Layer 0 - Gradient Weights Max: 0.1961763202121394 Min: -0.24556749880776904\n","ReLU Activation - Max: 5.650440408871047 Min: 0.0\n","ReLU Activation - Max: 4.039338395452134 Min: 0.0\n","Softmax Output - Max: 0.9651127077396576 Min: 6.594890405713937e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09752716687091162 Min: -0.11872493406068309\n","Layer 1 - Gradient Weights Max: 0.15820748676110147 Min: -0.14693953507534352\n","Layer 0 - Gradient Weights Max: 0.15845368520698014 Min: -0.13720983999242184\n","ReLU Activation - Max: 6.32695669724276 Min: 0.0\n","ReLU Activation - Max: 3.5212771210665976 Min: 0.0\n","Softmax Output - Max: 0.9417529005689388 Min: 2.5588804965937734e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.10553552456651923 Min: -0.18922449615978748\n","Layer 1 - Gradient Weights Max: 0.21554080257313127 Min: -0.17016276220952345\n","Layer 0 - Gradient Weights Max: 0.19989501822512618 Min: -0.16642000756914224\n","ReLU Activation - Max: 4.799290041105923 Min: 0.0\n","ReLU Activation - Max: 4.274575692659342 Min: 0.0\n","Softmax Output - Max: 0.9097281412856257 Min: 5.880730686630056e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10295879234058458 Min: -0.09337890461582923\n","Layer 1 - Gradient Weights Max: 0.18244882207935637 Min: -0.1757870275666082\n","Layer 0 - Gradient Weights Max: 0.16021108951473492 Min: -0.21273694897042278\n","ReLU Activation - Max: 5.04911379852467 Min: 0.0\n","ReLU Activation - Max: 4.799045458740991 Min: 0.0\n","Softmax Output - Max: 0.9507229635288613 Min: 1.6199238189353795e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1086598755847018 Min: -0.09600748149882009\n","Layer 1 - Gradient Weights Max: 0.17576727667434458 Min: -0.1997014818768368\n","Layer 0 - Gradient Weights Max: 0.18949534545987326 Min: -0.14028741643988576\n","ReLU Activation - Max: 5.165347722909255 Min: 0.0\n","ReLU Activation - Max: 4.186373224738307 Min: 0.0\n","Softmax Output - Max: 0.9177181271958857 Min: 3.2870109360657624e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.16297292090272114 Min: -0.11330389314379147\n","Layer 1 - Gradient Weights Max: 0.21960660231607596 Min: -0.18043028849178613\n","Layer 0 - Gradient Weights Max: 0.18562136036104068 Min: -0.22868269965199742\n","ReLU Activation - Max: 4.898806444594682 Min: 0.0\n","ReLU Activation - Max: 4.051318179987653 Min: 0.0\n","Softmax Output - Max: 0.9835017181301338 Min: 2.026404344740377e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.13734062245950324 Min: -0.15690424967109418\n","Layer 1 - Gradient Weights Max: 0.12436372973088768 Min: -0.18318010400400064\n","Layer 0 - Gradient Weights Max: 0.19045419872475497 Min: -0.16997701652123032\n","ReLU Activation - Max: 4.6262960215466915 Min: 0.0\n","ReLU Activation - Max: 3.7439296504887984 Min: 0.0\n","Softmax Output - Max: 0.9740543981816138 Min: 3.752248820769455e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.1112330154333214 Min: -0.1347776939324759\n","Layer 1 - Gradient Weights Max: 0.20334352607374886 Min: -0.18704341380253436\n","Layer 0 - Gradient Weights Max: 0.15117229215177777 Min: -0.2484945904063578\n","ReLU Activation - Max: 4.624798015369965 Min: 0.0\n","ReLU Activation - Max: 4.040847871763965 Min: 0.0\n","Softmax Output - Max: 0.9498242120615318 Min: 3.1865695709693814e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.08600537687062998 Min: -0.12657995162857882\n","Layer 1 - Gradient Weights Max: 0.13645092015798263 Min: -0.1965336398320439\n","Layer 0 - Gradient Weights Max: 0.1447164819439427 Min: -0.16214202066502906\n","ReLU Activation - Max: 5.254901431522178 Min: 0.0\n","ReLU Activation - Max: 3.799760060983267 Min: 0.0\n","Softmax Output - Max: 0.9224682475220103 Min: 7.765575032576228e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.0927513381220327 Min: -0.1261465068790443\n","Layer 1 - Gradient Weights Max: 0.1665899791222161 Min: -0.13817720280335027\n","Layer 0 - Gradient Weights Max: 0.19078248536247433 Min: -0.19647462168252874\n","ReLU Activation - Max: 4.883326586264838 Min: 0.0\n","ReLU Activation - Max: 4.133401075091009 Min: 0.0\n","Softmax Output - Max: 0.9745936205129846 Min: 2.1150851912712492e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.13794393059878163 Min: -0.07835879023967281\n","Layer 1 - Gradient Weights Max: 0.12369293962427422 Min: -0.1347795712345988\n","Layer 0 - Gradient Weights Max: 0.13940207371045793 Min: -0.13999056066557913\n","ReLU Activation - Max: 7.09462408268107 Min: 0.0\n","ReLU Activation - Max: 4.230602772624553 Min: 0.0\n","Softmax Output - Max: 0.9977120369195533 Min: 8.282198937602287e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1005448922086352 Min: -0.08404097470727932\n","Layer 1 - Gradient Weights Max: 0.16420994974536454 Min: -0.1716754978190716\n","Layer 0 - Gradient Weights Max: 0.18942820673393473 Min: -0.21121414273394784\n","ReLU Activation - Max: 4.50246724867138 Min: 0.0\n","ReLU Activation - Max: 4.051266217938564 Min: 0.0\n","Softmax Output - Max: 0.962558351117977 Min: 1.109974436371304e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07160217496228326 Min: -0.104461392426451\n","Layer 1 - Gradient Weights Max: 0.21167624931304652 Min: -0.13346602834592455\n","Layer 0 - Gradient Weights Max: 0.15020392035514174 Min: -0.1922996609528703\n","ReLU Activation - Max: 4.825170128909178 Min: 0.0\n","ReLU Activation - Max: 4.043567297111267 Min: 0.0\n","Softmax Output - Max: 0.9617241653467155 Min: 3.898739716071752e-05 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.09533616245444745 Min: -0.086408788923782\n","Layer 1 - Gradient Weights Max: 0.15704562580312328 Min: -0.1235077240114641\n","Layer 0 - Gradient Weights Max: 0.14858866567231657 Min: -0.18914509849697497\n","ReLU Activation - Max: 4.341835069072301 Min: 0.0\n","ReLU Activation - Max: 3.7425408324514504 Min: 0.0\n","Softmax Output - Max: 0.9879402889529281 Min: 3.614727756463052e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15037925806243913 Min: -0.15651053270818743\n","Layer 1 - Gradient Weights Max: 0.15361391633364452 Min: -0.17983879221146434\n","Layer 0 - Gradient Weights Max: 0.17685691854803887 Min: -0.2150029841806191\n","ReLU Activation - Max: 4.699245513423231 Min: 0.0\n","ReLU Activation - Max: 4.120495088989959 Min: 0.0\n","Softmax Output - Max: 0.961566586583153 Min: 5.020695202935228e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.0683661182943767 Min: -0.08459310994650414\n","Layer 1 - Gradient Weights Max: 0.11601207365565824 Min: -0.14080339909274814\n","Layer 0 - Gradient Weights Max: 0.13621038423853066 Min: -0.1949730598991255\n","ReLU Activation - Max: 4.225103355752762 Min: 0.0\n","ReLU Activation - Max: 4.71993027131127 Min: 0.0\n","Softmax Output - Max: 0.9950046274284539 Min: 3.1712792157956585e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1452599474722713 Min: -0.08845923224276792\n","Layer 1 - Gradient Weights Max: 0.1797671775882718 Min: -0.1888028906726433\n","Layer 0 - Gradient Weights Max: 0.22977355474906858 Min: -0.1797394461677245\n","ReLU Activation - Max: 5.024723355648791 Min: 0.0\n","ReLU Activation - Max: 4.003726451623292 Min: 0.0\n","Softmax Output - Max: 0.9952146589223201 Min: 1.433828971396223e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08658831887009757 Min: -0.11597794124141066\n","Layer 1 - Gradient Weights Max: 0.18924899816304627 Min: -0.1426691835348906\n","Layer 0 - Gradient Weights Max: 0.17166669050615546 Min: -0.17565312480873374\n","ReLU Activation - Max: 5.300293054638409 Min: 0.0\n","ReLU Activation - Max: 4.631430267364267 Min: 0.0\n","Softmax Output - Max: 0.9300991608042786 Min: 8.209658069427628e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07232923020452399 Min: -0.09872118508665108\n","Layer 1 - Gradient Weights Max: 0.118889285650566 Min: -0.14596114970319418\n","Layer 0 - Gradient Weights Max: 0.14373631127787787 Min: -0.15932423677858626\n","ReLU Activation - Max: 4.37122170094567 Min: 0.0\n","ReLU Activation - Max: 3.7824951905050175 Min: 0.0\n","Softmax Output - Max: 0.9799313120812987 Min: 5.7024426598151765e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07005588016635009 Min: -0.07670734504059248\n","Layer 1 - Gradient Weights Max: 0.12864373680380103 Min: -0.1450442751935574\n","Layer 0 - Gradient Weights Max: 0.15793502719161884 Min: -0.19156256383486997\n","ReLU Activation - Max: 4.201371694359306 Min: 0.0\n","ReLU Activation - Max: 3.8720864434948674 Min: 0.0\n","Softmax Output - Max: 0.9732151414194069 Min: 1.7935353541727513e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09630887248049944 Min: -0.08742619828447168\n","Layer 1 - Gradient Weights Max: 0.15596688915930482 Min: -0.14130131807006452\n","Layer 0 - Gradient Weights Max: 0.20161249660447614 Min: -0.2752312909100185\n","ReLU Activation - Max: 6.500628345832836 Min: 0.0\n","ReLU Activation - Max: 3.440703729899427 Min: 0.0\n","Softmax Output - Max: 0.9298863527427066 Min: 3.243338273909619e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09510675468974425 Min: -0.10088779016990655\n","Layer 1 - Gradient Weights Max: 0.15381463949411775 Min: -0.17685702302201256\n","Layer 0 - Gradient Weights Max: 0.24804509836098992 Min: -0.15863556502158183\n","ReLU Activation - Max: 5.339322075895307 Min: 0.0\n","ReLU Activation - Max: 4.205406042179886 Min: 0.0\n","Softmax Output - Max: 0.9687309107805201 Min: 4.590471310709479e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07231979801636002 Min: -0.11923517987439361\n","Layer 1 - Gradient Weights Max: 0.18553490182025037 Min: -0.15194366234322937\n","Layer 0 - Gradient Weights Max: 0.1422622599813854 Min: -0.19379052968073815\n","ReLU Activation - Max: 5.203800258718466 Min: 0.0\n","ReLU Activation - Max: 3.793781226946912 Min: 0.0\n","Softmax Output - Max: 0.9357829278529269 Min: 2.92751653671722e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06304912760790461 Min: -0.11929749393257469\n","Layer 1 - Gradient Weights Max: 0.10128082126736192 Min: -0.14014261499104594\n","Layer 0 - Gradient Weights Max: 0.1827562431277114 Min: -0.1702831153223878\n","ReLU Activation - Max: 4.658327817260699 Min: 0.0\n","ReLU Activation - Max: 3.810362088505619 Min: 0.0\n","Softmax Output - Max: 0.9822665396530764 Min: 1.7237603707613358e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.10215919640840596 Min: -0.12054837409435688\n","Layer 1 - Gradient Weights Max: 0.1689361515976764 Min: -0.12169295783554054\n","Layer 0 - Gradient Weights Max: 0.17143142005351192 Min: -0.23394888159632085\n","ReLU Activation - Max: 4.952293007823816 Min: 0.0\n","ReLU Activation - Max: 4.391286495588597 Min: 0.0\n","Softmax Output - Max: 0.9691695275838605 Min: 2.079937566883094e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.08641723524439192 Min: -0.09346069979663815\n","Layer 1 - Gradient Weights Max: 0.14835247419992983 Min: -0.20998741649897068\n","Layer 0 - Gradient Weights Max: 0.1980234406051006 Min: -0.21976915602955674\n","ReLU Activation - Max: 5.365110211630975 Min: 0.0\n","ReLU Activation - Max: 4.23708482843506 Min: 0.0\n","Softmax Output - Max: 0.9684976835908455 Min: 8.339326217936128e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12257226030872631 Min: -0.11729370954113771\n","Layer 1 - Gradient Weights Max: 0.15375595605078576 Min: -0.16526418216275823\n","Layer 0 - Gradient Weights Max: 0.23165003134987264 Min: -0.17525668602759428\n","ReLU Activation - Max: 6.333145964292682 Min: 0.0\n","ReLU Activation - Max: 4.339998477649878 Min: 0.0\n","Softmax Output - Max: 0.9498065879510479 Min: 0.0001588205944291251 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09999261163421122 Min: -0.14983045729671013\n","Layer 1 - Gradient Weights Max: 0.2214548261432769 Min: -0.18249765604720147\n","Layer 0 - Gradient Weights Max: 0.1681933519051578 Min: -0.1664661885317514\n","ReLU Activation - Max: 5.968684563510456 Min: 0.0\n","ReLU Activation - Max: 4.739924843384991 Min: 0.0\n","Softmax Output - Max: 0.9873051155498483 Min: 0.00011472726184450796 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.10556006118981553 Min: -0.15534145297491017\n","Layer 1 - Gradient Weights Max: 0.1765403761803303 Min: -0.18121497338638629\n","Layer 0 - Gradient Weights Max: 0.18172717551659334 Min: -0.2077412249155846\n","ReLU Activation - Max: 4.380085326957902 Min: 0.0\n","ReLU Activation - Max: 4.157248257295219 Min: 0.0\n","Softmax Output - Max: 0.9784027078892944 Min: 7.32998733943558e-06 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.07809113624885698 Min: -0.09411064371657037\n","Layer 1 - Gradient Weights Max: 0.17267765467729257 Min: -0.14524463745250285\n","Layer 0 - Gradient Weights Max: 0.15938994332077375 Min: -0.19859978408651455\n","ReLU Activation - Max: 5.012628740690386 Min: 0.0\n","ReLU Activation - Max: 5.070534522242437 Min: 0.0\n","Softmax Output - Max: 0.9861924967922951 Min: 5.036597173754506e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12147471191465734 Min: -0.14683802587826755\n","Layer 1 - Gradient Weights Max: 0.14354335739830207 Min: -0.22950908250080052\n","Layer 0 - Gradient Weights Max: 0.1582780567152721 Min: -0.17553420784591256\n","ReLU Activation - Max: 5.214167918861656 Min: 0.0\n","ReLU Activation - Max: 4.433305746677272 Min: 0.0\n","Softmax Output - Max: 0.9752184166646695 Min: 3.8252316221435104e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.12441863076543647 Min: -0.15314354042121922\n","Layer 1 - Gradient Weights Max: 0.1534789197563205 Min: -0.19652399226676653\n","Layer 0 - Gradient Weights Max: 0.2214676963985612 Min: -0.16595448883649214\n","ReLU Activation - Max: 5.879360105589888 Min: 0.0\n","ReLU Activation - Max: 4.534214345125154 Min: 0.0\n","Softmax Output - Max: 0.9752431871489539 Min: 2.871860125478357e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07390186606688633 Min: -0.13332974351166868\n","Layer 1 - Gradient Weights Max: 0.10841359981122982 Min: -0.1354328722855903\n","Layer 0 - Gradient Weights Max: 0.15679235746006548 Min: -0.15533712047020715\n","ReLU Activation - Max: 5.064085259268696 Min: 0.0\n","ReLU Activation - Max: 4.357195238591367 Min: 0.0\n","Softmax Output - Max: 0.984387428336588 Min: 2.47191231553417e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09586902137701797 Min: -0.13104079286696646\n","Layer 1 - Gradient Weights Max: 0.13012303405754863 Min: -0.14944762112850188\n","Layer 0 - Gradient Weights Max: 0.17907238187728644 Min: -0.18636590716140905\n","ReLU Activation - Max: 3.958478632750222 Min: 0.0\n","ReLU Activation - Max: 3.9774636753139254 Min: 0.0\n","Softmax Output - Max: 0.9667901431038757 Min: 1.00742751919567e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.15366700221828558 Min: -0.11552189087025631\n","Layer 1 - Gradient Weights Max: 0.1587552427573645 Min: -0.13971824294873156\n","Layer 0 - Gradient Weights Max: 0.17775551237387963 Min: -0.15960637484329354\n","ReLU Activation - Max: 5.030654923550763 Min: 0.0\n","ReLU Activation - Max: 3.728172757469525 Min: 0.0\n","Softmax Output - Max: 0.9582375977082144 Min: 6.977425495478642e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11128414336430384 Min: -0.10034821358736112\n","Layer 1 - Gradient Weights Max: 0.22261390828077887 Min: -0.1358289074064738\n","Layer 0 - Gradient Weights Max: 0.24416952353155258 Min: -0.24005578132434388\n","ReLU Activation - Max: 4.475843578559699 Min: 0.0\n","ReLU Activation - Max: 3.691636565997572 Min: 0.0\n","Softmax Output - Max: 0.9913340376213982 Min: 1.4721952126892857e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06689513881605043 Min: -0.0805460944201675\n","Layer 1 - Gradient Weights Max: 0.13524093198743348 Min: -0.13648665662950601\n","Layer 0 - Gradient Weights Max: 0.1908034044271291 Min: -0.25097712280856815\n","ReLU Activation - Max: 4.616800082099858 Min: 0.0\n","ReLU Activation - Max: 4.334582961824419 Min: 0.0\n","Softmax Output - Max: 0.9679499181723389 Min: 1.0688600526538692e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09391250241105072 Min: -0.08834686329104562\n","Layer 1 - Gradient Weights Max: 0.16090592111412796 Min: -0.11013421584081232\n","Layer 0 - Gradient Weights Max: 0.300248024687627 Min: -0.2602335834939634\n","ReLU Activation - Max: 4.687012679576303 Min: 0.0\n","ReLU Activation - Max: 4.1959380081827184 Min: 0.0\n","Softmax Output - Max: 0.9824320277615394 Min: 2.6670326114721843e-06 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.09379015052127034 Min: -0.08977634321233179\n","Layer 1 - Gradient Weights Max: 0.1729129243859025 Min: -0.1333458746972212\n","Layer 0 - Gradient Weights Max: 0.24354921730931214 Min: -0.1578073825566462\n","ReLU Activation - Max: 4.3149784037004215 Min: 0.0\n","ReLU Activation - Max: 3.8098580022929074 Min: 0.0\n","Softmax Output - Max: 0.9696922831110597 Min: 9.448062849831838e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1055619037575286 Min: -0.10777899493646005\n","Layer 1 - Gradient Weights Max: 0.15869686711755293 Min: -0.14807935357668178\n","Layer 0 - Gradient Weights Max: 0.25238941054539 Min: -0.17778553749722287\n","ReLU Activation - Max: 5.238645420436521 Min: 0.0\n","ReLU Activation - Max: 3.994569645668554 Min: 0.0\n","Softmax Output - Max: 0.9719893119136715 Min: 6.167581209134946e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.11391691979642633 Min: -0.1261944464197801\n","Layer 1 - Gradient Weights Max: 0.19924983915336736 Min: -0.20789791850846365\n","Layer 0 - Gradient Weights Max: 0.26068612597187585 Min: -0.16491278273370522\n","ReLU Activation - Max: 5.871693091644601 Min: 0.0\n","ReLU Activation - Max: 4.245425262146513 Min: 0.0\n","Softmax Output - Max: 0.8838282718785557 Min: 6.1789180013559315e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.06649869913632915 Min: -0.1072611632161349\n","Layer 1 - Gradient Weights Max: 0.1337690148347745 Min: -0.2516720246404166\n","Layer 0 - Gradient Weights Max: 0.19187713484845886 Min: -0.2395369654193132\n","ReLU Activation - Max: 5.599120714310718 Min: 0.0\n","ReLU Activation - Max: 4.425806636123145 Min: 0.0\n","Softmax Output - Max: 0.9546976971214002 Min: 1.2010909251024961e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.1084568396542474 Min: -0.16060086092539216\n","Layer 1 - Gradient Weights Max: 0.15039134986101688 Min: -0.13609270897063663\n","Layer 0 - Gradient Weights Max: 0.17230546887214418 Min: -0.20565280125573993\n","ReLU Activation - Max: 5.2354202219746835 Min: 0.0\n","ReLU Activation - Max: 3.7201728570074812 Min: 0.0\n","Softmax Output - Max: 0.9708307191894582 Min: 1.126271753824371e-06 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.09950206740262621 Min: -0.10031484947740231\n","Layer 1 - Gradient Weights Max: 0.14602360536558207 Min: -0.11240463047877497\n","Layer 0 - Gradient Weights Max: 0.18379836298065227 Min: -0.17922237713683908\n","ReLU Activation - Max: 4.537379598276601 Min: 0.0\n","ReLU Activation - Max: 4.648995314735571 Min: 0.0\n","Softmax Output - Max: 0.9537458334633329 Min: 8.290600742784419e-07 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.07689053456175973 Min: -0.14269580401357299\n","Layer 1 - Gradient Weights Max: 0.14968654690810226 Min: -0.12550426768963388\n","Layer 0 - Gradient Weights Max: 0.19842100875536034 Min: -0.17866646486033413\n","ReLU Activation - Max: 5.584636135815404 Min: 0.0\n","ReLU Activation - Max: 5.189253573320618 Min: 0.0\n","Softmax Output - Max: 0.9609026155376126 Min: 3.963712378026078e-05 Sum (first example): 0.9999999999999999\n","Layer 2 - Gradient Weights Max: 0.07686869560561033 Min: -0.1033483861353428\n","Layer 1 - Gradient Weights Max: 0.1342896156497425 Min: -0.09994483313864955\n","Layer 0 - Gradient Weights Max: 0.16575450902425465 Min: -0.2485177055668347\n","ReLU Activation - Max: 4.948057809449147 Min: 0.0\n","ReLU Activation - Max: 3.899726606656306 Min: 0.0\n","Softmax Output - Max: 0.9739495172493738 Min: 7.14111286378796e-05 Sum (first example): 1.0\n","Layer 2 - Gradient Weights Max: 0.060884926422756266 Min: -0.0727387557913202\n","Layer 1 - Gradient Weights Max: 0.11032245797206498 Min: -0.10747729201801509\n","Layer 0 - Gradient Weights Max: 0.14252117966239128 Min: -0.20216541529721027\n","ReLU Activation - Max: 5.244527600774607 Min: 0.0\n","ReLU Activation - Max: 4.128234421544145 Min: 0.0\n","Softmax Output - Max: 0.9655414172845513 Min: 0.0001669715954453537 Sum (first example): 1.0000000000000002\n","Layer 2 - Gradient Weights Max: 0.159792414248969 Min: -0.21334799650997563\n","Layer 1 - Gradient Weights Max: 0.28428313634808844 Min: -0.19672745492299645\n","Layer 0 - Gradient Weights Max: 0.17882599442670735 Min: -0.17430970551317015\n","ReLU Activation - Max: 8.482527859232203 Min: 0.0\n","ReLU Activation - Max: 5.698972910274555 Min: 0.0\n","Softmax Output - Max: 0.9975703641601107 Min: 1.5073018120835292e-07 Sum (first example): 1.0000000000000002\n","Epoch 100/100 - Loss: 1.3700 - Test Accuracy: 0.5189\n"]}],"source":["epochs = 100  # Number of epochs to train for\n","batch_size = 64  # Size of the batch\n","\n","# Training the model\n","training_losses, test_accuracies = train_model(nn_model, X_train_normalized, y_train_encoded, X_test_normalized, y_test_encoded, epochs, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":990,"status":"ok","timestamp":1711360533030,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"yzilcelu_Fhm","outputId":"1650baf3-4ae7-41c2-b681-e6039650cf4b"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1400x600 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.figure(figsize=(14, 6))\n","\n","# Plotting training loss\n","plt.subplot(1, 2, 1)\n","plt.plot(training_losses, label='Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss vs. Epoch')\n","plt.legend()\n","\n","# Plotting test accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(test_accuracies, label='Test Accuracy', color='red')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Test Accuracy vs. Epoch')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":609,"status":"ok","timestamp":1711361008115,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"a3XYM1qrECtH","outputId":"62eb0b1d-2f9f-43de-ff29-1595844ba93a"},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU Activation - Max: 8.482527859232203 Min: 0.0\n","ReLU Activation - Max: 5.698972910274555 Min: 0.0\n","Softmax Output - Max: 0.9975703641601107 Min: 1.5073018120835292e-07 Sum (first example): 1.0000000000000002\n","Confusion Matrix:\n","[[734  55  86  91  79  93  49  70  63  58]\n"," [ 87 756  71  56  96  83  83  55  68  74]\n"," [ 84  69 723  66  69  83  95  98  84  68]\n"," [ 86  49  89 648  71  94 112  83  82  73]\n"," [ 84  66  87  69 729  57  81  99  86  59]\n"," [ 77  68  68  56  63 826  60  68  63  62]\n"," [ 59  75  91  76  89  72 604  79  84 100]\n"," [ 65  38  83  80  85  71  77 774  83 100]\n"," [ 77  45  78  70  79  82  61  69 732  54]\n"," [ 61  57  69  64  67  87  92 107  64 739]]\n"]}],"source":["def get_predictions(model, X):\n","    \"\"\"\n","    Perform a forward pass using the model and return the predicted labels.\n","\n","    Parameters:\n","    - model: The trained neural network model.\n","    - X: The input features (normalized).\n","\n","    Returns:\n","    - predictions: The predicted class labels as integers.\n","    \"\"\"\n","    # Get the output probabilities from the model\n","    output, _ = model.forward_pass(X)\n","    # Convert probabilities to class labels\n","    predictions = np.argmax(output, axis=1)\n","    return predictions\n","\n","def compute_confusion_matrix(true_labels, predictions, num_classes):\n","    \"\"\"\n","    Computes the confusion matrix.\n","\n","    Parameters:\n","    - true_labels: The true labels as integers.\n","    - predictions: The predicted labels as integers.\n","    - num_classes: The number of classes.\n","\n","    Returns:\n","    - cm: The confusion matrix.\n","    \"\"\"\n","    cm = np.zeros((num_classes, num_classes), dtype=int)\n","    for true, pred in zip(true_labels, predictions):\n","        cm[true, pred] += 1\n","    return cm\n","\n","\n","true_labels = np.argmax(y_test_encoded, axis=1)  # Convert one-hot encoded labels back to integers\n","predictions = get_predictions(nn_model, X_test_normalized)\n","\n","# Assuming the number of classes is 10 (for labels 0-9)\n","num_classes = 10\n","confusion_matrix = compute_confusion_matrix(true_labels, predictions, num_classes)\n","\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":446,"status":"ok","timestamp":1711361066685,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"T4O71nGWEQdZ","outputId":"1c8cf143-cfc0-41d3-cd32-cd0eef0fc8a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class 0: F1 Score = 0.5258\n","Class 1: F1 Score = 0.5586\n","Class 2: F1 Score = 0.5014\n","Class 3: F1 Score = 0.4867\n","Class 4: F1 Score = 0.5127\n","Class 5: F1 Score = 0.5583\n","Class 6: F1 Score = 0.4571\n","Class 7: F1 Score = 0.5233\n","Class 8: F1 Score = 0.5312\n","Class 9: F1 Score = 0.5290\n"]}],"source":["def calculate_f1_scores(cm):\n","    \"\"\"\n","    Calculates F1 scores for each class given a confusion matrix.\n","\n","    Parameters:\n","    - cm: The confusion matrix\n","\n","    Returns:\n","    - f1_scores: A list of F1 scores for each class.\n","    \"\"\"\n","    num_classes = cm.shape[0]\n","    f1_scores = []\n","\n","    for i in range(num_classes):\n","        # True positives\n","        tp = cm[i, i]\n","        # False positives: sum of column i (excluding tp)\n","        fp = np.sum(cm[:, i]) - tp\n","        # False negatives: sum of row i (excluding tp)\n","        fn = np.sum(cm[i, :]) - tp\n","        # Precision and recall\n","        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        # F1 score\n","        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","        f1_scores.append(f1)\n","\n","    return f1_scores\n","\n","f1_scores = calculate_f1_scores(confusion_matrix)\n","class_labels = [f'Class {i}' for i in range(num_classes)]\n","\n","# Print F1 scores for each class\n","for label, score in zip(class_labels, f1_scores):\n","    print(f'{label}: F1 Score = {score:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"IRRCTHV9Em6f"},"source":["Model with MSE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4W-8K4GEo66"},"outputs":[],"source":["# Initialize the new neural network model with MSE loss\n","nn_model_mse = FullyConnectedNeuralNetwork(input_size=input_size,\n","                                           output_size=output_size,\n","                                           hidden_layers=hidden_layers,\n","                                           loss_function='mse',  # Changed to 'mse'\n","                                           learning_rate=learning_rate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifU7A_0vE3z1"},"outputs":[],"source":["def train_model_mse(model, X_train, y_train, X_test, y_test, epochs, batch_size):\n","    m = X_train.shape[0]\n","    n_batches = m // batch_size\n","\n","    training_losses = []\n","    test_accuracies = []\n","\n","    for epoch in range(epochs):\n","        epoch_losses = []\n","        for i in range(n_batches):\n","            batch_start, batch_end = i * batch_size, (i + 1) * batch_size\n","            X_batch, y_batch = X_train[batch_start:batch_end], y_train[batch_start:batch_end]\n","\n","            # Forward pass\n","            output, activations = model.forward_pass(X_batch)\n","\n","            # Compute loss specifically for MSE\n","            loss = model.compute_loss(output, y_batch)\n","            epoch_losses.append(loss)\n","\n","            # Backpropagation and weight update\n","            model.backpropagate(X_batch, y_batch, activations)\n","\n","        # Store the average loss for this epoch\n","        training_losses.append(np.mean(epoch_losses))\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss: {training_losses[-1]:.4f}\")\n","\n","    return training_losses  # Adjusted to focus on loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":631273,"status":"ok","timestamp":1711361850045,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"neNV6Fa4E8yY","outputId":"866a83b9-4dd5-496e-8274-737ccbb81911"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Layer 2 - Gradient Weights Max: 0.02053270542638156 Min: -0.02559590702546742\n","Layer 1 - Gradient Weights Max: 0.018598804950190378 Min: -0.021270663345669325\n","Layer 0 - Gradient Weights Max: 0.04048536685086043 Min: -0.034955154987785196\n","ReLU Activation - Max: 1.3125333577193763 Min: 0.0\n","ReLU Activation - Max: 0.6315486682678305 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01026188200039001 Min: -0.020447046647262688\n","Layer 1 - Gradient Weights Max: 0.01783288973784695 Min: -0.02198841470630543\n","Layer 0 - Gradient Weights Max: 0.024615739782494567 Min: -0.027971588579468645\n","ReLU Activation - Max: 1.8392430230727128 Min: 0.0\n","ReLU Activation - Max: 0.6210224910574986 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014588347295126154 Min: -0.013609061663349117\n","Layer 1 - Gradient Weights Max: 0.021826643619049742 Min: -0.02152044335401278\n","Layer 0 - Gradient Weights Max: 0.030781043382160346 Min: -0.024562794237177273\n","ReLU Activation - Max: 1.393638052778755 Min: 0.0\n","ReLU Activation - Max: 0.5827412094498334 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020677380056181974 Min: -0.02545524819658741\n","Layer 1 - Gradient Weights Max: 0.019938807383202247 Min: -0.01587051928330065\n","Layer 0 - Gradient Weights Max: 0.025868043152726655 Min: -0.029192782421769082\n","ReLU Activation - Max: 1.64362395857603 Min: 0.0\n","ReLU Activation - Max: 0.574251005411871 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011578230325602571 Min: -0.014363311824819907\n","Layer 1 - Gradient Weights Max: 0.014377872403006838 Min: -0.02207836229129443\n","Layer 0 - Gradient Weights Max: 0.028506749663271116 Min: -0.031039151359249617\n","ReLU Activation - Max: 1.87420324381411 Min: 0.0\n","ReLU Activation - Max: 0.652536475607242 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024957882446556362 Min: -0.02749556928436567\n","Layer 1 - Gradient Weights Max: 0.024559025830965513 Min: -0.022344405667667076\n","Layer 0 - Gradient Weights Max: 0.03705717516285273 Min: -0.045761925689004265\n","ReLU Activation - Max: 1.804740066933726 Min: 0.0\n","ReLU Activation - Max: 0.5756471393602323 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01926797219992904 Min: -0.03334124008683578\n","Layer 1 - Gradient Weights Max: 0.023231274137542793 Min: -0.0451529740618057\n","Layer 0 - Gradient Weights Max: 0.0364000614294109 Min: -0.029609699684712183\n","ReLU Activation - Max: 1.6059521278424307 Min: 0.0\n","ReLU Activation - Max: 0.6774911817091728 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01005622499740949 Min: -0.0177480351818224\n","Layer 1 - Gradient Weights Max: 0.020239221756937882 Min: -0.017007017244591277\n","Layer 0 - Gradient Weights Max: 0.028319310432509376 Min: -0.03575239880804201\n","ReLU Activation - Max: 1.339419303800595 Min: 0.0\n","ReLU Activation - Max: 0.5252846563608615 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014908617679031611 Min: -0.014668768844923413\n","Layer 1 - Gradient Weights Max: 0.02563575299347037 Min: -0.01940999407837084\n","Layer 0 - Gradient Weights Max: 0.030115994875938742 Min: -0.035838545633738915\n","ReLU Activation - Max: 1.5342636109280883 Min: 0.0\n","ReLU Activation - Max: 0.585508075045616 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017352038837919408 Min: -0.015214463531384797\n","Layer 1 - Gradient Weights Max: 0.01620690975179546 Min: -0.016006497570538653\n","Layer 0 - Gradient Weights Max: 0.02498078709526842 Min: -0.02797835127189891\n","ReLU Activation - Max: 1.6304051356197573 Min: 0.0\n","ReLU Activation - Max: 0.5739874437523466 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0073732235181198725 Min: -0.008803498849536852\n","Layer 1 - Gradient Weights Max: 0.011675135732797418 Min: -0.01389325198678383\n","Layer 0 - Gradient Weights Max: 0.03483663019129525 Min: -0.03061688943055362\n","ReLU Activation - Max: 1.426236114156188 Min: 0.0\n","ReLU Activation - Max: 0.632193528413411 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01597086089751595 Min: -0.022837395992953376\n","Layer 1 - Gradient Weights Max: 0.022218182671686796 Min: -0.023930785862773432\n","Layer 0 - Gradient Weights Max: 0.030174942950901385 Min: -0.03144207750985313\n","ReLU Activation - Max: 1.3181026340100048 Min: 0.0\n","ReLU Activation - Max: 0.6737657721811985 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025045988333072325 Min: -0.026685592683195914\n","Layer 1 - Gradient Weights Max: 0.018592431377419463 Min: -0.025960544285226428\n","Layer 0 - Gradient Weights Max: 0.030827254595287425 Min: -0.03570801991986974\n","ReLU Activation - Max: 1.3341979489082187 Min: 0.0\n","ReLU Activation - Max: 0.6011926890378534 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014587586506610083 Min: -0.014485877253593703\n","Layer 1 - Gradient Weights Max: 0.021676899111970186 Min: -0.014298919098127564\n","Layer 0 - Gradient Weights Max: 0.027001380840177093 Min: -0.026525540243285874\n","ReLU Activation - Max: 1.300571386430086 Min: 0.0\n","ReLU Activation - Max: 0.7499192973621293 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013744511441524306 Min: -0.02354911374011749\n","Layer 1 - Gradient Weights Max: 0.016100538444924536 Min: -0.017607246037015665\n","Layer 0 - Gradient Weights Max: 0.021668753015493068 Min: -0.026677329867462025\n","ReLU Activation - Max: 1.534473310687093 Min: 0.0\n","ReLU Activation - Max: 0.5395640837635622 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01090768543460747 Min: -0.021665627893931633\n","Layer 1 - Gradient Weights Max: 0.016302820369703 Min: -0.018782473665461305\n","Layer 0 - Gradient Weights Max: 0.028968278632118653 Min: -0.03128672641351234\n","ReLU Activation - Max: 1.4577886202890715 Min: 0.0\n","ReLU Activation - Max: 0.6379292576349948 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015570660265888082 Min: -0.014578563961238973\n","Layer 1 - Gradient Weights Max: 0.016397639916084623 Min: -0.017350189019783753\n","Layer 0 - Gradient Weights Max: 0.029223548738597446 Min: -0.02962946145203649\n","ReLU Activation - Max: 1.3883567113629953 Min: 0.0\n","ReLU Activation - Max: 0.6978601391707009 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025802900569054234 Min: -0.02372607183463519\n","Layer 1 - Gradient Weights Max: 0.018406228743616256 Min: -0.019160281097346306\n","Layer 0 - Gradient Weights Max: 0.028789005483006946 Min: -0.02600908175406959\n","ReLU Activation - Max: 1.5195022085536174 Min: 0.0\n","ReLU Activation - Max: 0.6266573837597575 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019310264136379658 Min: -0.01721532393259771\n","Layer 1 - Gradient Weights Max: 0.025194549498028206 Min: -0.01955417986691705\n","Layer 0 - Gradient Weights Max: 0.02669073695188562 Min: -0.023304490304706358\n","ReLU Activation - Max: 1.3709324938314238 Min: 0.0\n","ReLU Activation - Max: 0.7434465195409223 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015287379318957066 Min: -0.017141638701784867\n","Layer 1 - Gradient Weights Max: 0.022314142996547163 Min: -0.017589553847515777\n","Layer 0 - Gradient Weights Max: 0.03629296545972753 Min: -0.03297725082831112\n","ReLU Activation - Max: 1.7830717941308776 Min: 0.0\n","ReLU Activation - Max: 0.6276682174625604 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02064206430697612 Min: -0.018168088234664342\n","Layer 1 - Gradient Weights Max: 0.021021900876344497 Min: -0.021099297354786908\n","Layer 0 - Gradient Weights Max: 0.03744210452824783 Min: -0.029201374061438197\n","ReLU Activation - Max: 1.73182784689302 Min: 0.0\n","ReLU Activation - Max: 0.7456983957174761 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010194522808806327 Min: -0.015346543806203693\n","Layer 1 - Gradient Weights Max: 0.016428382545805312 Min: -0.016752991779301937\n","Layer 0 - Gradient Weights Max: 0.03675000848642166 Min: -0.025925023244385705\n","ReLU Activation - Max: 1.6518346567938402 Min: 0.0\n","ReLU Activation - Max: 0.5278067727330549 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011956797403258444 Min: -0.017344559198911945\n","Layer 1 - Gradient Weights Max: 0.019932998531113733 Min: -0.02066049246517231\n","Layer 0 - Gradient Weights Max: 0.023147946489146726 Min: -0.03511208767576328\n","ReLU Activation - Max: 1.4217036668476086 Min: 0.0\n","ReLU Activation - Max: 0.6338718211347399 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017696826040381388 Min: -0.018621175723076918\n","Layer 1 - Gradient Weights Max: 0.021048026787395754 Min: -0.016295898586508997\n","Layer 0 - Gradient Weights Max: 0.02881280125063791 Min: -0.026893943527000447\n","ReLU Activation - Max: 1.55897926366298 Min: 0.0\n","ReLU Activation - Max: 0.536335625941003 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011655988300076508 Min: -0.010787682102628714\n","Layer 1 - Gradient Weights Max: 0.015633551071675425 Min: -0.01464878020129033\n","Layer 0 - Gradient Weights Max: 0.026466129805163115 Min: -0.03115153063050799\n","ReLU Activation - Max: 1.6150329653239768 Min: 0.0\n","ReLU Activation - Max: 0.6152803716879566 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012729073506548961 Min: -0.015080194267773487\n","Layer 1 - Gradient Weights Max: 0.019877104997148076 Min: -0.021994660817752115\n","Layer 0 - Gradient Weights Max: 0.036875932851159324 Min: -0.04234359747885756\n","ReLU Activation - Max: 1.233191377765571 Min: 0.0\n","ReLU Activation - Max: 0.5483440514561795 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011211454543420636 Min: -0.02281803553475678\n","Layer 1 - Gradient Weights Max: 0.015798096051341703 Min: -0.020173882420751636\n","Layer 0 - Gradient Weights Max: 0.03438461812234056 Min: -0.029466832082214015\n","ReLU Activation - Max: 1.7314522592615604 Min: 0.0\n","ReLU Activation - Max: 0.6397449067450888 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012191891750975107 Min: -0.016312383863758505\n","Layer 1 - Gradient Weights Max: 0.024935575266108546 Min: -0.014884450681168431\n","Layer 0 - Gradient Weights Max: 0.0276748080938677 Min: -0.024814377160763984\n","ReLU Activation - Max: 1.3177835868697378 Min: 0.0\n","ReLU Activation - Max: 0.5206100447319781 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01221356918881737 Min: -0.025530837315629946\n","Layer 1 - Gradient Weights Max: 0.017328556821104027 Min: -0.01872632911912592\n","Layer 0 - Gradient Weights Max: 0.024259118464667308 Min: -0.023748468679481213\n","ReLU Activation - Max: 1.375579706524197 Min: 0.0\n","ReLU Activation - Max: 0.5261058917585567 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014591328526006694 Min: -0.02847382494144083\n","Layer 1 - Gradient Weights Max: 0.018331902707025012 Min: -0.018831401149388034\n","Layer 0 - Gradient Weights Max: 0.02448140328344073 Min: -0.02468542938795884\n","ReLU Activation - Max: 1.2844949517474675 Min: 0.0\n","ReLU Activation - Max: 0.5451778895089153 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01651757859794598 Min: -0.030203311242027662\n","Layer 1 - Gradient Weights Max: 0.013006201958596485 Min: -0.02557053926860876\n","Layer 0 - Gradient Weights Max: 0.028907398451863613 Min: -0.03259153592742645\n","ReLU Activation - Max: 1.2538217621718066 Min: 0.0\n","ReLU Activation - Max: 0.5944845262369509 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017066369782208123 Min: -0.020599969701090782\n","Layer 1 - Gradient Weights Max: 0.024027115596344614 Min: -0.018061186472431497\n","Layer 0 - Gradient Weights Max: 0.03052019600448003 Min: -0.028618235317004116\n","ReLU Activation - Max: 1.4114500262014984 Min: 0.0\n","ReLU Activation - Max: 0.5601046602756581 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020210342983715573 Min: -0.019367122367840554\n","Layer 1 - Gradient Weights Max: 0.024834534641324423 Min: -0.030302233369396858\n","Layer 0 - Gradient Weights Max: 0.02992453189323687 Min: -0.02711015163196392\n","ReLU Activation - Max: 1.606293835108184 Min: 0.0\n","ReLU Activation - Max: 0.5983410098954446 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018987469775352647 Min: -0.022273526175233375\n","Layer 1 - Gradient Weights Max: 0.026671343835052666 Min: -0.019149254766163998\n","Layer 0 - Gradient Weights Max: 0.04001145805871478 Min: -0.028189143914863924\n","ReLU Activation - Max: 1.4422424402908063 Min: 0.0\n","ReLU Activation - Max: 0.6102369122489698 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018713938892733945 Min: -0.017843052328730687\n","Layer 1 - Gradient Weights Max: 0.019193668978266708 Min: -0.01698509680010298\n","Layer 0 - Gradient Weights Max: 0.028003077700171644 Min: -0.02921421770054821\n","ReLU Activation - Max: 1.5502164309997122 Min: 0.0\n","ReLU Activation - Max: 0.6224768722881414 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02075275455882264 Min: -0.022427776868497038\n","Layer 1 - Gradient Weights Max: 0.02861426661509057 Min: -0.022436366739545046\n","Layer 0 - Gradient Weights Max: 0.02958504451151212 Min: -0.03188556281662105\n","ReLU Activation - Max: 1.4809033316990365 Min: 0.0\n","ReLU Activation - Max: 0.5652479583987692 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01673385224692389 Min: -0.012117476237015104\n","Layer 1 - Gradient Weights Max: 0.018002882947899812 Min: -0.019637742068988606\n","Layer 0 - Gradient Weights Max: 0.03515177378119218 Min: -0.02851954869741348\n","ReLU Activation - Max: 1.5009863644600054 Min: 0.0\n","ReLU Activation - Max: 0.593399095743874 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01346648736806744 Min: -0.011568926390843134\n","Layer 1 - Gradient Weights Max: 0.01904885927683287 Min: -0.017115675531532676\n","Layer 0 - Gradient Weights Max: 0.02764710101922433 Min: -0.031043925561534843\n","ReLU Activation - Max: 2.111755044453269 Min: 0.0\n","ReLU Activation - Max: 0.5731973260845533 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016673745572045134 Min: -0.02179558655090715\n","Layer 1 - Gradient Weights Max: 0.013665309060808019 Min: -0.02058534803841912\n","Layer 0 - Gradient Weights Max: 0.03055934288025322 Min: -0.028847031143316663\n","ReLU Activation - Max: 1.1107686896938778 Min: 0.0\n","ReLU Activation - Max: 0.5845860496714455 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017497786449603838 Min: -0.019446550484304322\n","Layer 1 - Gradient Weights Max: 0.013460609221035614 Min: -0.019354703916249008\n","Layer 0 - Gradient Weights Max: 0.03376476814326333 Min: -0.025429149471067107\n","ReLU Activation - Max: 1.2968018847338783 Min: 0.0\n","ReLU Activation - Max: 0.6060511281323192 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021368745927491915 Min: -0.018666301140840235\n","Layer 1 - Gradient Weights Max: 0.021348957553020093 Min: -0.023666738468160007\n","Layer 0 - Gradient Weights Max: 0.04304141200294772 Min: -0.029899472734719407\n","ReLU Activation - Max: 1.4315914229525912 Min: 0.0\n","ReLU Activation - Max: 0.527762891663653 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013496383908462112 Min: -0.02068312664024729\n","Layer 1 - Gradient Weights Max: 0.02444310748591006 Min: -0.01816095739820558\n","Layer 0 - Gradient Weights Max: 0.0275333238920472 Min: -0.029599031908054737\n","ReLU Activation - Max: 1.4994119563400354 Min: 0.0\n","ReLU Activation - Max: 0.5633434353647706 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0216175886650152 Min: -0.016826155514332575\n","Layer 1 - Gradient Weights Max: 0.019907615283439973 Min: -0.023202130867529012\n","Layer 0 - Gradient Weights Max: 0.03678746204359847 Min: -0.026619282481957482\n","ReLU Activation - Max: 1.3952777252680166 Min: 0.0\n","ReLU Activation - Max: 0.6697672024364612 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02193551485435844 Min: -0.02069892721960575\n","Layer 1 - Gradient Weights Max: 0.019435511709184557 Min: -0.019938310175142304\n","Layer 0 - Gradient Weights Max: 0.02566794172135143 Min: -0.029391084320218107\n","ReLU Activation - Max: 1.2321972442301452 Min: 0.0\n","ReLU Activation - Max: 0.5920073156274925 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021412176667893967 Min: -0.013640143606132039\n","Layer 1 - Gradient Weights Max: 0.021696653276047976 Min: -0.023933399522554882\n","Layer 0 - Gradient Weights Max: 0.029800958500468218 Min: -0.029225541836945288\n","ReLU Activation - Max: 1.7228657976753532 Min: 0.0\n","ReLU Activation - Max: 0.6187971224140747 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014162664105326718 Min: -0.016234552629403754\n","Layer 1 - Gradient Weights Max: 0.020496922617085186 Min: -0.016563054800032864\n","Layer 0 - Gradient Weights Max: 0.022050493705778145 Min: -0.02303784844264696\n","ReLU Activation - Max: 1.8150039402028466 Min: 0.0\n","ReLU Activation - Max: 0.6070838004903429 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01666832243000122 Min: -0.020682980823754377\n","Layer 1 - Gradient Weights Max: 0.02011568440739809 Min: -0.019994149824475928\n","Layer 0 - Gradient Weights Max: 0.04140324104816607 Min: -0.03228490301726654\n","ReLU Activation - Max: 1.4558278490675698 Min: 0.0\n","ReLU Activation - Max: 0.679095514999223 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011694303954096834 Min: -0.013279132897250504\n","Layer 1 - Gradient Weights Max: 0.01618681840174948 Min: -0.02088619640602181\n","Layer 0 - Gradient Weights Max: 0.023057186624508348 Min: -0.026892142958099895\n","ReLU Activation - Max: 1.6398635850362375 Min: 0.0\n","ReLU Activation - Max: 0.587521478122582 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013021239225415702 Min: -0.013022693102377332\n","Layer 1 - Gradient Weights Max: 0.015756083666002542 Min: -0.024259014735108955\n","Layer 0 - Gradient Weights Max: 0.03319183943058628 Min: -0.025571750782188693\n","ReLU Activation - Max: 1.4229195872483198 Min: 0.0\n","ReLU Activation - Max: 0.6842659326160777 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019958384109075175 Min: -0.028387968332379117\n","Layer 1 - Gradient Weights Max: 0.020718045243527737 Min: -0.02795208271020688\n","Layer 0 - Gradient Weights Max: 0.028804989568066992 Min: -0.03145557606367775\n","ReLU Activation - Max: 1.5439538907450399 Min: 0.0\n","ReLU Activation - Max: 0.7250816659570611 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019251018923142262 Min: -0.02752388521066785\n","Layer 1 - Gradient Weights Max: 0.021643855458542777 Min: -0.017570087195865006\n","Layer 0 - Gradient Weights Max: 0.02820729560605287 Min: -0.037946552776866906\n","ReLU Activation - Max: 1.4943106331299192 Min: 0.0\n","ReLU Activation - Max: 0.537833621069178 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014634236315486151 Min: -0.014287321956009253\n","Layer 1 - Gradient Weights Max: 0.025297214581644356 Min: -0.0262199512750535\n","Layer 0 - Gradient Weights Max: 0.03209915859123479 Min: -0.033793002297765065\n","ReLU Activation - Max: 1.8394435051959215 Min: 0.0\n","ReLU Activation - Max: 0.6291745529770874 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.026074208239053386 Min: -0.025145762312304164\n","Layer 1 - Gradient Weights Max: 0.026372714701731956 Min: -0.01875038792590395\n","Layer 0 - Gradient Weights Max: 0.027623029881833086 Min: -0.03605673648099932\n","ReLU Activation - Max: 1.4819863244742442 Min: 0.0\n","ReLU Activation - Max: 0.5935758517039355 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016584439813108802 Min: -0.017787032377724818\n","Layer 1 - Gradient Weights Max: 0.017640378482722027 Min: -0.017826899543219296\n","Layer 0 - Gradient Weights Max: 0.03114284200226939 Min: -0.031102687630955358\n","ReLU Activation - Max: 1.6770050957935596 Min: 0.0\n","ReLU Activation - Max: 0.5712243187316703 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013297344421417789 Min: -0.012154595872026415\n","Layer 1 - Gradient Weights Max: 0.015238993534319786 Min: -0.01692786161481628\n","Layer 0 - Gradient Weights Max: 0.03287781037091768 Min: -0.031888025046439505\n","ReLU Activation - Max: 1.7951042246878743 Min: 0.0\n","ReLU Activation - Max: 0.6375811226798941 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0157353775355639 Min: -0.02202818802163572\n","Layer 1 - Gradient Weights Max: 0.02090518933784095 Min: -0.02376768632510778\n","Layer 0 - Gradient Weights Max: 0.031100944213723575 Min: -0.028509241305526355\n","ReLU Activation - Max: 1.4873662329788626 Min: 0.0\n","ReLU Activation - Max: 0.5788069711726356 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01877071672321246 Min: -0.015682047165852763\n","Layer 1 - Gradient Weights Max: 0.015310515152169312 Min: -0.017600575772899734\n","Layer 0 - Gradient Weights Max: 0.027475805702273887 Min: -0.025504744522614434\n","ReLU Activation - Max: 1.6455374516714616 Min: 0.0\n","ReLU Activation - Max: 0.5892109204069893 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01784107811616313 Min: -0.023769345577906456\n","Layer 1 - Gradient Weights Max: 0.021208170826183835 Min: -0.02712887560737404\n","Layer 0 - Gradient Weights Max: 0.03262225843568804 Min: -0.029702240737880324\n","ReLU Activation - Max: 1.664744409518151 Min: 0.0\n","ReLU Activation - Max: 0.7145732894865809 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016003401060309636 Min: -0.019035080723669034\n","Layer 1 - Gradient Weights Max: 0.025067871617723024 Min: -0.016686522884174475\n","Layer 0 - Gradient Weights Max: 0.025914388433385033 Min: -0.02857523030098898\n","ReLU Activation - Max: 1.7607480112755578 Min: 0.0\n","ReLU Activation - Max: 0.5826469720436427 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01802102963564985 Min: -0.02091874540083052\n","Layer 1 - Gradient Weights Max: 0.017658196058524655 Min: -0.02120560862391902\n","Layer 0 - Gradient Weights Max: 0.032667341449920444 Min: -0.028361235457551006\n","ReLU Activation - Max: 1.362825209062692 Min: 0.0\n","ReLU Activation - Max: 0.5695791895265184 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014490110962527862 Min: -0.016332662252733447\n","Layer 1 - Gradient Weights Max: 0.015391480955426303 Min: -0.021094519146613033\n","Layer 0 - Gradient Weights Max: 0.0295309339846824 Min: -0.027059337662920787\n","ReLU Activation - Max: 1.4793197651726742 Min: 0.0\n","ReLU Activation - Max: 0.5990411098675215 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013500023659414695 Min: -0.014181087782147737\n","Layer 1 - Gradient Weights Max: 0.014412359750262019 Min: -0.015611780025104736\n","Layer 0 - Gradient Weights Max: 0.025739748870489618 Min: -0.032749722436480014\n","ReLU Activation - Max: 1.5513727529098276 Min: 0.0\n","ReLU Activation - Max: 0.7077300065124306 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015916429655994657 Min: -0.015931809455202874\n","Layer 1 - Gradient Weights Max: 0.017243757485044503 Min: -0.01820635416863242\n","Layer 0 - Gradient Weights Max: 0.02379325377958205 Min: -0.027526363111966955\n","ReLU Activation - Max: 1.4864474209519836 Min: 0.0\n","ReLU Activation - Max: 0.6882707498630929 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01852433281224495 Min: -0.021972620035278893\n","Layer 1 - Gradient Weights Max: 0.02832123952201618 Min: -0.02782344798687714\n","Layer 0 - Gradient Weights Max: 0.02879622261704563 Min: -0.03687544768382302\n","ReLU Activation - Max: 1.6588464415179018 Min: 0.0\n","ReLU Activation - Max: 0.6113647882169304 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01986874629755841 Min: -0.014453275658595516\n","Layer 1 - Gradient Weights Max: 0.020726544869300527 Min: -0.02250534461117653\n","Layer 0 - Gradient Weights Max: 0.03388972602558177 Min: -0.03135949308619445\n","ReLU Activation - Max: 1.280682993530902 Min: 0.0\n","ReLU Activation - Max: 0.5801950767193639 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021182551334831805 Min: -0.024194905128263586\n","Layer 1 - Gradient Weights Max: 0.018276458869297872 Min: -0.023690746151154633\n","Layer 0 - Gradient Weights Max: 0.029041555014601247 Min: -0.032675429021561335\n","ReLU Activation - Max: 1.373447428409243 Min: 0.0\n","ReLU Activation - Max: 0.5880501908115049 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01042005744975397 Min: -0.014683914252239224\n","Layer 1 - Gradient Weights Max: 0.014113358798295773 Min: -0.015330202009644477\n","Layer 0 - Gradient Weights Max: 0.02940801142039301 Min: -0.027040011435736235\n","ReLU Activation - Max: 2.1751266934207187 Min: 0.0\n","ReLU Activation - Max: 0.5700142856544694 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02374863260055562 Min: -0.020246958337247583\n","Layer 1 - Gradient Weights Max: 0.023652762234702564 Min: -0.017216631039932435\n","Layer 0 - Gradient Weights Max: 0.036027283186729854 Min: -0.030890826254748975\n","ReLU Activation - Max: 1.4008578569650512 Min: 0.0\n","ReLU Activation - Max: 0.6537894988175871 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016048700535320416 Min: -0.020691751105646805\n","Layer 1 - Gradient Weights Max: 0.019061666984389672 Min: -0.02063088881011045\n","Layer 0 - Gradient Weights Max: 0.026785406618929936 Min: -0.031223454092686325\n","ReLU Activation - Max: 1.2978415949514148 Min: 0.0\n","ReLU Activation - Max: 0.5575018207735075 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01488221141873873 Min: -0.02129451621324541\n","Layer 1 - Gradient Weights Max: 0.016532935022749343 Min: -0.02696342259611235\n","Layer 0 - Gradient Weights Max: 0.03225985135347486 Min: -0.02444483140957767\n","ReLU Activation - Max: 1.4208824896727164 Min: 0.0\n","ReLU Activation - Max: 0.5137291444825401 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01664430845590281 Min: -0.01910037495626197\n","Layer 1 - Gradient Weights Max: 0.01668135205313799 Min: -0.027690975938158523\n","Layer 0 - Gradient Weights Max: 0.038678630142355036 Min: -0.02790734593926754\n","ReLU Activation - Max: 1.2088064061442882 Min: 0.0\n","ReLU Activation - Max: 0.57767846054503 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012126129912978723 Min: -0.026233907398970198\n","Layer 1 - Gradient Weights Max: 0.01811349326911304 Min: -0.026352703999440327\n","Layer 0 - Gradient Weights Max: 0.03945648975584009 Min: -0.025876903480971162\n","ReLU Activation - Max: 1.6290640997412882 Min: 0.0\n","ReLU Activation - Max: 0.6156116381876025 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01393490743276493 Min: -0.02279936089602098\n","Layer 1 - Gradient Weights Max: 0.015137900246614615 Min: -0.01805187099941282\n","Layer 0 - Gradient Weights Max: 0.02911789123941247 Min: -0.03680888585063084\n","ReLU Activation - Max: 1.4225881690263549 Min: 0.0\n","ReLU Activation - Max: 0.5960432499240894 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019719550369445223 Min: -0.015410402083814608\n","Layer 1 - Gradient Weights Max: 0.013712327795941188 Min: -0.02086598292605489\n","Layer 0 - Gradient Weights Max: 0.027615618670286446 Min: -0.027044435620716366\n","ReLU Activation - Max: 1.535639738956101 Min: 0.0\n","ReLU Activation - Max: 0.5475455767902353 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011991684240820566 Min: -0.016722714583684628\n","Layer 1 - Gradient Weights Max: 0.021254872023257713 Min: -0.01703058688159921\n","Layer 0 - Gradient Weights Max: 0.025959543800239227 Min: -0.028673075954594\n","ReLU Activation - Max: 1.4119122658920475 Min: 0.0\n","ReLU Activation - Max: 0.5544815478195635 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013241533600634686 Min: -0.018143644921073995\n","Layer 1 - Gradient Weights Max: 0.01933730850692769 Min: -0.020305378254394206\n","Layer 0 - Gradient Weights Max: 0.03529487157029162 Min: -0.0255928153377636\n","ReLU Activation - Max: 1.4063062580838461 Min: 0.0\n","ReLU Activation - Max: 0.5681574092338517 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011021442732219259 Min: -0.02031986482636276\n","Layer 1 - Gradient Weights Max: 0.01841616855464589 Min: -0.02472825140706862\n","Layer 0 - Gradient Weights Max: 0.02893697046278578 Min: -0.04204326236966135\n","ReLU Activation - Max: 1.435129796339157 Min: 0.0\n","ReLU Activation - Max: 0.6307702836870857 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012780079810683583 Min: -0.011360999467922173\n","Layer 1 - Gradient Weights Max: 0.019154201385869297 Min: -0.019351285106367567\n","Layer 0 - Gradient Weights Max: 0.02782941495902082 Min: -0.03334287743295166\n","ReLU Activation - Max: 1.5344746026705551 Min: 0.0\n","ReLU Activation - Max: 0.6648330300422527 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01578075488929208 Min: -0.013188153603579037\n","Layer 1 - Gradient Weights Max: 0.017164509065209953 Min: -0.02243507220244669\n","Layer 0 - Gradient Weights Max: 0.027359250218960702 Min: -0.028608181549817157\n","ReLU Activation - Max: 1.5101890864799572 Min: 0.0\n","ReLU Activation - Max: 0.6163103168218462 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019453185722978474 Min: -0.020683974210868542\n","Layer 1 - Gradient Weights Max: 0.024117515542465846 Min: -0.02453285754645175\n","Layer 0 - Gradient Weights Max: 0.02598260944809965 Min: -0.02575870985667185\n","ReLU Activation - Max: 1.4318671072879718 Min: 0.0\n","ReLU Activation - Max: 0.6145299319032232 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015248180982423665 Min: -0.01666208248907937\n","Layer 1 - Gradient Weights Max: 0.016916201052714856 Min: -0.021526042209768895\n","Layer 0 - Gradient Weights Max: 0.04228394738361926 Min: -0.028426726855455615\n","ReLU Activation - Max: 1.5651968230354247 Min: 0.0\n","ReLU Activation - Max: 0.631232392136821 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015658810351476378 Min: -0.01220325700696977\n","Layer 1 - Gradient Weights Max: 0.015709291627741557 Min: -0.012774198411174602\n","Layer 0 - Gradient Weights Max: 0.024712627692009263 Min: -0.030776163877454632\n","ReLU Activation - Max: 1.9880664574123457 Min: 0.0\n","ReLU Activation - Max: 0.5388395453470894 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016085831131677195 Min: -0.01942155959256913\n","Layer 1 - Gradient Weights Max: 0.01738583923826289 Min: -0.020675044205525576\n","Layer 0 - Gradient Weights Max: 0.040980177519148356 Min: -0.031912907315402465\n","ReLU Activation - Max: 1.3274710210996976 Min: 0.0\n","ReLU Activation - Max: 0.6454037266994997 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018653600433954212 Min: -0.022391432995535407\n","Layer 1 - Gradient Weights Max: 0.026304051129805467 Min: -0.0262260836029517\n","Layer 0 - Gradient Weights Max: 0.03109141466494054 Min: -0.02906151696217832\n","ReLU Activation - Max: 1.3539128264503741 Min: 0.0\n","ReLU Activation - Max: 0.5883490171768667 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019622779369448506 Min: -0.016826452610702043\n","Layer 1 - Gradient Weights Max: 0.020781979811764224 Min: -0.01956651663359519\n","Layer 0 - Gradient Weights Max: 0.02947398656425929 Min: -0.03089760968706206\n","ReLU Activation - Max: 1.2868282943477574 Min: 0.0\n","ReLU Activation - Max: 0.5756650090070199 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019983275090424486 Min: -0.029139375923992194\n","Layer 1 - Gradient Weights Max: 0.019027168698963332 Min: -0.026646038835228907\n","Layer 0 - Gradient Weights Max: 0.026416857182987842 Min: -0.025146164258262767\n","ReLU Activation - Max: 1.612769476377842 Min: 0.0\n","ReLU Activation - Max: 0.6212070258648237 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010286047035309616 Min: -0.017605499245897032\n","Layer 1 - Gradient Weights Max: 0.02072103990363599 Min: -0.01970713011489377\n","Layer 0 - Gradient Weights Max: 0.028950280275608573 Min: -0.029755334497589887\n","ReLU Activation - Max: 1.5279123786048234 Min: 0.0\n","ReLU Activation - Max: 0.6448379438339856 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016453355690474174 Min: -0.011605176436154744\n","Layer 1 - Gradient Weights Max: 0.017316505288521033 Min: -0.01910294477193074\n","Layer 0 - Gradient Weights Max: 0.02778067388355759 Min: -0.026603518921485284\n","ReLU Activation - Max: 1.4580901854811394 Min: 0.0\n","ReLU Activation - Max: 0.5932311539243678 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02174943212471234 Min: -0.020649410016145023\n","Layer 1 - Gradient Weights Max: 0.0197672858216773 Min: -0.021306310284776635\n","Layer 0 - Gradient Weights Max: 0.029467096319899473 Min: -0.028387282504394658\n","ReLU Activation - Max: 1.3538559845317082 Min: 0.0\n","ReLU Activation - Max: 0.6436829685461171 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014082416389928286 Min: -0.018683203009931877\n","Layer 1 - Gradient Weights Max: 0.018109658460721673 Min: -0.018747879504021597\n","Layer 0 - Gradient Weights Max: 0.025457904346699416 Min: -0.023764619762746825\n","ReLU Activation - Max: 1.587170404463423 Min: 0.0\n","ReLU Activation - Max: 0.6066726226542002 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017006584680493864 Min: -0.017136514595059417\n","Layer 1 - Gradient Weights Max: 0.014963852512032277 Min: -0.015568515279039715\n","Layer 0 - Gradient Weights Max: 0.025045505326565548 Min: -0.026016180550196926\n","ReLU Activation - Max: 1.2698523036741205 Min: 0.0\n","ReLU Activation - Max: 0.6098173603471402 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018750710778872002 Min: -0.034129952786374644\n","Layer 1 - Gradient Weights Max: 0.02856618622918306 Min: -0.02476421504184085\n","Layer 0 - Gradient Weights Max: 0.033665886517745 Min: -0.039660573565467094\n","ReLU Activation - Max: 1.5989525985758515 Min: 0.0\n","ReLU Activation - Max: 0.533588762093698 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01578789494719451 Min: -0.013729931017784113\n","Layer 1 - Gradient Weights Max: 0.015567291592934933 Min: -0.021973577688382966\n","Layer 0 - Gradient Weights Max: 0.027487466221268138 Min: -0.02629920741108478\n","ReLU Activation - Max: 1.6647718045932742 Min: 0.0\n","ReLU Activation - Max: 0.6124432109518444 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022779458425619268 Min: -0.026078714705752046\n","Layer 1 - Gradient Weights Max: 0.01884482518259713 Min: -0.02499898541510884\n","Layer 0 - Gradient Weights Max: 0.03897614902217003 Min: -0.030491592087865307\n","ReLU Activation - Max: 1.6020590855932486 Min: 0.0\n","ReLU Activation - Max: 0.6669607659995895 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014295445676047272 Min: -0.019952977432266476\n","Layer 1 - Gradient Weights Max: 0.01775394032129641 Min: -0.022725430347877607\n","Layer 0 - Gradient Weights Max: 0.040622727723935464 Min: -0.026954879345515947\n","ReLU Activation - Max: 1.3893954419247208 Min: 0.0\n","ReLU Activation - Max: 0.6700640414447162 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012028179914927224 Min: -0.019374922070662937\n","Layer 1 - Gradient Weights Max: 0.026073269419064796 Min: -0.025702009719571588\n","Layer 0 - Gradient Weights Max: 0.037573042859450366 Min: -0.030816596941905763\n","ReLU Activation - Max: 1.4005435077065715 Min: 0.0\n","ReLU Activation - Max: 0.6362460934125661 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011464718760930274 Min: -0.012452063103551429\n","Layer 1 - Gradient Weights Max: 0.01767851919734315 Min: -0.015848028518458054\n","Layer 0 - Gradient Weights Max: 0.029704421572455918 Min: -0.027459396421843058\n","ReLU Activation - Max: 1.499191359442932 Min: 0.0\n","ReLU Activation - Max: 0.5589928315978367 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014089662532090954 Min: -0.021782799813161054\n","Layer 1 - Gradient Weights Max: 0.019904083759963064 Min: -0.0175171680058669\n","Layer 0 - Gradient Weights Max: 0.0328616894190673 Min: -0.037916700677100186\n","ReLU Activation - Max: 1.8868803843179627 Min: 0.0\n","ReLU Activation - Max: 0.6247597577509822 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011856267722949702 Min: -0.014297131979363917\n","Layer 1 - Gradient Weights Max: 0.017817753202876253 Min: -0.0211209721701633\n","Layer 0 - Gradient Weights Max: 0.032472997675352736 Min: -0.026773215942041018\n","ReLU Activation - Max: 1.2575116244911986 Min: 0.0\n","ReLU Activation - Max: 0.5441386138543512 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01619721596004257 Min: -0.01822060301040015\n","Layer 1 - Gradient Weights Max: 0.021550392948855336 Min: -0.02396249228981665\n","Layer 0 - Gradient Weights Max: 0.024959816535340726 Min: -0.03303348799804645\n","ReLU Activation - Max: 1.300716844573815 Min: 0.0\n","ReLU Activation - Max: 0.6233311749692633 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01342781816562073 Min: -0.01171428154847991\n","Layer 1 - Gradient Weights Max: 0.015407674276096467 Min: -0.017318445680078228\n","Layer 0 - Gradient Weights Max: 0.026743380785668628 Min: -0.025852136360318977\n","ReLU Activation - Max: 1.4267230078168245 Min: 0.0\n","ReLU Activation - Max: 0.5297889343104499 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01706267243134889 Min: -0.02523161983105425\n","Layer 1 - Gradient Weights Max: 0.02213802218685161 Min: -0.02399346114295161\n","Layer 0 - Gradient Weights Max: 0.027053278911368394 Min: -0.026721112115375388\n","ReLU Activation - Max: 1.482797789530895 Min: 0.0\n","ReLU Activation - Max: 0.6117655794688353 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015588056752733597 Min: -0.013475663856686613\n","Layer 1 - Gradient Weights Max: 0.016224938224938455 Min: -0.014249855248967873\n","Layer 0 - Gradient Weights Max: 0.02536066392476753 Min: -0.02698848819663277\n","ReLU Activation - Max: 1.2982933438505042 Min: 0.0\n","ReLU Activation - Max: 0.5365955482245738 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018688714832986182 Min: -0.019217444394970893\n","Layer 1 - Gradient Weights Max: 0.021063147574617632 Min: -0.022715658368849104\n","Layer 0 - Gradient Weights Max: 0.03331907693792034 Min: -0.02723388171144508\n","ReLU Activation - Max: 1.500006052066655 Min: 0.0\n","ReLU Activation - Max: 0.5970039563891676 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017142396081439682 Min: -0.019146721365664955\n","Layer 1 - Gradient Weights Max: 0.017651024284080535 Min: -0.024519411758248778\n","Layer 0 - Gradient Weights Max: 0.03395095225767289 Min: -0.029175314471246345\n","ReLU Activation - Max: 1.2419306154363756 Min: 0.0\n","ReLU Activation - Max: 0.741117299306287 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01234619496436262 Min: -0.0279414694839503\n","Layer 1 - Gradient Weights Max: 0.023716421568429923 Min: -0.01643686636997219\n","Layer 0 - Gradient Weights Max: 0.028235846909681308 Min: -0.025311755289976694\n","ReLU Activation - Max: 1.4467454252607599 Min: 0.0\n","ReLU Activation - Max: 0.6127140252416994 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015177336084220318 Min: -0.02092319696958965\n","Layer 1 - Gradient Weights Max: 0.02581317185172221 Min: -0.019949662279496187\n","Layer 0 - Gradient Weights Max: 0.026171678905777658 Min: -0.025769724142660752\n","ReLU Activation - Max: 1.7839956276288178 Min: 0.0\n","ReLU Activation - Max: 0.7051450678684372 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021140556103681516 Min: -0.022493571277251833\n","Layer 1 - Gradient Weights Max: 0.014197344794343997 Min: -0.03720566817253148\n","Layer 0 - Gradient Weights Max: 0.039297088985705025 Min: -0.0298228471764521\n","ReLU Activation - Max: 1.6733974484810803 Min: 0.0\n","ReLU Activation - Max: 0.6482856670100262 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012576076836479644 Min: -0.016662993413584296\n","Layer 1 - Gradient Weights Max: 0.016276256966135993 Min: -0.022843537655955894\n","Layer 0 - Gradient Weights Max: 0.034122808472402254 Min: -0.035708756926200465\n","ReLU Activation - Max: 1.4042807576209961 Min: 0.0\n","ReLU Activation - Max: 0.6054534637875352 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008752253061908752 Min: -0.012768655659660469\n","Layer 1 - Gradient Weights Max: 0.012623509198132054 Min: -0.021092639586811457\n","Layer 0 - Gradient Weights Max: 0.030719907252836975 Min: -0.026913355109011227\n","ReLU Activation - Max: 1.4203756594171997 Min: 0.0\n","ReLU Activation - Max: 0.706499173608777 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017699752231276875 Min: -0.02110974166957134\n","Layer 1 - Gradient Weights Max: 0.02073780580642691 Min: -0.020783434886422675\n","Layer 0 - Gradient Weights Max: 0.025856159572776483 Min: -0.02836490041967849\n","ReLU Activation - Max: 1.3990767086934675 Min: 0.0\n","ReLU Activation - Max: 0.692998273007659 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.027002281778029293 Min: -0.015214790804748418\n","Layer 1 - Gradient Weights Max: 0.020413601069786205 Min: -0.023076481181999446\n","Layer 0 - Gradient Weights Max: 0.03484730425225033 Min: -0.02391116057397656\n","ReLU Activation - Max: 1.3335330829172403 Min: 0.0\n","ReLU Activation - Max: 0.5671153800146708 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023148190692239317 Min: -0.019760466337891736\n","Layer 1 - Gradient Weights Max: 0.01987371107537779 Min: -0.02213756433438876\n","Layer 0 - Gradient Weights Max: 0.03492772697225112 Min: -0.03527140412010822\n","ReLU Activation - Max: 1.1133413817491586 Min: 0.0\n","ReLU Activation - Max: 0.638552320889887 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016620659341819333 Min: -0.01198669368803879\n","Layer 1 - Gradient Weights Max: 0.01769633355120862 Min: -0.01636685189550219\n","Layer 0 - Gradient Weights Max: 0.0278509650309522 Min: -0.026708726817952083\n","ReLU Activation - Max: 1.7895437021001055 Min: 0.0\n","ReLU Activation - Max: 0.7003549112745303 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018201627738686943 Min: -0.01788668384438273\n","Layer 1 - Gradient Weights Max: 0.018558154233362073 Min: -0.019890265733741017\n","Layer 0 - Gradient Weights Max: 0.024427016810227936 Min: -0.0334556977895821\n","ReLU Activation - Max: 1.6997395902418182 Min: 0.0\n","ReLU Activation - Max: 0.6440639219547046 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011625813671365975 Min: -0.013500886208961428\n","Layer 1 - Gradient Weights Max: 0.012998173832642044 Min: -0.018047639769934044\n","Layer 0 - Gradient Weights Max: 0.031496204113486394 Min: -0.027828655097974667\n","ReLU Activation - Max: 1.4742489258491467 Min: 0.0\n","ReLU Activation - Max: 0.6315864515931218 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.026650132199384433 Min: -0.01984442893888122\n","Layer 1 - Gradient Weights Max: 0.02579163229347381 Min: -0.027850591259474505\n","Layer 0 - Gradient Weights Max: 0.02485210936408064 Min: -0.026041496003094418\n","ReLU Activation - Max: 1.327649601052938 Min: 0.0\n","ReLU Activation - Max: 0.5961426445537428 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018602895234186408 Min: -0.025608600660212456\n","Layer 1 - Gradient Weights Max: 0.020454249922656428 Min: -0.02574351582720669\n","Layer 0 - Gradient Weights Max: 0.040731764294132417 Min: -0.027019938816378035\n","ReLU Activation - Max: 1.6057882875804699 Min: 0.0\n","ReLU Activation - Max: 0.5450047107456938 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017004988785593176 Min: -0.021826383008222164\n","Layer 1 - Gradient Weights Max: 0.023806332261208506 Min: -0.019648922089272935\n","Layer 0 - Gradient Weights Max: 0.030125041985991363 Min: -0.0328711357648851\n","ReLU Activation - Max: 1.370395262229167 Min: 0.0\n","ReLU Activation - Max: 0.5835477302469845 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018253483228526105 Min: -0.02323342274905912\n","Layer 1 - Gradient Weights Max: 0.02096825119348913 Min: -0.02337333943194501\n","Layer 0 - Gradient Weights Max: 0.0322296021707086 Min: -0.039248412862777575\n","ReLU Activation - Max: 1.655499960198327 Min: 0.0\n","ReLU Activation - Max: 0.6726909221249608 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015215502852111883 Min: -0.015388140782151187\n","Layer 1 - Gradient Weights Max: 0.0230613340963906 Min: -0.02126178297726137\n","Layer 0 - Gradient Weights Max: 0.02750463282825081 Min: -0.03138872830467465\n","ReLU Activation - Max: 1.4865785041800403 Min: 0.0\n","ReLU Activation - Max: 0.6068997261165443 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011432337669168317 Min: -0.02741322795923726\n","Layer 1 - Gradient Weights Max: 0.023362987251022434 Min: -0.02225244649751136\n","Layer 0 - Gradient Weights Max: 0.025671749221759764 Min: -0.030325865446666374\n","ReLU Activation - Max: 1.7555469108730017 Min: 0.0\n","ReLU Activation - Max: 0.6942502731272331 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009872024870317254 Min: -0.01267362903942047\n","Layer 1 - Gradient Weights Max: 0.016799084746913724 Min: -0.0154319962443262\n","Layer 0 - Gradient Weights Max: 0.02646984388090104 Min: -0.023131874019220663\n","ReLU Activation - Max: 1.2446084379262385 Min: 0.0\n","ReLU Activation - Max: 0.5865569190500425 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017715991502667967 Min: -0.013159634628869078\n","Layer 1 - Gradient Weights Max: 0.019086487917419438 Min: -0.02625328128104487\n","Layer 0 - Gradient Weights Max: 0.03506206253080648 Min: -0.02356346466714853\n","ReLU Activation - Max: 1.5514177976278791 Min: 0.0\n","ReLU Activation - Max: 0.6154624853675342 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020877997065082383 Min: -0.024780450151041425\n","Layer 1 - Gradient Weights Max: 0.02832506157917949 Min: -0.019789939921709156\n","Layer 0 - Gradient Weights Max: 0.0333991269684863 Min: -0.027829300293794604\n","Epoch 99/100 - Loss: 0.7448\n","ReLU Activation - Max: 1.3146312543920782 Min: 0.0\n","ReLU Activation - Max: 0.6404483051704436 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01710217824092406 Min: -0.02147166375153711\n","Layer 1 - Gradient Weights Max: 0.01591272945560541 Min: -0.030309085461384572\n","Layer 0 - Gradient Weights Max: 0.027175273991509145 Min: -0.027937221936939084\n","ReLU Activation - Max: 1.2797444909084612 Min: 0.0\n","ReLU Activation - Max: 0.5364117390727446 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010308036957725888 Min: -0.015779068160092634\n","Layer 1 - Gradient Weights Max: 0.016748672763313027 Min: -0.021640216108317234\n","Layer 0 - Gradient Weights Max: 0.034585640997956246 Min: -0.03661957073468947\n","ReLU Activation - Max: 1.6081877228748764 Min: 0.0\n","ReLU Activation - Max: 0.7089104569914682 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015943392533099676 Min: -0.02374855431004472\n","Layer 1 - Gradient Weights Max: 0.01681707075857635 Min: -0.024455829729147185\n","Layer 0 - Gradient Weights Max: 0.03049474689037285 Min: -0.030697839177164732\n","ReLU Activation - Max: 1.3225961746510433 Min: 0.0\n","ReLU Activation - Max: 0.6035553566283687 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010574216415572242 Min: -0.01525408221547257\n","Layer 1 - Gradient Weights Max: 0.019314341321547846 Min: -0.016145590235683897\n","Layer 0 - Gradient Weights Max: 0.037254412515373786 Min: -0.024957607048206737\n","ReLU Activation - Max: 1.3348286265702045 Min: 0.0\n","ReLU Activation - Max: 0.6157487171623791 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02149824740725398 Min: -0.01479634920377069\n","Layer 1 - Gradient Weights Max: 0.022325506041940748 Min: -0.020747591977503574\n","Layer 0 - Gradient Weights Max: 0.03150496582572825 Min: -0.025481967155919806\n","ReLU Activation - Max: 1.3811097449276757 Min: 0.0\n","ReLU Activation - Max: 0.5701500487469648 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01140079580547186 Min: -0.018816570545687642\n","Layer 1 - Gradient Weights Max: 0.020534862335883575 Min: -0.023130607878039777\n","Layer 0 - Gradient Weights Max: 0.03254148425283579 Min: -0.033044944518566724\n","ReLU Activation - Max: 1.4948491566615085 Min: 0.0\n","ReLU Activation - Max: 0.5952948188975796 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018040612113296134 Min: -0.016402808947041925\n","Layer 1 - Gradient Weights Max: 0.019568717731948813 Min: -0.01893882649615805\n","Layer 0 - Gradient Weights Max: 0.026104696639879355 Min: -0.029111038862135584\n","ReLU Activation - Max: 1.393265262870064 Min: 0.0\n","ReLU Activation - Max: 0.5754745076069352 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01358747369266606 Min: -0.018506518603955177\n","Layer 1 - Gradient Weights Max: 0.020070358131690128 Min: -0.020752293704014848\n","Layer 0 - Gradient Weights Max: 0.035441830243880784 Min: -0.03153877384708172\n","ReLU Activation - Max: 1.4717283371286172 Min: 0.0\n","ReLU Activation - Max: 0.6388853303787898 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021659778542353802 Min: -0.020180706252101943\n","Layer 1 - Gradient Weights Max: 0.02337242339086562 Min: -0.019471770784514914\n","Layer 0 - Gradient Weights Max: 0.030033960258715728 Min: -0.032077039968757305\n","ReLU Activation - Max: 1.376377186688263 Min: 0.0\n","ReLU Activation - Max: 0.6611072499747885 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.026574628052138626 Min: -0.024042287073841356\n","Layer 1 - Gradient Weights Max: 0.0254481920354842 Min: -0.034846556220490425\n","Layer 0 - Gradient Weights Max: 0.030636178180587316 Min: -0.029446198113834476\n","ReLU Activation - Max: 1.7031202649462205 Min: 0.0\n","ReLU Activation - Max: 0.7279995818639289 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014622952319676141 Min: -0.017883376115933443\n","Layer 1 - Gradient Weights Max: 0.023505973845412764 Min: -0.024205200735941807\n","Layer 0 - Gradient Weights Max: 0.03217988234971281 Min: -0.040986458290039586\n","ReLU Activation - Max: 1.3888079790660308 Min: 0.0\n","ReLU Activation - Max: 0.5615079737215467 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015411741768242756 Min: -0.025621085034329954\n","Layer 1 - Gradient Weights Max: 0.020595877153131936 Min: -0.019283917525462095\n","Layer 0 - Gradient Weights Max: 0.04211192998822121 Min: -0.03936746663253359\n","ReLU Activation - Max: 1.4175233768323425 Min: 0.0\n","ReLU Activation - Max: 0.5319975561133414 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022378523988704886 Min: -0.017715693941312366\n","Layer 1 - Gradient Weights Max: 0.015776130169848013 Min: -0.02148900157965862\n","Layer 0 - Gradient Weights Max: 0.024769847615474405 Min: -0.03315097900810713\n","ReLU Activation - Max: 1.413090949736939 Min: 0.0\n","ReLU Activation - Max: 0.6163610018967721 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01867925189198236 Min: -0.02530277224539469\n","Layer 1 - Gradient Weights Max: 0.021722098944921202 Min: -0.02313387563806449\n","Layer 0 - Gradient Weights Max: 0.0256788434877863 Min: -0.03230838666829457\n","ReLU Activation - Max: 1.532410693056722 Min: 0.0\n","ReLU Activation - Max: 0.53780003302133 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0128335385555607 Min: -0.028605095062784157\n","Layer 1 - Gradient Weights Max: 0.014634581109968027 Min: -0.02350888689261478\n","Layer 0 - Gradient Weights Max: 0.024795647513651868 Min: -0.027291051965145908\n","ReLU Activation - Max: 1.585928205757344 Min: 0.0\n","ReLU Activation - Max: 0.5769704871084893 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014455764094596021 Min: -0.012919042207762433\n","Layer 1 - Gradient Weights Max: 0.015501660887836883 Min: -0.0242254972790866\n","Layer 0 - Gradient Weights Max: 0.036505685315954675 Min: -0.030801128940491328\n","ReLU Activation - Max: 1.5839637246296403 Min: 0.0\n","ReLU Activation - Max: 0.5547837964429213 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01068123307438116 Min: -0.025636879583546355\n","Layer 1 - Gradient Weights Max: 0.019601890322674516 Min: -0.018329469382650572\n","Layer 0 - Gradient Weights Max: 0.026800143503695563 Min: -0.02463900268744502\n","ReLU Activation - Max: 1.7378077365890348 Min: 0.0\n","ReLU Activation - Max: 0.5890168643815437 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017959933085990813 Min: -0.01937232217748084\n","Layer 1 - Gradient Weights Max: 0.02603802726063336 Min: -0.021434376246031738\n","Layer 0 - Gradient Weights Max: 0.03181003055943015 Min: -0.045167818747039394\n","ReLU Activation - Max: 1.7020948967778244 Min: 0.0\n","ReLU Activation - Max: 0.5446509860359117 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013774247624983596 Min: -0.02251098232305574\n","Layer 1 - Gradient Weights Max: 0.016571793270903846 Min: -0.01899000120544557\n","Layer 0 - Gradient Weights Max: 0.03358270957016199 Min: -0.03338252383275736\n","ReLU Activation - Max: 1.4490844845859594 Min: 0.0\n","ReLU Activation - Max: 0.5680403651496087 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012777309556911604 Min: -0.021904377204938683\n","Layer 1 - Gradient Weights Max: 0.018408160797461798 Min: -0.020306532232459314\n","Layer 0 - Gradient Weights Max: 0.026334718487740723 Min: -0.029701065284228143\n","ReLU Activation - Max: 1.6807912581689106 Min: 0.0\n","ReLU Activation - Max: 0.5815519592018398 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015622118755297218 Min: -0.014530636388740694\n","Layer 1 - Gradient Weights Max: 0.0256243526154437 Min: -0.0169035919923148\n","Layer 0 - Gradient Weights Max: 0.033273329639404624 Min: -0.036458922778384896\n","ReLU Activation - Max: 1.3324849729095614 Min: 0.0\n","ReLU Activation - Max: 0.6419144553953137 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013833066018175827 Min: -0.015755458467916785\n","Layer 1 - Gradient Weights Max: 0.016412379311420575 Min: -0.017748976117846302\n","Layer 0 - Gradient Weights Max: 0.026076536270073763 Min: -0.03071795215710857\n","ReLU Activation - Max: 1.415333728554819 Min: 0.0\n","ReLU Activation - Max: 0.6420764201013812 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020553880631298406 Min: -0.021331662865315286\n","Layer 1 - Gradient Weights Max: 0.017172942622717482 Min: -0.02894476550349066\n","Layer 0 - Gradient Weights Max: 0.028021119425109747 Min: -0.03191458455128457\n","ReLU Activation - Max: 1.5219325551667844 Min: 0.0\n","ReLU Activation - Max: 0.5585869289113132 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015225198631177446 Min: -0.015072551475656537\n","Layer 1 - Gradient Weights Max: 0.02007341791782846 Min: -0.019264181716870006\n","Layer 0 - Gradient Weights Max: 0.026882290284233005 Min: -0.02693397862087881\n","ReLU Activation - Max: 1.2979306670151687 Min: 0.0\n","ReLU Activation - Max: 0.6051894041119074 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013435173002593679 Min: -0.022685281544042667\n","Layer 1 - Gradient Weights Max: 0.020375360674261312 Min: -0.0180286679515937\n","Layer 0 - Gradient Weights Max: 0.03459028689667954 Min: -0.04116371262633344\n","ReLU Activation - Max: 1.8335953364855682 Min: 0.0\n","ReLU Activation - Max: 0.6154537251877542 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020916358945571607 Min: -0.017050407025654486\n","Layer 1 - Gradient Weights Max: 0.02239120659049009 Min: -0.01545133855204331\n","Layer 0 - Gradient Weights Max: 0.02574622455520024 Min: -0.035148985948707356\n","ReLU Activation - Max: 1.3560953311706372 Min: 0.0\n","ReLU Activation - Max: 0.6420447158811811 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01047467237987288 Min: -0.013574004776331718\n","Layer 1 - Gradient Weights Max: 0.01711192312690708 Min: -0.020851133747897297\n","Layer 0 - Gradient Weights Max: 0.023075144438170735 Min: -0.026008675003780547\n","ReLU Activation - Max: 1.2381381383405845 Min: 0.0\n","ReLU Activation - Max: 0.6803783961238489 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02888189024006678 Min: -0.036486699390392376\n","Layer 1 - Gradient Weights Max: 0.02105783349547079 Min: -0.023727952425052087\n","Layer 0 - Gradient Weights Max: 0.035592864244411195 Min: -0.04478419537774995\n","ReLU Activation - Max: 1.4232895263485366 Min: 0.0\n","ReLU Activation - Max: 0.590491720404148 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014562415830051715 Min: -0.010108874601207702\n","Layer 1 - Gradient Weights Max: 0.016154335813903103 Min: -0.01684112953626969\n","Layer 0 - Gradient Weights Max: 0.027885811337535954 Min: -0.029115428926693474\n","ReLU Activation - Max: 1.5598012671987065 Min: 0.0\n","ReLU Activation - Max: 0.674886875901643 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012292573832669237 Min: -0.013938476051403314\n","Layer 1 - Gradient Weights Max: 0.015837043685287783 Min: -0.01950626599641015\n","Layer 0 - Gradient Weights Max: 0.04593994604710644 Min: -0.03009405575454148\n","ReLU Activation - Max: 1.541920267909163 Min: 0.0\n","ReLU Activation - Max: 0.5517601614620381 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019003821242926152 Min: -0.01949774961657376\n","Layer 1 - Gradient Weights Max: 0.021717989383512003 Min: -0.020525377322699885\n","Layer 0 - Gradient Weights Max: 0.032086473323377655 Min: -0.029039245484993394\n","ReLU Activation - Max: 1.5438114648847636 Min: 0.0\n","ReLU Activation - Max: 0.5183959469920902 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.036171815583665876 Min: -0.021127485244968318\n","Layer 1 - Gradient Weights Max: 0.030605409512531618 Min: -0.02335547881717662\n","Layer 0 - Gradient Weights Max: 0.025470373287005205 Min: -0.03594302431129895\n","ReLU Activation - Max: 1.3844639749773764 Min: 0.0\n","ReLU Activation - Max: 0.5764739267673153 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015552898569968157 Min: -0.01322543742955141\n","Layer 1 - Gradient Weights Max: 0.015607130309755078 Min: -0.018524852709829207\n","Layer 0 - Gradient Weights Max: 0.025372854256033094 Min: -0.02916701989773696\n","ReLU Activation - Max: 1.5186612950449088 Min: 0.0\n","ReLU Activation - Max: 0.5628942720650275 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018033922722288015 Min: -0.023380731982760954\n","Layer 1 - Gradient Weights Max: 0.02754650889095508 Min: -0.01609892188973307\n","Layer 0 - Gradient Weights Max: 0.02982171121476532 Min: -0.02792302552706733\n","ReLU Activation - Max: 1.3782591188858142 Min: 0.0\n","ReLU Activation - Max: 0.7141753345494948 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0089483298586795 Min: -0.014860421176043524\n","Layer 1 - Gradient Weights Max: 0.013395249661685298 Min: -0.024393315962518102\n","Layer 0 - Gradient Weights Max: 0.024225459514754622 Min: -0.02936382817672936\n","ReLU Activation - Max: 1.2078258000304087 Min: 0.0\n","ReLU Activation - Max: 0.6121487823205861 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0154497241254159 Min: -0.01530806525331399\n","Layer 1 - Gradient Weights Max: 0.019303540506070256 Min: -0.013370042034448778\n","Layer 0 - Gradient Weights Max: 0.024451379105173754 Min: -0.03284577588230789\n","ReLU Activation - Max: 1.2421612529982438 Min: 0.0\n","ReLU Activation - Max: 0.6719112405393118 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016711128630701882 Min: -0.01823653937164088\n","Layer 1 - Gradient Weights Max: 0.015998931932372992 Min: -0.01605340883268445\n","Layer 0 - Gradient Weights Max: 0.035818824514229755 Min: -0.02730317683899129\n","ReLU Activation - Max: 1.542411398557515 Min: 0.0\n","ReLU Activation - Max: 0.610515601381676 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01404270988510175 Min: -0.022212594658793283\n","Layer 1 - Gradient Weights Max: 0.018956178945762514 Min: -0.017764476749590825\n","Layer 0 - Gradient Weights Max: 0.029166882588681612 Min: -0.032235248224613655\n","ReLU Activation - Max: 1.245048909602753 Min: 0.0\n","ReLU Activation - Max: 0.5386588834472741 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010986053776447263 Min: -0.017826020719904415\n","Layer 1 - Gradient Weights Max: 0.019269315451169628 Min: -0.03480855710930062\n","Layer 0 - Gradient Weights Max: 0.028812787120938436 Min: -0.028206683566962567\n","ReLU Activation - Max: 1.4669274793521934 Min: 0.0\n","ReLU Activation - Max: 0.663356368844367 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012847688407096545 Min: -0.027491867145170316\n","Layer 1 - Gradient Weights Max: 0.02095481365633645 Min: -0.015219581796105086\n","Layer 0 - Gradient Weights Max: 0.024727932994138963 Min: -0.03571177061849656\n","ReLU Activation - Max: 1.4707565822428068 Min: 0.0\n","ReLU Activation - Max: 0.5926265297148934 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02105447541286189 Min: -0.022561065728542922\n","Layer 1 - Gradient Weights Max: 0.017597403968797106 Min: -0.018561694549632614\n","Layer 0 - Gradient Weights Max: 0.028206788315943834 Min: -0.024626586748181793\n","ReLU Activation - Max: 1.399700604876893 Min: 0.0\n","ReLU Activation - Max: 0.5742474440436236 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012795647238262146 Min: -0.009095470689872624\n","Layer 1 - Gradient Weights Max: 0.014044215806285102 Min: -0.024406433731076715\n","Layer 0 - Gradient Weights Max: 0.03262058846696305 Min: -0.027198099212462037\n","ReLU Activation - Max: 1.3472558281955962 Min: 0.0\n","ReLU Activation - Max: 0.6880659841609613 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01577694827263861 Min: -0.019160044276778556\n","Layer 1 - Gradient Weights Max: 0.017328130716351708 Min: -0.023365627737065995\n","Layer 0 - Gradient Weights Max: 0.02768827043492389 Min: -0.026545744229600196\n","ReLU Activation - Max: 1.4061584068714923 Min: 0.0\n","ReLU Activation - Max: 0.6572869191054307 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021659724362629033 Min: -0.016594200332309517\n","Layer 1 - Gradient Weights Max: 0.018769235120624804 Min: -0.023195385292847863\n","Layer 0 - Gradient Weights Max: 0.0272035833727019 Min: -0.027462272078458683\n","ReLU Activation - Max: 1.5621004788192643 Min: 0.0\n","ReLU Activation - Max: 0.6102412018880805 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021027810906821355 Min: -0.02343524263646923\n","Layer 1 - Gradient Weights Max: 0.018494466336610338 Min: -0.02314182380225204\n","Layer 0 - Gradient Weights Max: 0.022999922439892087 Min: -0.026999159801551622\n","ReLU Activation - Max: 1.3589012215051162 Min: 0.0\n","ReLU Activation - Max: 0.6326555930961937 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017620021256706735 Min: -0.01592628916727021\n","Layer 1 - Gradient Weights Max: 0.019130762702928336 Min: -0.021906707425005608\n","Layer 0 - Gradient Weights Max: 0.03187605803230148 Min: -0.02941983342978407\n","ReLU Activation - Max: 1.548950427828088 Min: 0.0\n","ReLU Activation - Max: 0.5094747662047029 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01699678627294092 Min: -0.025902152576954805\n","Layer 1 - Gradient Weights Max: 0.021140711654488352 Min: -0.020114680985364154\n","Layer 0 - Gradient Weights Max: 0.03433040789965521 Min: -0.03811046898527489\n","ReLU Activation - Max: 1.576038151206973 Min: 0.0\n","ReLU Activation - Max: 0.677863778231988 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010806763737955373 Min: -0.014663681353052486\n","Layer 1 - Gradient Weights Max: 0.013636220593024342 Min: -0.017448517397724263\n","Layer 0 - Gradient Weights Max: 0.026632840843283934 Min: -0.03467170047246613\n","ReLU Activation - Max: 1.5950875729935161 Min: 0.0\n","ReLU Activation - Max: 0.6667712560158106 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016492993189811057 Min: -0.013161390412201818\n","Layer 1 - Gradient Weights Max: 0.021420915880181197 Min: -0.020509772370347606\n","Layer 0 - Gradient Weights Max: 0.03396432455166532 Min: -0.031748998460608795\n","ReLU Activation - Max: 1.3590235294744324 Min: 0.0\n","ReLU Activation - Max: 0.6650822545329388 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015906111294991924 Min: -0.015534096090920832\n","Layer 1 - Gradient Weights Max: 0.020834972180953173 Min: -0.019902831272903113\n","Layer 0 - Gradient Weights Max: 0.025899898672256987 Min: -0.03773850421744499\n","ReLU Activation - Max: 1.2839098039840238 Min: 0.0\n","ReLU Activation - Max: 0.588084200673537 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013931532689669296 Min: -0.02464718853903986\n","Layer 1 - Gradient Weights Max: 0.015815123384693538 Min: -0.028693799488660278\n","Layer 0 - Gradient Weights Max: 0.0267407833422737 Min: -0.023056395797854207\n","ReLU Activation - Max: 1.6294054176161228 Min: 0.0\n","ReLU Activation - Max: 0.5734790391631684 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016948392055419963 Min: -0.014030599060109094\n","Layer 1 - Gradient Weights Max: 0.016494095257480544 Min: -0.019108130576774136\n","Layer 0 - Gradient Weights Max: 0.0347372030078083 Min: -0.028907076298298136\n","ReLU Activation - Max: 1.3836448517349274 Min: 0.0\n","ReLU Activation - Max: 0.6298040170647883 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01630513225976041 Min: -0.013773214565518369\n","Layer 1 - Gradient Weights Max: 0.015425264883375416 Min: -0.020845474153594907\n","Layer 0 - Gradient Weights Max: 0.03467589225958534 Min: -0.026767357401877453\n","ReLU Activation - Max: 1.3525526053443042 Min: 0.0\n","ReLU Activation - Max: 0.6249060163883948 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017764220217492896 Min: -0.02389474859145035\n","Layer 1 - Gradient Weights Max: 0.019869138887648338 Min: -0.022791807337003624\n","Layer 0 - Gradient Weights Max: 0.03074531508509256 Min: -0.036621100500335665\n","ReLU Activation - Max: 1.278420238325097 Min: 0.0\n","ReLU Activation - Max: 0.7221629988278078 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01709567708949695 Min: -0.017161730645647914\n","Layer 1 - Gradient Weights Max: 0.01574043128742724 Min: -0.0322659629420984\n","Layer 0 - Gradient Weights Max: 0.02915505348963219 Min: -0.02398280622043469\n","ReLU Activation - Max: 1.5060394523428604 Min: 0.0\n","ReLU Activation - Max: 0.6069038094942891 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012760860800887501 Min: -0.017864640988885693\n","Layer 1 - Gradient Weights Max: 0.018820175248157357 Min: -0.018117093140610975\n","Layer 0 - Gradient Weights Max: 0.03329193610086938 Min: -0.027329668252130738\n","ReLU Activation - Max: 1.6102840863526888 Min: 0.0\n","ReLU Activation - Max: 0.589329474292181 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012254914572584812 Min: -0.01904305319469958\n","Layer 1 - Gradient Weights Max: 0.014663933417107242 Min: -0.020179702819667016\n","Layer 0 - Gradient Weights Max: 0.02718386547100102 Min: -0.023670281987790514\n","ReLU Activation - Max: 2.1492523103807977 Min: 0.0\n","ReLU Activation - Max: 0.6529184034656407 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015458249372321767 Min: -0.016455338431222582\n","Layer 1 - Gradient Weights Max: 0.017117591511958522 Min: -0.021127468761714577\n","Layer 0 - Gradient Weights Max: 0.03076065410140718 Min: -0.027423344823596534\n","ReLU Activation - Max: 1.1703896091503085 Min: 0.0\n","ReLU Activation - Max: 0.6241647321921859 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012146638369630288 Min: -0.019510080210092418\n","Layer 1 - Gradient Weights Max: 0.021648866229407662 Min: -0.018028311397319825\n","Layer 0 - Gradient Weights Max: 0.024989153780984062 Min: -0.02520772348775497\n","ReLU Activation - Max: 1.506936177520955 Min: 0.0\n","ReLU Activation - Max: 0.5626010619086076 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018255916336165967 Min: -0.015075343332146577\n","Layer 1 - Gradient Weights Max: 0.02257556343847472 Min: -0.018122620204836192\n","Layer 0 - Gradient Weights Max: 0.032042437952544335 Min: -0.03739441849040298\n","ReLU Activation - Max: 1.5495667327868043 Min: 0.0\n","ReLU Activation - Max: 0.7607695829676168 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015193817339170555 Min: -0.019828079220292567\n","Layer 1 - Gradient Weights Max: 0.028179558839827912 Min: -0.026169855522018802\n","Layer 0 - Gradient Weights Max: 0.035577925414519734 Min: -0.030751289988745784\n","ReLU Activation - Max: 1.2281397334433015 Min: 0.0\n","ReLU Activation - Max: 0.6272086570412116 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011614649991789855 Min: -0.01088392113433219\n","Layer 1 - Gradient Weights Max: 0.015414454228561527 Min: -0.01628822751552272\n","Layer 0 - Gradient Weights Max: 0.03881036105068217 Min: -0.03644574008839635\n","ReLU Activation - Max: 1.3077668110348977 Min: 0.0\n","ReLU Activation - Max: 0.6141928557416205 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012378482721336355 Min: -0.020036978795991943\n","Layer 1 - Gradient Weights Max: 0.01842057185901341 Min: -0.020271814031876112\n","Layer 0 - Gradient Weights Max: 0.03780153742326072 Min: -0.03174738313368342\n","ReLU Activation - Max: 1.5523717518536984 Min: 0.0\n","ReLU Activation - Max: 0.5543656958526817 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018787166676846663 Min: -0.022647024700160463\n","Layer 1 - Gradient Weights Max: 0.019077152851658147 Min: -0.02710655653206586\n","Layer 0 - Gradient Weights Max: 0.028189862705061054 Min: -0.025780072871491366\n","ReLU Activation - Max: 1.7450187207242935 Min: 0.0\n","ReLU Activation - Max: 0.6223552140515194 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016021396416559832 Min: -0.025705739184972206\n","Layer 1 - Gradient Weights Max: 0.01678952839941096 Min: -0.01650711583383681\n","Layer 0 - Gradient Weights Max: 0.02811690327068847 Min: -0.02702393372525209\n","ReLU Activation - Max: 1.4744002129918643 Min: 0.0\n","ReLU Activation - Max: 0.5869790603959704 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020236897868230533 Min: -0.027035960652947704\n","Layer 1 - Gradient Weights Max: 0.01953881512436881 Min: -0.0168392131577273\n","Layer 0 - Gradient Weights Max: 0.03286681709335337 Min: -0.02382739225679409\n","ReLU Activation - Max: 1.760169019238889 Min: 0.0\n","ReLU Activation - Max: 0.5923429644762098 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011605481170692253 Min: -0.019035369145227365\n","Layer 1 - Gradient Weights Max: 0.0177814800218411 Min: -0.019090904173795564\n","Layer 0 - Gradient Weights Max: 0.03032173308881187 Min: -0.03245788047493207\n","ReLU Activation - Max: 1.6688335717479725 Min: 0.0\n","ReLU Activation - Max: 0.5497790610210195 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022212335325570282 Min: -0.020450343119467123\n","Layer 1 - Gradient Weights Max: 0.024982558752225246 Min: -0.024168012893565363\n","Layer 0 - Gradient Weights Max: 0.03577834012369957 Min: -0.028063658706378267\n","ReLU Activation - Max: 1.5212555339171434 Min: 0.0\n","ReLU Activation - Max: 0.6042311267779675 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011933795890098308 Min: -0.009943050863013092\n","Layer 1 - Gradient Weights Max: 0.014926121677988603 Min: -0.0188126792316317\n","Layer 0 - Gradient Weights Max: 0.028498022001685476 Min: -0.026520400156645547\n","ReLU Activation - Max: 1.3740938850115387 Min: 0.0\n","ReLU Activation - Max: 0.6402187017864488 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02089841191906196 Min: -0.023814277883334505\n","Layer 1 - Gradient Weights Max: 0.012393637210154176 Min: -0.019384723331437006\n","Layer 0 - Gradient Weights Max: 0.031160381754729332 Min: -0.026102583313567247\n","ReLU Activation - Max: 1.439337199780602 Min: 0.0\n","ReLU Activation - Max: 0.5301341487251701 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02175653261472117 Min: -0.02424603273944141\n","Layer 1 - Gradient Weights Max: 0.02272292893318417 Min: -0.028695607903878248\n","Layer 0 - Gradient Weights Max: 0.029216054423551338 Min: -0.027253964263845985\n","ReLU Activation - Max: 1.4222444376005936 Min: 0.0\n","ReLU Activation - Max: 0.6199479515284068 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01293266896955656 Min: -0.019249813529918967\n","Layer 1 - Gradient Weights Max: 0.02304874588927388 Min: -0.014255641418089915\n","Layer 0 - Gradient Weights Max: 0.031078269058268444 Min: -0.029295162599168003\n","ReLU Activation - Max: 1.7025330940702028 Min: 0.0\n","ReLU Activation - Max: 0.5309294644803251 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014699531599454132 Min: -0.02144874743679936\n","Layer 1 - Gradient Weights Max: 0.033678823262411686 Min: -0.022631092816531473\n","Layer 0 - Gradient Weights Max: 0.029855146556423778 Min: -0.0361658783713213\n","ReLU Activation - Max: 1.3886509923570114 Min: 0.0\n","ReLU Activation - Max: 0.5885224218034293 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014760439427563476 Min: -0.015478208437844353\n","Layer 1 - Gradient Weights Max: 0.02004026075311978 Min: -0.021805445883440588\n","Layer 0 - Gradient Weights Max: 0.0331444499284168 Min: -0.03569002596081372\n","ReLU Activation - Max: 1.5236617246871398 Min: 0.0\n","ReLU Activation - Max: 0.6264818224507039 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013869451784067654 Min: -0.011677321352521754\n","Layer 1 - Gradient Weights Max: 0.017493856968726434 Min: -0.019937938141208495\n","Layer 0 - Gradient Weights Max: 0.026185852728729583 Min: -0.02625514537470957\n","ReLU Activation - Max: 1.4564687894298662 Min: 0.0\n","ReLU Activation - Max: 0.7056799740525008 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011352707057108485 Min: -0.012627975631315205\n","Layer 1 - Gradient Weights Max: 0.016975487800200525 Min: -0.014774405890167274\n","Layer 0 - Gradient Weights Max: 0.02480738583810439 Min: -0.02368577809371106\n","ReLU Activation - Max: 1.6253315296940283 Min: 0.0\n","ReLU Activation - Max: 0.654556874440588 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017695755901833834 Min: -0.015525550531099481\n","Layer 1 - Gradient Weights Max: 0.016555998141246323 Min: -0.032144505433558436\n","Layer 0 - Gradient Weights Max: 0.028582418997474317 Min: -0.03575009729231934\n","ReLU Activation - Max: 1.5547813589159656 Min: 0.0\n","ReLU Activation - Max: 0.5460398064844287 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014466665619126854 Min: -0.024879178970603724\n","Layer 1 - Gradient Weights Max: 0.022589084319739692 Min: -0.016076980880597593\n","Layer 0 - Gradient Weights Max: 0.027189261074797792 Min: -0.031025146816800457\n","ReLU Activation - Max: 1.3017876388754301 Min: 0.0\n","ReLU Activation - Max: 0.5512551805623278 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023209902172338957 Min: -0.014963010806042287\n","Layer 1 - Gradient Weights Max: 0.020664698435938513 Min: -0.014648272111304594\n","Layer 0 - Gradient Weights Max: 0.03158119836599275 Min: -0.023676006869193512\n","ReLU Activation - Max: 1.8100804178662766 Min: 0.0\n","ReLU Activation - Max: 0.6457911210805728 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0143312013781535 Min: -0.02437179721966618\n","Layer 1 - Gradient Weights Max: 0.019476598007367247 Min: -0.028786579495616194\n","Layer 0 - Gradient Weights Max: 0.026464603820666635 Min: -0.02558812120996344\n","ReLU Activation - Max: 1.5591345728539245 Min: 0.0\n","ReLU Activation - Max: 0.4875684806188655 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016558034228065798 Min: -0.02029834964924693\n","Layer 1 - Gradient Weights Max: 0.02200911496876974 Min: -0.020014024715284743\n","Layer 0 - Gradient Weights Max: 0.03406081774516548 Min: -0.03105200305880955\n","ReLU Activation - Max: 1.498687186842525 Min: 0.0\n","ReLU Activation - Max: 0.5897064168808661 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019999422505767313 Min: -0.015735696149284127\n","Layer 1 - Gradient Weights Max: 0.01858180176391466 Min: -0.018872191901139767\n","Layer 0 - Gradient Weights Max: 0.03803904395955789 Min: -0.029318466461100416\n","ReLU Activation - Max: 1.4063614046334347 Min: 0.0\n","ReLU Activation - Max: 0.5830318323414541 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012679198052432334 Min: -0.013120452230520786\n","Layer 1 - Gradient Weights Max: 0.020407665604058003 Min: -0.01654553165084471\n","Layer 0 - Gradient Weights Max: 0.030072655413607572 Min: -0.03855182622890682\n","ReLU Activation - Max: 1.4614483166367556 Min: 0.0\n","ReLU Activation - Max: 0.7394990715652867 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01580702852014655 Min: -0.025186847414630453\n","Layer 1 - Gradient Weights Max: 0.01909723624041256 Min: -0.015099425322732082\n","Layer 0 - Gradient Weights Max: 0.03246015731366983 Min: -0.034569749899794715\n","ReLU Activation - Max: 1.3872176826515543 Min: 0.0\n","ReLU Activation - Max: 0.5911831618706319 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02204087157798408 Min: -0.014854168077162038\n","Layer 1 - Gradient Weights Max: 0.017339810366609247 Min: -0.019014510898801025\n","Layer 0 - Gradient Weights Max: 0.026837823531423777 Min: -0.036609993244942525\n","ReLU Activation - Max: 1.4364053692424192 Min: 0.0\n","ReLU Activation - Max: 0.5866226238826264 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017431601754038517 Min: -0.016550813558845363\n","Layer 1 - Gradient Weights Max: 0.028925973564251972 Min: -0.01708124434058549\n","Layer 0 - Gradient Weights Max: 0.029835410302324745 Min: -0.03148549260733273\n","ReLU Activation - Max: 1.558327154267403 Min: 0.0\n","ReLU Activation - Max: 0.5890241124493486 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012783066264186826 Min: -0.016639041661265862\n","Layer 1 - Gradient Weights Max: 0.017758238048458757 Min: -0.016654578017974864\n","Layer 0 - Gradient Weights Max: 0.026162826640045437 Min: -0.03239194338142629\n","ReLU Activation - Max: 1.4926971854952005 Min: 0.0\n","ReLU Activation - Max: 0.5306628699740062 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01670857939581739 Min: -0.013855851635267872\n","Layer 1 - Gradient Weights Max: 0.019633784587626907 Min: -0.024697942685053924\n","Layer 0 - Gradient Weights Max: 0.02988111746781482 Min: -0.028676052189750414\n","ReLU Activation - Max: 1.566002533810641 Min: 0.0\n","ReLU Activation - Max: 0.5398490228531577 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018013734375063015 Min: -0.021162768719949623\n","Layer 1 - Gradient Weights Max: 0.01459421896332012 Min: -0.02422655961341617\n","Layer 0 - Gradient Weights Max: 0.025393769147541222 Min: -0.03016538648092724\n","ReLU Activation - Max: 1.639248604414068 Min: 0.0\n","ReLU Activation - Max: 0.6958488031338638 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012838898792137992 Min: -0.026378531265017456\n","Layer 1 - Gradient Weights Max: 0.017108484766139985 Min: -0.020573088769452245\n","Layer 0 - Gradient Weights Max: 0.033050440402980906 Min: -0.028998759400247494\n","ReLU Activation - Max: 1.8408732909186365 Min: 0.0\n","ReLU Activation - Max: 0.6031411321352844 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021759784684916916 Min: -0.01468409604191262\n","Layer 1 - Gradient Weights Max: 0.016887013533669573 Min: -0.02167066401889452\n","Layer 0 - Gradient Weights Max: 0.025170754586366688 Min: -0.026703404600641693\n","ReLU Activation - Max: 1.3598683561164204 Min: 0.0\n","ReLU Activation - Max: 0.801408572087637 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017885703264500022 Min: -0.023474673016617712\n","Layer 1 - Gradient Weights Max: 0.01943558075409892 Min: -0.02177478694025387\n","Layer 0 - Gradient Weights Max: 0.03998446079286825 Min: -0.0305723811906772\n","ReLU Activation - Max: 1.5055543645872258 Min: 0.0\n","ReLU Activation - Max: 0.6224518707229704 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015185894167502796 Min: -0.01961078701574745\n","Layer 1 - Gradient Weights Max: 0.019385586213271487 Min: -0.023493522570621404\n","Layer 0 - Gradient Weights Max: 0.026329881063744995 Min: -0.029283685465686177\n","ReLU Activation - Max: 1.4894272819450987 Min: 0.0\n","ReLU Activation - Max: 0.6481357330828267 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014247894717607257 Min: -0.02088194484303337\n","Layer 1 - Gradient Weights Max: 0.02069453205164091 Min: -0.02626483796145945\n","Layer 0 - Gradient Weights Max: 0.03263953216450628 Min: -0.029530906945686382\n","ReLU Activation - Max: 1.2750703114170083 Min: 0.0\n","ReLU Activation - Max: 0.6145027304424354 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.028500522200046938 Min: -0.022196523613579242\n","Layer 1 - Gradient Weights Max: 0.021929293906440883 Min: -0.021038549644945607\n","Layer 0 - Gradient Weights Max: 0.033146788511251954 Min: -0.031010488509326087\n","ReLU Activation - Max: 1.155123132184802 Min: 0.0\n","ReLU Activation - Max: 0.616810968301672 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013617992514283212 Min: -0.028587279237225066\n","Layer 1 - Gradient Weights Max: 0.016218081313009412 Min: -0.02873671531811256\n","Layer 0 - Gradient Weights Max: 0.03952698757837671 Min: -0.031308529830618714\n","ReLU Activation - Max: 1.9038782021292324 Min: 0.0\n","ReLU Activation - Max: 0.586126171664476 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0228242405663275 Min: -0.030392045330433107\n","Layer 1 - Gradient Weights Max: 0.018141514513221286 Min: -0.02429732727726296\n","Layer 0 - Gradient Weights Max: 0.02700083745979568 Min: -0.03198250568310561\n","ReLU Activation - Max: 1.411678104465873 Min: 0.0\n","ReLU Activation - Max: 0.5559146491750213 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012613167558213487 Min: -0.02937575869509473\n","Layer 1 - Gradient Weights Max: 0.01717553174296618 Min: -0.01964882099728285\n","Layer 0 - Gradient Weights Max: 0.025986585494652513 Min: -0.02572565841137283\n","ReLU Activation - Max: 1.3248361073884676 Min: 0.0\n","ReLU Activation - Max: 0.6788459900779656 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01647676905987943 Min: -0.034513299606204\n","Layer 1 - Gradient Weights Max: 0.017984869131927674 Min: -0.02040321568347936\n","Layer 0 - Gradient Weights Max: 0.028331270487221612 Min: -0.02388607486221611\n","ReLU Activation - Max: 1.5448440037107638 Min: 0.0\n","ReLU Activation - Max: 0.6594021882316377 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01170687848269153 Min: -0.013899674535365798\n","Layer 1 - Gradient Weights Max: 0.013763916221133194 Min: -0.01926466222181053\n","Layer 0 - Gradient Weights Max: 0.027293414467546063 Min: -0.02266218313238638\n","ReLU Activation - Max: 1.4296716712514537 Min: 0.0\n","ReLU Activation - Max: 0.5655476877579418 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013884249582126354 Min: -0.013836730240661786\n","Layer 1 - Gradient Weights Max: 0.015224924009403432 Min: -0.022240669152436\n","Layer 0 - Gradient Weights Max: 0.02630588434318088 Min: -0.0302992805426833\n","ReLU Activation - Max: 1.6900039902277462 Min: 0.0\n","ReLU Activation - Max: 0.6377268504723497 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016828843491090702 Min: -0.03067855424780759\n","Layer 1 - Gradient Weights Max: 0.020856776496027223 Min: -0.024049307204169434\n","Layer 0 - Gradient Weights Max: 0.0260743154547401 Min: -0.028992765171869907\n","ReLU Activation - Max: 1.4349642581503192 Min: 0.0\n","ReLU Activation - Max: 0.6807571566558136 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013342603022481716 Min: -0.015484568713145248\n","Layer 1 - Gradient Weights Max: 0.02144583793618066 Min: -0.019251954671459393\n","Layer 0 - Gradient Weights Max: 0.03132761431617745 Min: -0.05103943401478459\n","ReLU Activation - Max: 1.2982230700368134 Min: 0.0\n","ReLU Activation - Max: 0.5519890417132288 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011900155177368194 Min: -0.01939176176248127\n","Layer 1 - Gradient Weights Max: 0.01956410735143137 Min: -0.016144482742946147\n","Layer 0 - Gradient Weights Max: 0.038558488500061 Min: -0.028423583936551755\n","ReLU Activation - Max: 1.6137441733818538 Min: 0.0\n","ReLU Activation - Max: 0.6113747504086486 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020310593365328415 Min: -0.04540334075464348\n","Layer 1 - Gradient Weights Max: 0.027677312809547123 Min: -0.026255590666446442\n","Layer 0 - Gradient Weights Max: 0.035771545673002114 Min: -0.035987611341494447\n","ReLU Activation - Max: 1.5481798221917502 Min: 0.0\n","ReLU Activation - Max: 0.5862955370702091 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014722723005868141 Min: -0.012180296086906977\n","Layer 1 - Gradient Weights Max: 0.025667023263836297 Min: -0.01699675633032084\n","Layer 0 - Gradient Weights Max: 0.02831510144521528 Min: -0.04507031117130396\n","ReLU Activation - Max: 1.7142212397088803 Min: 0.0\n","ReLU Activation - Max: 0.6583285299628292 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011099364691557935 Min: -0.01362122829797201\n","Layer 1 - Gradient Weights Max: 0.015281216025913464 Min: -0.0174429974254466\n","Layer 0 - Gradient Weights Max: 0.02722290330752498 Min: -0.023986831515098863\n","ReLU Activation - Max: 1.3324984562515922 Min: 0.0\n","ReLU Activation - Max: 0.5663298663051298 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011620711731607879 Min: -0.025091263914639884\n","Layer 1 - Gradient Weights Max: 0.018282408190799244 Min: -0.028304116598163714\n","Layer 0 - Gradient Weights Max: 0.041253291652769364 Min: -0.03448796719225682\n","ReLU Activation - Max: 1.4249852831529473 Min: 0.0\n","ReLU Activation - Max: 0.6582242046225861 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.029902454129108694 Min: -0.01585041400819235\n","Layer 1 - Gradient Weights Max: 0.023162712029512037 Min: -0.02092563628515231\n","Layer 0 - Gradient Weights Max: 0.03195057654483943 Min: -0.030845527070965054\n","ReLU Activation - Max: 1.4575791075765472 Min: 0.0\n","ReLU Activation - Max: 0.540935229075471 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0128203637375539 Min: -0.014955836595216253\n","Layer 1 - Gradient Weights Max: 0.0195661847134998 Min: -0.01743603841432078\n","Layer 0 - Gradient Weights Max: 0.03264837082566618 Min: -0.0279746409215211\n","ReLU Activation - Max: 1.5556533865129827 Min: 0.0\n","ReLU Activation - Max: 0.5373082264408867 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019428372522614378 Min: -0.021336924429775594\n","Layer 1 - Gradient Weights Max: 0.02148441407123054 Min: -0.023019570422812077\n","Layer 0 - Gradient Weights Max: 0.02753617545803133 Min: -0.028218425443268205\n","ReLU Activation - Max: 1.403253088374012 Min: 0.0\n","ReLU Activation - Max: 0.6285933693327186 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014314255952766626 Min: -0.02037310925070495\n","Layer 1 - Gradient Weights Max: 0.019624307984410286 Min: -0.02224090255597246\n","Layer 0 - Gradient Weights Max: 0.03285982392777929 Min: -0.025427594930515707\n","ReLU Activation - Max: 1.6471430858384517 Min: 0.0\n","ReLU Activation - Max: 0.6487015550763455 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022462086409154793 Min: -0.025916497820458354\n","Layer 1 - Gradient Weights Max: 0.02005596427947544 Min: -0.02820205330680353\n","Layer 0 - Gradient Weights Max: 0.02538729325535897 Min: -0.028808220114378395\n","ReLU Activation - Max: 1.3186892669848367 Min: 0.0\n","ReLU Activation - Max: 0.7209701075239108 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01750230390389335 Min: -0.013272486227137057\n","Layer 1 - Gradient Weights Max: 0.018835282830408705 Min: -0.022008026114598653\n","Layer 0 - Gradient Weights Max: 0.03035432285465803 Min: -0.04058714925749531\n","ReLU Activation - Max: 1.67302333060386 Min: 0.0\n","ReLU Activation - Max: 0.551804548688608 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011649657145243462 Min: -0.01779498924905399\n","Layer 1 - Gradient Weights Max: 0.022408176339245467 Min: -0.021091411298151362\n","Layer 0 - Gradient Weights Max: 0.024765787964856276 Min: -0.031650196160848416\n","ReLU Activation - Max: 1.4230069477722178 Min: 0.0\n","ReLU Activation - Max: 0.5470173291169843 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020074574480776666 Min: -0.019569000821623894\n","Layer 1 - Gradient Weights Max: 0.013895512390150147 Min: -0.024514101012045982\n","Layer 0 - Gradient Weights Max: 0.03359257644516984 Min: -0.028904611342872304\n","ReLU Activation - Max: 1.5501702499726546 Min: 0.0\n","ReLU Activation - Max: 0.7497920541763182 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02097042676926414 Min: -0.013613111000301815\n","Layer 1 - Gradient Weights Max: 0.01791122508713444 Min: -0.019390038656722636\n","Layer 0 - Gradient Weights Max: 0.02680963308649196 Min: -0.035806189623717344\n","ReLU Activation - Max: 1.3570664509442425 Min: 0.0\n","ReLU Activation - Max: 0.595981028122695 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024835155255725994 Min: -0.018426060506262193\n","Layer 1 - Gradient Weights Max: 0.022669064729141087 Min: -0.031968205932768515\n","Layer 0 - Gradient Weights Max: 0.03805969250467188 Min: -0.026637201913942896\n","ReLU Activation - Max: 1.6805475234253089 Min: 0.0\n","ReLU Activation - Max: 0.6566145858523451 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.03591614817228584 Min: -0.021129247694597605\n","Layer 1 - Gradient Weights Max: 0.03130213127009493 Min: -0.021423062484641757\n","Layer 0 - Gradient Weights Max: 0.032100460900987914 Min: -0.03565389660814168\n","ReLU Activation - Max: 1.6572606217420631 Min: 0.0\n","ReLU Activation - Max: 0.6140016102697842 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01128798324840657 Min: -0.013925380475414452\n","Layer 1 - Gradient Weights Max: 0.013341157280702598 Min: -0.019892124339465363\n","Layer 0 - Gradient Weights Max: 0.024707157541235546 Min: -0.027773663110405473\n","ReLU Activation - Max: 1.2059205754855031 Min: 0.0\n","ReLU Activation - Max: 0.6518269743276982 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014827218446529839 Min: -0.022561095341090198\n","Layer 1 - Gradient Weights Max: 0.019466163529118664 Min: -0.019063510211707636\n","Layer 0 - Gradient Weights Max: 0.043368560899875765 Min: -0.02944261738772894\n","ReLU Activation - Max: 1.7167246050836613 Min: 0.0\n","ReLU Activation - Max: 0.6903154423936021 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016874660253799773 Min: -0.020046337719510955\n","Layer 1 - Gradient Weights Max: 0.018956435469410962 Min: -0.03373123310789363\n","Layer 0 - Gradient Weights Max: 0.02713252801249584 Min: -0.032699628719705415\n","ReLU Activation - Max: 1.2761508038351255 Min: 0.0\n","ReLU Activation - Max: 0.6415342243584011 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011326150869090045 Min: -0.028446316789665818\n","Layer 1 - Gradient Weights Max: 0.01796105814343864 Min: -0.020054530971157503\n","Layer 0 - Gradient Weights Max: 0.03651950256688602 Min: -0.03625251988821788\n","ReLU Activation - Max: 1.3248033327678872 Min: 0.0\n","ReLU Activation - Max: 0.6396419213155318 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022193041510501728 Min: -0.013297213135100391\n","Layer 1 - Gradient Weights Max: 0.018468681275302667 Min: -0.015306878443497752\n","Layer 0 - Gradient Weights Max: 0.02871647599509233 Min: -0.021967036166254356\n","ReLU Activation - Max: 1.3461963127528795 Min: 0.0\n","ReLU Activation - Max: 0.5412338992541758 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015787571882820615 Min: -0.015506083310851121\n","Layer 1 - Gradient Weights Max: 0.02164313700143977 Min: -0.024498237599646745\n","Layer 0 - Gradient Weights Max: 0.030473227034070877 Min: -0.03216845888026289\n","ReLU Activation - Max: 1.7314848091311943 Min: 0.0\n","ReLU Activation - Max: 0.5720723714042847 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011470993205960631 Min: -0.01748753230641576\n","Layer 1 - Gradient Weights Max: 0.018586103036414562 Min: -0.016450932378073355\n","Layer 0 - Gradient Weights Max: 0.02540291438064953 Min: -0.03243961707947246\n","ReLU Activation - Max: 1.5264549198661976 Min: 0.0\n","ReLU Activation - Max: 0.6704460824715781 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02161012535784676 Min: -0.015948748186222952\n","Layer 1 - Gradient Weights Max: 0.017320976320848197 Min: -0.022743903651670925\n","Layer 0 - Gradient Weights Max: 0.03344319509673792 Min: -0.026156073659225638\n","ReLU Activation - Max: 1.241580486504959 Min: 0.0\n","ReLU Activation - Max: 0.5147403913249176 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019424925878961455 Min: -0.03415466397784502\n","Layer 1 - Gradient Weights Max: 0.0252376684983291 Min: -0.02404769845818408\n","Layer 0 - Gradient Weights Max: 0.02878474944690744 Min: -0.037019129167911544\n","ReLU Activation - Max: 2.0280917782998023 Min: 0.0\n","ReLU Activation - Max: 0.6180788896287924 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024362884621226582 Min: -0.015353228347423238\n","Layer 1 - Gradient Weights Max: 0.02149458114168404 Min: -0.026869034209342647\n","Layer 0 - Gradient Weights Max: 0.029138632630000008 Min: -0.029964109600831054\n","ReLU Activation - Max: 1.4032080144830275 Min: 0.0\n","ReLU Activation - Max: 0.592044655179981 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01771302562499212 Min: -0.01438271962143181\n","Layer 1 - Gradient Weights Max: 0.021543319134700683 Min: -0.020681727758405124\n","Layer 0 - Gradient Weights Max: 0.02525669383128236 Min: -0.026221131861100706\n","ReLU Activation - Max: 1.4081795171255256 Min: 0.0\n","ReLU Activation - Max: 0.6019944459676352 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014628569413134222 Min: -0.018940534387571734\n","Layer 1 - Gradient Weights Max: 0.01939547808711536 Min: -0.02772529204104755\n","Layer 0 - Gradient Weights Max: 0.028560313193912792 Min: -0.026634731274149098\n","ReLU Activation - Max: 1.5895750092553924 Min: 0.0\n","ReLU Activation - Max: 0.5819300281144093 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01238799358617749 Min: -0.03600207726259315\n","Layer 1 - Gradient Weights Max: 0.016338039062723023 Min: -0.025816887353651355\n","Layer 0 - Gradient Weights Max: 0.04241057259570325 Min: -0.0411160634074641\n","ReLU Activation - Max: 1.510811277179824 Min: 0.0\n","ReLU Activation - Max: 0.6425131434275503 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01672152522867941 Min: -0.02013108047604116\n","Layer 1 - Gradient Weights Max: 0.02173996998222421 Min: -0.021004275830051704\n","Layer 0 - Gradient Weights Max: 0.03231905133172955 Min: -0.03241442343406933\n","ReLU Activation - Max: 1.4905811240983688 Min: 0.0\n","ReLU Activation - Max: 0.6416737189707042 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01698301039325287 Min: -0.01807279106867039\n","Layer 1 - Gradient Weights Max: 0.015794269561048332 Min: -0.024995675608474346\n","Layer 0 - Gradient Weights Max: 0.03238022240868338 Min: -0.022646802874981883\n","ReLU Activation - Max: 1.302586632162922 Min: 0.0\n","ReLU Activation - Max: 0.7192335623164395 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012435138523986849 Min: -0.01808545062089735\n","Layer 1 - Gradient Weights Max: 0.017668291910612716 Min: -0.01869684420541183\n","Layer 0 - Gradient Weights Max: 0.028534266869678256 Min: -0.03294307973922946\n","ReLU Activation - Max: 1.6674432699925192 Min: 0.0\n","ReLU Activation - Max: 0.5549339833540881 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017496966664618557 Min: -0.013596131244844229\n","Layer 1 - Gradient Weights Max: 0.01947753962598904 Min: -0.016687466270345577\n","Layer 0 - Gradient Weights Max: 0.026358805120890203 Min: -0.026938847267410478\n","ReLU Activation - Max: 1.6075909227697711 Min: 0.0\n","ReLU Activation - Max: 0.6193041001037782 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01694892684742612 Min: -0.02760511458813501\n","Layer 1 - Gradient Weights Max: 0.017056796760075606 Min: -0.02012908005421387\n","Layer 0 - Gradient Weights Max: 0.026688948891335475 Min: -0.03237627688509286\n","ReLU Activation - Max: 1.5874782469011994 Min: 0.0\n","ReLU Activation - Max: 0.6094790085901207 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018963074780350895 Min: -0.015646670854499393\n","Layer 1 - Gradient Weights Max: 0.014907088628806535 Min: -0.021934105679839958\n","Layer 0 - Gradient Weights Max: 0.02808671202550342 Min: -0.032909341641140163\n","ReLU Activation - Max: 1.3983582466987308 Min: 0.0\n","ReLU Activation - Max: 0.5649906683314083 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0166298100835946 Min: -0.01451737012936016\n","Layer 1 - Gradient Weights Max: 0.018514340435432874 Min: -0.01904116204072244\n","Layer 0 - Gradient Weights Max: 0.03496642227352854 Min: -0.032968538583348944\n","ReLU Activation - Max: 1.6471001371110168 Min: 0.0\n","ReLU Activation - Max: 0.6522975471773984 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020340572112844145 Min: -0.015734017163209605\n","Layer 1 - Gradient Weights Max: 0.017218432622480878 Min: -0.014556920442805559\n","Layer 0 - Gradient Weights Max: 0.033270133183435344 Min: -0.028301868168992324\n","ReLU Activation - Max: 1.2359509765677827 Min: 0.0\n","ReLU Activation - Max: 0.5597942360860313 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015348989044131074 Min: -0.016230178132432396\n","Layer 1 - Gradient Weights Max: 0.016197800315912735 Min: -0.021853172065875212\n","Layer 0 - Gradient Weights Max: 0.02407330589181913 Min: -0.025985570260602726\n","ReLU Activation - Max: 1.605130692174396 Min: 0.0\n","ReLU Activation - Max: 0.5331151103834733 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01810123886405008 Min: -0.02033579125109645\n","Layer 1 - Gradient Weights Max: 0.020130801605375424 Min: -0.029655724847517072\n","Layer 0 - Gradient Weights Max: 0.02853945316805699 Min: -0.02646005936554052\n","ReLU Activation - Max: 1.6576577351696202 Min: 0.0\n","ReLU Activation - Max: 0.7923274836691584 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023118368705048117 Min: -0.024643764590517644\n","Layer 1 - Gradient Weights Max: 0.02212165838171818 Min: -0.027981157673840323\n","Layer 0 - Gradient Weights Max: 0.034478943013424704 Min: -0.024317583572614078\n","ReLU Activation - Max: 1.8281944748599899 Min: 0.0\n","ReLU Activation - Max: 0.6746406469449403 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022448294210810862 Min: -0.01660545341746342\n","Layer 1 - Gradient Weights Max: 0.018262009417612433 Min: -0.03352872050473705\n","Layer 0 - Gradient Weights Max: 0.029218589441423973 Min: -0.02843792441368864\n","ReLU Activation - Max: 1.2763295815642024 Min: 0.0\n","ReLU Activation - Max: 0.5842387454001648 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020729412881824144 Min: -0.016184060510040746\n","Layer 1 - Gradient Weights Max: 0.019541390914595592 Min: -0.014965851693209895\n","Layer 0 - Gradient Weights Max: 0.026565532925537774 Min: -0.035658431527159855\n","ReLU Activation - Max: 1.2678958181339863 Min: 0.0\n","ReLU Activation - Max: 0.6181722696158741 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008749871528735577 Min: -0.01117055571010317\n","Layer 1 - Gradient Weights Max: 0.015666906717347516 Min: -0.01865869692663085\n","Layer 0 - Gradient Weights Max: 0.030650979698409152 Min: -0.03465310306181298\n","ReLU Activation - Max: 1.2420281947964658 Min: 0.0\n","ReLU Activation - Max: 0.5649534928548259 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01985014929086568 Min: -0.018684990904620686\n","Layer 1 - Gradient Weights Max: 0.02233712521770369 Min: -0.019333264899503825\n","Layer 0 - Gradient Weights Max: 0.02886347485646215 Min: -0.033639998026324815\n","ReLU Activation - Max: 1.5794299120900661 Min: 0.0\n","ReLU Activation - Max: 0.6557525522119307 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02365685211788106 Min: -0.014534870987519937\n","Layer 1 - Gradient Weights Max: 0.01790255861003839 Min: -0.02294174988267841\n","Layer 0 - Gradient Weights Max: 0.03505707599514701 Min: -0.030816617850487258\n","ReLU Activation - Max: 1.3179407460082566 Min: 0.0\n","ReLU Activation - Max: 0.5161474313845642 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009802351659880942 Min: -0.01216457250982536\n","Layer 1 - Gradient Weights Max: 0.012612228579847486 Min: -0.017381041987616103\n","Layer 0 - Gradient Weights Max: 0.025260673658950145 Min: -0.02888988416418923\n","ReLU Activation - Max: 1.4483041553638483 Min: 0.0\n","ReLU Activation - Max: 0.5886027432640595 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009616770680667583 Min: -0.01417526430411684\n","Layer 1 - Gradient Weights Max: 0.016566969831282582 Min: -0.017834309253787164\n","Layer 0 - Gradient Weights Max: 0.03007359867928498 Min: -0.029436200929031247\n","ReLU Activation - Max: 1.7289696418152118 Min: 0.0\n","ReLU Activation - Max: 0.5871645196315151 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01591796607674488 Min: -0.020231359534729203\n","Layer 1 - Gradient Weights Max: 0.017465905239891173 Min: -0.021750634898101803\n","Layer 0 - Gradient Weights Max: 0.027032846024541445 Min: -0.02757378352039791\n","ReLU Activation - Max: 1.6129415173033261 Min: 0.0\n","ReLU Activation - Max: 0.6445897813572853 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014256718783403982 Min: -0.01652769646560038\n","Layer 1 - Gradient Weights Max: 0.01742704408852284 Min: -0.026143338985266072\n","Layer 0 - Gradient Weights Max: 0.03307551348867539 Min: -0.041830981264925085\n","ReLU Activation - Max: 1.2812162427213951 Min: 0.0\n","ReLU Activation - Max: 0.5025054169790315 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010110367413086994 Min: -0.010822460250126151\n","Layer 1 - Gradient Weights Max: 0.015498139864497157 Min: -0.012765023247462653\n","Layer 0 - Gradient Weights Max: 0.02521606690494845 Min: -0.02960290109124895\n","ReLU Activation - Max: 1.4008897136728695 Min: 0.0\n","ReLU Activation - Max: 0.5579736006035706 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.027026947255086354 Min: -0.024570639873120855\n","Layer 1 - Gradient Weights Max: 0.023630078706521892 Min: -0.020041011940037778\n","Layer 0 - Gradient Weights Max: 0.03253412108052552 Min: -0.03245910448339953\n","ReLU Activation - Max: 1.2904844759702465 Min: 0.0\n","ReLU Activation - Max: 0.5945216229935661 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015901154611818694 Min: -0.020704621897272395\n","Layer 1 - Gradient Weights Max: 0.02100009832697728 Min: -0.023947205624804062\n","Layer 0 - Gradient Weights Max: 0.025603684018776583 Min: -0.025738744429893188\n","ReLU Activation - Max: 1.3650682764226374 Min: 0.0\n","ReLU Activation - Max: 0.6425044181102029 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019108285273100507 Min: -0.021569157726933724\n","Layer 1 - Gradient Weights Max: 0.022173118934246495 Min: -0.014444812253533\n","Layer 0 - Gradient Weights Max: 0.027873436949267978 Min: -0.03220265117848076\n","ReLU Activation - Max: 1.3742431771709678 Min: 0.0\n","ReLU Activation - Max: 0.628285823311578 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010208047237247539 Min: -0.013555154077601172\n","Layer 1 - Gradient Weights Max: 0.019646806550522034 Min: -0.02450672244566228\n","Layer 0 - Gradient Weights Max: 0.02598706252276948 Min: -0.03400911650188372\n","ReLU Activation - Max: 1.287471918188748 Min: 0.0\n","ReLU Activation - Max: 0.5982787636134876 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015365905047146595 Min: -0.022802720396020337\n","Layer 1 - Gradient Weights Max: 0.020675993253867573 Min: -0.02065097569485149\n","Layer 0 - Gradient Weights Max: 0.03492694312653708 Min: -0.04422599591937086\n","ReLU Activation - Max: 1.5467814414348515 Min: 0.0\n","ReLU Activation - Max: 0.6030703586518268 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016730001484564425 Min: -0.01968942468031824\n","Layer 1 - Gradient Weights Max: 0.014790451773794748 Min: -0.017251711911682108\n","Layer 0 - Gradient Weights Max: 0.02938737939564402 Min: -0.03547138501221701\n","ReLU Activation - Max: 1.5466634607962273 Min: 0.0\n","ReLU Activation - Max: 0.5813623035115665 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02739825525023378 Min: -0.020945860501580472\n","Layer 1 - Gradient Weights Max: 0.017629397323815642 Min: -0.02423947534393618\n","Layer 0 - Gradient Weights Max: 0.03552391412703886 Min: -0.026261063768888707\n","ReLU Activation - Max: 1.709216560564027 Min: 0.0\n","ReLU Activation - Max: 0.6978623133246502 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01314855733408349 Min: -0.013512798547907033\n","Layer 1 - Gradient Weights Max: 0.021339655680788057 Min: -0.020266822286048584\n","Layer 0 - Gradient Weights Max: 0.038330250019439516 Min: -0.029797379580166882\n","ReLU Activation - Max: 1.9718147460388478 Min: 0.0\n","ReLU Activation - Max: 0.5592525874251439 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01943737355471264 Min: -0.030777997613968752\n","Layer 1 - Gradient Weights Max: 0.02188749200167664 Min: -0.020350964576974672\n","Layer 0 - Gradient Weights Max: 0.030336711649979784 Min: -0.02384221455268254\n","ReLU Activation - Max: 1.6749171240522265 Min: 0.0\n","ReLU Activation - Max: 0.5320479990508991 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014846321363637284 Min: -0.010981949261944469\n","Layer 1 - Gradient Weights Max: 0.01824499408580763 Min: -0.016559595795517454\n","Layer 0 - Gradient Weights Max: 0.028597507248013097 Min: -0.030398043030622324\n","ReLU Activation - Max: 1.429328781448244 Min: 0.0\n","ReLU Activation - Max: 0.6356218457419702 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013324361207820086 Min: -0.011483083167767477\n","Layer 1 - Gradient Weights Max: 0.017522294938394422 Min: -0.02595704325597343\n","Layer 0 - Gradient Weights Max: 0.03253362889107959 Min: -0.02940867782952621\n","ReLU Activation - Max: 1.5011288413394135 Min: 0.0\n","ReLU Activation - Max: 0.5395831473266511 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016179309434773904 Min: -0.015840906289065808\n","Layer 1 - Gradient Weights Max: 0.01726179228926168 Min: -0.018052089389891845\n","Layer 0 - Gradient Weights Max: 0.02937951125853046 Min: -0.02793798939669425\n","ReLU Activation - Max: 1.762852474353667 Min: 0.0\n","ReLU Activation - Max: 0.6050360325388461 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02023877074543874 Min: -0.018741470704155164\n","Layer 1 - Gradient Weights Max: 0.021000794010353842 Min: -0.016826548590263946\n","Layer 0 - Gradient Weights Max: 0.037008351018449105 Min: -0.03476628554323168\n","ReLU Activation - Max: 1.6402395809284809 Min: 0.0\n","ReLU Activation - Max: 0.515836807221218 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017613569122753088 Min: -0.014729396461396484\n","Layer 1 - Gradient Weights Max: 0.01984215875509123 Min: -0.019802010230850212\n","Layer 0 - Gradient Weights Max: 0.026601815553577487 Min: -0.02849660262492351\n","ReLU Activation - Max: 1.6446839069089036 Min: 0.0\n","ReLU Activation - Max: 0.6819336029321355 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014199841965507038 Min: -0.022627683252649206\n","Layer 1 - Gradient Weights Max: 0.014915189800517061 Min: -0.0320141736726185\n","Layer 0 - Gradient Weights Max: 0.02364634347555621 Min: -0.02384757090041895\n","ReLU Activation - Max: 1.199316367818119 Min: 0.0\n","ReLU Activation - Max: 0.5453187345927402 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014379056723424695 Min: -0.019433824779560127\n","Layer 1 - Gradient Weights Max: 0.015126838336628212 Min: -0.019477978580365367\n","Layer 0 - Gradient Weights Max: 0.023700201694068522 Min: -0.025657757094020878\n","ReLU Activation - Max: 1.4697442718761198 Min: 0.0\n","ReLU Activation - Max: 0.6526167648383006 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014986731906173667 Min: -0.016076278427054426\n","Layer 1 - Gradient Weights Max: 0.017947990960424282 Min: -0.017791188510675994\n","Layer 0 - Gradient Weights Max: 0.028995495741157402 Min: -0.03295147269084577\n","ReLU Activation - Max: 1.6362206577984486 Min: 0.0\n","ReLU Activation - Max: 0.6795434653476197 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013594317174999931 Min: -0.014655282165865547\n","Layer 1 - Gradient Weights Max: 0.019626725670104966 Min: -0.03282954670481308\n","Layer 0 - Gradient Weights Max: 0.027734178247365692 Min: -0.04274606765714669\n","ReLU Activation - Max: 1.541180894142751 Min: 0.0\n","ReLU Activation - Max: 0.5984332392246072 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01300545573913727 Min: -0.02947156134222832\n","Layer 1 - Gradient Weights Max: 0.022822410527956014 Min: -0.019198907492586428\n","Layer 0 - Gradient Weights Max: 0.026955070724007043 Min: -0.02825795760585256\n","ReLU Activation - Max: 1.6002869621244282 Min: 0.0\n","ReLU Activation - Max: 0.6186651394777438 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013511713250778394 Min: -0.025836329062701738\n","Layer 1 - Gradient Weights Max: 0.017561372256357264 Min: -0.024222601190903567\n","Layer 0 - Gradient Weights Max: 0.026416886675241637 Min: -0.026573974365542277\n","ReLU Activation - Max: 1.3571322543170339 Min: 0.0\n","ReLU Activation - Max: 0.6394636496011818 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02036540600471759 Min: -0.01619029786643768\n","Layer 1 - Gradient Weights Max: 0.02473401240749716 Min: -0.01848270145870032\n","Layer 0 - Gradient Weights Max: 0.031710783397073866 Min: -0.027704782571820134\n","ReLU Activation - Max: 1.6986919395670805 Min: 0.0\n","ReLU Activation - Max: 0.5873499455388675 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02071023423234703 Min: -0.02127686968599006\n","Layer 1 - Gradient Weights Max: 0.026171296165685122 Min: -0.020988883761187956\n","Layer 0 - Gradient Weights Max: 0.031479612323915974 Min: -0.02657069133668405\n","ReLU Activation - Max: 1.8246109527988938 Min: 0.0\n","ReLU Activation - Max: 0.6836711988865466 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020402703115640592 Min: -0.01972349458314638\n","Layer 1 - Gradient Weights Max: 0.016908951233826987 Min: -0.018797823449124753\n","Layer 0 - Gradient Weights Max: 0.03205805353682383 Min: -0.0254629779206386\n","ReLU Activation - Max: 1.3139212643750957 Min: 0.0\n","ReLU Activation - Max: 0.613767802264419 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012972134056335698 Min: -0.02127508484762617\n","Layer 1 - Gradient Weights Max: 0.014690137037623563 Min: -0.01848517607832409\n","Layer 0 - Gradient Weights Max: 0.023710022187289882 Min: -0.0253006588229161\n","ReLU Activation - Max: 2.0248249727750673 Min: 0.0\n","ReLU Activation - Max: 0.6190242389942755 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020492932147657586 Min: -0.01618563609534878\n","Layer 1 - Gradient Weights Max: 0.020657642575239905 Min: -0.024643998298989722\n","Layer 0 - Gradient Weights Max: 0.03537210195772184 Min: -0.031051075508246852\n","ReLU Activation - Max: 1.2324231960843568 Min: 0.0\n","ReLU Activation - Max: 0.5246422908008745 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01581592908717798 Min: -0.01229420359353876\n","Layer 1 - Gradient Weights Max: 0.01791056551331915 Min: -0.022309585800500693\n","Layer 0 - Gradient Weights Max: 0.026002588928465947 Min: -0.02690006895022538\n","ReLU Activation - Max: 1.539704153060561 Min: 0.0\n","ReLU Activation - Max: 0.632631751468484 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009875208389572464 Min: -0.020759461510352725\n","Layer 1 - Gradient Weights Max: 0.02098387547060669 Min: -0.019237046186254738\n","Layer 0 - Gradient Weights Max: 0.027573680155431844 Min: -0.0359096031698478\n","ReLU Activation - Max: 1.5178713269360997 Min: 0.0\n","ReLU Activation - Max: 0.5629639363140256 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014460585211804456 Min: -0.026364353081263444\n","Layer 1 - Gradient Weights Max: 0.027379681239979727 Min: -0.02104807662008977\n","Layer 0 - Gradient Weights Max: 0.03607388105525032 Min: -0.029841545432556765\n","ReLU Activation - Max: 1.766590026966328 Min: 0.0\n","ReLU Activation - Max: 0.5793833964968444 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01638189477401168 Min: -0.02061634449535906\n","Layer 1 - Gradient Weights Max: 0.016002362311629187 Min: -0.02536240626467422\n","Layer 0 - Gradient Weights Max: 0.026963491240547328 Min: -0.02961407559007578\n","ReLU Activation - Max: 1.433177677419513 Min: 0.0\n","ReLU Activation - Max: 0.5852655882898504 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012327008932859816 Min: -0.024022521268541146\n","Layer 1 - Gradient Weights Max: 0.023987701983891403 Min: -0.02364631647958631\n","Layer 0 - Gradient Weights Max: 0.026714106999494402 Min: -0.02558724206782608\n","ReLU Activation - Max: 1.4546165751262539 Min: 0.0\n","ReLU Activation - Max: 0.7347072968381413 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01366250506335294 Min: -0.019762285860240047\n","Layer 1 - Gradient Weights Max: 0.016154533648302622 Min: -0.025368409877667863\n","Layer 0 - Gradient Weights Max: 0.0231940165923707 Min: -0.023268537686981398\n","ReLU Activation - Max: 1.6332358428232991 Min: 0.0\n","ReLU Activation - Max: 0.6310697157292642 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01674725576155345 Min: -0.025759060262810128\n","Layer 1 - Gradient Weights Max: 0.027033703651464222 Min: -0.017986195675386063\n","Layer 0 - Gradient Weights Max: 0.028754764564870406 Min: -0.03182232165073583\n","ReLU Activation - Max: 1.5075876558540837 Min: 0.0\n","ReLU Activation - Max: 0.5730275736714974 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014340042254324377 Min: -0.015493322349705642\n","Layer 1 - Gradient Weights Max: 0.02421538277026678 Min: -0.019971908947859576\n","Layer 0 - Gradient Weights Max: 0.026303049462114727 Min: -0.03330637429868983\n","ReLU Activation - Max: 1.3932031188507192 Min: 0.0\n","ReLU Activation - Max: 0.5670689823362636 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008523384569603425 Min: -0.011875837905224734\n","Layer 1 - Gradient Weights Max: 0.012942905784551763 Min: -0.017609864584644044\n","Layer 0 - Gradient Weights Max: 0.03115777808699054 Min: -0.0267484951757712\n","ReLU Activation - Max: 1.6548163886632226 Min: 0.0\n","ReLU Activation - Max: 0.6729661758584965 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.026870177997308713 Min: -0.01366024121604584\n","Layer 1 - Gradient Weights Max: 0.015754219784915417 Min: -0.01769084813221152\n","Layer 0 - Gradient Weights Max: 0.022095107341935157 Min: -0.0291258961704495\n","ReLU Activation - Max: 1.3700732359855563 Min: 0.0\n","ReLU Activation - Max: 0.6698375181657771 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013878278505671039 Min: -0.01606903977900491\n","Layer 1 - Gradient Weights Max: 0.021976435300200138 Min: -0.018233355071734805\n","Layer 0 - Gradient Weights Max: 0.034357054033991793 Min: -0.03575207294111436\n","ReLU Activation - Max: 1.3516377887562063 Min: 0.0\n","ReLU Activation - Max: 0.7108707522753634 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014213172774571329 Min: -0.016061376688156413\n","Layer 1 - Gradient Weights Max: 0.020253615727427295 Min: -0.017078947494523655\n","Layer 0 - Gradient Weights Max: 0.029005260721594095 Min: -0.02758423504527507\n","ReLU Activation - Max: 1.2854361603270335 Min: 0.0\n","ReLU Activation - Max: 0.5689146158372769 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016890309042961692 Min: -0.03005807945478649\n","Layer 1 - Gradient Weights Max: 0.018258475761940167 Min: -0.022902723144429045\n","Layer 0 - Gradient Weights Max: 0.02609087340417577 Min: -0.037924283715877846\n","ReLU Activation - Max: 1.6309519438139 Min: 0.0\n","ReLU Activation - Max: 0.6695653598936389 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015141805935050627 Min: -0.01570417575272265\n","Layer 1 - Gradient Weights Max: 0.015852024135916604 Min: -0.024666464042882343\n","Layer 0 - Gradient Weights Max: 0.029704048645161747 Min: -0.026333490919146923\n","ReLU Activation - Max: 2.136189774883444 Min: 0.0\n","ReLU Activation - Max: 0.5926840676834267 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01967480525189357 Min: -0.019243032754275576\n","Layer 1 - Gradient Weights Max: 0.019739085400236854 Min: -0.020919347532212577\n","Layer 0 - Gradient Weights Max: 0.025548746216567966 Min: -0.029097180061589782\n","ReLU Activation - Max: 1.3269448155150176 Min: 0.0\n","ReLU Activation - Max: 0.5746937386252658 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01482546531803706 Min: -0.02200521479801683\n","Layer 1 - Gradient Weights Max: 0.01594854003075927 Min: -0.02993994581135995\n","Layer 0 - Gradient Weights Max: 0.024638398257320095 Min: -0.02815475723469792\n","ReLU Activation - Max: 1.4200414064981666 Min: 0.0\n","ReLU Activation - Max: 0.584174660095664 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016368323183959678 Min: -0.01824630769137947\n","Layer 1 - Gradient Weights Max: 0.02386382299581168 Min: -0.018295405903535567\n","Layer 0 - Gradient Weights Max: 0.026071865584082688 Min: -0.03585016962512497\n","ReLU Activation - Max: 1.4897398937123596 Min: 0.0\n","ReLU Activation - Max: 0.6940706721258499 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011853450984561833 Min: -0.015247313760563807\n","Layer 1 - Gradient Weights Max: 0.01735079467160746 Min: -0.017157042534945682\n","Layer 0 - Gradient Weights Max: 0.02606034235367139 Min: -0.029032345628399236\n","ReLU Activation - Max: 1.5341393049096803 Min: 0.0\n","ReLU Activation - Max: 0.6191039674760199 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013882327991820017 Min: -0.023230181294824043\n","Layer 1 - Gradient Weights Max: 0.017341093054156178 Min: -0.026752778159220978\n","Layer 0 - Gradient Weights Max: 0.026297128590248613 Min: -0.025813080080202482\n","ReLU Activation - Max: 1.9248093716060461 Min: 0.0\n","ReLU Activation - Max: 0.6390178484874445 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013456491591298936 Min: -0.012342306638776338\n","Layer 1 - Gradient Weights Max: 0.017045725576917883 Min: -0.014814232272512448\n","Layer 0 - Gradient Weights Max: 0.03575735718589885 Min: -0.03168955591866054\n","ReLU Activation - Max: 1.401008521113395 Min: 0.0\n","ReLU Activation - Max: 0.8136188065015469 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016274736681203238 Min: -0.02277810878243242\n","Layer 1 - Gradient Weights Max: 0.014724853762695465 Min: -0.0249443705614934\n","Layer 0 - Gradient Weights Max: 0.026007769313434646 Min: -0.037947602967042\n","ReLU Activation - Max: 1.5240290172321276 Min: 0.0\n","ReLU Activation - Max: 0.57928234457586 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015163361581081234 Min: -0.022256206432364353\n","Layer 1 - Gradient Weights Max: 0.018835443077468993 Min: -0.01941063699062896\n","Layer 0 - Gradient Weights Max: 0.026259417864261468 Min: -0.03057489774618013\n","ReLU Activation - Max: 1.4669637646257194 Min: 0.0\n","ReLU Activation - Max: 0.6875089981514213 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010008993471932251 Min: -0.017860621278551106\n","Layer 1 - Gradient Weights Max: 0.014921248527707693 Min: -0.02113873903189425\n","Layer 0 - Gradient Weights Max: 0.03078186371005358 Min: -0.028193441786346654\n","ReLU Activation - Max: 1.3683984234347226 Min: 0.0\n","ReLU Activation - Max: 0.6683757336968901 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013424270083706525 Min: -0.022507453548145557\n","Layer 1 - Gradient Weights Max: 0.02168965215779778 Min: -0.021696029516138665\n","Layer 0 - Gradient Weights Max: 0.039553258053659106 Min: -0.033785445310495855\n","ReLU Activation - Max: 1.4046605378159753 Min: 0.0\n","ReLU Activation - Max: 0.5453526248879892 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01597972280769954 Min: -0.022327627142923457\n","Layer 1 - Gradient Weights Max: 0.017107089901787425 Min: -0.02440977643534869\n","Layer 0 - Gradient Weights Max: 0.03311060880818727 Min: -0.028480057397916873\n","ReLU Activation - Max: 1.4866091950750888 Min: 0.0\n","ReLU Activation - Max: 0.5910461270238201 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018459510850884812 Min: -0.02547124286071671\n","Layer 1 - Gradient Weights Max: 0.016986263026522747 Min: -0.020757327853279557\n","Layer 0 - Gradient Weights Max: 0.03316461486466352 Min: -0.029226990858075654\n","ReLU Activation - Max: 1.680516721478163 Min: 0.0\n","ReLU Activation - Max: 0.7465316650257067 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01866343050941294 Min: -0.02282876031087506\n","Layer 1 - Gradient Weights Max: 0.01983318325510103 Min: -0.029159649487562077\n","Layer 0 - Gradient Weights Max: 0.029347967050980986 Min: -0.03587043142539579\n","ReLU Activation - Max: 1.6253826687457267 Min: 0.0\n","ReLU Activation - Max: 0.6491083171532745 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014250438651673235 Min: -0.012758622127447081\n","Layer 1 - Gradient Weights Max: 0.019028328435349448 Min: -0.019756002638846432\n","Layer 0 - Gradient Weights Max: 0.024467876678219355 Min: -0.037271566150639285\n","ReLU Activation - Max: 1.3694958049316166 Min: 0.0\n","ReLU Activation - Max: 0.547799707679823 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015469249118605227 Min: -0.019711223155685235\n","Layer 1 - Gradient Weights Max: 0.016044896022254555 Min: -0.021550583806666274\n","Layer 0 - Gradient Weights Max: 0.026789428115472066 Min: -0.031472217086553816\n","ReLU Activation - Max: 1.5475444002699537 Min: 0.0\n","ReLU Activation - Max: 0.7277596622827629 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015629910915261388 Min: -0.016324860661292396\n","Layer 1 - Gradient Weights Max: 0.026771241741156828 Min: -0.022766484082905384\n","Layer 0 - Gradient Weights Max: 0.03445253848584455 Min: -0.030181376611389525\n","ReLU Activation - Max: 1.606987884616459 Min: 0.0\n","ReLU Activation - Max: 0.5638212059648121 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023539466141597826 Min: -0.046244085508581674\n","Layer 1 - Gradient Weights Max: 0.019645635317329886 Min: -0.025840298021708934\n","Layer 0 - Gradient Weights Max: 0.030003353063613313 Min: -0.03129673741087316\n","ReLU Activation - Max: 1.4780097155052738 Min: 0.0\n","ReLU Activation - Max: 0.5656214236043049 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0094527408490683 Min: -0.014351064483322944\n","Layer 1 - Gradient Weights Max: 0.024482525345640698 Min: -0.01777673626373846\n","Layer 0 - Gradient Weights Max: 0.0258325552923993 Min: -0.028748278102753336\n","ReLU Activation - Max: 1.3367887049214444 Min: 0.0\n","ReLU Activation - Max: 0.5380605690744898 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016369752532220064 Min: -0.0161176227528805\n","Layer 1 - Gradient Weights Max: 0.018761445297407208 Min: -0.028566790039224563\n","Layer 0 - Gradient Weights Max: 0.03443046365221376 Min: -0.0375054922131496\n","ReLU Activation - Max: 1.2630729550157824 Min: 0.0\n","ReLU Activation - Max: 0.5178648282923649 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0153182780052684 Min: -0.03251882893736194\n","Layer 1 - Gradient Weights Max: 0.02738260983274075 Min: -0.024423986887839196\n","Layer 0 - Gradient Weights Max: 0.026459484557481608 Min: -0.029420048753476997\n","ReLU Activation - Max: 1.4143836267255916 Min: 0.0\n","ReLU Activation - Max: 0.6603820643650501 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01589455549280413 Min: -0.018106598337640906\n","Layer 1 - Gradient Weights Max: 0.01887670866717347 Min: -0.0171434301811109\n","Layer 0 - Gradient Weights Max: 0.026481744007194132 Min: -0.02757848523004022\n","ReLU Activation - Max: 1.892341805738185 Min: 0.0\n","ReLU Activation - Max: 0.5642565444140726 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013791106548280583 Min: -0.01557948531853911\n","Layer 1 - Gradient Weights Max: 0.0185606633494981 Min: -0.015219297827574684\n","Layer 0 - Gradient Weights Max: 0.02467755885972394 Min: -0.03116369717600249\n","ReLU Activation - Max: 1.5169642409943698 Min: 0.0\n","ReLU Activation - Max: 0.5470066122766382 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0238527849483582 Min: -0.021681276105236687\n","Layer 1 - Gradient Weights Max: 0.01717962790562689 Min: -0.0208250264956054\n","Layer 0 - Gradient Weights Max: 0.029975921524748508 Min: -0.03636078468774824\n","ReLU Activation - Max: 1.6303950035980217 Min: 0.0\n","ReLU Activation - Max: 0.5792843001936238 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01449821214260628 Min: -0.01816794149346097\n","Layer 1 - Gradient Weights Max: 0.019317710993398136 Min: -0.03202939119986775\n","Layer 0 - Gradient Weights Max: 0.02574653254690015 Min: -0.03510319906108476\n","ReLU Activation - Max: 1.4046964758672655 Min: 0.0\n","ReLU Activation - Max: 0.5561441175599743 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019866621303476103 Min: -0.019841994896317185\n","Layer 1 - Gradient Weights Max: 0.020023329362287443 Min: -0.017453166118953173\n","Layer 0 - Gradient Weights Max: 0.02455653005785487 Min: -0.027552646210894025\n","ReLU Activation - Max: 1.6884300326176724 Min: 0.0\n","ReLU Activation - Max: 0.6566880640273525 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010517337015511713 Min: -0.01933356864921738\n","Layer 1 - Gradient Weights Max: 0.015290745956482662 Min: -0.02076302648933794\n","Layer 0 - Gradient Weights Max: 0.027024511330746584 Min: -0.033997930008336445\n","ReLU Activation - Max: 1.379138458683677 Min: 0.0\n","ReLU Activation - Max: 0.6688549993722374 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020597744861731382 Min: -0.01827352990524075\n","Layer 1 - Gradient Weights Max: 0.018409736874452916 Min: -0.017320677764400865\n","Layer 0 - Gradient Weights Max: 0.030581975124576367 Min: -0.025975819412969874\n","ReLU Activation - Max: 1.239827819541726 Min: 0.0\n","ReLU Activation - Max: 0.6664079237462047 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02050045300684558 Min: -0.01921149842706496\n","Layer 1 - Gradient Weights Max: 0.016408213322598494 Min: -0.02649869718692807\n","Layer 0 - Gradient Weights Max: 0.025009602648726353 Min: -0.021806718882670557\n","ReLU Activation - Max: 1.7465042908584254 Min: 0.0\n","ReLU Activation - Max: 0.5352512173453375 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017936036453142134 Min: -0.01702363250371557\n","Layer 1 - Gradient Weights Max: 0.015477863317173752 Min: -0.019007591687932493\n","Layer 0 - Gradient Weights Max: 0.027993880635685742 Min: -0.02681057625769187\n","ReLU Activation - Max: 1.510834834356798 Min: 0.0\n","ReLU Activation - Max: 0.7031016428806381 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01584315725122789 Min: -0.015855920019187453\n","Layer 1 - Gradient Weights Max: 0.015582984203306562 Min: -0.018090388044802633\n","Layer 0 - Gradient Weights Max: 0.034307958555776806 Min: -0.03775941864760063\n","ReLU Activation - Max: 1.4737766374723924 Min: 0.0\n","ReLU Activation - Max: 0.5226039718705658 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018176879566562366 Min: -0.020170775557947253\n","Layer 1 - Gradient Weights Max: 0.021338201490438777 Min: -0.03163360289772051\n","Layer 0 - Gradient Weights Max: 0.0396152755767107 Min: -0.03832667651734435\n","ReLU Activation - Max: 1.683771463050691 Min: 0.0\n","ReLU Activation - Max: 0.6434487042648673 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017442209209840732 Min: -0.012186072562379632\n","Layer 1 - Gradient Weights Max: 0.01721948343838269 Min: -0.02467515929646301\n","Layer 0 - Gradient Weights Max: 0.03254993783145701 Min: -0.027135304071661495\n","ReLU Activation - Max: 1.7921978837812274 Min: 0.0\n","ReLU Activation - Max: 0.5970591295460568 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017878285309316493 Min: -0.03258023871762\n","Layer 1 - Gradient Weights Max: 0.024920544287688584 Min: -0.02459375512468665\n","Layer 0 - Gradient Weights Max: 0.028868442496961754 Min: -0.02882797543901788\n","ReLU Activation - Max: 1.35555026004283 Min: 0.0\n","ReLU Activation - Max: 0.5739161630182068 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012520434062085313 Min: -0.024398202795437408\n","Layer 1 - Gradient Weights Max: 0.01949490899840055 Min: -0.01845314246566336\n","Layer 0 - Gradient Weights Max: 0.04286436483240105 Min: -0.03172226402441209\n","ReLU Activation - Max: 1.816134212766605 Min: 0.0\n","ReLU Activation - Max: 0.7259743910103759 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01264045793710369 Min: -0.019797639695792135\n","Layer 1 - Gradient Weights Max: 0.015222086384434045 Min: -0.019745456243234368\n","Layer 0 - Gradient Weights Max: 0.027299049526986138 Min: -0.03897152550556489\n","ReLU Activation - Max: 1.556052876162948 Min: 0.0\n","ReLU Activation - Max: 0.7358024152441618 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017959845423307745 Min: -0.016368763095432212\n","Layer 1 - Gradient Weights Max: 0.018453535259600147 Min: -0.023044741187827657\n","Layer 0 - Gradient Weights Max: 0.03355872578530109 Min: -0.0351460221722727\n","ReLU Activation - Max: 1.885311852197008 Min: 0.0\n","ReLU Activation - Max: 0.5596033263702482 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012249315650719116 Min: -0.025689135401193604\n","Layer 1 - Gradient Weights Max: 0.021762529077723556 Min: -0.019691578517980285\n","Layer 0 - Gradient Weights Max: 0.030770979338241126 Min: -0.02476994657678577\n","ReLU Activation - Max: 1.4413444978776002 Min: 0.0\n","ReLU Activation - Max: 0.680843173393585 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019555653710150445 Min: -0.014312162065451191\n","Layer 1 - Gradient Weights Max: 0.016893068990945385 Min: -0.018101415036948437\n","Layer 0 - Gradient Weights Max: 0.03451829601165262 Min: -0.028189694986331296\n","ReLU Activation - Max: 1.4640624549856707 Min: 0.0\n","ReLU Activation - Max: 0.8180554322535017 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013340491636245656 Min: -0.020803589280419327\n","Layer 1 - Gradient Weights Max: 0.018055306208482398 Min: -0.02227673243760354\n","Layer 0 - Gradient Weights Max: 0.038343829550448676 Min: -0.02821221225822584\n","ReLU Activation - Max: 1.5153525427292154 Min: 0.0\n","ReLU Activation - Max: 0.5271152545178205 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019148678456017212 Min: -0.020407996130338158\n","Layer 1 - Gradient Weights Max: 0.022201697934159114 Min: -0.027370759336330385\n","Layer 0 - Gradient Weights Max: 0.024194959560597546 Min: -0.028990778923687253\n","ReLU Activation - Max: 1.4308667575492997 Min: 0.0\n","ReLU Activation - Max: 0.5866093272541529 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012656982156723624 Min: -0.012760721613327218\n","Layer 1 - Gradient Weights Max: 0.022793804611636735 Min: -0.02066678657847342\n","Layer 0 - Gradient Weights Max: 0.02847959737203641 Min: -0.026635210430065732\n","ReLU Activation - Max: 1.5688352708165725 Min: 0.0\n","ReLU Activation - Max: 0.5795039138341593 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012427147795345296 Min: -0.0254731969107415\n","Layer 1 - Gradient Weights Max: 0.02191318531491763 Min: -0.020952172516923082\n","Layer 0 - Gradient Weights Max: 0.036619761615498646 Min: -0.03282740037440798\n","ReLU Activation - Max: 1.624978088455899 Min: 0.0\n","ReLU Activation - Max: 0.6585400092463501 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014961698328567892 Min: -0.013958056034305388\n","Layer 1 - Gradient Weights Max: 0.030891191985037737 Min: -0.025140309643763133\n","Layer 0 - Gradient Weights Max: 0.028352878837073953 Min: -0.032746641915731664\n","ReLU Activation - Max: 1.2246005623201355 Min: 0.0\n","ReLU Activation - Max: 0.6223877425583343 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013243487593321045 Min: -0.015603391947799564\n","Layer 1 - Gradient Weights Max: 0.017854684628689683 Min: -0.01706036059707092\n","Layer 0 - Gradient Weights Max: 0.03552624089964698 Min: -0.030151499952845984\n","ReLU Activation - Max: 1.6916052690932248 Min: 0.0\n","ReLU Activation - Max: 0.6243234238409637 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010112489229737943 Min: -0.015674919937802963\n","Layer 1 - Gradient Weights Max: 0.017406570749801967 Min: -0.016787155834013773\n","Layer 0 - Gradient Weights Max: 0.021908317046870338 Min: -0.028774441844525836\n","ReLU Activation - Max: 1.3084382359973978 Min: 0.0\n","ReLU Activation - Max: 0.5156927240645388 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010326501613855793 Min: -0.032968427469322256\n","Layer 1 - Gradient Weights Max: 0.022945402788506177 Min: -0.019899228616455666\n","Layer 0 - Gradient Weights Max: 0.029453296556992123 Min: -0.02577159803022821\n","ReLU Activation - Max: 1.521955379434183 Min: 0.0\n","ReLU Activation - Max: 0.620939673376192 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022736795902766834 Min: -0.021225517101291926\n","Layer 1 - Gradient Weights Max: 0.016292210515307465 Min: -0.02090251450798688\n","Layer 0 - Gradient Weights Max: 0.028120113147427975 Min: -0.03625674781923303\n","ReLU Activation - Max: 1.5961732362062893 Min: 0.0\n","ReLU Activation - Max: 0.6991522516788257 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013249337529888348 Min: -0.01619888403723792\n","Layer 1 - Gradient Weights Max: 0.01632591347696026 Min: -0.027594054433206994\n","Layer 0 - Gradient Weights Max: 0.0322764887417473 Min: -0.027597044798531856\n","ReLU Activation - Max: 1.416270664755035 Min: 0.0\n","ReLU Activation - Max: 0.6376020203357875 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014409478802447487 Min: -0.021278284934549297\n","Layer 1 - Gradient Weights Max: 0.021699555411975813 Min: -0.01467424813333766\n","Layer 0 - Gradient Weights Max: 0.03102017062145152 Min: -0.023511454477039655\n","ReLU Activation - Max: 1.4826089358198924 Min: 0.0\n","ReLU Activation - Max: 0.5122294588911211 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013465267830106459 Min: -0.01538535049904489\n","Layer 1 - Gradient Weights Max: 0.021338173256660205 Min: -0.01570515189839615\n","Layer 0 - Gradient Weights Max: 0.0274363226799058 Min: -0.03399263801491911\n","ReLU Activation - Max: 1.5132363035556593 Min: 0.0\n","ReLU Activation - Max: 0.5464444363912226 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021379169587498897 Min: -0.018640564367203332\n","Layer 1 - Gradient Weights Max: 0.022654579236549757 Min: -0.018968599704008136\n","Layer 0 - Gradient Weights Max: 0.030674419052574613 Min: -0.02626448935189833\n","ReLU Activation - Max: 1.305713331211793 Min: 0.0\n","ReLU Activation - Max: 0.5316262875331393 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01582516118025523 Min: -0.01732004586217791\n","Layer 1 - Gradient Weights Max: 0.01642482280709529 Min: -0.031434222586283186\n","Layer 0 - Gradient Weights Max: 0.038751512005869695 Min: -0.032298251918391696\n","ReLU Activation - Max: 1.5950101065859903 Min: 0.0\n","ReLU Activation - Max: 0.5107348724124854 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014108926287569245 Min: -0.02188866255230232\n","Layer 1 - Gradient Weights Max: 0.02281676080163834 Min: -0.021167143831230435\n","Layer 0 - Gradient Weights Max: 0.02730405608402218 Min: -0.029926164234694696\n","ReLU Activation - Max: 1.5497229003465443 Min: 0.0\n","ReLU Activation - Max: 0.615917397572391 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020543058327044034 Min: -0.0249371546121167\n","Layer 1 - Gradient Weights Max: 0.018241200223198844 Min: -0.02116153922321161\n","Layer 0 - Gradient Weights Max: 0.034282961918077864 Min: -0.032010178962828095\n","ReLU Activation - Max: 1.2059139321800807 Min: 0.0\n","ReLU Activation - Max: 0.6484078275411937 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010731528078511458 Min: -0.01661611175440261\n","Layer 1 - Gradient Weights Max: 0.023460842847419377 Min: -0.01828269789077387\n","Layer 0 - Gradient Weights Max: 0.02638141754126962 Min: -0.03282479259625182\n","ReLU Activation - Max: 1.325373166228875 Min: 0.0\n","ReLU Activation - Max: 0.5071835682928764 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019679803376763564 Min: -0.014649635722278885\n","Layer 1 - Gradient Weights Max: 0.020920902705400063 Min: -0.017111703560326545\n","Layer 0 - Gradient Weights Max: 0.03597139015029205 Min: -0.02811990602127259\n","ReLU Activation - Max: 1.363444777399927 Min: 0.0\n","ReLU Activation - Max: 0.5832630620308618 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.030921048227207613 Min: -0.04793451172679334\n","Layer 1 - Gradient Weights Max: 0.025369759020929326 Min: -0.024763064776971513\n","Layer 0 - Gradient Weights Max: 0.040913721351396246 Min: -0.04241290849206155\n","ReLU Activation - Max: 1.318180262520953 Min: 0.0\n","ReLU Activation - Max: 0.47115905954426784 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019506546294888252 Min: -0.02607337723008538\n","Layer 1 - Gradient Weights Max: 0.025756797916317506 Min: -0.020706951940376507\n","Layer 0 - Gradient Weights Max: 0.03058767056101277 Min: -0.034143019152656426\n","ReLU Activation - Max: 1.1919391748505885 Min: 0.0\n","ReLU Activation - Max: 0.5368460015308837 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012497493428676133 Min: -0.024782508676110787\n","Layer 1 - Gradient Weights Max: 0.01467425034596326 Min: -0.018052774967982497\n","Layer 0 - Gradient Weights Max: 0.031355140316549686 Min: -0.03228364965867349\n","ReLU Activation - Max: 1.34323184632709 Min: 0.0\n","ReLU Activation - Max: 0.6435808695780201 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02493519358476071 Min: -0.016367149485228355\n","Layer 1 - Gradient Weights Max: 0.01967668797900516 Min: -0.017024748093845892\n","Layer 0 - Gradient Weights Max: 0.038417291432080274 Min: -0.029134925631554866\n","ReLU Activation - Max: 1.4674861331767202 Min: 0.0\n","ReLU Activation - Max: 0.6554229834468942 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02731440436672915 Min: -0.022935741303270203\n","Layer 1 - Gradient Weights Max: 0.018494604592887886 Min: -0.016569966252964276\n","Layer 0 - Gradient Weights Max: 0.0287244180781251 Min: -0.026246137877518158\n","ReLU Activation - Max: 1.7469161656040386 Min: 0.0\n","ReLU Activation - Max: 0.6199218844489565 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011186423959079742 Min: -0.011393384366095527\n","Layer 1 - Gradient Weights Max: 0.020934111033633473 Min: -0.01721553097092024\n","Layer 0 - Gradient Weights Max: 0.03753135660935123 Min: -0.03303939539916323\n","ReLU Activation - Max: 1.4378926556554834 Min: 0.0\n","ReLU Activation - Max: 0.619554606013401 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01507484121733782 Min: -0.017750711543940912\n","Layer 1 - Gradient Weights Max: 0.015100706878026122 Min: -0.022512233724048727\n","Layer 0 - Gradient Weights Max: 0.03598240700628777 Min: -0.026924296440292428\n","ReLU Activation - Max: 1.3500524368613835 Min: 0.0\n","ReLU Activation - Max: 0.5666018788356366 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016273107202550315 Min: -0.011883862191056032\n","Layer 1 - Gradient Weights Max: 0.01726175252517714 Min: -0.016723928940343972\n","Layer 0 - Gradient Weights Max: 0.0337377258185971 Min: -0.029265923107944288\n","ReLU Activation - Max: 1.2342263497148986 Min: 0.0\n","ReLU Activation - Max: 0.6541950032639264 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010978433223952241 Min: -0.014589121270550343\n","Layer 1 - Gradient Weights Max: 0.017344845957123114 Min: -0.017693159431704103\n","Layer 0 - Gradient Weights Max: 0.024862922524812238 Min: -0.024912701300815823\n","ReLU Activation - Max: 1.5051695114684023 Min: 0.0\n","ReLU Activation - Max: 0.5855112257072245 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01492227327418634 Min: -0.021805089162137012\n","Layer 1 - Gradient Weights Max: 0.02780304962054464 Min: -0.02030400044421908\n","Layer 0 - Gradient Weights Max: 0.03806095706890644 Min: -0.03489981166578979\n","ReLU Activation - Max: 1.3486989635226194 Min: 0.0\n","ReLU Activation - Max: 0.5212531022399391 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009781104255978721 Min: -0.012530990951239792\n","Layer 1 - Gradient Weights Max: 0.018224955664366542 Min: -0.018246640230581746\n","Layer 0 - Gradient Weights Max: 0.026817761885381628 Min: -0.031232541249935256\n","ReLU Activation - Max: 1.871822935252825 Min: 0.0\n","ReLU Activation - Max: 0.7803325856785164 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016748314306509766 Min: -0.016711304407030843\n","Layer 1 - Gradient Weights Max: 0.022056633405401465 Min: -0.01976368211558497\n","Layer 0 - Gradient Weights Max: 0.028285489731929608 Min: -0.029834272638260856\n","ReLU Activation - Max: 1.3490026740844192 Min: 0.0\n","ReLU Activation - Max: 0.549748671863877 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015920753525968404 Min: -0.020280610683042946\n","Layer 1 - Gradient Weights Max: 0.017846762311071563 Min: -0.0178969223578241\n","Layer 0 - Gradient Weights Max: 0.028991092710925193 Min: -0.02573151833868942\n","ReLU Activation - Max: 1.349338977527695 Min: 0.0\n","ReLU Activation - Max: 0.5582413462242428 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021092142336716288 Min: -0.030706169281941826\n","Layer 1 - Gradient Weights Max: 0.0242538358762225 Min: -0.017342104850983812\n","Layer 0 - Gradient Weights Max: 0.030517969913781733 Min: -0.025478874669121215\n","ReLU Activation - Max: 1.141874973260636 Min: 0.0\n","ReLU Activation - Max: 0.5104196634239776 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024053054865502695 Min: -0.021331133084433674\n","Layer 1 - Gradient Weights Max: 0.020052285429731705 Min: -0.021956783440983597\n","Layer 0 - Gradient Weights Max: 0.032975250761565225 Min: -0.02832114189940701\n","ReLU Activation - Max: 1.754821246873355 Min: 0.0\n","ReLU Activation - Max: 0.6061175136128198 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01670139224227977 Min: -0.015507478914034896\n","Layer 1 - Gradient Weights Max: 0.018226905762948525 Min: -0.02185586223485502\n","Layer 0 - Gradient Weights Max: 0.024938620724602782 Min: -0.030201059940943534\n","ReLU Activation - Max: 1.2352848204051021 Min: 0.0\n","ReLU Activation - Max: 0.5954813616080106 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01864678360285761 Min: -0.01850978391296074\n","Layer 1 - Gradient Weights Max: 0.016709669731878398 Min: -0.019536753737539422\n","Layer 0 - Gradient Weights Max: 0.02755030529157952 Min: -0.03393014437718642\n","ReLU Activation - Max: 1.7141695598315243 Min: 0.0\n","ReLU Activation - Max: 0.6710487205511022 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022097735239176748 Min: -0.022090791498840127\n","Layer 1 - Gradient Weights Max: 0.019062272396240098 Min: -0.026182399302049793\n","Layer 0 - Gradient Weights Max: 0.03048457537289796 Min: -0.028275457497937948\n","ReLU Activation - Max: 1.4675434520198465 Min: 0.0\n","ReLU Activation - Max: 0.5894290420711632 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02208653798438415 Min: -0.036927979224814854\n","Layer 1 - Gradient Weights Max: 0.017551527216035864 Min: -0.027033533329954392\n","Layer 0 - Gradient Weights Max: 0.0323750517830262 Min: -0.030667053938950355\n","ReLU Activation - Max: 1.712094553124118 Min: 0.0\n","ReLU Activation - Max: 0.6231406186856873 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014168960814256235 Min: -0.020464565046227474\n","Layer 1 - Gradient Weights Max: 0.018155854388142358 Min: -0.022773288473758983\n","Layer 0 - Gradient Weights Max: 0.033892208905324926 Min: -0.04504111237604302\n","ReLU Activation - Max: 1.6739436581671518 Min: 0.0\n","ReLU Activation - Max: 0.603623492276871 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01753455289611724 Min: -0.012240555776917614\n","Layer 1 - Gradient Weights Max: 0.015656352202056134 Min: -0.01783416475061972\n","Layer 0 - Gradient Weights Max: 0.029180872770325446 Min: -0.03118039177738079\n","ReLU Activation - Max: 1.4716827486635655 Min: 0.0\n","ReLU Activation - Max: 0.6777164149475008 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011866696949075101 Min: -0.016306923396843365\n","Layer 1 - Gradient Weights Max: 0.018611369508656432 Min: -0.01918832438222161\n","Layer 0 - Gradient Weights Max: 0.025220646231139284 Min: -0.029867169183364954\n","ReLU Activation - Max: 1.4634504355142592 Min: 0.0\n","ReLU Activation - Max: 0.6751633252410317 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015120228310761572 Min: -0.02159977370603996\n","Layer 1 - Gradient Weights Max: 0.025312911870347315 Min: -0.02460070248109586\n","Layer 0 - Gradient Weights Max: 0.035460984744498984 Min: -0.037113416671041795\n","ReLU Activation - Max: 1.3713217819685717 Min: 0.0\n","ReLU Activation - Max: 0.7157513668304373 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009686041703140772 Min: -0.01443312710997424\n","Layer 1 - Gradient Weights Max: 0.022408926665813567 Min: -0.020047732278529795\n","Layer 0 - Gradient Weights Max: 0.02498458911622554 Min: -0.027650658649571575\n","ReLU Activation - Max: 1.385702102759973 Min: 0.0\n","ReLU Activation - Max: 0.5796694004657612 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01358105317013346 Min: -0.02830241123777252\n","Layer 1 - Gradient Weights Max: 0.022595700327943835 Min: -0.02638239818357902\n","Layer 0 - Gradient Weights Max: 0.029262343018969662 Min: -0.02616953620956577\n","ReLU Activation - Max: 1.2990808648148156 Min: 0.0\n","ReLU Activation - Max: 0.6069609750200317 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01744773178326048 Min: -0.024071142603669025\n","Layer 1 - Gradient Weights Max: 0.022362959543665187 Min: -0.02672343691587608\n","Layer 0 - Gradient Weights Max: 0.026280570527531723 Min: -0.03005052254724389\n","ReLU Activation - Max: 1.750814073782942 Min: 0.0\n","ReLU Activation - Max: 0.5849617774012231 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012778364102549093 Min: -0.018358921419224225\n","Layer 1 - Gradient Weights Max: 0.01944830000017171 Min: -0.01926052078853053\n","Layer 0 - Gradient Weights Max: 0.030635289626144 Min: -0.03462940686466536\n","ReLU Activation - Max: 1.3847472457167955 Min: 0.0\n","ReLU Activation - Max: 0.5828191716213814 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016806743089898168 Min: -0.026699057047141952\n","Layer 1 - Gradient Weights Max: 0.021628627458047646 Min: -0.03212156670056216\n","Layer 0 - Gradient Weights Max: 0.0362892424850849 Min: -0.03199955649047374\n","ReLU Activation - Max: 1.6587568872443816 Min: 0.0\n","ReLU Activation - Max: 0.5197832622492621 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013196538487406675 Min: -0.013491796497150639\n","Layer 1 - Gradient Weights Max: 0.019612881898618043 Min: -0.0189636818018809\n","Layer 0 - Gradient Weights Max: 0.02860141222717087 Min: -0.029140699606019577\n","ReLU Activation - Max: 1.275018813130568 Min: 0.0\n","ReLU Activation - Max: 0.6999762016419115 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014980254512342847 Min: -0.017312535359317847\n","Layer 1 - Gradient Weights Max: 0.021078042651304685 Min: -0.02785520346964214\n","Layer 0 - Gradient Weights Max: 0.03969063856368048 Min: -0.04037994648284694\n","ReLU Activation - Max: 1.6403589422350604 Min: 0.0\n","ReLU Activation - Max: 0.6114926007981836 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01740976618613013 Min: -0.025209669495833302\n","Layer 1 - Gradient Weights Max: 0.019891775725163097 Min: -0.023370102112780285\n","Layer 0 - Gradient Weights Max: 0.03042976583086941 Min: -0.025484342486512518\n","ReLU Activation - Max: 1.2522875178126773 Min: 0.0\n","ReLU Activation - Max: 0.542117581102355 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017877251980628207 Min: -0.02125142357267428\n","Layer 1 - Gradient Weights Max: 0.022918000360493095 Min: -0.017509667628853625\n","Layer 0 - Gradient Weights Max: 0.0333531116040535 Min: -0.0373726857602725\n","ReLU Activation - Max: 1.8354376992118913 Min: 0.0\n","ReLU Activation - Max: 0.6731641099259639 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015867619595982693 Min: -0.013624193216818944\n","Layer 1 - Gradient Weights Max: 0.017516562866974606 Min: -0.02151027572671368\n","Layer 0 - Gradient Weights Max: 0.026460521410528657 Min: -0.03183890164343823\n","ReLU Activation - Max: 1.236320327914824 Min: 0.0\n","ReLU Activation - Max: 0.6776851206110405 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016941121329688905 Min: -0.015014714140696411\n","Layer 1 - Gradient Weights Max: 0.021831601844020953 Min: -0.022549773058346972\n","Layer 0 - Gradient Weights Max: 0.029924186403905477 Min: -0.04259850791334911\n","ReLU Activation - Max: 1.3730216321382112 Min: 0.0\n","ReLU Activation - Max: 0.5961569080628358 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012785249662336727 Min: -0.015054503553926022\n","Layer 1 - Gradient Weights Max: 0.02446683690920569 Min: -0.022611612609304242\n","Layer 0 - Gradient Weights Max: 0.029915467250261216 Min: -0.0283125486617895\n","ReLU Activation - Max: 1.241320528644746 Min: 0.0\n","ReLU Activation - Max: 0.6282599381082085 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01969191726103158 Min: -0.03259663937669074\n","Layer 1 - Gradient Weights Max: 0.016601438269530984 Min: -0.01803065809469122\n","Layer 0 - Gradient Weights Max: 0.028379489861085228 Min: -0.031056744800367006\n","ReLU Activation - Max: 1.2696353478807987 Min: 0.0\n","ReLU Activation - Max: 0.5491307199053759 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01741913601749498 Min: -0.01871342486736048\n","Layer 1 - Gradient Weights Max: 0.021057849641291825 Min: -0.02329439712587351\n","Layer 0 - Gradient Weights Max: 0.024102509536045875 Min: -0.025675624873163563\n","ReLU Activation - Max: 1.7930235119036584 Min: 0.0\n","ReLU Activation - Max: 0.6506394801615649 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016855603163002542 Min: -0.0146619467644565\n","Layer 1 - Gradient Weights Max: 0.017544847781871334 Min: -0.016434695474652254\n","Layer 0 - Gradient Weights Max: 0.028415215359254276 Min: -0.039266528962052225\n","ReLU Activation - Max: 1.351352341105827 Min: 0.0\n","ReLU Activation - Max: 0.5065353010418832 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013928904612297295 Min: -0.01571423983011851\n","Layer 1 - Gradient Weights Max: 0.016302194013727955 Min: -0.019885367831360082\n","Layer 0 - Gradient Weights Max: 0.028175159197796156 Min: -0.03464761685142207\n","ReLU Activation - Max: 1.5006495484020286 Min: 0.0\n","ReLU Activation - Max: 0.6282160920476927 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012923486276795201 Min: -0.023194638433872938\n","Layer 1 - Gradient Weights Max: 0.019146249384460974 Min: -0.021372762832547364\n","Layer 0 - Gradient Weights Max: 0.03003736989094401 Min: -0.02874850398931318\n","ReLU Activation - Max: 1.423294727588817 Min: 0.0\n","ReLU Activation - Max: 0.6519495972393581 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015399108933178312 Min: -0.017141290784513416\n","Layer 1 - Gradient Weights Max: 0.016700731773896183 Min: -0.015355014044833243\n","Layer 0 - Gradient Weights Max: 0.029560757428612944 Min: -0.025805900734402355\n","ReLU Activation - Max: 1.6070516151370062 Min: 0.0\n","ReLU Activation - Max: 0.5458941980080697 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017389355290006583 Min: -0.011835308823184782\n","Layer 1 - Gradient Weights Max: 0.01603694989227799 Min: -0.016562779061348263\n","Layer 0 - Gradient Weights Max: 0.030276343949719176 Min: -0.027364846311562722\n","ReLU Activation - Max: 1.4598689122717061 Min: 0.0\n","ReLU Activation - Max: 0.7391547053472414 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020880957424455605 Min: -0.023513152955198457\n","Layer 1 - Gradient Weights Max: 0.020027482440940027 Min: -0.024870551224828428\n","Layer 0 - Gradient Weights Max: 0.03700163320213994 Min: -0.04119552220273585\n","ReLU Activation - Max: 1.369145146521845 Min: 0.0\n","ReLU Activation - Max: 0.6945615789359418 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017133201661441732 Min: -0.018109852945635278\n","Layer 1 - Gradient Weights Max: 0.013787612439167492 Min: -0.018662597905981237\n","Layer 0 - Gradient Weights Max: 0.03759434180367632 Min: -0.03415654594968172\n","ReLU Activation - Max: 1.4670020740210614 Min: 0.0\n","ReLU Activation - Max: 0.5762742366961336 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014437292629070038 Min: -0.01737366807481151\n","Layer 1 - Gradient Weights Max: 0.01915385262497528 Min: -0.017481861572611918\n","Layer 0 - Gradient Weights Max: 0.02616295989956297 Min: -0.03362006022058074\n","ReLU Activation - Max: 1.305386464051653 Min: 0.0\n","ReLU Activation - Max: 0.5496684185307087 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021938562202871846 Min: -0.022026845042052566\n","Layer 1 - Gradient Weights Max: 0.018423129044659817 Min: -0.025718825389510546\n","Layer 0 - Gradient Weights Max: 0.026799161492411027 Min: -0.03249545819672128\n","ReLU Activation - Max: 1.6183743195729436 Min: 0.0\n","ReLU Activation - Max: 0.5822771360871535 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021678751290738218 Min: -0.016844316434166104\n","Layer 1 - Gradient Weights Max: 0.022310359854174296 Min: -0.01873870288132151\n","Layer 0 - Gradient Weights Max: 0.021766698542443154 Min: -0.03650039108910722\n","ReLU Activation - Max: 1.2953053632724534 Min: 0.0\n","ReLU Activation - Max: 0.5541216099441064 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018716221231278297 Min: -0.023857796595088143\n","Layer 1 - Gradient Weights Max: 0.022595842558283497 Min: -0.017323754049859724\n","Layer 0 - Gradient Weights Max: 0.02622335650021155 Min: -0.0321850340514793\n","ReLU Activation - Max: 1.9982655294716964 Min: 0.0\n","ReLU Activation - Max: 0.7021716240075087 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0205194331452836 Min: -0.01856833118519338\n","Layer 1 - Gradient Weights Max: 0.024088920255446452 Min: -0.021025307279899978\n","Layer 0 - Gradient Weights Max: 0.04217783345220463 Min: -0.028641777159224672\n","ReLU Activation - Max: 1.6990437595266243 Min: 0.0\n","ReLU Activation - Max: 0.6480983170079131 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014070265543834401 Min: -0.015511512809081796\n","Layer 1 - Gradient Weights Max: 0.018101092828563148 Min: -0.01568723449415616\n","Layer 0 - Gradient Weights Max: 0.025228618001310616 Min: -0.03404360105673381\n","ReLU Activation - Max: 1.4561925633766075 Min: 0.0\n","ReLU Activation - Max: 0.6271404124639081 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015499878601461623 Min: -0.021967371088139252\n","Layer 1 - Gradient Weights Max: 0.016205441848002645 Min: -0.025026193310469392\n","Layer 0 - Gradient Weights Max: 0.027397339116993363 Min: -0.03191669768357692\n","ReLU Activation - Max: 1.3571272792946145 Min: 0.0\n","ReLU Activation - Max: 0.5348606087107023 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012915149176135711 Min: -0.014943070541822307\n","Layer 1 - Gradient Weights Max: 0.01548911596486836 Min: -0.021839832341886228\n","Layer 0 - Gradient Weights Max: 0.027832532925409773 Min: -0.025040599175538455\n","ReLU Activation - Max: 1.3036213856954746 Min: 0.0\n","ReLU Activation - Max: 0.5604023841884194 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01543691184435977 Min: -0.035277310696874224\n","Layer 1 - Gradient Weights Max: 0.023017723465433418 Min: -0.025123236230882546\n","Layer 0 - Gradient Weights Max: 0.04119145130077265 Min: -0.04667666828462618\n","ReLU Activation - Max: 1.3709107167824783 Min: 0.0\n","ReLU Activation - Max: 0.6485000632038413 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01751414440262494 Min: -0.021440786974001893\n","Layer 1 - Gradient Weights Max: 0.017239611561050566 Min: -0.020457007466929637\n","Layer 0 - Gradient Weights Max: 0.03046898499597029 Min: -0.02625610083422655\n","ReLU Activation - Max: 1.8048252079565097 Min: 0.0\n","ReLU Activation - Max: 0.6536593138284027 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024635707629680034 Min: -0.018478844043966686\n","Layer 1 - Gradient Weights Max: 0.021750569115159553 Min: -0.026032038790264878\n","Layer 0 - Gradient Weights Max: 0.03027592592943514 Min: -0.0384031880558242\n","ReLU Activation - Max: 1.483554943731734 Min: 0.0\n","ReLU Activation - Max: 0.588558018943451 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021102446338356246 Min: -0.01738152761710514\n","Layer 1 - Gradient Weights Max: 0.021135282275476477 Min: -0.019992193562717135\n","Layer 0 - Gradient Weights Max: 0.041351411594804745 Min: -0.04320327939924767\n","ReLU Activation - Max: 1.4471673943797352 Min: 0.0\n","ReLU Activation - Max: 0.6622625295628489 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014552425377419218 Min: -0.013053625871273056\n","Layer 1 - Gradient Weights Max: 0.0178592132387595 Min: -0.01611711976733178\n","Layer 0 - Gradient Weights Max: 0.028146232679349494 Min: -0.028292521220960976\n","ReLU Activation - Max: 1.2682077032791859 Min: 0.0\n","ReLU Activation - Max: 0.6009003170703445 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013869172320870019 Min: -0.022155096915221703\n","Layer 1 - Gradient Weights Max: 0.022562770669018954 Min: -0.024586610926130386\n","Layer 0 - Gradient Weights Max: 0.033908925036369136 Min: -0.0283782700449682\n","ReLU Activation - Max: 1.743165021602598 Min: 0.0\n","ReLU Activation - Max: 0.5465666713047118 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010977197662107283 Min: -0.013345570330358884\n","Layer 1 - Gradient Weights Max: 0.022541449628918558 Min: -0.017649058985536658\n","Layer 0 - Gradient Weights Max: 0.031930713596971895 Min: -0.03836776589737405\n","ReLU Activation - Max: 1.333486035870431 Min: 0.0\n","ReLU Activation - Max: 0.5830939945992917 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015924513631791405 Min: -0.021429801225101172\n","Layer 1 - Gradient Weights Max: 0.019758880295860804 Min: -0.01853997937651814\n","Layer 0 - Gradient Weights Max: 0.035731631596219456 Min: -0.0364156362057458\n","ReLU Activation - Max: 1.7204379241991326 Min: 0.0\n","ReLU Activation - Max: 0.5095104858031693 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017078103417644323 Min: -0.028455848606465845\n","Layer 1 - Gradient Weights Max: 0.02069613599858062 Min: -0.03573595833154225\n","Layer 0 - Gradient Weights Max: 0.030676116450891907 Min: -0.030608763961886748\n","ReLU Activation - Max: 1.6043776103003409 Min: 0.0\n","ReLU Activation - Max: 0.6448245901288798 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018528369423172625 Min: -0.01804386315101891\n","Layer 1 - Gradient Weights Max: 0.014565667234756972 Min: -0.019995928200869255\n","Layer 0 - Gradient Weights Max: 0.0262586856691812 Min: -0.025072388149055953\n","ReLU Activation - Max: 1.5035431717823227 Min: 0.0\n","ReLU Activation - Max: 0.7803588336162273 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01437147761508431 Min: -0.016460296430328706\n","Layer 1 - Gradient Weights Max: 0.01593950766003688 Min: -0.020958464635821364\n","Layer 0 - Gradient Weights Max: 0.023822697699144284 Min: -0.029800153137555095\n","ReLU Activation - Max: 1.4003402556404 Min: 0.0\n","ReLU Activation - Max: 0.6887863976189348 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013288502097893342 Min: -0.018256885469434436\n","Layer 1 - Gradient Weights Max: 0.01672311596233425 Min: -0.021226276767000975\n","Layer 0 - Gradient Weights Max: 0.02427699842211631 Min: -0.028636536467048068\n","ReLU Activation - Max: 1.6075033334901792 Min: 0.0\n","ReLU Activation - Max: 0.7842806309746404 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02108757450826119 Min: -0.02066722393352307\n","Layer 1 - Gradient Weights Max: 0.01844330538674907 Min: -0.019346839689019683\n","Layer 0 - Gradient Weights Max: 0.02919067955475422 Min: -0.021872458928387038\n","ReLU Activation - Max: 1.4453319867045806 Min: 0.0\n","ReLU Activation - Max: 0.8112540521380831 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01524629522967986 Min: -0.033500854962228974\n","Layer 1 - Gradient Weights Max: 0.021161145805996047 Min: -0.02693723330293793\n","Layer 0 - Gradient Weights Max: 0.02977153610706744 Min: -0.026695856458541323\n","ReLU Activation - Max: 1.718306503719635 Min: 0.0\n","ReLU Activation - Max: 0.6310538381649081 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02041732110869593 Min: -0.01815840560323321\n","Layer 1 - Gradient Weights Max: 0.0200200774496869 Min: -0.029492061012424294\n","Layer 0 - Gradient Weights Max: 0.02407571576242298 Min: -0.026961918854444218\n","ReLU Activation - Max: 1.4534546969714 Min: 0.0\n","ReLU Activation - Max: 0.5446759768158143 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01685875021897048 Min: -0.02294736693132699\n","Layer 1 - Gradient Weights Max: 0.030178733089516314 Min: -0.013021264202332626\n","Layer 0 - Gradient Weights Max: 0.02954653506533169 Min: -0.027178003379450012\n","ReLU Activation - Max: 1.32974143775069 Min: 0.0\n","ReLU Activation - Max: 0.5832089701074182 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016548337171662358 Min: -0.032368746968501796\n","Layer 1 - Gradient Weights Max: 0.02303518982488939 Min: -0.030417273494778834\n","Layer 0 - Gradient Weights Max: 0.042448264202094466 Min: -0.030125784765008594\n","ReLU Activation - Max: 1.432657732727054 Min: 0.0\n","ReLU Activation - Max: 0.5765283964442384 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01334416435657357 Min: -0.014779186203106193\n","Layer 1 - Gradient Weights Max: 0.015842094555943445 Min: -0.016368586534985855\n","Layer 0 - Gradient Weights Max: 0.026927736050074057 Min: -0.03220941290568908\n","ReLU Activation - Max: 1.5050763373206848 Min: 0.0\n","ReLU Activation - Max: 0.6165854063153439 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013978355770306941 Min: -0.018308945701441728\n","Layer 1 - Gradient Weights Max: 0.014384753350955846 Min: -0.025466585756825793\n","Layer 0 - Gradient Weights Max: 0.022046099244530545 Min: -0.031161599226321\n","ReLU Activation - Max: 1.5589096874225745 Min: 0.0\n","ReLU Activation - Max: 0.5179365236073797 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008744861502085994 Min: -0.008220922589336986\n","Layer 1 - Gradient Weights Max: 0.018245951858629414 Min: -0.015553571807003706\n","Layer 0 - Gradient Weights Max: 0.03303460161240511 Min: -0.02587373966534075\n","ReLU Activation - Max: 1.9431972965555633 Min: 0.0\n","ReLU Activation - Max: 0.626772784354724 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0166201404546871 Min: -0.015866781781538414\n","Layer 1 - Gradient Weights Max: 0.020872222287131342 Min: -0.021293671540034698\n","Layer 0 - Gradient Weights Max: 0.029972604610302333 Min: -0.031570836975589336\n","ReLU Activation - Max: 1.2463088970492033 Min: 0.0\n","ReLU Activation - Max: 0.5276347451098986 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011594721523784334 Min: -0.01407573918569831\n","Layer 1 - Gradient Weights Max: 0.01694573406568865 Min: -0.020714383918991955\n","Layer 0 - Gradient Weights Max: 0.028786232250607616 Min: -0.02789292008266277\n","ReLU Activation - Max: 1.5955980818683346 Min: 0.0\n","ReLU Activation - Max: 0.5576989607099032 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009892977126437821 Min: -0.013263724688655157\n","Layer 1 - Gradient Weights Max: 0.01596170004031682 Min: -0.017774414361440203\n","Layer 0 - Gradient Weights Max: 0.03268719781616448 Min: -0.02395133509345896\n","ReLU Activation - Max: 1.2985707376868207 Min: 0.0\n","ReLU Activation - Max: 0.7158264767478396 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014609218075622678 Min: -0.02693614362826849\n","Layer 1 - Gradient Weights Max: 0.018419227804034797 Min: -0.016528989278164444\n","Layer 0 - Gradient Weights Max: 0.026315011729299016 Min: -0.029109553455605675\n","ReLU Activation - Max: 1.799049547120647 Min: 0.0\n","ReLU Activation - Max: 0.7661927469933711 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018487131082998338 Min: -0.01273094049932941\n","Layer 1 - Gradient Weights Max: 0.015864112258954496 Min: -0.03221593608180811\n","Layer 0 - Gradient Weights Max: 0.03671778630379951 Min: -0.024936592298428793\n","ReLU Activation - Max: 1.390061718067199 Min: 0.0\n","ReLU Activation - Max: 0.5444128505036085 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013217964227808663 Min: -0.015922346659152087\n","Layer 1 - Gradient Weights Max: 0.017198314136723805 Min: -0.017294983251129433\n","Layer 0 - Gradient Weights Max: 0.0320265650795462 Min: -0.025497816392888346\n","ReLU Activation - Max: 1.2680172911775704 Min: 0.0\n","ReLU Activation - Max: 0.5837029708281314 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01564888026231568 Min: -0.016089392411246437\n","Layer 1 - Gradient Weights Max: 0.017515541744535686 Min: -0.017282832448694763\n","Layer 0 - Gradient Weights Max: 0.025731124215814232 Min: -0.03403135313408012\n","ReLU Activation - Max: 1.430256812271311 Min: 0.0\n","ReLU Activation - Max: 0.6438654635590442 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015702516263100076 Min: -0.027788550850166627\n","Layer 1 - Gradient Weights Max: 0.015257584771103975 Min: -0.02231296925420092\n","Layer 0 - Gradient Weights Max: 0.03548436704819328 Min: -0.03582264721896971\n","ReLU Activation - Max: 1.5507639497420356 Min: 0.0\n","ReLU Activation - Max: 0.6350076681270035 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013535323906692053 Min: -0.02600160781543123\n","Layer 1 - Gradient Weights Max: 0.02152879465236756 Min: -0.02762290856771447\n","Layer 0 - Gradient Weights Max: 0.0256554530655733 Min: -0.02800018638298312\n","ReLU Activation - Max: 1.2556334051473217 Min: 0.0\n","ReLU Activation - Max: 0.6398559155291076 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01623956889863352 Min: -0.013121649082435667\n","Layer 1 - Gradient Weights Max: 0.019642264625064784 Min: -0.020091957308014146\n","Layer 0 - Gradient Weights Max: 0.028259363741589282 Min: -0.02875546488886779\n","ReLU Activation - Max: 1.3854412772342324 Min: 0.0\n","ReLU Activation - Max: 0.6284545887667722 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02362159719403869 Min: -0.01619895077782584\n","Layer 1 - Gradient Weights Max: 0.020529754143998335 Min: -0.02105477777616475\n","Layer 0 - Gradient Weights Max: 0.03023226787988872 Min: -0.031828987887482854\n","ReLU Activation - Max: 1.4660164213385196 Min: 0.0\n","ReLU Activation - Max: 0.6703554815755104 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01718626278445894 Min: -0.021565462096124605\n","Layer 1 - Gradient Weights Max: 0.023680301210972622 Min: -0.023737628253112243\n","Layer 0 - Gradient Weights Max: 0.0324299460762524 Min: -0.03600781238060406\n","ReLU Activation - Max: 1.3129467819013498 Min: 0.0\n","ReLU Activation - Max: 0.48078291390799077 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012796129284657325 Min: -0.009929469866429147\n","Layer 1 - Gradient Weights Max: 0.019358068781877232 Min: -0.017528631822131668\n","Layer 0 - Gradient Weights Max: 0.026238356927777807 Min: -0.029393912402956787\n","ReLU Activation - Max: 1.6701265579067675 Min: 0.0\n","ReLU Activation - Max: 0.882248799389519 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016676787588570937 Min: -0.014172277177550997\n","Layer 1 - Gradient Weights Max: 0.016797320105518234 Min: -0.01844263330739955\n","Layer 0 - Gradient Weights Max: 0.02935148778582699 Min: -0.025672237393161247\n","ReLU Activation - Max: 1.7317170829317523 Min: 0.0\n","ReLU Activation - Max: 0.6608350462328318 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008910931595509352 Min: -0.017375776971558957\n","Layer 1 - Gradient Weights Max: 0.019226409312027397 Min: -0.033024576156858734\n","Layer 0 - Gradient Weights Max: 0.033099091246708054 Min: -0.0311922996140989\n","ReLU Activation - Max: 1.4348131265936623 Min: 0.0\n","ReLU Activation - Max: 0.5692072691309038 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01328335366909374 Min: -0.011844460638072363\n","Layer 1 - Gradient Weights Max: 0.017351463100653306 Min: -0.01972293480056256\n","Layer 0 - Gradient Weights Max: 0.033398047685185075 Min: -0.03162442796565837\n","ReLU Activation - Max: 1.277507479784653 Min: 0.0\n","ReLU Activation - Max: 0.5782436730635457 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016100133606123904 Min: -0.022458418946246946\n","Layer 1 - Gradient Weights Max: 0.019740740947383914 Min: -0.01497998274517608\n","Layer 0 - Gradient Weights Max: 0.031713179942218554 Min: -0.030970435102833985\n","ReLU Activation - Max: 1.4223729099933546 Min: 0.0\n","ReLU Activation - Max: 0.6040342608442192 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012788042696818762 Min: -0.0237577987215627\n","Layer 1 - Gradient Weights Max: 0.020276386267021385 Min: -0.023595614039203353\n","Layer 0 - Gradient Weights Max: 0.026929110059467153 Min: -0.030521855563955477\n","ReLU Activation - Max: 1.6603198592915214 Min: 0.0\n","ReLU Activation - Max: 0.5348970643986436 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02063817622016069 Min: -0.017068139802028197\n","Layer 1 - Gradient Weights Max: 0.019737407612372342 Min: -0.018744001212959004\n","Layer 0 - Gradient Weights Max: 0.027396260101295814 Min: -0.027158848790263975\n","ReLU Activation - Max: 1.6032589477956072 Min: 0.0\n","ReLU Activation - Max: 0.6128752490133029 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01872913057422293 Min: -0.02089600409420264\n","Layer 1 - Gradient Weights Max: 0.017585523055710775 Min: -0.021669374000447213\n","Layer 0 - Gradient Weights Max: 0.024408051268082726 Min: -0.029405793019294354\n","ReLU Activation - Max: 1.4818225801116978 Min: 0.0\n","ReLU Activation - Max: 0.6032631728147869 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014264672744206798 Min: -0.024583833418786446\n","Layer 1 - Gradient Weights Max: 0.027776888997079342 Min: -0.023935451537789185\n","Layer 0 - Gradient Weights Max: 0.030959029145353676 Min: -0.040782638388448915\n","ReLU Activation - Max: 1.3398658705386146 Min: 0.0\n","ReLU Activation - Max: 0.7174042720135408 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0159529439584581 Min: -0.012368337192123005\n","Layer 1 - Gradient Weights Max: 0.019553356526635406 Min: -0.02310890520697188\n","Layer 0 - Gradient Weights Max: 0.02861519532666294 Min: -0.02472967198646725\n","ReLU Activation - Max: 1.4167021100816162 Min: 0.0\n","ReLU Activation - Max: 0.5517056349141157 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014134989885672735 Min: -0.01571821149211026\n","Layer 1 - Gradient Weights Max: 0.019867320967613903 Min: -0.018498939016280847\n","Layer 0 - Gradient Weights Max: 0.029793304231348062 Min: -0.033389041893696444\n","ReLU Activation - Max: 1.5929507965382947 Min: 0.0\n","ReLU Activation - Max: 0.6309992235101741 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023701927967521103 Min: -0.020859238032706753\n","Layer 1 - Gradient Weights Max: 0.0277046434884186 Min: -0.020037563173538688\n","Layer 0 - Gradient Weights Max: 0.02734016901466741 Min: -0.03131707853910834\n","ReLU Activation - Max: 1.7938573769142367 Min: 0.0\n","ReLU Activation - Max: 0.6019584292053952 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010782270405646869 Min: -0.015340670590426236\n","Layer 1 - Gradient Weights Max: 0.02009013102489708 Min: -0.025837365498552048\n","Layer 0 - Gradient Weights Max: 0.03246451154903779 Min: -0.0352692569071424\n","ReLU Activation - Max: 1.451513261579632 Min: 0.0\n","ReLU Activation - Max: 0.6656704910610932 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015700346012969633 Min: -0.026396349597052646\n","Layer 1 - Gradient Weights Max: 0.01952968650342324 Min: -0.03318154356708642\n","Layer 0 - Gradient Weights Max: 0.03078041781925646 Min: -0.03118836299173483\n","ReLU Activation - Max: 1.4139661686537854 Min: 0.0\n","ReLU Activation - Max: 0.5585331740103606 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014964498566663424 Min: -0.018618350017094006\n","Layer 1 - Gradient Weights Max: 0.024952410600786488 Min: -0.021676693545224043\n","Layer 0 - Gradient Weights Max: 0.024742723390082377 Min: -0.02827948524340194\n","ReLU Activation - Max: 1.3796408242567926 Min: 0.0\n","ReLU Activation - Max: 0.6770124757114129 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0211157562830255 Min: -0.02602638346832569\n","Layer 1 - Gradient Weights Max: 0.0187964333680737 Min: -0.030193259075066436\n","Layer 0 - Gradient Weights Max: 0.029478293298217252 Min: -0.03138685971162614\n","ReLU Activation - Max: 2.1489858101966455 Min: 0.0\n","ReLU Activation - Max: 0.7337441491646044 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01746275402347544 Min: -0.01847021502427597\n","Layer 1 - Gradient Weights Max: 0.017112917849092114 Min: -0.02440875143182134\n","Layer 0 - Gradient Weights Max: 0.0241405042335536 Min: -0.03694962370039208\n","ReLU Activation - Max: 1.496510125701913 Min: 0.0\n","ReLU Activation - Max: 0.6944705080086077 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017497497678043 Min: -0.019602290832038738\n","Layer 1 - Gradient Weights Max: 0.019271286574983768 Min: -0.01939164923460937\n","Layer 0 - Gradient Weights Max: 0.02659902574844118 Min: -0.025622341519952033\n","ReLU Activation - Max: 1.5234849471044845 Min: 0.0\n","ReLU Activation - Max: 0.5691018897731306 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.029533679509283456 Min: -0.03764430941273004\n","Layer 1 - Gradient Weights Max: 0.0197393198230732 Min: -0.01668235273233482\n","Layer 0 - Gradient Weights Max: 0.029826875540334693 Min: -0.02677027234965557\n","ReLU Activation - Max: 1.3414579598173466 Min: 0.0\n","ReLU Activation - Max: 0.5306439781123881 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022438998982760406 Min: -0.029077221485427407\n","Layer 1 - Gradient Weights Max: 0.023974825945977142 Min: -0.020343903393556996\n","Layer 0 - Gradient Weights Max: 0.02618461130459117 Min: -0.02741087863728584\n","ReLU Activation - Max: 1.7965675381376291 Min: 0.0\n","ReLU Activation - Max: 0.7013557135300384 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020787392528084862 Min: -0.013588113221338106\n","Layer 1 - Gradient Weights Max: 0.021652506230128963 Min: -0.02214438200482915\n","Layer 0 - Gradient Weights Max: 0.030252159730464925 Min: -0.02891425875497381\n","ReLU Activation - Max: 1.3006924660449275 Min: 0.0\n","ReLU Activation - Max: 0.5921139014880518 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01505777119102187 Min: -0.031066110188955408\n","Layer 1 - Gradient Weights Max: 0.022741973613202012 Min: -0.029607086488792136\n","Layer 0 - Gradient Weights Max: 0.03155597433531809 Min: -0.028208691257019004\n","ReLU Activation - Max: 1.2879834616288177 Min: 0.0\n","ReLU Activation - Max: 0.6131050581490071 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021664962682452323 Min: -0.023317621508000538\n","Layer 1 - Gradient Weights Max: 0.024027586814399406 Min: -0.017129162353696596\n","Layer 0 - Gradient Weights Max: 0.02575779359887472 Min: -0.0267944514362472\n","ReLU Activation - Max: 1.3438593468022844 Min: 0.0\n","ReLU Activation - Max: 0.5882229187712418 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01335406679336201 Min: -0.01543846121261078\n","Layer 1 - Gradient Weights Max: 0.026657646680033783 Min: -0.017474800183468447\n","Layer 0 - Gradient Weights Max: 0.03825035438541475 Min: -0.026091597218163537\n","ReLU Activation - Max: 1.6521142569333336 Min: 0.0\n","ReLU Activation - Max: 0.6848626973270664 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02324729012592415 Min: -0.015158250103171934\n","Layer 1 - Gradient Weights Max: 0.018881615350906687 Min: -0.023644275158033003\n","Layer 0 - Gradient Weights Max: 0.032274353832544914 Min: -0.027142505637248633\n","ReLU Activation - Max: 1.4491575811341186 Min: 0.0\n","ReLU Activation - Max: 0.5623485009085243 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016504776200160956 Min: -0.022271749959228402\n","Layer 1 - Gradient Weights Max: 0.01998461632415362 Min: -0.020308691726505967\n","Layer 0 - Gradient Weights Max: 0.02626763218469096 Min: -0.04584806551571424\n","ReLU Activation - Max: 1.3202684383045165 Min: 0.0\n","ReLU Activation - Max: 0.5496452133846502 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.027063699177936308 Min: -0.01922971582142863\n","Layer 1 - Gradient Weights Max: 0.018395140358656827 Min: -0.024797267679536224\n","Layer 0 - Gradient Weights Max: 0.030570249035287905 Min: -0.030826385613898395\n","ReLU Activation - Max: 1.8266394893049864 Min: 0.0\n","ReLU Activation - Max: 0.6717860986234154 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01728149537972271 Min: -0.022016850129904963\n","Layer 1 - Gradient Weights Max: 0.01976011951996798 Min: -0.019075120471507383\n","Layer 0 - Gradient Weights Max: 0.028013150669251467 Min: -0.029463220312148482\n","ReLU Activation - Max: 1.3475469832458264 Min: 0.0\n","ReLU Activation - Max: 0.641100024242554 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014841078175076482 Min: -0.013885855263784745\n","Layer 1 - Gradient Weights Max: 0.017560722985442603 Min: -0.015249494658886774\n","Layer 0 - Gradient Weights Max: 0.027818189435457327 Min: -0.03994951174607014\n","ReLU Activation - Max: 1.438271895788553 Min: 0.0\n","ReLU Activation - Max: 0.5832887231295036 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008689253702826251 Min: -0.01737787521412671\n","Layer 1 - Gradient Weights Max: 0.019460309100178732 Min: -0.019100550770399582\n","Layer 0 - Gradient Weights Max: 0.0319483875160592 Min: -0.029154968902374224\n","ReLU Activation - Max: 1.2991664604498265 Min: 0.0\n","ReLU Activation - Max: 0.5275178833701846 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021436092354198785 Min: -0.014519923885487137\n","Layer 1 - Gradient Weights Max: 0.019847904294545277 Min: -0.02284106505445982\n","Layer 0 - Gradient Weights Max: 0.02758554660018816 Min: -0.029156729642817562\n","ReLU Activation - Max: 1.622728229391609 Min: 0.0\n","ReLU Activation - Max: 0.5903730841339627 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010135519921974924 Min: -0.015034679239013743\n","Layer 1 - Gradient Weights Max: 0.018899755935026016 Min: -0.023473322575259574\n","Layer 0 - Gradient Weights Max: 0.0239514270777292 Min: -0.0333949647711246\n","ReLU Activation - Max: 1.3691513099617394 Min: 0.0\n","ReLU Activation - Max: 0.6132146333604773 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02274974739543313 Min: -0.032052685437529614\n","Layer 1 - Gradient Weights Max: 0.021020035954182406 Min: -0.029603063000547686\n","Layer 0 - Gradient Weights Max: 0.03252993805300412 Min: -0.039360334701687025\n","ReLU Activation - Max: 1.3486815286636036 Min: 0.0\n","ReLU Activation - Max: 0.5947708003146059 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019040983737506418 Min: -0.014129435003303216\n","Layer 1 - Gradient Weights Max: 0.017794231557778328 Min: -0.03163568911211936\n","Layer 0 - Gradient Weights Max: 0.0344113756527375 Min: -0.032316927089796665\n","ReLU Activation - Max: 1.4093875497076358 Min: 0.0\n","ReLU Activation - Max: 0.5358325053254896 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012211432281909974 Min: -0.01641050563101531\n","Layer 1 - Gradient Weights Max: 0.018607604875764735 Min: -0.018594086909230013\n","Layer 0 - Gradient Weights Max: 0.02899038155236045 Min: -0.05402480413606129\n","ReLU Activation - Max: 1.5805523403643762 Min: 0.0\n","ReLU Activation - Max: 0.6581541107509574 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021166677328438622 Min: -0.030329351831865088\n","Layer 1 - Gradient Weights Max: 0.01983913489545628 Min: -0.018991904024792037\n","Layer 0 - Gradient Weights Max: 0.02705573327806163 Min: -0.026904266183630853\n","ReLU Activation - Max: 2.063794055670634 Min: 0.0\n","ReLU Activation - Max: 0.676765093940745 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015392458941364364 Min: -0.024715702458214542\n","Layer 1 - Gradient Weights Max: 0.01606521090334904 Min: -0.023424414890014806\n","Layer 0 - Gradient Weights Max: 0.02601680589199362 Min: -0.02645816257683938\n","ReLU Activation - Max: 1.6944777026005071 Min: 0.0\n","ReLU Activation - Max: 0.6753130434112515 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01113049245589984 Min: -0.017636841288678314\n","Layer 1 - Gradient Weights Max: 0.02593753730139437 Min: -0.019632057397131353\n","Layer 0 - Gradient Weights Max: 0.03234642124766972 Min: -0.03773487374938283\n","ReLU Activation - Max: 1.328742216588396 Min: 0.0\n","ReLU Activation - Max: 0.5483292606151368 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018285913950694874 Min: -0.014578205092192804\n","Layer 1 - Gradient Weights Max: 0.02053605184991019 Min: -0.025706273904643112\n","Layer 0 - Gradient Weights Max: 0.03820505432659079 Min: -0.034929373831296726\n","ReLU Activation - Max: 1.5969729683430613 Min: 0.0\n","ReLU Activation - Max: 0.5705607964424365 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01511195663306267 Min: -0.020818048400451955\n","Layer 1 - Gradient Weights Max: 0.019556434557056725 Min: -0.018551268011172923\n","Layer 0 - Gradient Weights Max: 0.023379354827430927 Min: -0.03818640833903705\n","ReLU Activation - Max: 1.3974276879378353 Min: 0.0\n","ReLU Activation - Max: 0.5668761888388201 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01973662997543822 Min: -0.013194879999884545\n","Layer 1 - Gradient Weights Max: 0.0213229819734696 Min: -0.03385226179154821\n","Layer 0 - Gradient Weights Max: 0.034919145479408845 Min: -0.03259792379848022\n","ReLU Activation - Max: 1.1721257797029343 Min: 0.0\n","ReLU Activation - Max: 0.554304373037694 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010824251148799044 Min: -0.01769646624531337\n","Layer 1 - Gradient Weights Max: 0.014965769339398643 Min: -0.01952572332663387\n","Layer 0 - Gradient Weights Max: 0.0344229149718771 Min: -0.0275425179587351\n","ReLU Activation - Max: 1.6531745404114917 Min: 0.0\n","ReLU Activation - Max: 0.6444542260978964 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0165409440269498 Min: -0.02178210966971227\n","Layer 1 - Gradient Weights Max: 0.016896226941134036 Min: -0.0219295157949363\n","Layer 0 - Gradient Weights Max: 0.025525580008613494 Min: -0.029626798417610085\n","ReLU Activation - Max: 1.376159251847819 Min: 0.0\n","ReLU Activation - Max: 0.5831626559001892 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015664135780411065 Min: -0.021999196744712308\n","Layer 1 - Gradient Weights Max: 0.015600363590256926 Min: -0.024355090257482392\n","Layer 0 - Gradient Weights Max: 0.024020045469164624 Min: -0.023269064213104672\n","ReLU Activation - Max: 1.6667022679405163 Min: 0.0\n","ReLU Activation - Max: 0.5575267540625538 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016536464122075697 Min: -0.01370571639091005\n","Layer 1 - Gradient Weights Max: 0.018257273079120385 Min: -0.016365180343181233\n","Layer 0 - Gradient Weights Max: 0.02462953651580282 Min: -0.030720747858290286\n","ReLU Activation - Max: 1.2457820127974393 Min: 0.0\n","ReLU Activation - Max: 0.5434517815225723 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01341977470517033 Min: -0.015467298211190916\n","Layer 1 - Gradient Weights Max: 0.019129559646681765 Min: -0.02097478555755128\n","Layer 0 - Gradient Weights Max: 0.03203205711296239 Min: -0.031728412769833514\n","ReLU Activation - Max: 1.407109176459688 Min: 0.0\n","ReLU Activation - Max: 0.5843590258573993 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016595765294693508 Min: -0.02654915107266915\n","Layer 1 - Gradient Weights Max: 0.013704838064292518 Min: -0.018779052371911985\n","Layer 0 - Gradient Weights Max: 0.03473278544514655 Min: -0.033427287804269026\n","ReLU Activation - Max: 1.7795450227123006 Min: 0.0\n","ReLU Activation - Max: 0.564828531168355 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017756510454649217 Min: -0.012191825046303598\n","Layer 1 - Gradient Weights Max: 0.018030396015246735 Min: -0.020502456855686687\n","Layer 0 - Gradient Weights Max: 0.026935043208662555 Min: -0.03334325146479881\n","ReLU Activation - Max: 1.3904007585526317 Min: 0.0\n","ReLU Activation - Max: 0.6104634811614156 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013389752327462909 Min: -0.021935995708179522\n","Layer 1 - Gradient Weights Max: 0.019709105987959084 Min: -0.021223283205272465\n","Layer 0 - Gradient Weights Max: 0.03191586151459489 Min: -0.030831042526383464\n","ReLU Activation - Max: 1.3476753906060266 Min: 0.0\n","ReLU Activation - Max: 0.6905718190715457 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021576211354912802 Min: -0.015470619518073088\n","Layer 1 - Gradient Weights Max: 0.017848416793401385 Min: -0.02972741009249184\n","Layer 0 - Gradient Weights Max: 0.04224162285729692 Min: -0.03662108287881867\n","ReLU Activation - Max: 1.5466884397454437 Min: 0.0\n","ReLU Activation - Max: 0.6629642912506576 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011384597410475313 Min: -0.012724584959891707\n","Layer 1 - Gradient Weights Max: 0.019621639222704504 Min: -0.02041053517236692\n","Layer 0 - Gradient Weights Max: 0.028991133020842087 Min: -0.030740374971132207\n","ReLU Activation - Max: 1.416016408859733 Min: 0.0\n","ReLU Activation - Max: 0.6170247005013397 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011410665267327934 Min: -0.011098398846426792\n","Layer 1 - Gradient Weights Max: 0.014536789134541919 Min: -0.01733696228635883\n","Layer 0 - Gradient Weights Max: 0.031214484782855014 Min: -0.030051163926249416\n","ReLU Activation - Max: 1.5014212369002171 Min: 0.0\n","ReLU Activation - Max: 0.6461139867509608 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008574583023835273 Min: -0.011366396805322702\n","Layer 1 - Gradient Weights Max: 0.015849327320550204 Min: -0.017101319245271942\n","Layer 0 - Gradient Weights Max: 0.028005557269533642 Min: -0.02571593626553895\n","ReLU Activation - Max: 1.3793005219715115 Min: 0.0\n","ReLU Activation - Max: 0.5536400048344806 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014594681771558916 Min: -0.022299785070881702\n","Layer 1 - Gradient Weights Max: 0.017319156102919373 Min: -0.01867314074169275\n","Layer 0 - Gradient Weights Max: 0.03063368716395974 Min: -0.027552921075090095\n","ReLU Activation - Max: 1.3882308081824113 Min: 0.0\n","ReLU Activation - Max: 0.5999936297331182 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022772771608246336 Min: -0.02060406512826936\n","Layer 1 - Gradient Weights Max: 0.020987309150495483 Min: -0.020180137611001337\n","Layer 0 - Gradient Weights Max: 0.030905513741368417 Min: -0.03208757149564176\n","ReLU Activation - Max: 1.4258381863435443 Min: 0.0\n","ReLU Activation - Max: 0.6888285381178583 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023142785444155264 Min: -0.01641732821927097\n","Layer 1 - Gradient Weights Max: 0.016589606035084175 Min: -0.01913164366927033\n","Layer 0 - Gradient Weights Max: 0.03097886316641274 Min: -0.02933780188690915\n","ReLU Activation - Max: 1.4950382761398642 Min: 0.0\n","ReLU Activation - Max: 0.6642413031615197 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01099525399204977 Min: -0.014314297593682824\n","Layer 1 - Gradient Weights Max: 0.028180571699693874 Min: -0.01806628884060715\n","Layer 0 - Gradient Weights Max: 0.03381460795068706 Min: -0.028084475858092775\n","ReLU Activation - Max: 1.5209260604614585 Min: 0.0\n","ReLU Activation - Max: 0.6966698929074304 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01552713592807683 Min: -0.02137939782289635\n","Layer 1 - Gradient Weights Max: 0.016987045158346637 Min: -0.017802520076082903\n","Layer 0 - Gradient Weights Max: 0.03127734753076861 Min: -0.029819302539256933\n","ReLU Activation - Max: 1.2526145467084202 Min: 0.0\n","ReLU Activation - Max: 0.6020252391399857 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021062889358432972 Min: -0.01922718170406342\n","Layer 1 - Gradient Weights Max: 0.01766373884887201 Min: -0.020992485942722815\n","Layer 0 - Gradient Weights Max: 0.023414504759519787 Min: -0.025184813202506262\n","ReLU Activation - Max: 1.5334360529958913 Min: 0.0\n","ReLU Activation - Max: 0.6201576984986478 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018145431209216516 Min: -0.01904817365659883\n","Layer 1 - Gradient Weights Max: 0.021151867576426654 Min: -0.014699832670213187\n","Layer 0 - Gradient Weights Max: 0.024778310425985082 Min: -0.028983581098540274\n","ReLU Activation - Max: 1.3371623405425952 Min: 0.0\n","ReLU Activation - Max: 0.5434733546086843 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01127449658330834 Min: -0.0196392040285241\n","Layer 1 - Gradient Weights Max: 0.017843217493450803 Min: -0.02207681709516368\n","Layer 0 - Gradient Weights Max: 0.0266217615374108 Min: -0.030756202899318692\n","ReLU Activation - Max: 1.2891686347970748 Min: 0.0\n","ReLU Activation - Max: 0.6551649128625201 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014265941529193223 Min: -0.022854809417416563\n","Layer 1 - Gradient Weights Max: 0.019116141854397355 Min: -0.01339735543390177\n","Layer 0 - Gradient Weights Max: 0.037397414444573916 Min: -0.026880874410486554\n","ReLU Activation - Max: 1.9918473201171376 Min: 0.0\n","ReLU Activation - Max: 0.6816607962011798 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015233531183091512 Min: -0.025297423867301383\n","Layer 1 - Gradient Weights Max: 0.01596388925941176 Min: -0.028164179508904773\n","Layer 0 - Gradient Weights Max: 0.03351749137804455 Min: -0.028159420487970974\n","ReLU Activation - Max: 1.5763333423861712 Min: 0.0\n","ReLU Activation - Max: 0.5482838213419092 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017288238119305657 Min: -0.02083249396087307\n","Layer 1 - Gradient Weights Max: 0.020157859990277693 Min: -0.0196047424273119\n","Layer 0 - Gradient Weights Max: 0.0264603834258999 Min: -0.03236896786655851\n","ReLU Activation - Max: 1.788340558875639 Min: 0.0\n","ReLU Activation - Max: 0.6119005162258081 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009824527421226496 Min: -0.012543269860359654\n","Layer 1 - Gradient Weights Max: 0.018024710076983967 Min: -0.017426685540909783\n","Layer 0 - Gradient Weights Max: 0.029108211691549778 Min: -0.033385054561309004\n","ReLU Activation - Max: 1.947962228185776 Min: 0.0\n","ReLU Activation - Max: 0.8680901347225685 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015881684752689598 Min: -0.030174793478234648\n","Layer 1 - Gradient Weights Max: 0.021499119582570544 Min: -0.02042889446042961\n","Layer 0 - Gradient Weights Max: 0.027196606015672628 Min: -0.031934500466381056\n","ReLU Activation - Max: 1.7685344938867453 Min: 0.0\n","ReLU Activation - Max: 0.6574178712530346 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01656138329620676 Min: -0.012303331322738422\n","Layer 1 - Gradient Weights Max: 0.019664663440228103 Min: -0.016145898591425643\n","Layer 0 - Gradient Weights Max: 0.034509758581825124 Min: -0.03387910448537827\n","ReLU Activation - Max: 1.4801246953800853 Min: 0.0\n","ReLU Activation - Max: 0.4792013196225766 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01693759777818734 Min: -0.017351870880303665\n","Layer 1 - Gradient Weights Max: 0.020216059168230684 Min: -0.015432946212241372\n","Layer 0 - Gradient Weights Max: 0.031348870072920566 Min: -0.03225594898416039\n","ReLU Activation - Max: 1.3861799310185705 Min: 0.0\n","ReLU Activation - Max: 0.5619750281917907 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012391608169975189 Min: -0.02080056199474702\n","Layer 1 - Gradient Weights Max: 0.018907833300944886 Min: -0.014177641262368845\n","Layer 0 - Gradient Weights Max: 0.03849421326128732 Min: -0.04295885189882675\n","ReLU Activation - Max: 1.3029025173121964 Min: 0.0\n","ReLU Activation - Max: 0.6046130280539779 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016004485390687002 Min: -0.022691183778819304\n","Layer 1 - Gradient Weights Max: 0.01717339963244856 Min: -0.021314272101841964\n","Layer 0 - Gradient Weights Max: 0.03204543668238295 Min: -0.03210809351334239\n","ReLU Activation - Max: 1.606697608291877 Min: 0.0\n","ReLU Activation - Max: 0.637319557989901 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013393375290210504 Min: -0.011200061084133068\n","Layer 1 - Gradient Weights Max: 0.017782121115335826 Min: -0.019120626210178193\n","Layer 0 - Gradient Weights Max: 0.029606352806726657 Min: -0.026321066485314546\n","ReLU Activation - Max: 1.45390340491325 Min: 0.0\n","ReLU Activation - Max: 0.5835335235469998 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01627255991493043 Min: -0.0184333649748654\n","Layer 1 - Gradient Weights Max: 0.01749408824374645 Min: -0.019529167192557848\n","Layer 0 - Gradient Weights Max: 0.0279839176907077 Min: -0.032909746699900246\n","ReLU Activation - Max: 1.3662645782595217 Min: 0.0\n","ReLU Activation - Max: 0.543143516742825 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014990996386047225 Min: -0.019030963503677237\n","Layer 1 - Gradient Weights Max: 0.02135066594997826 Min: -0.02204751252546228\n","Layer 0 - Gradient Weights Max: 0.030483118995729967 Min: -0.025275999164005476\n","ReLU Activation - Max: 1.6550132794155845 Min: 0.0\n","ReLU Activation - Max: 0.6801872693481485 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018520612283469236 Min: -0.013569286868323236\n","Layer 1 - Gradient Weights Max: 0.021799787297760803 Min: -0.02841478924403084\n","Layer 0 - Gradient Weights Max: 0.034633228669231776 Min: -0.030765546788075193\n","ReLU Activation - Max: 1.3265404285527425 Min: 0.0\n","ReLU Activation - Max: 0.7793834076610373 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011859991020999869 Min: -0.010681633756846512\n","Layer 1 - Gradient Weights Max: 0.02000786191786267 Min: -0.01950731196082611\n","Layer 0 - Gradient Weights Max: 0.028185318484447903 Min: -0.032260399988818175\n","ReLU Activation - Max: 1.467806687162536 Min: 0.0\n","ReLU Activation - Max: 0.6180238098628875 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018310469910621515 Min: -0.015973801134920855\n","Layer 1 - Gradient Weights Max: 0.023598896981320863 Min: -0.017650444850198455\n","Layer 0 - Gradient Weights Max: 0.026580852180146452 Min: -0.029290294315601758\n","ReLU Activation - Max: 1.3028603422311313 Min: 0.0\n","ReLU Activation - Max: 0.5402993403397556 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010322993384061743 Min: -0.017121745342635265\n","Layer 1 - Gradient Weights Max: 0.014468691276947304 Min: -0.02253641675179179\n","Layer 0 - Gradient Weights Max: 0.0337544924564972 Min: -0.026623519099695278\n","ReLU Activation - Max: 1.480518433392821 Min: 0.0\n","ReLU Activation - Max: 0.5749158459161277 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01941228266002579 Min: -0.019379862824885945\n","Layer 1 - Gradient Weights Max: 0.020233525323079125 Min: -0.020785617656554997\n","Layer 0 - Gradient Weights Max: 0.0300845809001777 Min: -0.028492666251091044\n","ReLU Activation - Max: 1.6886665277789459 Min: 0.0\n","ReLU Activation - Max: 0.535906180128052 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01943230597928343 Min: -0.02063175960081353\n","Layer 1 - Gradient Weights Max: 0.025971393092687374 Min: -0.02642278499772254\n","Layer 0 - Gradient Weights Max: 0.026025745606692376 Min: -0.031246234146999383\n","ReLU Activation - Max: 1.7213134054073531 Min: 0.0\n","ReLU Activation - Max: 0.5649811934033657 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015625299145743867 Min: -0.025637792818297367\n","Layer 1 - Gradient Weights Max: 0.030106556950661327 Min: -0.030080549802873495\n","Layer 0 - Gradient Weights Max: 0.03377520965581649 Min: -0.028498718053538333\n","ReLU Activation - Max: 1.5972176539832197 Min: 0.0\n","ReLU Activation - Max: 0.6192860033765206 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012821094641601857 Min: -0.01161807966269623\n","Layer 1 - Gradient Weights Max: 0.015873622312153918 Min: -0.03498021713926167\n","Layer 0 - Gradient Weights Max: 0.024048299061199298 Min: -0.0317162164418964\n","ReLU Activation - Max: 1.6294512368117515 Min: 0.0\n","ReLU Activation - Max: 0.5616299243177547 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012303442041175719 Min: -0.020745003961747416\n","Layer 1 - Gradient Weights Max: 0.025082166883204703 Min: -0.018973628786086248\n","Layer 0 - Gradient Weights Max: 0.025332298897740604 Min: -0.03288411850473605\n","ReLU Activation - Max: 1.766382498591262 Min: 0.0\n","ReLU Activation - Max: 0.669100164814152 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014199478051950506 Min: -0.019166021252860425\n","Layer 1 - Gradient Weights Max: 0.015808505364479933 Min: -0.028700564951726678\n","Layer 0 - Gradient Weights Max: 0.026259854737538814 Min: -0.04068091146861413\n","ReLU Activation - Max: 1.3819498036437736 Min: 0.0\n","ReLU Activation - Max: 0.5796776703533689 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010303350359352929 Min: -0.018785218173044727\n","Layer 1 - Gradient Weights Max: 0.015938686419037854 Min: -0.01892031418384406\n","Layer 0 - Gradient Weights Max: 0.02637206048125071 Min: -0.039500177119536885\n","ReLU Activation - Max: 1.3939031504015607 Min: 0.0\n","ReLU Activation - Max: 0.7506827190175227 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015393106573192702 Min: -0.030781379106130703\n","Layer 1 - Gradient Weights Max: 0.02901239163717983 Min: -0.024237490151213854\n","Layer 0 - Gradient Weights Max: 0.024952910353613564 Min: -0.030650178407002675\n","ReLU Activation - Max: 1.6633574442695251 Min: 0.0\n","ReLU Activation - Max: 0.7343716322285113 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014831400072335466 Min: -0.017206987143251244\n","Layer 1 - Gradient Weights Max: 0.023431054038364766 Min: -0.01743555318935879\n","Layer 0 - Gradient Weights Max: 0.024408578393715196 Min: -0.027014587725361045\n","ReLU Activation - Max: 1.4602664456303973 Min: 0.0\n","ReLU Activation - Max: 0.5634603524386905 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017105050681919656 Min: -0.016745303714339715\n","Layer 1 - Gradient Weights Max: 0.017737136218665742 Min: -0.024123849663930396\n","Layer 0 - Gradient Weights Max: 0.03425592538960231 Min: -0.027628141639546792\n","ReLU Activation - Max: 1.4083059410389303 Min: 0.0\n","ReLU Activation - Max: 0.7292948506711421 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010312800761780302 Min: -0.013274694967148861\n","Layer 1 - Gradient Weights Max: 0.019931918396389 Min: -0.01917210658869\n","Layer 0 - Gradient Weights Max: 0.03151906908003834 Min: -0.028029251763912362\n","ReLU Activation - Max: 1.9193916501087478 Min: 0.0\n","ReLU Activation - Max: 0.6494054716483965 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01210645112928411 Min: -0.013589993424774476\n","Layer 1 - Gradient Weights Max: 0.01776406156361278 Min: -0.03521515235711983\n","Layer 0 - Gradient Weights Max: 0.03001123589446001 Min: -0.03319897678447741\n","ReLU Activation - Max: 1.334152805666308 Min: 0.0\n","ReLU Activation - Max: 0.566019335223146 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015777601577648556 Min: -0.019668822769399345\n","Layer 1 - Gradient Weights Max: 0.01749822382968887 Min: -0.019776225300921418\n","Layer 0 - Gradient Weights Max: 0.03090556992387739 Min: -0.025738373528230506\n","ReLU Activation - Max: 1.5129375009688246 Min: 0.0\n","ReLU Activation - Max: 0.6049594769187554 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017592656450154367 Min: -0.01475548317384239\n","Layer 1 - Gradient Weights Max: 0.017742728059423393 Min: -0.02325583558908143\n","Layer 0 - Gradient Weights Max: 0.026618979980898173 Min: -0.025219029261664447\n","ReLU Activation - Max: 1.5148903696860292 Min: 0.0\n","ReLU Activation - Max: 0.5289694680114981 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.027626735199944585 Min: -0.02148236341409023\n","Layer 1 - Gradient Weights Max: 0.014795253288514377 Min: -0.02103867669580354\n","Layer 0 - Gradient Weights Max: 0.02798298461839728 Min: -0.029849086085432013\n","ReLU Activation - Max: 1.3226003847157428 Min: 0.0\n","ReLU Activation - Max: 0.6071404399363218 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0136279200855561 Min: -0.015951102978404016\n","Layer 1 - Gradient Weights Max: 0.01516764702686197 Min: -0.01448570025203708\n","Layer 0 - Gradient Weights Max: 0.027636378627243385 Min: -0.029056433694283988\n","ReLU Activation - Max: 1.3906823671829418 Min: 0.0\n","ReLU Activation - Max: 0.5718011564069517 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021107334732059377 Min: -0.019701522489216007\n","Layer 1 - Gradient Weights Max: 0.02259680464220997 Min: -0.020541542566465235\n","Layer 0 - Gradient Weights Max: 0.027582507020371187 Min: -0.02590991511341946\n","ReLU Activation - Max: 1.3201976453556545 Min: 0.0\n","ReLU Activation - Max: 0.590770894170656 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015770215795626066 Min: -0.01288178316116967\n","Layer 1 - Gradient Weights Max: 0.012976147831505625 Min: -0.02197145565533228\n","Layer 0 - Gradient Weights Max: 0.031726228792972926 Min: -0.02691956527936792\n","ReLU Activation - Max: 1.4145011837361572 Min: 0.0\n","ReLU Activation - Max: 0.675250458176111 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017452000386497444 Min: -0.012652442552937937\n","Layer 1 - Gradient Weights Max: 0.017887475660200398 Min: -0.01615382228280656\n","Layer 0 - Gradient Weights Max: 0.028708277120337047 Min: -0.028898914747930878\n","ReLU Activation - Max: 1.6112449537061582 Min: 0.0\n","ReLU Activation - Max: 0.723643339023121 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012915289466736786 Min: -0.01994533299447881\n","Layer 1 - Gradient Weights Max: 0.015085362449069165 Min: -0.024135537788363692\n","Layer 0 - Gradient Weights Max: 0.027821832114314046 Min: -0.027460360098804516\n","ReLU Activation - Max: 1.8155996703027482 Min: 0.0\n","ReLU Activation - Max: 0.5769445637950046 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015437526621665693 Min: -0.035541469312865635\n","Layer 1 - Gradient Weights Max: 0.01842010603748164 Min: -0.0235271885826677\n","Layer 0 - Gradient Weights Max: 0.03489356631172902 Min: -0.032876752098322096\n","ReLU Activation - Max: 1.652308952073632 Min: 0.0\n","ReLU Activation - Max: 0.572789814270062 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014032570332517353 Min: -0.01175116476769595\n","Layer 1 - Gradient Weights Max: 0.01781619307149519 Min: -0.018967504223069014\n","Layer 0 - Gradient Weights Max: 0.02778809338557669 Min: -0.028099701404703147\n","ReLU Activation - Max: 1.8244372252369856 Min: 0.0\n","ReLU Activation - Max: 0.632838615762293 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012331049633907108 Min: -0.022236957935755464\n","Layer 1 - Gradient Weights Max: 0.022548461520530097 Min: -0.018260084289457003\n","Layer 0 - Gradient Weights Max: 0.02628622120902475 Min: -0.031551738342118316\n","ReLU Activation - Max: 1.5386335538860503 Min: 0.0\n","ReLU Activation - Max: 0.636002818383319 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015211754008338954 Min: -0.018200147201508466\n","Layer 1 - Gradient Weights Max: 0.014818194006240537 Min: -0.022267128127738622\n","Layer 0 - Gradient Weights Max: 0.024627468499157288 Min: -0.024579232447176486\n","ReLU Activation - Max: 1.7520142959938754 Min: 0.0\n","ReLU Activation - Max: 0.5380751999588178 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015899884838958676 Min: -0.017548414174089394\n","Layer 1 - Gradient Weights Max: 0.020885306620504904 Min: -0.029206362364565308\n","Layer 0 - Gradient Weights Max: 0.03639076218767151 Min: -0.03226182350843015\n","ReLU Activation - Max: 1.7880425472122543 Min: 0.0\n","ReLU Activation - Max: 0.6788346403458279 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010873736020278784 Min: -0.0167327994774784\n","Layer 1 - Gradient Weights Max: 0.019329784734646686 Min: -0.013974873516446395\n","Layer 0 - Gradient Weights Max: 0.02853965637671819 Min: -0.026455552592933574\n","ReLU Activation - Max: 1.3367785157213332 Min: 0.0\n","ReLU Activation - Max: 0.5734203091667996 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0147950065711635 Min: -0.014113203960859499\n","Layer 1 - Gradient Weights Max: 0.013965989960223132 Min: -0.020499384427667522\n","Layer 0 - Gradient Weights Max: 0.024042220664210627 Min: -0.03082126542585523\n","ReLU Activation - Max: 1.433346995541051 Min: 0.0\n","ReLU Activation - Max: 0.6405585639040797 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01627933972810047 Min: -0.018760718073602654\n","Layer 1 - Gradient Weights Max: 0.015240671022874824 Min: -0.02694461510839889\n","Layer 0 - Gradient Weights Max: 0.02415866032180889 Min: -0.02471536034259189\n","ReLU Activation - Max: 1.6742811138348255 Min: 0.0\n","ReLU Activation - Max: 0.5697844779582654 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024288129260074995 Min: -0.02317113022952195\n","Layer 1 - Gradient Weights Max: 0.026131587430427167 Min: -0.021354209096969577\n","Layer 0 - Gradient Weights Max: 0.03867336047621387 Min: -0.03154048646117249\n","ReLU Activation - Max: 1.3330152423999504 Min: 0.0\n","ReLU Activation - Max: 0.6348203673616123 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01868085301798688 Min: -0.01341554152231076\n","Layer 1 - Gradient Weights Max: 0.021750910335828016 Min: -0.021494340788412836\n","Layer 0 - Gradient Weights Max: 0.03549040135449686 Min: -0.048394131142597795\n","ReLU Activation - Max: 1.330315235296553 Min: 0.0\n","ReLU Activation - Max: 0.5900951859539053 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01036363058890604 Min: -0.02525907252877043\n","Layer 1 - Gradient Weights Max: 0.01767987012099424 Min: -0.01928519249742365\n","Layer 0 - Gradient Weights Max: 0.024912747803859867 Min: -0.021605444573932996\n","ReLU Activation - Max: 1.3983691052484573 Min: 0.0\n","ReLU Activation - Max: 0.6466193821894082 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.027873285416506584 Min: -0.01698024985889915\n","Layer 1 - Gradient Weights Max: 0.023539175708232826 Min: -0.022500120834390652\n","Layer 0 - Gradient Weights Max: 0.030631872564216563 Min: -0.03149825356623608\n","ReLU Activation - Max: 1.2944619152067338 Min: 0.0\n","ReLU Activation - Max: 0.5260161295942528 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021965562205635395 Min: -0.021650784090134732\n","Layer 1 - Gradient Weights Max: 0.01872226365187773 Min: -0.021581924167001148\n","Layer 0 - Gradient Weights Max: 0.03291041214842995 Min: -0.023656273895966647\n","ReLU Activation - Max: 1.3187379349895378 Min: 0.0\n","ReLU Activation - Max: 0.6248067059489785 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.026094432158699245 Min: -0.023918205122686494\n","Layer 1 - Gradient Weights Max: 0.020222679267758465 Min: -0.02164488294188475\n","Layer 0 - Gradient Weights Max: 0.02759720677795117 Min: -0.02839714383166951\n","ReLU Activation - Max: 1.379321398791607 Min: 0.0\n","ReLU Activation - Max: 0.6137255004256293 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014580962965401257 Min: -0.017794208270090702\n","Layer 1 - Gradient Weights Max: 0.01571521714970284 Min: -0.02145641318648889\n","Layer 0 - Gradient Weights Max: 0.02905308907922062 Min: -0.02729991808122523\n","ReLU Activation - Max: 1.6891359562615234 Min: 0.0\n","ReLU Activation - Max: 0.5823011032178279 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010987601804973202 Min: -0.024944869614274116\n","Layer 1 - Gradient Weights Max: 0.018095080233824487 Min: -0.021768533768090518\n","Layer 0 - Gradient Weights Max: 0.027689285747662086 Min: -0.027910462092104626\n","ReLU Activation - Max: 1.5689437169281093 Min: 0.0\n","ReLU Activation - Max: 0.5839590023579257 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01813772431640921 Min: -0.014151908066544587\n","Layer 1 - Gradient Weights Max: 0.02349041509914001 Min: -0.023898461776361544\n","Layer 0 - Gradient Weights Max: 0.029509101836357163 Min: -0.03517078867920292\n","ReLU Activation - Max: 1.4700507897050326 Min: 0.0\n","ReLU Activation - Max: 0.5574788169161231 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019326312864238228 Min: -0.02118771348492491\n","Layer 1 - Gradient Weights Max: 0.02018069548044061 Min: -0.021894816921230076\n","Layer 0 - Gradient Weights Max: 0.033322112322379825 Min: -0.03967072153862167\n","ReLU Activation - Max: 1.5879573898912376 Min: 0.0\n","ReLU Activation - Max: 0.6778980534266703 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013823995606782874 Min: -0.020195662673859884\n","Layer 1 - Gradient Weights Max: 0.017583547258746123 Min: -0.02731588408031369\n","Layer 0 - Gradient Weights Max: 0.029788751420509844 Min: -0.029484787534819146\n","ReLU Activation - Max: 1.3289340552663738 Min: 0.0\n","ReLU Activation - Max: 0.6064032394327513 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01850207351325398 Min: -0.020797193663862075\n","Layer 1 - Gradient Weights Max: 0.023535938720970807 Min: -0.023687081154779337\n","Layer 0 - Gradient Weights Max: 0.029119297322749114 Min: -0.022480014399465488\n","ReLU Activation - Max: 1.7483727810369605 Min: 0.0\n","ReLU Activation - Max: 0.5026067049154 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014487304127619784 Min: -0.021736314205819285\n","Layer 1 - Gradient Weights Max: 0.02097076581802937 Min: -0.02375293937441772\n","Layer 0 - Gradient Weights Max: 0.032859558028098 Min: -0.027797952565607933\n","ReLU Activation - Max: 1.5195115571349238 Min: 0.0\n","ReLU Activation - Max: 0.583545561708365 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015649942396478145 Min: -0.025140007977816403\n","Layer 1 - Gradient Weights Max: 0.01823413399068376 Min: -0.02381332696169414\n","Layer 0 - Gradient Weights Max: 0.0311109387669472 Min: -0.025415752256601143\n","ReLU Activation - Max: 1.3522457794576188 Min: 0.0\n","ReLU Activation - Max: 0.6393504118673263 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014056739365517427 Min: -0.020392556929576408\n","Layer 1 - Gradient Weights Max: 0.022726147151082243 Min: -0.020084352295537396\n","Layer 0 - Gradient Weights Max: 0.030115379800321362 Min: -0.035107614901861615\n","ReLU Activation - Max: 1.4180497712434128 Min: 0.0\n","ReLU Activation - Max: 0.5192259739783732 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017818228822084985 Min: -0.020021783789754218\n","Layer 1 - Gradient Weights Max: 0.021858092322007135 Min: -0.022011841303678926\n","Layer 0 - Gradient Weights Max: 0.0332168985586761 Min: -0.031739127982218666\n","ReLU Activation - Max: 1.4610203429908475 Min: 0.0\n","ReLU Activation - Max: 0.5637310671018113 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013830698098507841 Min: -0.026833866398067122\n","Layer 1 - Gradient Weights Max: 0.02049426807840056 Min: -0.016255548137889392\n","Layer 0 - Gradient Weights Max: 0.02442964097397466 Min: -0.02534184585893727\n","ReLU Activation - Max: 1.397805675814284 Min: 0.0\n","ReLU Activation - Max: 0.5880439094789862 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022765873330079057 Min: -0.0282211053658157\n","Layer 1 - Gradient Weights Max: 0.021699296365314463 Min: -0.01913561050770637\n","Layer 0 - Gradient Weights Max: 0.035141602341614044 Min: -0.02816009030932114\n","ReLU Activation - Max: 1.7352442172590847 Min: 0.0\n","ReLU Activation - Max: 0.5706446784396039 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02220657001948519 Min: -0.01266974127769446\n","Layer 1 - Gradient Weights Max: 0.015832826823275196 Min: -0.02149234948445658\n","Layer 0 - Gradient Weights Max: 0.04431054823980719 Min: -0.03127596507291326\n","ReLU Activation - Max: 1.5472639955754488 Min: 0.0\n","ReLU Activation - Max: 0.6094314519879216 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014226677906677233 Min: -0.01660181130089076\n","Layer 1 - Gradient Weights Max: 0.024086600659657387 Min: -0.016781138316175323\n","Layer 0 - Gradient Weights Max: 0.029153791178876894 Min: -0.02500160546529501\n","ReLU Activation - Max: 1.479321244515819 Min: 0.0\n","ReLU Activation - Max: 0.6275855382427757 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020458665162521823 Min: -0.014111878402676872\n","Layer 1 - Gradient Weights Max: 0.023986315223032306 Min: -0.016093359491230176\n","Layer 0 - Gradient Weights Max: 0.03145857451807526 Min: -0.026320859991313977\n","ReLU Activation - Max: 1.5325918241719776 Min: 0.0\n","ReLU Activation - Max: 0.5533181537128669 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013659493640721278 Min: -0.023755474901320014\n","Layer 1 - Gradient Weights Max: 0.020381935092214934 Min: -0.021572304495310116\n","Layer 0 - Gradient Weights Max: 0.036325645902488796 Min: -0.034972640816153\n","ReLU Activation - Max: 1.1977101735712459 Min: 0.0\n","ReLU Activation - Max: 0.5862759317205379 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012027548552926001 Min: -0.01810712626838105\n","Layer 1 - Gradient Weights Max: 0.019706974383854277 Min: -0.02698331635430631\n","Layer 0 - Gradient Weights Max: 0.03041385821736092 Min: -0.03091294370596375\n","ReLU Activation - Max: 1.718858792582038 Min: 0.0\n","ReLU Activation - Max: 0.6972030594096513 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02066605662738933 Min: -0.01721154318183432\n","Layer 1 - Gradient Weights Max: 0.017890066277149393 Min: -0.019410148050977623\n","Layer 0 - Gradient Weights Max: 0.031185628262659686 Min: -0.0326200061660305\n","ReLU Activation - Max: 1.5317036390460788 Min: 0.0\n","ReLU Activation - Max: 0.7003016443335619 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013048071792600906 Min: -0.016786267705593258\n","Layer 1 - Gradient Weights Max: 0.014697708857420663 Min: -0.028093606756850426\n","Layer 0 - Gradient Weights Max: 0.03829913703313826 Min: -0.03599659845666729\n","ReLU Activation - Max: 1.3126801216148665 Min: 0.0\n","ReLU Activation - Max: 0.5904731194962911 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016625687117122246 Min: -0.020371837956308928\n","Layer 1 - Gradient Weights Max: 0.02434815933002452 Min: -0.017620444418281797\n","Layer 0 - Gradient Weights Max: 0.02558363280063183 Min: -0.03632294050364972\n","ReLU Activation - Max: 1.6878542840543416 Min: 0.0\n","ReLU Activation - Max: 0.5679482243730153 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013436601925908839 Min: -0.02306668441339993\n","Layer 1 - Gradient Weights Max: 0.017839685377440594 Min: -0.01968798325414489\n","Layer 0 - Gradient Weights Max: 0.027075257460435674 Min: -0.030426575079389673\n","ReLU Activation - Max: 1.4000197135927808 Min: 0.0\n","ReLU Activation - Max: 0.5521207228656294 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020279674705563144 Min: -0.015594919859711495\n","Layer 1 - Gradient Weights Max: 0.024532310802355708 Min: -0.01808463928405375\n","Layer 0 - Gradient Weights Max: 0.03238122206928609 Min: -0.038741453646799304\n","ReLU Activation - Max: 1.569680023847983 Min: 0.0\n","ReLU Activation - Max: 0.5815005114610691 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01542419964269722 Min: -0.024428672785413996\n","Layer 1 - Gradient Weights Max: 0.018016157407702824 Min: -0.019212193044634715\n","Layer 0 - Gradient Weights Max: 0.021351894101890905 Min: -0.024588282549717906\n","ReLU Activation - Max: 1.516199119780618 Min: 0.0\n","ReLU Activation - Max: 0.553705684119845 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01344938455235253 Min: -0.014540153758316016\n","Layer 1 - Gradient Weights Max: 0.014768120568212967 Min: -0.016969798487822468\n","Layer 0 - Gradient Weights Max: 0.058611022680605515 Min: -0.03615452444033344\n","ReLU Activation - Max: 1.3992163766447838 Min: 0.0\n","ReLU Activation - Max: 0.5692025086041554 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022002315949523277 Min: -0.018890629908827523\n","Layer 1 - Gradient Weights Max: 0.023553557511930362 Min: -0.02356719168275366\n","Layer 0 - Gradient Weights Max: 0.03108313453578084 Min: -0.03596506847062262\n","ReLU Activation - Max: 1.440078310845158 Min: 0.0\n","ReLU Activation - Max: 0.6247850415211579 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012824795107643514 Min: -0.010570507837636066\n","Layer 1 - Gradient Weights Max: 0.015590526371853448 Min: -0.018519928084674467\n","Layer 0 - Gradient Weights Max: 0.02819985060577547 Min: -0.027722255856337332\n","ReLU Activation - Max: 1.3623701288811838 Min: 0.0\n","ReLU Activation - Max: 0.5692835917544715 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013344250194184896 Min: -0.01667267384713674\n","Layer 1 - Gradient Weights Max: 0.01696740645821575 Min: -0.014280732205316024\n","Layer 0 - Gradient Weights Max: 0.029420718902776418 Min: -0.025193556095408385\n","ReLU Activation - Max: 1.3434066912997034 Min: 0.0\n","ReLU Activation - Max: 0.5244281540967164 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011914568618146189 Min: -0.012330979039704227\n","Layer 1 - Gradient Weights Max: 0.017029789188333515 Min: -0.0153689318401273\n","Layer 0 - Gradient Weights Max: 0.02813271397558489 Min: -0.038647540273043776\n","ReLU Activation - Max: 1.4179639343092076 Min: 0.0\n","ReLU Activation - Max: 0.6116426380901385 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018606845754096737 Min: -0.024702083816374447\n","Layer 1 - Gradient Weights Max: 0.022177292993951044 Min: -0.02020742488617767\n","Layer 0 - Gradient Weights Max: 0.03520584878683936 Min: -0.029644593060909136\n","ReLU Activation - Max: 1.617794109518519 Min: 0.0\n","ReLU Activation - Max: 0.5974417286733543 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0171512529227678 Min: -0.0201924952985011\n","Layer 1 - Gradient Weights Max: 0.019545197355618472 Min: -0.022235114530426203\n","Layer 0 - Gradient Weights Max: 0.029537397932762742 Min: -0.026392224309837838\n","ReLU Activation - Max: 1.6863199555324098 Min: 0.0\n","ReLU Activation - Max: 0.6011359037362013 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.00948871765935083 Min: -0.009146169800589153\n","Layer 1 - Gradient Weights Max: 0.016070378799038942 Min: -0.018157648749155913\n","Layer 0 - Gradient Weights Max: 0.028268909270706678 Min: -0.02649932839407151\n","ReLU Activation - Max: 1.6318304842444202 Min: 0.0\n","ReLU Activation - Max: 0.5354008070135826 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01752089705115418 Min: -0.019270363308250615\n","Layer 1 - Gradient Weights Max: 0.021662803925786813 Min: -0.024590472474745065\n","Layer 0 - Gradient Weights Max: 0.030473212293122835 Min: -0.02993773626197971\n","ReLU Activation - Max: 1.4010803804389453 Min: 0.0\n","ReLU Activation - Max: 0.6536477129717132 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015547405691467397 Min: -0.032782040145513\n","Layer 1 - Gradient Weights Max: 0.018665515395472083 Min: -0.025625894873651977\n","Layer 0 - Gradient Weights Max: 0.02519444153481658 Min: -0.027346843439088615\n","ReLU Activation - Max: 1.3769077086073132 Min: 0.0\n","ReLU Activation - Max: 0.5550758911533827 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013663906055906849 Min: -0.009390682720114311\n","Layer 1 - Gradient Weights Max: 0.013257450350828717 Min: -0.01906056914474571\n","Layer 0 - Gradient Weights Max: 0.02994468071448928 Min: -0.026910716760016124\n","ReLU Activation - Max: 1.6050243493172904 Min: 0.0\n","ReLU Activation - Max: 0.6482560609812101 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019190726265185715 Min: -0.01338022882940432\n","Layer 1 - Gradient Weights Max: 0.02133806925402838 Min: -0.014563866687409286\n","Layer 0 - Gradient Weights Max: 0.029487593163784603 Min: -0.02680353905172642\n","ReLU Activation - Max: 1.460913615719792 Min: 0.0\n","ReLU Activation - Max: 0.6302586812239204 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011599404918251241 Min: -0.015027216786771198\n","Layer 1 - Gradient Weights Max: 0.018424027399508266 Min: -0.018382648905831172\n","Layer 0 - Gradient Weights Max: 0.03336628765560232 Min: -0.02663471394276336\n","ReLU Activation - Max: 1.5028178493283375 Min: 0.0\n","ReLU Activation - Max: 0.5812838353914356 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01742020261765009 Min: -0.024412286481909864\n","Layer 1 - Gradient Weights Max: 0.025298419124648613 Min: -0.027695209759577424\n","Layer 0 - Gradient Weights Max: 0.02629601464396007 Min: -0.025327564538861716\n","ReLU Activation - Max: 1.595258818237755 Min: 0.0\n","ReLU Activation - Max: 0.6702052544106875 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01248253469036853 Min: -0.018976513573342953\n","Layer 1 - Gradient Weights Max: 0.029830454413682497 Min: -0.01644716963534999\n","Layer 0 - Gradient Weights Max: 0.032772890664214765 Min: -0.029346238903732433\n","ReLU Activation - Max: 1.8723424584741604 Min: 0.0\n","ReLU Activation - Max: 0.5821229141639042 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020170925349101577 Min: -0.02196160303468478\n","Layer 1 - Gradient Weights Max: 0.016799991516862303 Min: -0.01671916163324832\n","Layer 0 - Gradient Weights Max: 0.0286808632746717 Min: -0.03698082925541721\n","ReLU Activation - Max: 1.4674097329676972 Min: 0.0\n","ReLU Activation - Max: 0.5416730297872071 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014757245747949654 Min: -0.016188533336821846\n","Layer 1 - Gradient Weights Max: 0.022361731415783424 Min: -0.020834781510506144\n","Layer 0 - Gradient Weights Max: 0.030187944676255803 Min: -0.0304560944577915\n","ReLU Activation - Max: 1.3816176166558796 Min: 0.0\n","ReLU Activation - Max: 0.5703008759914204 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022380533290888217 Min: -0.017836846638599017\n","Layer 1 - Gradient Weights Max: 0.018511351613842444 Min: -0.017568834637214186\n","Layer 0 - Gradient Weights Max: 0.023721726078893272 Min: -0.030671321581275706\n","ReLU Activation - Max: 1.7092944001707813 Min: 0.0\n","ReLU Activation - Max: 0.6861009763692667 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023210633981223593 Min: -0.03888952845383477\n","Layer 1 - Gradient Weights Max: 0.029512759154118426 Min: -0.029815962506664833\n","Layer 0 - Gradient Weights Max: 0.034789685246726314 Min: -0.03217123944066864\n","ReLU Activation - Max: 1.3612549657089312 Min: 0.0\n","ReLU Activation - Max: 0.6040951032285748 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01703969652188752 Min: -0.016997214478241776\n","Layer 1 - Gradient Weights Max: 0.015403510646672804 Min: -0.022930753791454015\n","Layer 0 - Gradient Weights Max: 0.028146978005829368 Min: -0.03100073121570229\n","ReLU Activation - Max: 1.5595389601155905 Min: 0.0\n","ReLU Activation - Max: 0.5702015283056187 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017312349004014918 Min: -0.017948699263812824\n","Layer 1 - Gradient Weights Max: 0.019295731526351705 Min: -0.01740323404639033\n","Layer 0 - Gradient Weights Max: 0.03279744484273152 Min: -0.026013277491591002\n","ReLU Activation - Max: 1.3046484445219513 Min: 0.0\n","ReLU Activation - Max: 0.5871443178613418 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014056275521954013 Min: -0.013677571694020781\n","Layer 1 - Gradient Weights Max: 0.017107122669150782 Min: -0.01727537567945972\n","Layer 0 - Gradient Weights Max: 0.03042713592785951 Min: -0.03255917644129372\n","ReLU Activation - Max: 1.5859760600324722 Min: 0.0\n","ReLU Activation - Max: 0.6350868972993068 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014720884516836698 Min: -0.022994847243385123\n","Layer 1 - Gradient Weights Max: 0.02008591171927914 Min: -0.03385342539197254\n","Layer 0 - Gradient Weights Max: 0.029276745942787365 Min: -0.022769483761638953\n","ReLU Activation - Max: 1.4193395942578786 Min: 0.0\n","ReLU Activation - Max: 0.7779459246472211 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019849860557509775 Min: -0.017876458333926256\n","Layer 1 - Gradient Weights Max: 0.02126595852985668 Min: -0.020396482038326513\n","Layer 0 - Gradient Weights Max: 0.02515809322761384 Min: -0.028836002535082315\n","ReLU Activation - Max: 1.2442192065647788 Min: 0.0\n","ReLU Activation - Max: 0.620845384261159 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01468724365153155 Min: -0.018751545821927996\n","Layer 1 - Gradient Weights Max: 0.019492784173266765 Min: -0.019957022681537564\n","Layer 0 - Gradient Weights Max: 0.031162093982655348 Min: -0.025463773503733002\n","ReLU Activation - Max: 1.314554560839054 Min: 0.0\n","ReLU Activation - Max: 0.6462359133433662 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009434600478001482 Min: -0.014946657710377018\n","Layer 1 - Gradient Weights Max: 0.025211067600972248 Min: -0.017379303479423392\n","Layer 0 - Gradient Weights Max: 0.028064547474566232 Min: -0.03583546259615182\n","ReLU Activation - Max: 1.367837091324657 Min: 0.0\n","ReLU Activation - Max: 0.6550856349235818 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01697336003511833 Min: -0.02036818081983496\n","Layer 1 - Gradient Weights Max: 0.014140856275303138 Min: -0.025206314259828947\n","Layer 0 - Gradient Weights Max: 0.030899503057707473 Min: -0.03558462130850011\n","ReLU Activation - Max: 1.4694738903397164 Min: 0.0\n","ReLU Activation - Max: 0.6295859946188516 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011853297818871561 Min: -0.01523204397044752\n","Layer 1 - Gradient Weights Max: 0.015815641474493172 Min: -0.02250611737585946\n","Layer 0 - Gradient Weights Max: 0.026175385405714006 Min: -0.031095246294285922\n","ReLU Activation - Max: 1.4368222879461092 Min: 0.0\n","ReLU Activation - Max: 0.5067113766261444 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014087029196907274 Min: -0.014574857700106768\n","Layer 1 - Gradient Weights Max: 0.017011791335173113 Min: -0.01981236256186471\n","Layer 0 - Gradient Weights Max: 0.025612577337037694 Min: -0.02858406962361039\n","ReLU Activation - Max: 1.6558433388701241 Min: 0.0\n","ReLU Activation - Max: 0.7283803888072659 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013359275481779675 Min: -0.017605418533217704\n","Layer 1 - Gradient Weights Max: 0.015318668785508029 Min: -0.01637854759045353\n","Layer 0 - Gradient Weights Max: 0.02722952650450471 Min: -0.025583569485474213\n","ReLU Activation - Max: 1.5543497416174201 Min: 0.0\n","ReLU Activation - Max: 0.6303213047789049 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011164693638302754 Min: -0.012037437215972977\n","Layer 1 - Gradient Weights Max: 0.021570125406958785 Min: -0.015160318917711621\n","Layer 0 - Gradient Weights Max: 0.028128984970241782 Min: -0.031127025099860713\n","ReLU Activation - Max: 1.543843908506635 Min: 0.0\n","ReLU Activation - Max: 0.6720967085891112 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011044999733474772 Min: -0.01234812027914982\n","Layer 1 - Gradient Weights Max: 0.015551782208313568 Min: -0.02035284763837507\n","Layer 0 - Gradient Weights Max: 0.02682985284686694 Min: -0.03386931357183838\n","ReLU Activation - Max: 1.4563579788983159 Min: 0.0\n","ReLU Activation - Max: 0.5219007061236376 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01937488733677434 Min: -0.026513396422596053\n","Layer 1 - Gradient Weights Max: 0.022383295378777365 Min: -0.017559934057629113\n","Layer 0 - Gradient Weights Max: 0.03431919457660546 Min: -0.02994703194710258\n","ReLU Activation - Max: 1.5200733863534115 Min: 0.0\n","ReLU Activation - Max: 0.6961167081405872 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020574517872147247 Min: -0.027904551174684683\n","Layer 1 - Gradient Weights Max: 0.026578945758757598 Min: -0.029541872689522276\n","Layer 0 - Gradient Weights Max: 0.036464053892441096 Min: -0.033545616228394\n","ReLU Activation - Max: 1.433080973446611 Min: 0.0\n","ReLU Activation - Max: 0.5058348216117026 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014430866792342616 Min: -0.01883016649962414\n","Layer 1 - Gradient Weights Max: 0.017822786773528432 Min: -0.018998596237795675\n","Layer 0 - Gradient Weights Max: 0.027943182583832913 Min: -0.03231815497707217\n","ReLU Activation - Max: 2.1342522481904336 Min: 0.0\n","ReLU Activation - Max: 0.5656107934392212 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018035613572032733 Min: -0.015165888505892202\n","Layer 1 - Gradient Weights Max: 0.022191733221331462 Min: -0.01630180192592273\n","Layer 0 - Gradient Weights Max: 0.030601858823051987 Min: -0.026189753224192758\n","ReLU Activation - Max: 1.9719914076787564 Min: 0.0\n","ReLU Activation - Max: 0.7013427380457374 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016798173993361327 Min: -0.019774756361145184\n","Layer 1 - Gradient Weights Max: 0.02011817163121895 Min: -0.018604933094034277\n","Layer 0 - Gradient Weights Max: 0.028161651899469183 Min: -0.026791569979183392\n","ReLU Activation - Max: 1.609590292016225 Min: 0.0\n","ReLU Activation - Max: 0.5804456329322563 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017937963505326456 Min: -0.013348147938653887\n","Layer 1 - Gradient Weights Max: 0.016326144930760855 Min: -0.019953088484020123\n","Layer 0 - Gradient Weights Max: 0.03296890086643926 Min: -0.03141582237668795\n","ReLU Activation - Max: 1.2675433807440264 Min: 0.0\n","ReLU Activation - Max: 0.5715367514463129 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014370532880506099 Min: -0.016735974234309125\n","Layer 1 - Gradient Weights Max: 0.02261423799892934 Min: -0.02131865354824616\n","Layer 0 - Gradient Weights Max: 0.033917395018858466 Min: -0.038538814930411315\n","ReLU Activation - Max: 1.3613132214824621 Min: 0.0\n","ReLU Activation - Max: 0.6262841747091404 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014443926082668273 Min: -0.028699561752063778\n","Layer 1 - Gradient Weights Max: 0.021496999189314053 Min: -0.019750842246497466\n","Layer 0 - Gradient Weights Max: 0.0316948914437636 Min: -0.033009742936408126\n","ReLU Activation - Max: 1.483255458683274 Min: 0.0\n","ReLU Activation - Max: 0.632573617402431 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017456583239559658 Min: -0.009892233705647211\n","Layer 1 - Gradient Weights Max: 0.016611534345018553 Min: -0.019084811215113812\n","Layer 0 - Gradient Weights Max: 0.03132542114802321 Min: -0.02989628985733072\n","ReLU Activation - Max: 1.4488178977477215 Min: 0.0\n","ReLU Activation - Max: 0.5493660151980593 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01638203461391985 Min: -0.013788836992848646\n","Layer 1 - Gradient Weights Max: 0.01911511136777794 Min: -0.016170717693137163\n","Layer 0 - Gradient Weights Max: 0.03047166925098335 Min: -0.029021836926570906\n","ReLU Activation - Max: 1.530746121095314 Min: 0.0\n","ReLU Activation - Max: 0.6205350974153626 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01566793902822275 Min: -0.01320235195313556\n","Layer 1 - Gradient Weights Max: 0.01915890711826986 Min: -0.013905478750940704\n","Layer 0 - Gradient Weights Max: 0.027558293383002194 Min: -0.02856204789931802\n","ReLU Activation - Max: 1.5886730922454768 Min: 0.0\n","ReLU Activation - Max: 0.6205074680742743 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021816346707567758 Min: -0.019334339583989252\n","Layer 1 - Gradient Weights Max: 0.021552411591798663 Min: -0.0284906447044499\n","Layer 0 - Gradient Weights Max: 0.04206206188531073 Min: -0.045169545478914976\n","ReLU Activation - Max: 1.7951546882223144 Min: 0.0\n","ReLU Activation - Max: 0.5602910206881394 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013882418331102922 Min: -0.027885188947488815\n","Layer 1 - Gradient Weights Max: 0.016752036650401952 Min: -0.02253786483415897\n","Layer 0 - Gradient Weights Max: 0.03452725497189892 Min: -0.025748939937823637\n","ReLU Activation - Max: 1.4290203990222359 Min: 0.0\n","ReLU Activation - Max: 0.6276363883584261 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02061971526508768 Min: -0.02970818308863224\n","Layer 1 - Gradient Weights Max: 0.02052593535251906 Min: -0.018543080597294474\n","Layer 0 - Gradient Weights Max: 0.031249832491659357 Min: -0.026295061853630924\n","ReLU Activation - Max: 1.640018569079387 Min: 0.0\n","ReLU Activation - Max: 0.6143964763321296 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017350103940785965 Min: -0.025731460503718495\n","Layer 1 - Gradient Weights Max: 0.02200520505515742 Min: -0.029763636199163473\n","Layer 0 - Gradient Weights Max: 0.03580299858635177 Min: -0.030614327564038354\n","ReLU Activation - Max: 1.3532069838408267 Min: 0.0\n","ReLU Activation - Max: 0.6513817358872573 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019616361834728084 Min: -0.02226894013315388\n","Layer 1 - Gradient Weights Max: 0.013152392670255947 Min: -0.02208778470859155\n","Layer 0 - Gradient Weights Max: 0.025724146636307097 Min: -0.027068421780419866\n","ReLU Activation - Max: 1.6713174797073032 Min: 0.0\n","ReLU Activation - Max: 0.6307034924357021 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012851995108807803 Min: -0.017844731183703322\n","Layer 1 - Gradient Weights Max: 0.021753453574811836 Min: -0.016897164045522887\n","Layer 0 - Gradient Weights Max: 0.034230671598524165 Min: -0.02244493263578916\n","ReLU Activation - Max: 1.6240384904225047 Min: 0.0\n","ReLU Activation - Max: 0.5686033669423605 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015167667365789004 Min: -0.014742141701585002\n","Layer 1 - Gradient Weights Max: 0.01629416715615896 Min: -0.021481535210521305\n","Layer 0 - Gradient Weights Max: 0.03314635419149744 Min: -0.03256100956637833\n","ReLU Activation - Max: 1.5262928617554343 Min: 0.0\n","ReLU Activation - Max: 0.7106500658220407 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013527943524034153 Min: -0.03684287941909241\n","Layer 1 - Gradient Weights Max: 0.015034627933147962 Min: -0.019389289644998173\n","Layer 0 - Gradient Weights Max: 0.028145399686899237 Min: -0.028644090220939524\n","ReLU Activation - Max: 1.4177551217103026 Min: 0.0\n","ReLU Activation - Max: 0.7749808722018681 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01759285419236582 Min: -0.022858580365952643\n","Layer 1 - Gradient Weights Max: 0.01838159906873515 Min: -0.01841344465430629\n","Layer 0 - Gradient Weights Max: 0.028264622836794484 Min: -0.02988650469533846\n","ReLU Activation - Max: 1.757052569582033 Min: 0.0\n","ReLU Activation - Max: 0.586052051129102 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009528959932693195 Min: -0.017992437688686847\n","Layer 1 - Gradient Weights Max: 0.024073142540173897 Min: -0.01816440747094248\n","Layer 0 - Gradient Weights Max: 0.04386441164462255 Min: -0.02624899452462751\n","ReLU Activation - Max: 1.3673072962887611 Min: 0.0\n","ReLU Activation - Max: 0.5962527149167473 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013521203553446218 Min: -0.01618740374554934\n","Layer 1 - Gradient Weights Max: 0.01574156522809746 Min: -0.018048427620646495\n","Layer 0 - Gradient Weights Max: 0.03351161061150675 Min: -0.03861594130765632\n","ReLU Activation - Max: 1.580167307132786 Min: 0.0\n","ReLU Activation - Max: 0.6726989335112852 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025921593271696484 Min: -0.026295609646261448\n","Layer 1 - Gradient Weights Max: 0.019581195117463333 Min: -0.022844373703002022\n","Layer 0 - Gradient Weights Max: 0.03083018210390136 Min: -0.03205356406527309\n","ReLU Activation - Max: 1.4695598858492707 Min: 0.0\n","ReLU Activation - Max: 0.6544642646016817 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01766936686952393 Min: -0.01864371219633868\n","Layer 1 - Gradient Weights Max: 0.025438841075857803 Min: -0.018376218846770404\n","Layer 0 - Gradient Weights Max: 0.027690704108219742 Min: -0.0365575197907576\n","ReLU Activation - Max: 1.3013023720409294 Min: 0.0\n","ReLU Activation - Max: 0.570201679701573 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014221016602548714 Min: -0.013083293072425211\n","Layer 1 - Gradient Weights Max: 0.01781399576677219 Min: -0.025350216814072138\n","Layer 0 - Gradient Weights Max: 0.03648730681025372 Min: -0.02921700642187572\n","ReLU Activation - Max: 1.5912373410602418 Min: 0.0\n","ReLU Activation - Max: 0.7180838191670972 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013046295733138142 Min: -0.019030821250675668\n","Layer 1 - Gradient Weights Max: 0.022846108118375716 Min: -0.028613690372800326\n","Layer 0 - Gradient Weights Max: 0.03258964337514653 Min: -0.024766057869931257\n","ReLU Activation - Max: 1.3598246936618055 Min: 0.0\n","ReLU Activation - Max: 0.628804516566142 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01332606672707072 Min: -0.016054325206297824\n","Layer 1 - Gradient Weights Max: 0.014604935952326937 Min: -0.015010964779405953\n","Layer 0 - Gradient Weights Max: 0.02726956863468473 Min: -0.026225679675349466\n","ReLU Activation - Max: 1.648452048762117 Min: 0.0\n","ReLU Activation - Max: 0.5129376116377702 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01865006554133776 Min: -0.020020484586287357\n","Layer 1 - Gradient Weights Max: 0.020652108606837236 Min: -0.02234810311728852\n","Layer 0 - Gradient Weights Max: 0.03199283796251096 Min: -0.02855903713270224\n","ReLU Activation - Max: 1.596084297233932 Min: 0.0\n","ReLU Activation - Max: 0.5782718128306567 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01804967115851427 Min: -0.031078329385417807\n","Layer 1 - Gradient Weights Max: 0.02587187545827652 Min: -0.017647435507870218\n","Layer 0 - Gradient Weights Max: 0.028758664606823848 Min: -0.03292260260469927\n","ReLU Activation - Max: 1.697150127618789 Min: 0.0\n","ReLU Activation - Max: 0.6159184629696031 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01665474750667886 Min: -0.016131107180914275\n","Layer 1 - Gradient Weights Max: 0.016048621978641108 Min: -0.018220571358882196\n","Layer 0 - Gradient Weights Max: 0.023781611102157468 Min: -0.044133097092272505\n","ReLU Activation - Max: 1.664320751320986 Min: 0.0\n","ReLU Activation - Max: 0.8018285323753424 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015792896651520796 Min: -0.022059528039768257\n","Layer 1 - Gradient Weights Max: 0.01583530454007056 Min: -0.04119839998736996\n","Layer 0 - Gradient Weights Max: 0.030409649384068864 Min: -0.03498689960060703\n","ReLU Activation - Max: 1.324506944885451 Min: 0.0\n","ReLU Activation - Max: 0.5728535154967103 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015179533189325854 Min: -0.01693009145810686\n","Layer 1 - Gradient Weights Max: 0.020905843447996997 Min: -0.027262549470843354\n","Layer 0 - Gradient Weights Max: 0.03138599898907909 Min: -0.033610932907958094\n","ReLU Activation - Max: 1.3918067911173921 Min: 0.0\n","ReLU Activation - Max: 0.5570075890775978 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01936857439286335 Min: -0.014296213003851933\n","Layer 1 - Gradient Weights Max: 0.0161316023704041 Min: -0.023169746167189146\n","Layer 0 - Gradient Weights Max: 0.0359555045152424 Min: -0.02783945539123913\n","ReLU Activation - Max: 1.9530961658324095 Min: 0.0\n","ReLU Activation - Max: 0.5646772164656608 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018712845138045917 Min: -0.015159628471964526\n","Layer 1 - Gradient Weights Max: 0.01922968436262028 Min: -0.017727944071956523\n","Layer 0 - Gradient Weights Max: 0.029016821500428485 Min: -0.031843927417910554\n","ReLU Activation - Max: 1.4302552356598774 Min: 0.0\n","ReLU Activation - Max: 0.6260621727742518 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011770250161105382 Min: -0.02132400915969488\n","Layer 1 - Gradient Weights Max: 0.017079793328950935 Min: -0.016851489738999522\n","Layer 0 - Gradient Weights Max: 0.04452087304995957 Min: -0.028833567458204194\n","ReLU Activation - Max: 1.4935045515962058 Min: 0.0\n","ReLU Activation - Max: 0.6667420526654533 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02052034386099372 Min: -0.033628715350703275\n","Layer 1 - Gradient Weights Max: 0.026389472939117913 Min: -0.0276520730861527\n","Layer 0 - Gradient Weights Max: 0.03417594323808946 Min: -0.03575623425259054\n","ReLU Activation - Max: 1.4073023276863041 Min: 0.0\n","ReLU Activation - Max: 0.5057915910344963 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015110672523876972 Min: -0.02167695403456542\n","Layer 1 - Gradient Weights Max: 0.02890169038367177 Min: -0.02193685039715215\n","Layer 0 - Gradient Weights Max: 0.02935820095042148 Min: -0.028365151908927896\n","ReLU Activation - Max: 1.541336779046825 Min: 0.0\n","ReLU Activation - Max: 0.5997613764073553 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0168196542571689 Min: -0.02219873310908957\n","Layer 1 - Gradient Weights Max: 0.03169290053071656 Min: -0.026741134673775094\n","Layer 0 - Gradient Weights Max: 0.04006115112845 Min: -0.03153213177762336\n","ReLU Activation - Max: 1.4079977198345404 Min: 0.0\n","ReLU Activation - Max: 0.5470515735848921 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015389391378068218 Min: -0.018466679779771206\n","Layer 1 - Gradient Weights Max: 0.01753080172774238 Min: -0.01744686928885341\n","Layer 0 - Gradient Weights Max: 0.025589206196338404 Min: -0.03092088854384045\n","ReLU Activation - Max: 1.3329409671061427 Min: 0.0\n","ReLU Activation - Max: 0.592880509393813 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018415657912127254 Min: -0.015483036168825187\n","Layer 1 - Gradient Weights Max: 0.022062770356960134 Min: -0.021242967833657277\n","Layer 0 - Gradient Weights Max: 0.030385450554696925 Min: -0.02994232698080865\n","ReLU Activation - Max: 1.6393017642267291 Min: 0.0\n","ReLU Activation - Max: 0.5163785346072557 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02395715513152972 Min: -0.02923652128014429\n","Layer 1 - Gradient Weights Max: 0.020501780259083176 Min: -0.028197542133730363\n","Layer 0 - Gradient Weights Max: 0.02820664578133776 Min: -0.02803757925615784\n","ReLU Activation - Max: 1.3270633562624095 Min: 0.0\n","ReLU Activation - Max: 0.5232962504512639 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015869079490160556 Min: -0.023608644590861438\n","Layer 1 - Gradient Weights Max: 0.013737831622593732 Min: -0.018755807383072396\n","Layer 0 - Gradient Weights Max: 0.02372373732161817 Min: -0.029767450550252463\n","ReLU Activation - Max: 1.3895275778427383 Min: 0.0\n","ReLU Activation - Max: 0.6014459292149239 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01299750367184219 Min: -0.018118231060007022\n","Layer 1 - Gradient Weights Max: 0.023683932830302498 Min: -0.015864595956568497\n","Layer 0 - Gradient Weights Max: 0.026406680620789578 Min: -0.024206101285397522\n","ReLU Activation - Max: 1.3894111130842661 Min: 0.0\n","ReLU Activation - Max: 0.5152561005683255 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017668695932664496 Min: -0.014128666464601175\n","Layer 1 - Gradient Weights Max: 0.018197524203920682 Min: -0.017980748684644245\n","Layer 0 - Gradient Weights Max: 0.027215294092870265 Min: -0.032525172988296575\n","ReLU Activation - Max: 1.4793752524485138 Min: 0.0\n","ReLU Activation - Max: 0.634121956895108 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022549963600859348 Min: -0.018304090183312166\n","Layer 1 - Gradient Weights Max: 0.0236010346980478 Min: -0.02752728552437537\n","Layer 0 - Gradient Weights Max: 0.031301235403548316 Min: -0.02820853254321813\n","ReLU Activation - Max: 1.3607297313405708 Min: 0.0\n","ReLU Activation - Max: 0.6158424918581435 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01379458400845606 Min: -0.013569047265806083\n","Layer 1 - Gradient Weights Max: 0.027710860372503865 Min: -0.02319041044112589\n","Layer 0 - Gradient Weights Max: 0.026795323369500853 Min: -0.028839611223411915\n","ReLU Activation - Max: 1.8075652211063282 Min: 0.0\n","ReLU Activation - Max: 0.6938826317050213 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020103674620115673 Min: -0.023523003815595772\n","Layer 1 - Gradient Weights Max: 0.024736479097313926 Min: -0.028578713198367838\n","Layer 0 - Gradient Weights Max: 0.04254555164693559 Min: -0.040587563909863726\n","ReLU Activation - Max: 1.5983792529998468 Min: 0.0\n","ReLU Activation - Max: 0.6377878614135143 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013529906640023927 Min: -0.01526967029206444\n","Layer 1 - Gradient Weights Max: 0.01798799434294898 Min: -0.02124400884549515\n","Layer 0 - Gradient Weights Max: 0.02805359696425467 Min: -0.029591942091358694\n","ReLU Activation - Max: 1.758749899079769 Min: 0.0\n","ReLU Activation - Max: 0.6823919825218348 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015448248444089596 Min: -0.021160103361688082\n","Layer 1 - Gradient Weights Max: 0.02218508758428544 Min: -0.026522465717521742\n","Layer 0 - Gradient Weights Max: 0.028704484769224315 Min: -0.03337053699353649\n","ReLU Activation - Max: 1.660655805275621 Min: 0.0\n","ReLU Activation - Max: 0.736317434523226 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016769830632033356 Min: -0.02705286542356721\n","Layer 1 - Gradient Weights Max: 0.01900498912231681 Min: -0.031881548100600794\n","Layer 0 - Gradient Weights Max: 0.02947726237825723 Min: -0.03353377274295327\n","ReLU Activation - Max: 1.4017377268610764 Min: 0.0\n","ReLU Activation - Max: 0.5872658006549 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01413105016755055 Min: -0.022818333835025215\n","Layer 1 - Gradient Weights Max: 0.02383039061221232 Min: -0.015783904166925638\n","Layer 0 - Gradient Weights Max: 0.027977790818841303 Min: -0.030234989056017028\n","ReLU Activation - Max: 1.470585436493633 Min: 0.0\n","ReLU Activation - Max: 0.5336062940098909 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01780214081557296 Min: -0.0365484683297627\n","Layer 1 - Gradient Weights Max: 0.018885110550953786 Min: -0.03272130363166482\n","Layer 0 - Gradient Weights Max: 0.05665654322884486 Min: -0.03853074434278683\n","ReLU Activation - Max: 1.4050171192924727 Min: 0.0\n","ReLU Activation - Max: 0.6577047917385338 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009080901362540071 Min: -0.016676700925136217\n","Layer 1 - Gradient Weights Max: 0.01818880388281649 Min: -0.01586223680893832\n","Layer 0 - Gradient Weights Max: 0.03484254296329323 Min: -0.027819997053685843\n","ReLU Activation - Max: 1.7057935128374175 Min: 0.0\n","ReLU Activation - Max: 0.6279763019368927 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018895193443121093 Min: -0.03678636831483961\n","Layer 1 - Gradient Weights Max: 0.021567165760290916 Min: -0.021150953115899495\n","Layer 0 - Gradient Weights Max: 0.0309321843428142 Min: -0.028645083465451936\n","ReLU Activation - Max: 1.3244016859350536 Min: 0.0\n","ReLU Activation - Max: 0.5853948031453261 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020111326142147567 Min: -0.027725382363651924\n","Layer 1 - Gradient Weights Max: 0.019125566524435204 Min: -0.02109195323207911\n","Layer 0 - Gradient Weights Max: 0.034901796162290465 Min: -0.03218090983456058\n","ReLU Activation - Max: 1.3912582364288222 Min: 0.0\n","ReLU Activation - Max: 0.5623595833112363 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015460307980862573 Min: -0.023964150759773897\n","Layer 1 - Gradient Weights Max: 0.022997171955736508 Min: -0.019754707240570483\n","Layer 0 - Gradient Weights Max: 0.027015481574012894 Min: -0.029639481607822375\n","ReLU Activation - Max: 1.8370126891861571 Min: 0.0\n","ReLU Activation - Max: 0.6917059170979216 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01447809580095624 Min: -0.018340023140684013\n","Layer 1 - Gradient Weights Max: 0.017862576071228417 Min: -0.021769605636206453\n","Layer 0 - Gradient Weights Max: 0.032693084455510196 Min: -0.03071578831455897\n","ReLU Activation - Max: 1.5948672631400511 Min: 0.0\n","ReLU Activation - Max: 0.8146308805218189 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01462903927481548 Min: -0.01960375301054184\n","Layer 1 - Gradient Weights Max: 0.018575912083483245 Min: -0.02271413946694142\n","Layer 0 - Gradient Weights Max: 0.027225911667805823 Min: -0.02740190379595794\n","ReLU Activation - Max: 1.3839365892230975 Min: 0.0\n","ReLU Activation - Max: 0.6964404143594672 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01445861935490446 Min: -0.021003764988035495\n","Layer 1 - Gradient Weights Max: 0.018440560142259813 Min: -0.014687063346855132\n","Layer 0 - Gradient Weights Max: 0.025039096756028622 Min: -0.022302655379453664\n","ReLU Activation - Max: 1.4402661697938368 Min: 0.0\n","ReLU Activation - Max: 0.5961629741178779 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018178533045481997 Min: -0.02669155886133828\n","Layer 1 - Gradient Weights Max: 0.0223970165046572 Min: -0.029228217476677178\n","Layer 0 - Gradient Weights Max: 0.032751919668947305 Min: -0.03124981601770677\n","ReLU Activation - Max: 1.4500274860750861 Min: 0.0\n","ReLU Activation - Max: 0.5464621771863186 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009728575114858074 Min: -0.016490914545094836\n","Layer 1 - Gradient Weights Max: 0.01366553241766217 Min: -0.022773882891356432\n","Layer 0 - Gradient Weights Max: 0.029993118805253127 Min: -0.023922232361818962\n","ReLU Activation - Max: 1.3125201145929675 Min: 0.0\n","ReLU Activation - Max: 0.5396314485701375 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011393786654540198 Min: -0.01568322892779122\n","Layer 1 - Gradient Weights Max: 0.01462015008029713 Min: -0.02329352567409156\n","Layer 0 - Gradient Weights Max: 0.03673406117451548 Min: -0.03735283459161779\n","ReLU Activation - Max: 1.8626482434285114 Min: 0.0\n","ReLU Activation - Max: 0.6037481039369943 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010795964934169299 Min: -0.013253751397231065\n","Layer 1 - Gradient Weights Max: 0.018393946938642596 Min: -0.01592248340296797\n","Layer 0 - Gradient Weights Max: 0.032899502799558646 Min: -0.03804759068583018\n","ReLU Activation - Max: 1.3043260350191141 Min: 0.0\n","ReLU Activation - Max: 0.5868629782385782 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01509758743359052 Min: -0.037210479226701794\n","Layer 1 - Gradient Weights Max: 0.01672269455698249 Min: -0.03010584765448028\n","Layer 0 - Gradient Weights Max: 0.037007011696499434 Min: -0.026887188959681473\n","ReLU Activation - Max: 2.1005621002188533 Min: 0.0\n","ReLU Activation - Max: 0.8361196771354896 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015632105235563687 Min: -0.021933351231682687\n","Layer 1 - Gradient Weights Max: 0.027220606137095302 Min: -0.024622656502494122\n","Layer 0 - Gradient Weights Max: 0.02946848913369194 Min: -0.031141399623653716\n","ReLU Activation - Max: 1.4398791718599548 Min: 0.0\n","ReLU Activation - Max: 0.7516257729109462 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013130800725458417 Min: -0.023426512179344168\n","Layer 1 - Gradient Weights Max: 0.02016437576181731 Min: -0.02633683438055834\n","Layer 0 - Gradient Weights Max: 0.028954803363122648 Min: -0.031876349973834185\n","ReLU Activation - Max: 1.5976663575589625 Min: 0.0\n","ReLU Activation - Max: 0.6172841795883406 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019350484571911672 Min: -0.019011303582559657\n","Layer 1 - Gradient Weights Max: 0.029402489220399883 Min: -0.017916576206399523\n","Layer 0 - Gradient Weights Max: 0.028096460863641703 Min: -0.030210550385514342\n","ReLU Activation - Max: 1.6191298180891598 Min: 0.0\n","ReLU Activation - Max: 0.5319271430068546 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02291344438065745 Min: -0.021613344146086764\n","Layer 1 - Gradient Weights Max: 0.01523953693413318 Min: -0.0232299800854503\n","Layer 0 - Gradient Weights Max: 0.031157540074872075 Min: -0.029939201421825642\n","ReLU Activation - Max: 1.517923638710284 Min: 0.0\n","ReLU Activation - Max: 0.6613243764908234 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02800538960173115 Min: -0.022674630581173\n","Layer 1 - Gradient Weights Max: 0.020141100654359714 Min: -0.02494107711247496\n","Layer 0 - Gradient Weights Max: 0.0317635200816151 Min: -0.037827261436727155\n","ReLU Activation - Max: 1.4651316239998182 Min: 0.0\n","ReLU Activation - Max: 0.5937386411133818 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025774744377440723 Min: -0.024819039376592873\n","Layer 1 - Gradient Weights Max: 0.018005265108864372 Min: -0.02671834611512132\n","Layer 0 - Gradient Weights Max: 0.024714299259171284 Min: -0.02756545384246394\n","ReLU Activation - Max: 1.8736293984146182 Min: 0.0\n","ReLU Activation - Max: 0.5999474366503532 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01779319373557742 Min: -0.017458491558585525\n","Layer 1 - Gradient Weights Max: 0.021281918782499444 Min: -0.02198765597696019\n","Layer 0 - Gradient Weights Max: 0.033727730920660254 Min: -0.033738308259610085\n","ReLU Activation - Max: 1.5140643397340035 Min: 0.0\n","ReLU Activation - Max: 0.6428917942886994 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013691424254915702 Min: -0.015230041133135017\n","Layer 1 - Gradient Weights Max: 0.016284451749745278 Min: -0.012240511292223026\n","Layer 0 - Gradient Weights Max: 0.03257221043799548 Min: -0.026296655279823108\n","ReLU Activation - Max: 1.6403726762412447 Min: 0.0\n","ReLU Activation - Max: 0.6507951151246217 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020572355251692015 Min: -0.01299433894891853\n","Layer 1 - Gradient Weights Max: 0.025643339486227174 Min: -0.01743683169767155\n","Layer 0 - Gradient Weights Max: 0.02637782227895273 Min: -0.03342461244716114\n","ReLU Activation - Max: 1.550599224105936 Min: 0.0\n","ReLU Activation - Max: 0.5602968985618808 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019229072463722206 Min: -0.018086715181360964\n","Layer 1 - Gradient Weights Max: 0.024521051407400022 Min: -0.019416780276850062\n","Layer 0 - Gradient Weights Max: 0.023075416662690123 Min: -0.026880982313625783\n","ReLU Activation - Max: 1.4175487857873206 Min: 0.0\n","ReLU Activation - Max: 0.6467256712573285 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013824656458265246 Min: -0.02191442478295963\n","Layer 1 - Gradient Weights Max: 0.0180951575161566 Min: -0.024875526399000515\n","Layer 0 - Gradient Weights Max: 0.029544652674172743 Min: -0.03965006571807\n","ReLU Activation - Max: 1.435475160464654 Min: 0.0\n","ReLU Activation - Max: 0.5547567089352249 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01420813870112285 Min: -0.025250740690065953\n","Layer 1 - Gradient Weights Max: 0.020258953421470834 Min: -0.01925719010918911\n","Layer 0 - Gradient Weights Max: 0.03163881780495437 Min: -0.03494433846845673\n","ReLU Activation - Max: 1.4593669088210204 Min: 0.0\n","ReLU Activation - Max: 0.6378223142130913 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01817695956757879 Min: -0.01979061578499518\n","Layer 1 - Gradient Weights Max: 0.023771760844197753 Min: -0.019128441371390786\n","Layer 0 - Gradient Weights Max: 0.02790137513935258 Min: -0.03130375649390849\n","ReLU Activation - Max: 1.8819050461762048 Min: 0.0\n","ReLU Activation - Max: 0.5881125933012148 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013152119139740508 Min: -0.010855503711576238\n","Layer 1 - Gradient Weights Max: 0.01776500735969348 Min: -0.02071372928679583\n","Layer 0 - Gradient Weights Max: 0.027121430791516063 Min: -0.03203175296142641\n","ReLU Activation - Max: 1.304091779760126 Min: 0.0\n","ReLU Activation - Max: 0.5736549461353133 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013316059644676671 Min: -0.02069621956788615\n","Layer 1 - Gradient Weights Max: 0.018444425274163605 Min: -0.015951971127923688\n","Layer 0 - Gradient Weights Max: 0.031323386231545826 Min: -0.027166585862652005\n","ReLU Activation - Max: 1.5636265791419652 Min: 0.0\n","ReLU Activation - Max: 0.5859678242999229 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020682606091521005 Min: -0.012718712607783192\n","Layer 1 - Gradient Weights Max: 0.018349971180464268 Min: -0.021611016461733624\n","Layer 0 - Gradient Weights Max: 0.03507860687475634 Min: -0.024377492174087266\n","ReLU Activation - Max: 1.454267646372362 Min: 0.0\n","ReLU Activation - Max: 0.7616675200253757 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016979659094214172 Min: -0.02017944679652131\n","Layer 1 - Gradient Weights Max: 0.019462161383100442 Min: -0.018080587437749783\n","Layer 0 - Gradient Weights Max: 0.026444641798051872 Min: -0.026341794919242545\n","ReLU Activation - Max: 1.3293248712288384 Min: 0.0\n","ReLU Activation - Max: 0.5622726543538467 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011061829758976405 Min: -0.013741649118035791\n","Layer 1 - Gradient Weights Max: 0.018518282414765552 Min: -0.01880398613994424\n","Layer 0 - Gradient Weights Max: 0.03228772304812636 Min: -0.026751461216630367\n","ReLU Activation - Max: 1.4253521633097261 Min: 0.0\n","ReLU Activation - Max: 0.6743304722936119 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015927675455607866 Min: -0.021755053493950682\n","Layer 1 - Gradient Weights Max: 0.018757718889664002 Min: -0.026474099935384892\n","Layer 0 - Gradient Weights Max: 0.029445333538765956 Min: -0.029063679120061355\n","ReLU Activation - Max: 1.4280841851062338 Min: 0.0\n","ReLU Activation - Max: 0.5714139826461461 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025630319768303354 Min: -0.019115051385291182\n","Layer 1 - Gradient Weights Max: 0.018781631499441728 Min: -0.021719838783169625\n","Layer 0 - Gradient Weights Max: 0.03441561861559386 Min: -0.026978295619402903\n","ReLU Activation - Max: 1.5220000579304442 Min: 0.0\n","ReLU Activation - Max: 0.701970530597206 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01728890842835922 Min: -0.018834856146853582\n","Layer 1 - Gradient Weights Max: 0.023054957100809126 Min: -0.02443363486289455\n","Layer 0 - Gradient Weights Max: 0.028028286056890762 Min: -0.034744875541931855\n","ReLU Activation - Max: 1.2918794445496957 Min: 0.0\n","ReLU Activation - Max: 0.6070609013045798 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017293722569546227 Min: -0.01992944561858139\n","Layer 1 - Gradient Weights Max: 0.017059365282368708 Min: -0.01893525523403044\n","Layer 0 - Gradient Weights Max: 0.031713863237272044 Min: -0.028298260495998928\n","ReLU Activation - Max: 1.681482741324118 Min: 0.0\n","ReLU Activation - Max: 0.6169860888992583 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012810736523029689 Min: -0.01741279280544719\n","Layer 1 - Gradient Weights Max: 0.017766707657166814 Min: -0.035786871320044315\n","Layer 0 - Gradient Weights Max: 0.03947774171834843 Min: -0.029772987625275166\n","ReLU Activation - Max: 1.433703316145672 Min: 0.0\n","ReLU Activation - Max: 0.593165338846364 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015279476299370974 Min: -0.018534905780558303\n","Layer 1 - Gradient Weights Max: 0.016551133058139694 Min: -0.015768928650140013\n","Layer 0 - Gradient Weights Max: 0.025549535919349893 Min: -0.023001847125977092\n","ReLU Activation - Max: 1.2942916588405817 Min: 0.0\n","ReLU Activation - Max: 0.5732292442064921 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014687704176312086 Min: -0.015367050341536804\n","Layer 1 - Gradient Weights Max: 0.019184874637380032 Min: -0.01843259333824038\n","Layer 0 - Gradient Weights Max: 0.025420572506781737 Min: -0.02751203437101746\n","ReLU Activation - Max: 1.5598579717902235 Min: 0.0\n","ReLU Activation - Max: 0.5642024837680643 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023121879348243807 Min: -0.02005798139226316\n","Layer 1 - Gradient Weights Max: 0.02338723131295972 Min: -0.026662942870305465\n","Layer 0 - Gradient Weights Max: 0.028684554731473733 Min: -0.027200559807626697\n","ReLU Activation - Max: 1.553014451339919 Min: 0.0\n","ReLU Activation - Max: 0.5504301351636991 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015241137812465116 Min: -0.01228463166377436\n","Layer 1 - Gradient Weights Max: 0.025212711058006033 Min: -0.017240070624486056\n","Layer 0 - Gradient Weights Max: 0.03142712579550919 Min: -0.034170217588060305\n","ReLU Activation - Max: 1.566251950990293 Min: 0.0\n","ReLU Activation - Max: 0.7400235294093991 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020316699256525766 Min: -0.018394143171606172\n","Layer 1 - Gradient Weights Max: 0.02034973427336423 Min: -0.019256726324795508\n","Layer 0 - Gradient Weights Max: 0.031233980061508695 Min: -0.031791155763369806\n","ReLU Activation - Max: 1.2091728204220793 Min: 0.0\n","ReLU Activation - Max: 0.5835792499921475 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017837972826867405 Min: -0.024857297393905853\n","Layer 1 - Gradient Weights Max: 0.020014843414806 Min: -0.024728865999398733\n","Layer 0 - Gradient Weights Max: 0.02842235489998886 Min: -0.040331107536614175\n","ReLU Activation - Max: 1.4951597745368084 Min: 0.0\n","ReLU Activation - Max: 0.519627754509624 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019983309693553795 Min: -0.01760328127162763\n","Layer 1 - Gradient Weights Max: 0.019103488734095054 Min: -0.01842909080954187\n","Layer 0 - Gradient Weights Max: 0.029967366724836738 Min: -0.031259207134854125\n","ReLU Activation - Max: 1.294668788307404 Min: 0.0\n","ReLU Activation - Max: 0.6532967834803579 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015463552869321058 Min: -0.0169472829966013\n","Layer 1 - Gradient Weights Max: 0.013937156807790542 Min: -0.020311442977356292\n","Layer 0 - Gradient Weights Max: 0.031771442434866765 Min: -0.033958988030759224\n","ReLU Activation - Max: 1.3789687608682204 Min: 0.0\n","ReLU Activation - Max: 0.6248728611123585 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01103998614689261 Min: -0.017640508713613282\n","Layer 1 - Gradient Weights Max: 0.01493632863557044 Min: -0.017334222852358267\n","Layer 0 - Gradient Weights Max: 0.030843266644388963 Min: -0.03031632741468577\n","ReLU Activation - Max: 1.3638637160792608 Min: 0.0\n","ReLU Activation - Max: 0.6186127183493808 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014734107795735864 Min: -0.02225310826852801\n","Layer 1 - Gradient Weights Max: 0.020448436449809176 Min: -0.02369164378335368\n","Layer 0 - Gradient Weights Max: 0.02945462050808559 Min: -0.040467385127264506\n","ReLU Activation - Max: 1.8322587140374649 Min: 0.0\n","ReLU Activation - Max: 0.7560977167271379 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022379615207329284 Min: -0.014641070073759522\n","Layer 1 - Gradient Weights Max: 0.02112594332347371 Min: -0.017993487528777267\n","Layer 0 - Gradient Weights Max: 0.027211544285058512 Min: -0.02319384082560274\n","ReLU Activation - Max: 1.3839031431476936 Min: 0.0\n","ReLU Activation - Max: 0.6579883155401179 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01364561576678186 Min: -0.0248821482798943\n","Layer 1 - Gradient Weights Max: 0.017096337484584997 Min: -0.019699933473232224\n","Layer 0 - Gradient Weights Max: 0.02674839412030144 Min: -0.03059301060188055\n","ReLU Activation - Max: 1.4650424337723529 Min: 0.0\n","ReLU Activation - Max: 0.7589570237483743 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011816096313738247 Min: -0.011949580809948677\n","Layer 1 - Gradient Weights Max: 0.023511381142562186 Min: -0.019692959429483424\n","Layer 0 - Gradient Weights Max: 0.03397454100102538 Min: -0.029599818261119817\n","ReLU Activation - Max: 1.474473287381928 Min: 0.0\n","ReLU Activation - Max: 0.5694422459003656 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013367350968788183 Min: -0.017694026982914758\n","Layer 1 - Gradient Weights Max: 0.016639729952780947 Min: -0.0166434189845948\n","Layer 0 - Gradient Weights Max: 0.02443064984727528 Min: -0.03226723185842167\n","ReLU Activation - Max: 1.4911768568644836 Min: 0.0\n","ReLU Activation - Max: 0.6824437930392359 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01877381866180658 Min: -0.02901809441451924\n","Layer 1 - Gradient Weights Max: 0.015472936366284022 Min: -0.020692928359064278\n","Layer 0 - Gradient Weights Max: 0.029915974307563188 Min: -0.027733113819504666\n","ReLU Activation - Max: 1.470869095374789 Min: 0.0\n","ReLU Activation - Max: 0.5759707716855266 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011101724625799368 Min: -0.009876613012964587\n","Layer 1 - Gradient Weights Max: 0.0204182198999137 Min: -0.018061768261697485\n","Layer 0 - Gradient Weights Max: 0.03292369290583085 Min: -0.030389579468984705\n","ReLU Activation - Max: 1.6339721721960243 Min: 0.0\n","ReLU Activation - Max: 0.6124730854206386 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014935317489523314 Min: -0.01490461324065898\n","Layer 1 - Gradient Weights Max: 0.016967250266635526 Min: -0.017363037959346486\n","Layer 0 - Gradient Weights Max: 0.04205032645098806 Min: -0.02644950901109686\n","ReLU Activation - Max: 1.85703730393181 Min: 0.0\n","ReLU Activation - Max: 0.6960907874909861 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019502561829564495 Min: -0.021800636544896276\n","Layer 1 - Gradient Weights Max: 0.021345407011959135 Min: -0.018935633180195727\n","Layer 0 - Gradient Weights Max: 0.0322993107712482 Min: -0.03364316722130668\n","ReLU Activation - Max: 1.2768309903719925 Min: 0.0\n","ReLU Activation - Max: 0.5940660557192622 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021399828314037323 Min: -0.019236275383047067\n","Layer 1 - Gradient Weights Max: 0.02138675860104488 Min: -0.020837113582736926\n","Layer 0 - Gradient Weights Max: 0.02971383572783188 Min: -0.03480639913593367\n","ReLU Activation - Max: 1.4751480499107539 Min: 0.0\n","ReLU Activation - Max: 0.5548778832810104 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02221891951787236 Min: -0.026963166243963246\n","Layer 1 - Gradient Weights Max: 0.01701598878263572 Min: -0.017170603858395302\n","Layer 0 - Gradient Weights Max: 0.029251128553589786 Min: -0.03635720630149252\n","ReLU Activation - Max: 1.2051286345572438 Min: 0.0\n","ReLU Activation - Max: 0.6617335139113707 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01526352971853309 Min: -0.02037473057470614\n","Layer 1 - Gradient Weights Max: 0.02062321002372655 Min: -0.016689589585176173\n","Layer 0 - Gradient Weights Max: 0.029914038097146843 Min: -0.04369418716945071\n","ReLU Activation - Max: 1.4645421869263109 Min: 0.0\n","ReLU Activation - Max: 0.7934024564910525 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017565137495786243 Min: -0.018093625640555996\n","Layer 1 - Gradient Weights Max: 0.019468268950003437 Min: -0.016442979549133194\n","Layer 0 - Gradient Weights Max: 0.026070265744261942 Min: -0.025869276499535743\n","ReLU Activation - Max: 1.4454348522090466 Min: 0.0\n","ReLU Activation - Max: 0.5851699046423655 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013217147134362886 Min: -0.014481880172825763\n","Layer 1 - Gradient Weights Max: 0.016876228283534384 Min: -0.02104154706010908\n","Layer 0 - Gradient Weights Max: 0.036029676586198635 Min: -0.032775563247681515\n","ReLU Activation - Max: 1.6996200475124672 Min: 0.0\n","ReLU Activation - Max: 0.5332209050935277 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014169326267698451 Min: -0.015330591968610785\n","Layer 1 - Gradient Weights Max: 0.023148864563258764 Min: -0.015805963483530482\n","Layer 0 - Gradient Weights Max: 0.047688728963192144 Min: -0.03247851587346934\n","ReLU Activation - Max: 1.5115860550390405 Min: 0.0\n","ReLU Activation - Max: 0.6864618037046459 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016130567764318048 Min: -0.015822695974689542\n","Layer 1 - Gradient Weights Max: 0.022524897737793 Min: -0.018627113476359498\n","Layer 0 - Gradient Weights Max: 0.026416017944120056 Min: -0.02952303483753642\n","ReLU Activation - Max: 1.3629369268658396 Min: 0.0\n","ReLU Activation - Max: 0.5989756575007475 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012208658712667648 Min: -0.011581048026463512\n","Layer 1 - Gradient Weights Max: 0.0176962642139693 Min: -0.024794064823896487\n","Layer 0 - Gradient Weights Max: 0.02858575858812725 Min: -0.031237345826156337\n","ReLU Activation - Max: 1.5507506138117921 Min: 0.0\n","ReLU Activation - Max: 0.5108972442325133 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.00878279529547705 Min: -0.013679351097308116\n","Layer 1 - Gradient Weights Max: 0.014038936154734113 Min: -0.021919608336572957\n","Layer 0 - Gradient Weights Max: 0.026065120911998044 Min: -0.02594255703641593\n","ReLU Activation - Max: 1.279608277007986 Min: 0.0\n","ReLU Activation - Max: 0.647199590623126 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017496240117810327 Min: -0.02135943960024011\n","Layer 1 - Gradient Weights Max: 0.01823995617961755 Min: -0.022863253079431413\n","Layer 0 - Gradient Weights Max: 0.03049892757630591 Min: -0.028576797462823655\n","ReLU Activation - Max: 1.547351392354222 Min: 0.0\n","ReLU Activation - Max: 0.6313550741041007 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017364791478952597 Min: -0.019148058192966125\n","Layer 1 - Gradient Weights Max: 0.02056677288712686 Min: -0.02309446781698475\n","Layer 0 - Gradient Weights Max: 0.028263930263716156 Min: -0.025269701784198104\n","ReLU Activation - Max: 1.5744557176382292 Min: 0.0\n","ReLU Activation - Max: 0.6999727975997 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011695232023870435 Min: -0.014185890331112215\n","Layer 1 - Gradient Weights Max: 0.012112619653340392 Min: -0.023077848116072757\n","Layer 0 - Gradient Weights Max: 0.03407009506437076 Min: -0.03010510814926184\n","ReLU Activation - Max: 1.515199308360162 Min: 0.0\n","ReLU Activation - Max: 0.5633046901202472 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017538168980161525 Min: -0.02225413034666908\n","Layer 1 - Gradient Weights Max: 0.01946254551168859 Min: -0.01784945750842992\n","Layer 0 - Gradient Weights Max: 0.024996910597695852 Min: -0.028151803697093847\n","ReLU Activation - Max: 1.3466478284280625 Min: 0.0\n","ReLU Activation - Max: 0.601487188350703 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01652676479423653 Min: -0.018787965387384566\n","Layer 1 - Gradient Weights Max: 0.018786563809423864 Min: -0.022662496175149306\n","Layer 0 - Gradient Weights Max: 0.03643584801982219 Min: -0.03145974776567173\n","ReLU Activation - Max: 1.4770027045369256 Min: 0.0\n","ReLU Activation - Max: 0.6376034121371025 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023487192864023686 Min: -0.02710925058776251\n","Layer 1 - Gradient Weights Max: 0.030517357051139428 Min: -0.020266876423633874\n","Layer 0 - Gradient Weights Max: 0.03271061389089322 Min: -0.03256938810572276\n","ReLU Activation - Max: 1.8082325552017766 Min: 0.0\n","ReLU Activation - Max: 0.597636160793404 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016267025144372803 Min: -0.023535108212448596\n","Layer 1 - Gradient Weights Max: 0.021000179319979766 Min: -0.02008546125822868\n","Layer 0 - Gradient Weights Max: 0.023438420180915652 Min: -0.02985254246040715\n","ReLU Activation - Max: 1.4969633236805193 Min: 0.0\n","ReLU Activation - Max: 0.5354043432538909 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012480538194145234 Min: -0.031355159014562546\n","Layer 1 - Gradient Weights Max: 0.021990921164909156 Min: -0.020260927081032578\n","Layer 0 - Gradient Weights Max: 0.026940116489468112 Min: -0.028489622916920088\n","ReLU Activation - Max: 1.327560201765413 Min: 0.0\n","ReLU Activation - Max: 0.6840474887350629 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020193337573171845 Min: -0.024535005378759755\n","Layer 1 - Gradient Weights Max: 0.023163706724303127 Min: -0.02083825583657768\n","Layer 0 - Gradient Weights Max: 0.03281145387154739 Min: -0.033216524043537865\n","ReLU Activation - Max: 1.5345262639151893 Min: 0.0\n","ReLU Activation - Max: 0.5094873999798306 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013991949740587668 Min: -0.021803073140013788\n","Layer 1 - Gradient Weights Max: 0.01620034744091814 Min: -0.02253590549096755\n","Layer 0 - Gradient Weights Max: 0.03213045423501029 Min: -0.02978001253559284\n","ReLU Activation - Max: 1.7057787592636107 Min: 0.0\n","ReLU Activation - Max: 0.6318277861349277 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017943403145139328 Min: -0.023463772543362054\n","Layer 1 - Gradient Weights Max: 0.02118097818247625 Min: -0.015584220444018783\n","Layer 0 - Gradient Weights Max: 0.026369248644319904 Min: -0.027767194957418303\n","ReLU Activation - Max: 1.429118727762915 Min: 0.0\n","ReLU Activation - Max: 0.601105868841059 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014817682127194772 Min: -0.014129446560060292\n","Layer 1 - Gradient Weights Max: 0.02140436938402004 Min: -0.018490853505730456\n","Layer 0 - Gradient Weights Max: 0.02628806097775728 Min: -0.029031888266485308\n","ReLU Activation - Max: 1.6449392475798525 Min: 0.0\n","ReLU Activation - Max: 0.4877264788290735 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02121374088409216 Min: -0.029276083804787235\n","Layer 1 - Gradient Weights Max: 0.024656866146388835 Min: -0.0281464489876936\n","Layer 0 - Gradient Weights Max: 0.026622548471754024 Min: -0.025031318912439867\n","ReLU Activation - Max: 1.5040732487127904 Min: 0.0\n","ReLU Activation - Max: 0.5740339158919904 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012751320441883776 Min: -0.01261253859251106\n","Layer 1 - Gradient Weights Max: 0.015969441394051053 Min: -0.01748724777715531\n","Layer 0 - Gradient Weights Max: 0.033566938475949146 Min: -0.02608547559833127\n","ReLU Activation - Max: 1.6963078162095064 Min: 0.0\n","ReLU Activation - Max: 0.8304540448116889 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015872625470441326 Min: -0.01857669223861412\n","Layer 1 - Gradient Weights Max: 0.01772126086656024 Min: -0.02252889469338278\n","Layer 0 - Gradient Weights Max: 0.030803032916903977 Min: -0.03172203541350179\n","ReLU Activation - Max: 1.4399229024685707 Min: 0.0\n","ReLU Activation - Max: 0.7033695956423776 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01998827358152292 Min: -0.01792047494503252\n","Layer 1 - Gradient Weights Max: 0.01850584601608483 Min: -0.019780183392590363\n","Layer 0 - Gradient Weights Max: 0.029539721228129017 Min: -0.03253571703403168\n","ReLU Activation - Max: 1.4490895447483965 Min: 0.0\n","ReLU Activation - Max: 0.5583868222607544 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013885214605740041 Min: -0.029957298319978596\n","Layer 1 - Gradient Weights Max: 0.02647833496086997 Min: -0.025745404623536194\n","Layer 0 - Gradient Weights Max: 0.029347704016479757 Min: -0.030359714163233163\n","ReLU Activation - Max: 1.4286161262740147 Min: 0.0\n","ReLU Activation - Max: 0.6580703668176802 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013587429858789997 Min: -0.012710133801643502\n","Layer 1 - Gradient Weights Max: 0.025169647226191763 Min: -0.02792445388267162\n","Layer 0 - Gradient Weights Max: 0.02663659467327227 Min: -0.029000533251414717\n","ReLU Activation - Max: 1.4759425444536138 Min: 0.0\n","ReLU Activation - Max: 0.5502873524565404 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0187486548867547 Min: -0.020969033771958252\n","Layer 1 - Gradient Weights Max: 0.017915413110616736 Min: -0.047191132309036134\n","Layer 0 - Gradient Weights Max: 0.024713392253645456 Min: -0.027919962383410886\n","ReLU Activation - Max: 1.3792955465470451 Min: 0.0\n","ReLU Activation - Max: 0.5826563737043522 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019756584025844875 Min: -0.01986737821801246\n","Layer 1 - Gradient Weights Max: 0.029859023824537593 Min: -0.021585098491271965\n","Layer 0 - Gradient Weights Max: 0.02740328243554223 Min: -0.039767104069613674\n","ReLU Activation - Max: 1.5107516491265285 Min: 0.0\n","ReLU Activation - Max: 0.640154933416858 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016518137212103472 Min: -0.019183034581494514\n","Layer 1 - Gradient Weights Max: 0.020839542948027947 Min: -0.01922389736027498\n","Layer 0 - Gradient Weights Max: 0.0265968613208166 Min: -0.030722167326568888\n","ReLU Activation - Max: 1.520431114272746 Min: 0.0\n","ReLU Activation - Max: 0.5660124132971601 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011145943524411497 Min: -0.023558805202708654\n","Layer 1 - Gradient Weights Max: 0.019869287993112127 Min: -0.023705961487671283\n","Layer 0 - Gradient Weights Max: 0.03371820390971686 Min: -0.039596901136822356\n","ReLU Activation - Max: 1.3254430718847328 Min: 0.0\n","ReLU Activation - Max: 0.5459513911851219 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011395736145347397 Min: -0.017077454237671624\n","Layer 1 - Gradient Weights Max: 0.014333207627973835 Min: -0.02375289902128297\n","Layer 0 - Gradient Weights Max: 0.03544525729834996 Min: -0.03641008882722453\n","ReLU Activation - Max: 1.3474198568278706 Min: 0.0\n","ReLU Activation - Max: 0.7648565201305202 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.026398584572672665 Min: -0.02296331900472986\n","Layer 1 - Gradient Weights Max: 0.018533552166778454 Min: -0.020219585814374025\n","Layer 0 - Gradient Weights Max: 0.0346647220785422 Min: -0.028813085190750324\n","ReLU Activation - Max: 1.3179830125603453 Min: 0.0\n","ReLU Activation - Max: 0.5008769102315479 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015301498041149353 Min: -0.018306726564232247\n","Layer 1 - Gradient Weights Max: 0.021399670375229878 Min: -0.021414095824977254\n","Layer 0 - Gradient Weights Max: 0.02835244887471445 Min: -0.027104438161628674\n","ReLU Activation - Max: 1.5162857752056738 Min: 0.0\n","ReLU Activation - Max: 0.6264042766715611 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02234840571913188 Min: -0.01327605604192876\n","Layer 1 - Gradient Weights Max: 0.01922009598175362 Min: -0.015571552083827735\n","Layer 0 - Gradient Weights Max: 0.02506704551014905 Min: -0.03051014736710899\n","ReLU Activation - Max: 1.3300930683644137 Min: 0.0\n","ReLU Activation - Max: 0.6047095894142562 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021014256020608964 Min: -0.03186521878454333\n","Layer 1 - Gradient Weights Max: 0.01934898854459055 Min: -0.031095052249174752\n","Layer 0 - Gradient Weights Max: 0.035997861356846995 Min: -0.0287657318797701\n","ReLU Activation - Max: 1.7295142860722776 Min: 0.0\n","ReLU Activation - Max: 0.5112545028102113 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013171737935198019 Min: -0.021953834263702603\n","Layer 1 - Gradient Weights Max: 0.020621000591747367 Min: -0.027196405341833502\n","Layer 0 - Gradient Weights Max: 0.030417786941937655 Min: -0.035492503017400305\n","ReLU Activation - Max: 1.4287816465200733 Min: 0.0\n","ReLU Activation - Max: 0.7335447225841049 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015755821700368215 Min: -0.024409125591961074\n","Layer 1 - Gradient Weights Max: 0.017236002140569626 Min: -0.024130446538465272\n","Layer 0 - Gradient Weights Max: 0.02664019760894672 Min: -0.033785460517575576\n","ReLU Activation - Max: 1.7793104982842742 Min: 0.0\n","ReLU Activation - Max: 0.5510743215029968 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010680290057979516 Min: -0.01826789162566308\n","Layer 1 - Gradient Weights Max: 0.01570253181931794 Min: -0.021148460237206325\n","Layer 0 - Gradient Weights Max: 0.032188564345910414 Min: -0.02674783347280896\n","ReLU Activation - Max: 1.321693515010034 Min: 0.0\n","ReLU Activation - Max: 0.6093051673202354 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015877237710387758 Min: -0.014789243409463191\n","Layer 1 - Gradient Weights Max: 0.013109353129990209 Min: -0.025024039090136375\n","Layer 0 - Gradient Weights Max: 0.02704832346806834 Min: -0.026800275223550427\n","ReLU Activation - Max: 1.365116633296332 Min: 0.0\n","ReLU Activation - Max: 0.5517087767515659 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020456661528173117 Min: -0.01577332202355836\n","Layer 1 - Gradient Weights Max: 0.01870254649655377 Min: -0.020635702065625695\n","Layer 0 - Gradient Weights Max: 0.024990359723495188 Min: -0.03285052529169862\n","ReLU Activation - Max: 1.7149717086031468 Min: 0.0\n","ReLU Activation - Max: 0.5803343803548663 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0165109432979372 Min: -0.017973335808509905\n","Layer 1 - Gradient Weights Max: 0.020844119878566082 Min: -0.01305831131410396\n","Layer 0 - Gradient Weights Max: 0.027828569416845656 Min: -0.027249345225522536\n","ReLU Activation - Max: 1.5006087813302562 Min: 0.0\n","ReLU Activation - Max: 0.7013847991229333 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01315171999404963 Min: -0.024041175921247892\n","Layer 1 - Gradient Weights Max: 0.01905922843313659 Min: -0.01915435757097576\n","Layer 0 - Gradient Weights Max: 0.030301091732070212 Min: -0.025180312991568587\n","ReLU Activation - Max: 1.5211606532565094 Min: 0.0\n","ReLU Activation - Max: 0.5224802962275494 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01665915984204825 Min: -0.0189798903018909\n","Layer 1 - Gradient Weights Max: 0.017336239209671485 Min: -0.02652658096985887\n","Layer 0 - Gradient Weights Max: 0.031488066593330706 Min: -0.03058687426883302\n","ReLU Activation - Max: 1.205759677810732 Min: 0.0\n","ReLU Activation - Max: 0.6908956628286214 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017324060081035028 Min: -0.030876600662682996\n","Layer 1 - Gradient Weights Max: 0.021644109592743776 Min: -0.03174170390929505\n","Layer 0 - Gradient Weights Max: 0.028570054376330752 Min: -0.0318221118622889\n","ReLU Activation - Max: 1.3153037155744658 Min: 0.0\n","ReLU Activation - Max: 0.5704823723820865 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012530586451065426 Min: -0.02011359850149348\n","Layer 1 - Gradient Weights Max: 0.02194780860020688 Min: -0.02609878203515295\n","Layer 0 - Gradient Weights Max: 0.034849187071217966 Min: -0.034461590295660385\n","ReLU Activation - Max: 1.5484058194160077 Min: 0.0\n","ReLU Activation - Max: 0.6347081281943241 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01013652695621977 Min: -0.01642193254523128\n","Layer 1 - Gradient Weights Max: 0.01447858750311073 Min: -0.025170798172582547\n","Layer 0 - Gradient Weights Max: 0.026524538158085396 Min: -0.038341124018651494\n","ReLU Activation - Max: 1.79492710589926 Min: 0.0\n","ReLU Activation - Max: 0.6409173519585979 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022750408399411146 Min: -0.022439335355602455\n","Layer 1 - Gradient Weights Max: 0.015289005384955284 Min: -0.025126489326587644\n","Layer 0 - Gradient Weights Max: 0.030220299251630917 Min: -0.029766307156090733\n","ReLU Activation - Max: 1.4113252138583936 Min: 0.0\n","ReLU Activation - Max: 0.5923968318552304 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020416800921273025 Min: -0.02646545777098944\n","Layer 1 - Gradient Weights Max: 0.021478399162606902 Min: -0.021948974005605828\n","Layer 0 - Gradient Weights Max: 0.031901395304679316 Min: -0.03129045238765661\n","ReLU Activation - Max: 1.447311313806388 Min: 0.0\n","ReLU Activation - Max: 0.5836955012388518 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013461595131856753 Min: -0.03047475772119525\n","Layer 1 - Gradient Weights Max: 0.02677303740347859 Min: -0.020598202129125828\n","Layer 0 - Gradient Weights Max: 0.02866116658572637 Min: -0.03238449579725564\n","ReLU Activation - Max: 1.2821818843746464 Min: 0.0\n","ReLU Activation - Max: 0.5537722808195311 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010979975297333246 Min: -0.011207965723147304\n","Layer 1 - Gradient Weights Max: 0.019987042501104637 Min: -0.026067335647480937\n","Layer 0 - Gradient Weights Max: 0.031939293411258854 Min: -0.03755139777003265\n","ReLU Activation - Max: 1.4014691433062203 Min: 0.0\n","ReLU Activation - Max: 0.6859062063728544 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012527517425303667 Min: -0.011624497738483853\n","Layer 1 - Gradient Weights Max: 0.01848363488428116 Min: -0.017680048429456918\n","Layer 0 - Gradient Weights Max: 0.038899003037553545 Min: -0.04236716614482875\n","ReLU Activation - Max: 1.424953025159682 Min: 0.0\n","ReLU Activation - Max: 0.5980161063305648 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012632066286001496 Min: -0.02238506506407251\n","Layer 1 - Gradient Weights Max: 0.016951013898377387 Min: -0.021728424777011004\n","Layer 0 - Gradient Weights Max: 0.02831869670456974 Min: -0.03209339111648507\n","ReLU Activation - Max: 1.4639804110682166 Min: 0.0\n","ReLU Activation - Max: 0.5545976897838925 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014604636249772712 Min: -0.022100945128755076\n","Layer 1 - Gradient Weights Max: 0.018925501328376705 Min: -0.015810521783975427\n","Layer 0 - Gradient Weights Max: 0.02713779136229122 Min: -0.02964262582098763\n","ReLU Activation - Max: 1.4826715853983656 Min: 0.0\n","ReLU Activation - Max: 0.5805320988623927 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020846842163701012 Min: -0.020346862353152785\n","Layer 1 - Gradient Weights Max: 0.024101522297651477 Min: -0.017341355175294608\n","Layer 0 - Gradient Weights Max: 0.027741563901401617 Min: -0.028947779534854515\n","ReLU Activation - Max: 1.3306184587522576 Min: 0.0\n","ReLU Activation - Max: 0.607933958773937 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013413175379833884 Min: -0.018176847485235046\n","Layer 1 - Gradient Weights Max: 0.01786722986815758 Min: -0.016908155367590248\n","Layer 0 - Gradient Weights Max: 0.03288310479551523 Min: -0.030136146775183198\n","ReLU Activation - Max: 1.48431952060943 Min: 0.0\n","ReLU Activation - Max: 0.5921281315142098 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020710348659869912 Min: -0.024488351964428515\n","Layer 1 - Gradient Weights Max: 0.024530401823046125 Min: -0.02448212200072685\n","Layer 0 - Gradient Weights Max: 0.025820758898072525 Min: -0.025278194882625688\n","ReLU Activation - Max: 1.518970833328439 Min: 0.0\n","ReLU Activation - Max: 0.6474911563819972 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008767440827795256 Min: -0.011606658326972325\n","Layer 1 - Gradient Weights Max: 0.019451981691800155 Min: -0.024410119923146765\n","Layer 0 - Gradient Weights Max: 0.0306894570164072 Min: -0.02596303000132506\n","ReLU Activation - Max: 1.5935141477206722 Min: 0.0\n","ReLU Activation - Max: 0.610689809955864 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018505431332107164 Min: -0.02779124359018772\n","Layer 1 - Gradient Weights Max: 0.020972548414796094 Min: -0.020526759931481585\n","Layer 0 - Gradient Weights Max: 0.023368143654375718 Min: -0.02572006473807835\n","ReLU Activation - Max: 1.9699798216645585 Min: 0.0\n","ReLU Activation - Max: 0.5350774384814219 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014627627337141545 Min: -0.015188171472830442\n","Layer 1 - Gradient Weights Max: 0.01832888153865695 Min: -0.02070140192681815\n","Layer 0 - Gradient Weights Max: 0.028915295940489277 Min: -0.031347875549291554\n","ReLU Activation - Max: 1.4805518418074028 Min: 0.0\n","ReLU Activation - Max: 0.6375097534519316 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013259758823056167 Min: -0.022769014440827195\n","Layer 1 - Gradient Weights Max: 0.02915407421355911 Min: -0.012726830032549737\n","Layer 0 - Gradient Weights Max: 0.03954225013454996 Min: -0.027210039935343294\n","ReLU Activation - Max: 1.4121122201224707 Min: 0.0\n","ReLU Activation - Max: 0.625209327179454 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01279557144538929 Min: -0.030413486602627337\n","Layer 1 - Gradient Weights Max: 0.019241914154022165 Min: -0.023697516490681473\n","Layer 0 - Gradient Weights Max: 0.03183475559841075 Min: -0.03682028149271419\n","ReLU Activation - Max: 1.4697071533830168 Min: 0.0\n","ReLU Activation - Max: 0.6556172583816595 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016304517535829934 Min: -0.01889084686973944\n","Layer 1 - Gradient Weights Max: 0.02054103660381296 Min: -0.04055523809425664\n","Layer 0 - Gradient Weights Max: 0.040554367423806664 Min: -0.052374816104581154\n","ReLU Activation - Max: 1.4251222061035762 Min: 0.0\n","ReLU Activation - Max: 0.5585764034430325 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01870379261326456 Min: -0.013538409051940174\n","Layer 1 - Gradient Weights Max: 0.014653693869969783 Min: -0.019423094608589535\n","Layer 0 - Gradient Weights Max: 0.025684445646135607 Min: -0.025653161276595778\n","ReLU Activation - Max: 1.3821543579492055 Min: 0.0\n","ReLU Activation - Max: 0.5578876466120083 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011327609300473631 Min: -0.02061968913775674\n","Layer 1 - Gradient Weights Max: 0.017190711036486627 Min: -0.017513831668491787\n","Layer 0 - Gradient Weights Max: 0.031601933257723945 Min: -0.025712192984365365\n","ReLU Activation - Max: 1.3081815709994828 Min: 0.0\n","ReLU Activation - Max: 0.5695486906659667 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017251828556941365 Min: -0.01940983900758075\n","Layer 1 - Gradient Weights Max: 0.01678081108630012 Min: -0.025436318644073756\n","Layer 0 - Gradient Weights Max: 0.03131658713046689 Min: -0.0251972885922811\n","ReLU Activation - Max: 1.2545680608696297 Min: 0.0\n","ReLU Activation - Max: 0.5874837567017551 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01748663575780306 Min: -0.023603123918217693\n","Layer 1 - Gradient Weights Max: 0.022558414437468068 Min: -0.03202848304559792\n","Layer 0 - Gradient Weights Max: 0.03322158200307615 Min: -0.027701514373389136\n","ReLU Activation - Max: 1.4943331740536516 Min: 0.0\n","ReLU Activation - Max: 0.6705007246578121 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018277690692536604 Min: -0.02738112544825826\n","Layer 1 - Gradient Weights Max: 0.02050744268367891 Min: -0.020039510244498043\n","Layer 0 - Gradient Weights Max: 0.04736212881935875 Min: -0.027040816398234004\n","ReLU Activation - Max: 1.6381369712881275 Min: 0.0\n","ReLU Activation - Max: 0.616495914980118 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014039298252073397 Min: -0.014378347865770951\n","Layer 1 - Gradient Weights Max: 0.01649629215392598 Min: -0.02154190113636085\n","Layer 0 - Gradient Weights Max: 0.030147196612757277 Min: -0.035667942494311086\n","ReLU Activation - Max: 1.615436098109808 Min: 0.0\n","ReLU Activation - Max: 0.586795735999778 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01585446669352177 Min: -0.026275479856478698\n","Layer 1 - Gradient Weights Max: 0.019369206536199297 Min: -0.03274551527772788\n","Layer 0 - Gradient Weights Max: 0.031868419259533175 Min: -0.04129413433515829\n","ReLU Activation - Max: 1.3380069111769461 Min: 0.0\n","ReLU Activation - Max: 0.5454881858529104 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015720783277912563 Min: -0.016989789484176746\n","Layer 1 - Gradient Weights Max: 0.01713650644176409 Min: -0.023543061732318536\n","Layer 0 - Gradient Weights Max: 0.030303749575347674 Min: -0.02981576077346657\n","ReLU Activation - Max: 1.623682813611612 Min: 0.0\n","ReLU Activation - Max: 0.5503671719167171 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012754803910442609 Min: -0.012262896451561556\n","Layer 1 - Gradient Weights Max: 0.015620426515266413 Min: -0.022076011059329083\n","Layer 0 - Gradient Weights Max: 0.03067168589533726 Min: -0.0299033291139413\n","ReLU Activation - Max: 1.4293647395907014 Min: 0.0\n","ReLU Activation - Max: 0.5295911166508604 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012124684629565233 Min: -0.02568620016093742\n","Layer 1 - Gradient Weights Max: 0.018527698316018467 Min: -0.018699385978964173\n","Layer 0 - Gradient Weights Max: 0.022170764065197888 Min: -0.0280755119595623\n","ReLU Activation - Max: 1.355961902139019 Min: 0.0\n","ReLU Activation - Max: 0.5200834462952323 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011040245813939092 Min: -0.013488627980433538\n","Layer 1 - Gradient Weights Max: 0.02410065363884137 Min: -0.019408598390125133\n","Layer 0 - Gradient Weights Max: 0.03068047948892986 Min: -0.03558584394756147\n","ReLU Activation - Max: 1.5188371564770593 Min: 0.0\n","ReLU Activation - Max: 0.6182074658293203 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012690925333617648 Min: -0.015130631534099246\n","Layer 1 - Gradient Weights Max: 0.01707662146780809 Min: -0.02019146691549684\n","Layer 0 - Gradient Weights Max: 0.027913006514734738 Min: -0.030129817444867847\n","ReLU Activation - Max: 1.492804048790217 Min: 0.0\n","ReLU Activation - Max: 0.5455537498994609 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010493640986577658 Min: -0.012670479275205659\n","Layer 1 - Gradient Weights Max: 0.01934079517230281 Min: -0.01724234306197898\n","Layer 0 - Gradient Weights Max: 0.03141590874823297 Min: -0.034448565948048356\n","ReLU Activation - Max: 1.1931857214260255 Min: 0.0\n","ReLU Activation - Max: 0.5280372605804109 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016129547166824066 Min: -0.01756241292225226\n","Layer 1 - Gradient Weights Max: 0.017345937083495817 Min: -0.01836660879674552\n","Layer 0 - Gradient Weights Max: 0.02598182765477619 Min: -0.027264856280714126\n","ReLU Activation - Max: 1.766316643685997 Min: 0.0\n","ReLU Activation - Max: 0.6679580632100803 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015725570986110365 Min: -0.01799235115992696\n","Layer 1 - Gradient Weights Max: 0.020680063494325725 Min: -0.021271418251058195\n","Layer 0 - Gradient Weights Max: 0.03401697041975861 Min: -0.02545103894756574\n","ReLU Activation - Max: 1.4871588966923686 Min: 0.0\n","ReLU Activation - Max: 0.6032496538438665 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020171960357699328 Min: -0.020550945722700307\n","Layer 1 - Gradient Weights Max: 0.019842729025285253 Min: -0.019553608453544785\n","Layer 0 - Gradient Weights Max: 0.040829359669895655 Min: -0.026970017277598762\n","ReLU Activation - Max: 1.4757809989521915 Min: 0.0\n","ReLU Activation - Max: 0.5510869940999754 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024201420682873686 Min: -0.020531479680287047\n","Layer 1 - Gradient Weights Max: 0.020141574159190212 Min: -0.028016131498613345\n","Layer 0 - Gradient Weights Max: 0.025387104287738247 Min: -0.03571493325953003\n","ReLU Activation - Max: 1.3476553468906451 Min: 0.0\n","ReLU Activation - Max: 0.6960003996124792 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018043717788016313 Min: -0.020267354632334714\n","Layer 1 - Gradient Weights Max: 0.02195737594153423 Min: -0.01935513344775959\n","Layer 0 - Gradient Weights Max: 0.030283661381626324 Min: -0.025652004556153546\n","ReLU Activation - Max: 1.4890168375212929 Min: 0.0\n","ReLU Activation - Max: 0.7455441264786338 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01730483373060951 Min: -0.020385191975899025\n","Layer 1 - Gradient Weights Max: 0.024270943421024708 Min: -0.021877447531933405\n","Layer 0 - Gradient Weights Max: 0.027937677193672135 Min: -0.030072894793222574\n","ReLU Activation - Max: 1.6430305595979895 Min: 0.0\n","ReLU Activation - Max: 0.5804313278187986 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019478378128122954 Min: -0.014148213280262223\n","Layer 1 - Gradient Weights Max: 0.02538906977130623 Min: -0.015048202448802549\n","Layer 0 - Gradient Weights Max: 0.03561495633099663 Min: -0.025976402236990485\n","ReLU Activation - Max: 1.8806556444860407 Min: 0.0\n","ReLU Activation - Max: 0.6722711740527083 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020320641325920746 Min: -0.02772984705177583\n","Layer 1 - Gradient Weights Max: 0.016806218909619412 Min: -0.024968405899349618\n","Layer 0 - Gradient Weights Max: 0.022569532397184792 Min: -0.03281705233778168\n","ReLU Activation - Max: 1.3965915141087795 Min: 0.0\n","ReLU Activation - Max: 0.5079284510845565 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018755834453069994 Min: -0.016828001522976362\n","Layer 1 - Gradient Weights Max: 0.018862786958071698 Min: -0.02905084270260295\n","Layer 0 - Gradient Weights Max: 0.028738520152015296 Min: -0.03452044010191064\n","ReLU Activation - Max: 1.5456907379160054 Min: 0.0\n","ReLU Activation - Max: 0.6483503186298127 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015022514801161565 Min: -0.024026992479001134\n","Layer 1 - Gradient Weights Max: 0.02004969511082076 Min: -0.023702242701808214\n","Layer 0 - Gradient Weights Max: 0.023653744231535753 Min: -0.031487149354880804\n","ReLU Activation - Max: 2.1188446379753803 Min: 0.0\n","ReLU Activation - Max: 0.5353401551646184 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018059703913914676 Min: -0.014022207924628624\n","Layer 1 - Gradient Weights Max: 0.023071299425791975 Min: -0.02831462579653194\n","Layer 0 - Gradient Weights Max: 0.03182975872526527 Min: -0.03572478612267915\n","ReLU Activation - Max: 1.3302538958197134 Min: 0.0\n","ReLU Activation - Max: 0.5615737635602418 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012379127080410586 Min: -0.01685156543298055\n","Layer 1 - Gradient Weights Max: 0.018079446901801778 Min: -0.020087929325622766\n","Layer 0 - Gradient Weights Max: 0.0249044498553702 Min: -0.02792379195727611\n","ReLU Activation - Max: 1.5015909734519615 Min: 0.0\n","ReLU Activation - Max: 0.5184142222372399 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025065005069440146 Min: -0.01222634117107032\n","Layer 1 - Gradient Weights Max: 0.019008791320483037 Min: -0.027825171877768503\n","Layer 0 - Gradient Weights Max: 0.04865741780556712 Min: -0.030776266294211502\n","ReLU Activation - Max: 1.5406330201259923 Min: 0.0\n","ReLU Activation - Max: 0.6608178742724972 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0208504613878985 Min: -0.02526639267814591\n","Layer 1 - Gradient Weights Max: 0.024776049321093372 Min: -0.029534963416581118\n","Layer 0 - Gradient Weights Max: 0.02774243190081793 Min: -0.03968573936434073\n","ReLU Activation - Max: 1.3465117026969013 Min: 0.0\n","ReLU Activation - Max: 0.5061918731200065 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014425944675300971 Min: -0.015984474965970233\n","Layer 1 - Gradient Weights Max: 0.019040419608158114 Min: -0.01962110505785736\n","Layer 0 - Gradient Weights Max: 0.03142907080218496 Min: -0.032733121913128715\n","ReLU Activation - Max: 1.4954701395479173 Min: 0.0\n","ReLU Activation - Max: 0.6961093753340507 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017601396685311025 Min: -0.011626688743565958\n","Layer 1 - Gradient Weights Max: 0.01770872409636406 Min: -0.01623414454638614\n","Layer 0 - Gradient Weights Max: 0.03629395462583361 Min: -0.025184440517906952\n","ReLU Activation - Max: 1.382243961242296 Min: 0.0\n","ReLU Activation - Max: 0.49735962085015184 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01944494689743126 Min: -0.018238615649028168\n","Layer 1 - Gradient Weights Max: 0.013237593085537628 Min: -0.02484486524040853\n","Layer 0 - Gradient Weights Max: 0.02677733586040816 Min: -0.027772806084882085\n","ReLU Activation - Max: 1.2937629386456824 Min: 0.0\n","ReLU Activation - Max: 0.599812023583628 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010873253236353247 Min: -0.015609948543670687\n","Layer 1 - Gradient Weights Max: 0.02608006257550606 Min: -0.017681875776893365\n","Layer 0 - Gradient Weights Max: 0.03943431792044694 Min: -0.034585215707424954\n","ReLU Activation - Max: 1.397587314012893 Min: 0.0\n","ReLU Activation - Max: 0.5277788334207177 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011445919797236137 Min: -0.020222092866235674\n","Layer 1 - Gradient Weights Max: 0.01706113758776535 Min: -0.03298850220926075\n","Layer 0 - Gradient Weights Max: 0.029806096051803257 Min: -0.024748157868329117\n","ReLU Activation - Max: 1.6904612576228926 Min: 0.0\n","ReLU Activation - Max: 0.6851317945867148 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021970354908308978 Min: -0.0240006426100033\n","Layer 1 - Gradient Weights Max: 0.021517294978144925 Min: -0.02646182502736531\n","Layer 0 - Gradient Weights Max: 0.03441819331047781 Min: -0.03494707943765792\n","ReLU Activation - Max: 1.4157219114040962 Min: 0.0\n","ReLU Activation - Max: 0.74626924861448 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014404498599450758 Min: -0.01584261492153564\n","Layer 1 - Gradient Weights Max: 0.020369217134854976 Min: -0.020692598001311898\n","Layer 0 - Gradient Weights Max: 0.0315410662921969 Min: -0.02839176295608144\n","ReLU Activation - Max: 1.4974690668546142 Min: 0.0\n","ReLU Activation - Max: 0.6582973280345645 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01823065289558777 Min: -0.019658409427257537\n","Layer 1 - Gradient Weights Max: 0.023515817052965595 Min: -0.03029932175602403\n","Layer 0 - Gradient Weights Max: 0.035744678086666044 Min: -0.03718468020540066\n","ReLU Activation - Max: 1.4023303928528765 Min: 0.0\n","ReLU Activation - Max: 0.6554214708551181 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014145873264900523 Min: -0.012578779426138874\n","Layer 1 - Gradient Weights Max: 0.01909539763944572 Min: -0.01925588018346173\n","Layer 0 - Gradient Weights Max: 0.03045060691685397 Min: -0.026243102956065318\n","ReLU Activation - Max: 1.599476043776943 Min: 0.0\n","ReLU Activation - Max: 0.6998853201790314 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010995542449167206 Min: -0.024826601751790484\n","Layer 1 - Gradient Weights Max: 0.021580083716662007 Min: -0.023882407125952597\n","Layer 0 - Gradient Weights Max: 0.025803696264427526 Min: -0.02585752470402547\n","ReLU Activation - Max: 1.609776096468622 Min: 0.0\n","ReLU Activation - Max: 0.7829878448547921 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018691248919253913 Min: -0.023871749720737464\n","Layer 1 - Gradient Weights Max: 0.014592648811998772 Min: -0.027288466568849694\n","Layer 0 - Gradient Weights Max: 0.022701644406012966 Min: -0.03206765091438166\n","ReLU Activation - Max: 1.3509751405839514 Min: 0.0\n","ReLU Activation - Max: 0.5060294074280901 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012545451332689616 Min: -0.014714372704412374\n","Layer 1 - Gradient Weights Max: 0.017267092692040242 Min: -0.019152083611794973\n","Layer 0 - Gradient Weights Max: 0.0322229895172726 Min: -0.029710919009365875\n","ReLU Activation - Max: 1.389988725290685 Min: 0.0\n","ReLU Activation - Max: 0.5526266026480112 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015360140884930554 Min: -0.024688385620518765\n","Layer 1 - Gradient Weights Max: 0.020564878221000813 Min: -0.019550711457898325\n","Layer 0 - Gradient Weights Max: 0.038080907156185825 Min: -0.03438589029251829\n","ReLU Activation - Max: 1.4470604456278893 Min: 0.0\n","ReLU Activation - Max: 0.5928143388700247 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01947876370146335 Min: -0.02132083283667398\n","Layer 1 - Gradient Weights Max: 0.01603693172313644 Min: -0.020769049238986845\n","Layer 0 - Gradient Weights Max: 0.027371115867838573 Min: -0.029989539099977404\n","ReLU Activation - Max: 1.4997017371529546 Min: 0.0\n","ReLU Activation - Max: 0.6136871578646185 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012931719890682454 Min: -0.017118812733549073\n","Layer 1 - Gradient Weights Max: 0.017999184834483452 Min: -0.012883385384836157\n","Layer 0 - Gradient Weights Max: 0.05194818483021803 Min: -0.024999874654906166\n","ReLU Activation - Max: 1.4545305553055896 Min: 0.0\n","ReLU Activation - Max: 0.5862435298576352 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01660426427319549 Min: -0.02323380890934709\n","Layer 1 - Gradient Weights Max: 0.019145238067299754 Min: -0.0274683179418137\n","Layer 0 - Gradient Weights Max: 0.02745721655478636 Min: -0.030237961087105705\n","ReLU Activation - Max: 1.6361815024030457 Min: 0.0\n","ReLU Activation - Max: 0.5659295010190387 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01397326413797785 Min: -0.025130313836273753\n","Layer 1 - Gradient Weights Max: 0.01609037689398151 Min: -0.03887741072133339\n","Layer 0 - Gradient Weights Max: 0.03588223244781015 Min: -0.033062239873598284\n","ReLU Activation - Max: 1.1674176370424094 Min: 0.0\n","ReLU Activation - Max: 0.5339805004343293 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015843325772911147 Min: -0.01873497916856516\n","Layer 1 - Gradient Weights Max: 0.022639819056626646 Min: -0.020178951447853106\n","Layer 0 - Gradient Weights Max: 0.030388256855204167 Min: -0.030238029345353516\n","ReLU Activation - Max: 1.2280203052558998 Min: 0.0\n","ReLU Activation - Max: 0.5120276136457235 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01596813323941366 Min: -0.020531681184490603\n","Layer 1 - Gradient Weights Max: 0.01804809348918841 Min: -0.020779429878714763\n","Layer 0 - Gradient Weights Max: 0.02290218382385528 Min: -0.028770346298566434\n","ReLU Activation - Max: 1.3609943952603265 Min: 0.0\n","ReLU Activation - Max: 0.6388502709394498 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019420081101503867 Min: -0.02401819827678234\n","Layer 1 - Gradient Weights Max: 0.019867132922199496 Min: -0.030487369781188253\n","Layer 0 - Gradient Weights Max: 0.028326626242474347 Min: -0.03231306296924908\n","ReLU Activation - Max: 1.81318978013913 Min: 0.0\n","ReLU Activation - Max: 0.6218194360151813 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013901768608708957 Min: -0.016771528765926955\n","Layer 1 - Gradient Weights Max: 0.028121082197046985 Min: -0.029125588068006487\n","Layer 0 - Gradient Weights Max: 0.027553064637367942 Min: -0.026462424251549023\n","ReLU Activation - Max: 1.4379314850346105 Min: 0.0\n","ReLU Activation - Max: 0.5364974094292098 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02006573030580422 Min: -0.02078346995228646\n","Layer 1 - Gradient Weights Max: 0.02097928416626204 Min: -0.019968256007533998\n","Layer 0 - Gradient Weights Max: 0.029202147078987332 Min: -0.03512290913684364\n","ReLU Activation - Max: 1.5544071546121156 Min: 0.0\n","ReLU Activation - Max: 0.560644596461148 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010395542241603495 Min: -0.014373013088759523\n","Layer 1 - Gradient Weights Max: 0.016322802577689068 Min: -0.021347580692764545\n","Layer 0 - Gradient Weights Max: 0.030440843388734033 Min: -0.025955196828537037\n","ReLU Activation - Max: 1.8866589848444084 Min: 0.0\n","ReLU Activation - Max: 0.6401591041702803 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025738467534553767 Min: -0.02627609356279059\n","Layer 1 - Gradient Weights Max: 0.03225104940357402 Min: -0.022205958539100876\n","Layer 0 - Gradient Weights Max: 0.05110525369381439 Min: -0.027589062467437853\n","ReLU Activation - Max: 1.565531683846691 Min: 0.0\n","ReLU Activation - Max: 0.5941909943656556 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014859490399837395 Min: -0.01518082926237649\n","Layer 1 - Gradient Weights Max: 0.016186724182817754 Min: -0.022702768530570638\n","Layer 0 - Gradient Weights Max: 0.028984932530350276 Min: -0.029596463227870257\n","ReLU Activation - Max: 1.7748196305185047 Min: 0.0\n","ReLU Activation - Max: 0.6130563967624068 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016952069108460574 Min: -0.01999355285053473\n","Layer 1 - Gradient Weights Max: 0.023402915161655657 Min: -0.043639792571057615\n","Layer 0 - Gradient Weights Max: 0.036371730571473686 Min: -0.0382967422168258\n","ReLU Activation - Max: 1.3817844422255388 Min: 0.0\n","ReLU Activation - Max: 0.5379867902523222 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015705748474833758 Min: -0.026361024608164103\n","Layer 1 - Gradient Weights Max: 0.01640432579107185 Min: -0.022448573828927374\n","Layer 0 - Gradient Weights Max: 0.02629480948641284 Min: -0.02576166663978067\n","ReLU Activation - Max: 1.7879628772659437 Min: 0.0\n","ReLU Activation - Max: 0.6268554303341425 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018309215795171748 Min: -0.026849802589819078\n","Layer 1 - Gradient Weights Max: 0.01626226489631384 Min: -0.029092315069962245\n","Layer 0 - Gradient Weights Max: 0.022852680439935522 Min: -0.033594760861747744\n","ReLU Activation - Max: 1.383390455058226 Min: 0.0\n","ReLU Activation - Max: 0.5196842690570503 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014615390030440214 Min: -0.01921132480837554\n","Layer 1 - Gradient Weights Max: 0.01591913186827279 Min: -0.01735575240719964\n","Layer 0 - Gradient Weights Max: 0.034013354861502115 Min: -0.029450949762324666\n","ReLU Activation - Max: 1.694070145770933 Min: 0.0\n","ReLU Activation - Max: 0.6147546742312557 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011270160775127071 Min: -0.017725575735372586\n","Layer 1 - Gradient Weights Max: 0.019495595998692848 Min: -0.018693844908192567\n","Layer 0 - Gradient Weights Max: 0.023160947298940648 Min: -0.028903879939113326\n","ReLU Activation - Max: 1.264153710273886 Min: 0.0\n","ReLU Activation - Max: 0.6925813780842793 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019794295034981077 Min: -0.02314889216285475\n","Layer 1 - Gradient Weights Max: 0.016435745705154808 Min: -0.025923179609273275\n","Layer 0 - Gradient Weights Max: 0.029026630678967266 Min: -0.03632991322606834\n","ReLU Activation - Max: 1.3929616017655155 Min: 0.0\n","ReLU Activation - Max: 0.5741757477680515 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014951252699504192 Min: -0.014773055032681068\n","Layer 1 - Gradient Weights Max: 0.02165670439824891 Min: -0.021737506234400118\n","Layer 0 - Gradient Weights Max: 0.0291779192295454 Min: -0.027872063365107447\n","ReLU Activation - Max: 1.3637216765700928 Min: 0.0\n","ReLU Activation - Max: 0.6485611686704704 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015997442023813268 Min: -0.014314353532682563\n","Layer 1 - Gradient Weights Max: 0.02563272766520742 Min: -0.018929540307095486\n","Layer 0 - Gradient Weights Max: 0.03371242872798006 Min: -0.02730738873520165\n","ReLU Activation - Max: 1.5047425455121957 Min: 0.0\n","ReLU Activation - Max: 0.5832379743298536 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01958126799084214 Min: -0.019921744318869006\n","Layer 1 - Gradient Weights Max: 0.018091361358403193 Min: -0.017105984161317676\n","Layer 0 - Gradient Weights Max: 0.032021435983797135 Min: -0.02669522404687943\n","ReLU Activation - Max: 1.5297071144736494 Min: 0.0\n","ReLU Activation - Max: 0.6189793496550723 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021960044845938456 Min: -0.016713811073566058\n","Layer 1 - Gradient Weights Max: 0.027172611129912976 Min: -0.02989902350466888\n","Layer 0 - Gradient Weights Max: 0.03297162123629067 Min: -0.037901885030490405\n","ReLU Activation - Max: 1.4441108887592633 Min: 0.0\n","ReLU Activation - Max: 0.569828503302476 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01217663110737611 Min: -0.013629092010513162\n","Layer 1 - Gradient Weights Max: 0.024278995504120122 Min: -0.030774214041356836\n","Layer 0 - Gradient Weights Max: 0.031922331648001186 Min: -0.02891977049002804\n","ReLU Activation - Max: 1.3574292918257906 Min: 0.0\n","ReLU Activation - Max: 0.6538741312475982 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0188785438781658 Min: -0.019114955225327624\n","Layer 1 - Gradient Weights Max: 0.01661333791917886 Min: -0.0270708242124429\n","Layer 0 - Gradient Weights Max: 0.029966594354049587 Min: -0.029751088781116822\n","ReLU Activation - Max: 1.5846298974638549 Min: 0.0\n","ReLU Activation - Max: 0.584453490879409 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011182998378041521 Min: -0.013521532405178102\n","Layer 1 - Gradient Weights Max: 0.018460910896172144 Min: -0.020000562017049254\n","Layer 0 - Gradient Weights Max: 0.034614952112280696 Min: -0.03472739265242771\n","ReLU Activation - Max: 1.3845913230061029 Min: 0.0\n","ReLU Activation - Max: 0.6254260146999069 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013708661569338726 Min: -0.02075754398396965\n","Layer 1 - Gradient Weights Max: 0.020454070231519024 Min: -0.03050513668313746\n","Layer 0 - Gradient Weights Max: 0.03501617671588691 Min: -0.03155619197395852\n","ReLU Activation - Max: 1.762255412285254 Min: 0.0\n","ReLU Activation - Max: 0.5277282408026853 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014061522412012872 Min: -0.01760426490175808\n","Layer 1 - Gradient Weights Max: 0.019121815218533164 Min: -0.01821680772208161\n","Layer 0 - Gradient Weights Max: 0.026068302295862756 Min: -0.035808082182833205\n","ReLU Activation - Max: 1.7604302630196704 Min: 0.0\n","ReLU Activation - Max: 0.6731880338324361 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013841195458937933 Min: -0.012685971408218616\n","Layer 1 - Gradient Weights Max: 0.01731450331437243 Min: -0.016855826385786147\n","Layer 0 - Gradient Weights Max: 0.037004804608870026 Min: -0.030247931436335338\n","ReLU Activation - Max: 1.6358878635028333 Min: 0.0\n","ReLU Activation - Max: 0.6761322533379401 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014133553405243342 Min: -0.020884682673800663\n","Layer 1 - Gradient Weights Max: 0.018311486958813537 Min: -0.022796273242374883\n","Layer 0 - Gradient Weights Max: 0.031086054359783892 Min: -0.025065150225242354\n","ReLU Activation - Max: 1.4648342312418388 Min: 0.0\n","ReLU Activation - Max: 0.6105479013077029 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012606662966357303 Min: -0.014145331232696866\n","Layer 1 - Gradient Weights Max: 0.018725185888309494 Min: -0.025066482166398608\n","Layer 0 - Gradient Weights Max: 0.029605154671276303 Min: -0.030305540540259166\n","ReLU Activation - Max: 1.5090209871510563 Min: 0.0\n","ReLU Activation - Max: 0.6125834529596562 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013321976606122369 Min: -0.021414900882927693\n","Layer 1 - Gradient Weights Max: 0.01934284761112032 Min: -0.036991883815780315\n","Layer 0 - Gradient Weights Max: 0.03777032920391146 Min: -0.029774953972264526\n","ReLU Activation - Max: 1.4221139037756887 Min: 0.0\n","ReLU Activation - Max: 0.5594371197771849 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022559449698636012 Min: -0.02305610964239192\n","Layer 1 - Gradient Weights Max: 0.025115354589667356 Min: -0.020857654324963684\n","Layer 0 - Gradient Weights Max: 0.027070014286191917 Min: -0.02761619454052016\n","ReLU Activation - Max: 1.6527285903506153 Min: 0.0\n","ReLU Activation - Max: 0.5261175080779107 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011212175188570523 Min: -0.013781439820561462\n","Layer 1 - Gradient Weights Max: 0.01831962933923756 Min: -0.023057177645484487\n","Layer 0 - Gradient Weights Max: 0.026212772784163254 Min: -0.030205883000265294\n","ReLU Activation - Max: 1.671263402732118 Min: 0.0\n","ReLU Activation - Max: 0.6176436284482273 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02376623850389911 Min: -0.019201303351571244\n","Layer 1 - Gradient Weights Max: 0.029704652834603093 Min: -0.015144835583391456\n","Layer 0 - Gradient Weights Max: 0.04628970308404851 Min: -0.028526847610052346\n","ReLU Activation - Max: 1.8328409966786183 Min: 0.0\n","ReLU Activation - Max: 0.6012348136250834 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01947844129161869 Min: -0.015556767763247424\n","Layer 1 - Gradient Weights Max: 0.019904415138829432 Min: -0.017800333459475067\n","Layer 0 - Gradient Weights Max: 0.02780281600675452 Min: -0.04379574062630976\n","ReLU Activation - Max: 1.336893264069052 Min: 0.0\n","ReLU Activation - Max: 0.5589762299196233 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008771006932832748 Min: -0.011586238734618228\n","Layer 1 - Gradient Weights Max: 0.01676999860922639 Min: -0.022222821033540945\n","Layer 0 - Gradient Weights Max: 0.02870913405494718 Min: -0.04015746166604777\n","ReLU Activation - Max: 1.9953129462860677 Min: 0.0\n","ReLU Activation - Max: 0.5525254892296649 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016798953042339432 Min: -0.01186007939719681\n","Layer 1 - Gradient Weights Max: 0.016500204395275855 Min: -0.0195904655548008\n","Layer 0 - Gradient Weights Max: 0.025973541078484253 Min: -0.02719458889860425\n","ReLU Activation - Max: 1.3579224214458543 Min: 0.0\n","ReLU Activation - Max: 0.5270470976746845 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012562389178115783 Min: -0.011368223413909403\n","Layer 1 - Gradient Weights Max: 0.013889616090975167 Min: -0.017416479088308744\n","Layer 0 - Gradient Weights Max: 0.028280052607549566 Min: -0.02795661291425901\n","ReLU Activation - Max: 1.7026043884090503 Min: 0.0\n","ReLU Activation - Max: 0.5872393796320281 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020991046516195933 Min: -0.018433563165514397\n","Layer 1 - Gradient Weights Max: 0.02288449956377013 Min: -0.021233012011003187\n","Layer 0 - Gradient Weights Max: 0.026678284657704537 Min: -0.03464766534180973\n","ReLU Activation - Max: 1.9446618155114668 Min: 0.0\n","ReLU Activation - Max: 0.5441905901952326 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02465912046771551 Min: -0.01945957887671839\n","Layer 1 - Gradient Weights Max: 0.029259866423644334 Min: -0.02758669969733171\n","Layer 0 - Gradient Weights Max: 0.02601571774956629 Min: -0.034758522476893365\n","ReLU Activation - Max: 1.4382661049991952 Min: 0.0\n","ReLU Activation - Max: 0.5868103568628392 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019242770433946572 Min: -0.015161087435066029\n","Layer 1 - Gradient Weights Max: 0.019295565824213143 Min: -0.016805552682765904\n","Layer 0 - Gradient Weights Max: 0.02383108309688411 Min: -0.03288169376622145\n","ReLU Activation - Max: 1.3880554052240168 Min: 0.0\n","ReLU Activation - Max: 0.5884918394177124 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013778948460339335 Min: -0.02487174478864171\n","Layer 1 - Gradient Weights Max: 0.013451109139054642 Min: -0.02429895091326793\n","Layer 0 - Gradient Weights Max: 0.03171244174859148 Min: -0.026652428976475004\n","ReLU Activation - Max: 1.840789466226565 Min: 0.0\n","ReLU Activation - Max: 0.673954196221455 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014873952783962469 Min: -0.030597656659053927\n","Layer 1 - Gradient Weights Max: 0.018799944536938406 Min: -0.029980238417558376\n","Layer 0 - Gradient Weights Max: 0.027096059026477364 Min: -0.03590535855383313\n","ReLU Activation - Max: 1.5919755439288374 Min: 0.0\n","ReLU Activation - Max: 0.5795217477733503 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015435536919069312 Min: -0.020109036010533845\n","Layer 1 - Gradient Weights Max: 0.02315294628780718 Min: -0.028637338981574365\n","Layer 0 - Gradient Weights Max: 0.033409802027884176 Min: -0.02828943663425943\n","ReLU Activation - Max: 1.2844113133522181 Min: 0.0\n","ReLU Activation - Max: 0.5913411471926552 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02044426290692492 Min: -0.025237017033661567\n","Layer 1 - Gradient Weights Max: 0.01883907415240808 Min: -0.022172312238333772\n","Layer 0 - Gradient Weights Max: 0.04149416704996111 Min: -0.03538852395364734\n","ReLU Activation - Max: 1.317811163138641 Min: 0.0\n","ReLU Activation - Max: 0.6373738055261887 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01007492395191245 Min: -0.020248604900377155\n","Layer 1 - Gradient Weights Max: 0.01778673540073181 Min: -0.022256228719914506\n","Layer 0 - Gradient Weights Max: 0.02488925603550308 Min: -0.028203300998157287\n","ReLU Activation - Max: 1.8529239565283615 Min: 0.0\n","ReLU Activation - Max: 0.6214977952403261 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014611847482681883 Min: -0.013539525557610992\n","Layer 1 - Gradient Weights Max: 0.02211572561035337 Min: -0.021982488274508123\n","Layer 0 - Gradient Weights Max: 0.03116816548491526 Min: -0.024871286157513384\n","ReLU Activation - Max: 1.4038674238066797 Min: 0.0\n","ReLU Activation - Max: 0.5871632794910627 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020576673738520148 Min: -0.025298218243126416\n","Layer 1 - Gradient Weights Max: 0.019638113479643386 Min: -0.017132834076064397\n","Layer 0 - Gradient Weights Max: 0.027367631123059025 Min: -0.029591315928533556\n","ReLU Activation - Max: 1.6540429952050637 Min: 0.0\n","ReLU Activation - Max: 0.574016286602688 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011576692460253796 Min: -0.014255560012829396\n","Layer 1 - Gradient Weights Max: 0.014695170514445193 Min: -0.02326460447481315\n","Layer 0 - Gradient Weights Max: 0.029071612006682124 Min: -0.029500829029839894\n","ReLU Activation - Max: 1.8862312227117615 Min: 0.0\n","ReLU Activation - Max: 0.6493489392010688 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.024945717952973408 Min: -0.02759128177538023\n","Layer 1 - Gradient Weights Max: 0.024537940455827092 Min: -0.022600154140393767\n","Layer 0 - Gradient Weights Max: 0.03724352463080098 Min: -0.046148677873339615\n","ReLU Activation - Max: 1.8216453719057049 Min: 0.0\n","ReLU Activation - Max: 0.5710968935208389 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019035048652091286 Min: -0.03344623440401126\n","Layer 1 - Gradient Weights Max: 0.023623684557470385 Min: -0.04258595763943088\n","Layer 0 - Gradient Weights Max: 0.03678970732810772 Min: -0.030075165608966645\n","ReLU Activation - Max: 1.6141396885621666 Min: 0.0\n","ReLU Activation - Max: 0.6784319543913219 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010110324062035474 Min: -0.017647283389420763\n","Layer 1 - Gradient Weights Max: 0.019501573226464126 Min: -0.017037612925587742\n","Layer 0 - Gradient Weights Max: 0.028652172804457275 Min: -0.035880362131657594\n","ReLU Activation - Max: 1.3533771276246194 Min: 0.0\n","ReLU Activation - Max: 0.5260548153634242 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01488157407933118 Min: -0.01490434979685879\n","Layer 1 - Gradient Weights Max: 0.02635081305007617 Min: -0.020099581428693565\n","Layer 0 - Gradient Weights Max: 0.03037977240390679 Min: -0.036300089851195615\n","ReLU Activation - Max: 1.5542232878677558 Min: 0.0\n","ReLU Activation - Max: 0.5858300052917962 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01745586943350819 Min: -0.015191530373695446\n","Layer 1 - Gradient Weights Max: 0.016162873081663386 Min: -0.016214577113826997\n","Layer 0 - Gradient Weights Max: 0.024854842054127375 Min: -0.027572865144513767\n","ReLU Activation - Max: 1.6412348903296217 Min: 0.0\n","ReLU Activation - Max: 0.574359598356633 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.00744396045396867 Min: -0.008533460169040689\n","Layer 1 - Gradient Weights Max: 0.011861910105901758 Min: -0.014109801355252168\n","Layer 0 - Gradient Weights Max: 0.03529475664966842 Min: -0.03162349754608621\n","ReLU Activation - Max: 1.428484016220917 Min: 0.0\n","ReLU Activation - Max: 0.630380736348145 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016018137021189344 Min: -0.02264599259660321\n","Layer 1 - Gradient Weights Max: 0.022397899704784424 Min: -0.02421015329756729\n","Layer 0 - Gradient Weights Max: 0.030306487157609446 Min: -0.03193001124356787\n","ReLU Activation - Max: 1.3416041837069899 Min: 0.0\n","ReLU Activation - Max: 0.6676543535446429 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025180808853167765 Min: -0.026841952132522634\n","Layer 1 - Gradient Weights Max: 0.01896099474111177 Min: -0.02649835840073302\n","Layer 0 - Gradient Weights Max: 0.03109186011569059 Min: -0.03697985113994816\n","ReLU Activation - Max: 1.3331067682013602 Min: 0.0\n","ReLU Activation - Max: 0.598792372956628 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014791804737979907 Min: -0.014476070024806841\n","Layer 1 - Gradient Weights Max: 0.021878534430148054 Min: -0.014429861906130841\n","Layer 0 - Gradient Weights Max: 0.0272508184362556 Min: -0.027166416325287428\n","ReLU Activation - Max: 1.3088859433884588 Min: 0.0\n","ReLU Activation - Max: 0.7553158222084645 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013882157998533836 Min: -0.023426463600432445\n","Layer 1 - Gradient Weights Max: 0.016014320344453912 Min: -0.017705621012613436\n","Layer 0 - Gradient Weights Max: 0.022610297405032845 Min: -0.026016012023278415\n","ReLU Activation - Max: 1.53982868244064 Min: 0.0\n","ReLU Activation - Max: 0.5410668316707006 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011037397751399534 Min: -0.021651481225719686\n","Layer 1 - Gradient Weights Max: 0.01648820734768695 Min: -0.018853788734377236\n","Layer 0 - Gradient Weights Max: 0.029473261176147594 Min: -0.03144817023267186\n","ReLU Activation - Max: 1.4675054862001922 Min: 0.0\n","ReLU Activation - Max: 0.6412327666395868 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015469832560411838 Min: -0.01471497812200843\n","Layer 1 - Gradient Weights Max: 0.016422778153075808 Min: -0.017451108258347132\n","Layer 0 - Gradient Weights Max: 0.029671391772299438 Min: -0.02968614058262889\n","ReLU Activation - Max: 1.3997258163487842 Min: 0.0\n","ReLU Activation - Max: 0.6964525688975042 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.025923874175405766 Min: -0.023859946731498702\n","Layer 1 - Gradient Weights Max: 0.01855795755975515 Min: -0.01957003419591608\n","Layer 0 - Gradient Weights Max: 0.023908290636877633 Min: -0.026574770428579263\n","ReLU Activation - Max: 1.5287778969903718 Min: 0.0\n","ReLU Activation - Max: 0.6265099816776223 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019311048196790612 Min: -0.017028149396067078\n","Layer 1 - Gradient Weights Max: 0.02341866383096528 Min: -0.020017321389842992\n","Layer 0 - Gradient Weights Max: 0.027517496398525614 Min: -0.02335543783187604\n","ReLU Activation - Max: 1.4011507123287128 Min: 0.0\n","ReLU Activation - Max: 0.7438238852358756 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015407701074265454 Min: -0.01728126515158978\n","Layer 1 - Gradient Weights Max: 0.022425774780011676 Min: -0.017868499162245997\n","Layer 0 - Gradient Weights Max: 0.036749247260469194 Min: -0.03353371824730012\n","ReLU Activation - Max: 1.8022442813372235 Min: 0.0\n","ReLU Activation - Max: 0.6298624146494375 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020491592713040775 Min: -0.01822348236454125\n","Layer 1 - Gradient Weights Max: 0.02129523643808308 Min: -0.021197702530617556\n","Layer 0 - Gradient Weights Max: 0.037013217698234915 Min: -0.029158759089490997\n","ReLU Activation - Max: 1.7340100854681089 Min: 0.0\n","ReLU Activation - Max: 0.7456334402543017 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010085084325618351 Min: -0.015073585621999404\n","Layer 1 - Gradient Weights Max: 0.016744231164148027 Min: -0.01814290491507521\n","Layer 0 - Gradient Weights Max: 0.037327247527811414 Min: -0.026092485308267355\n","ReLU Activation - Max: 1.6691547680221306 Min: 0.0\n","ReLU Activation - Max: 0.5290384804566253 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011999193567785926 Min: -0.017557022251246873\n","Layer 1 - Gradient Weights Max: 0.020091509244704923 Min: -0.02092833108112019\n","Layer 0 - Gradient Weights Max: 0.0230522330371335 Min: -0.035893041771879276\n","ReLU Activation - Max: 1.4308169965215751 Min: 0.0\n","ReLU Activation - Max: 0.6301312314685872 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01767765182865458 Min: -0.01854909887584179\n","Layer 1 - Gradient Weights Max: 0.02118546834324097 Min: -0.016490283492235875\n","Layer 0 - Gradient Weights Max: 0.028978648083934043 Min: -0.02549592859722799\n","ReLU Activation - Max: 1.5664077423817344 Min: 0.0\n","ReLU Activation - Max: 0.5370740769556265 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011563952058746007 Min: -0.010837528725859942\n","Layer 1 - Gradient Weights Max: 0.01605127276017762 Min: -0.01487959255611967\n","Layer 0 - Gradient Weights Max: 0.02664252527006141 Min: -0.031236773645857963\n","ReLU Activation - Max: 1.6318052224889952 Min: 0.0\n","ReLU Activation - Max: 0.6160421756722148 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012786116174564034 Min: -0.015246784083678624\n","Layer 1 - Gradient Weights Max: 0.020265973672419016 Min: -0.0221605278859967\n","Layer 0 - Gradient Weights Max: 0.03684090654217281 Min: -0.04189786539031269\n","ReLU Activation - Max: 1.2411668426993885 Min: 0.0\n","ReLU Activation - Max: 0.5456288676713529 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010996547175629425 Min: -0.02266056842753759\n","Layer 1 - Gradient Weights Max: 0.016024703665535628 Min: -0.020542165398702142\n","Layer 0 - Gradient Weights Max: 0.033273135306962465 Min: -0.02982105829311162\n","ReLU Activation - Max: 1.7353678741934573 Min: 0.0\n","ReLU Activation - Max: 0.6416761964511049 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01216712186795653 Min: -0.016351816368936633\n","Layer 1 - Gradient Weights Max: 0.024842055760634953 Min: -0.015129882872222854\n","Layer 0 - Gradient Weights Max: 0.02827389089260907 Min: -0.025330619378925213\n","ReLU Activation - Max: 1.323278693629293 Min: 0.0\n","ReLU Activation - Max: 0.5180710344904823 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012122404814605168 Min: -0.02545850478179201\n","Layer 1 - Gradient Weights Max: 0.01698265821252894 Min: -0.01530128847882126\n","Layer 0 - Gradient Weights Max: 0.024078306390931007 Min: -0.023567448080421657\n","ReLU Activation - Max: 1.3914911106394614 Min: 0.0\n","ReLU Activation - Max: 0.5241461509324236 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014578730648670464 Min: -0.02855221065468535\n","Layer 1 - Gradient Weights Max: 0.018537551974814343 Min: -0.019222835979450063\n","Layer 0 - Gradient Weights Max: 0.024738295726977784 Min: -0.024936921002343085\n","ReLU Activation - Max: 1.2960843853572233 Min: 0.0\n","ReLU Activation - Max: 0.5435435912491196 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016144084099186776 Min: -0.030226448616402068\n","Layer 1 - Gradient Weights Max: 0.013095062655124248 Min: -0.02614975349270441\n","Layer 0 - Gradient Weights Max: 0.030506170454264788 Min: -0.032695149354793875\n","ReLU Activation - Max: 1.2562427412668677 Min: 0.0\n","ReLU Activation - Max: 0.5989662692205171 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01687323902949653 Min: -0.020298672910389667\n","Layer 1 - Gradient Weights Max: 0.02419968365934058 Min: -0.018135718790509445\n","Layer 0 - Gradient Weights Max: 0.031212158778329644 Min: -0.02881151858938774\n","ReLU Activation - Max: 1.4237061261610906 Min: 0.0\n","ReLU Activation - Max: 0.5652742657048708 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020218954665632755 Min: -0.019496792114275237\n","Layer 1 - Gradient Weights Max: 0.024789311811651393 Min: -0.030691248078734756\n","Layer 0 - Gradient Weights Max: 0.030097753074137857 Min: -0.027775499805042465\n","ReLU Activation - Max: 1.6090321604453666 Min: 0.0\n","ReLU Activation - Max: 0.5923707961928978 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018876857168984112 Min: -0.021953676959042212\n","Layer 1 - Gradient Weights Max: 0.0269380899896001 Min: -0.019264847145401894\n","Layer 0 - Gradient Weights Max: 0.039213642780393444 Min: -0.02829013061384068\n","ReLU Activation - Max: 1.4503798411232862 Min: 0.0\n","ReLU Activation - Max: 0.6076108373015994 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0185151344648967 Min: -0.018104776547365937\n","Layer 1 - Gradient Weights Max: 0.019645547174789675 Min: -0.017450492780026906\n","Layer 0 - Gradient Weights Max: 0.028701685322338837 Min: -0.029945366101375423\n","ReLU Activation - Max: 1.5640843186739664 Min: 0.0\n","ReLU Activation - Max: 0.6251920170519929 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02055222227256432 Min: -0.022488753784715463\n","Layer 1 - Gradient Weights Max: 0.029357220280905736 Min: -0.02256296772633668\n","Layer 0 - Gradient Weights Max: 0.029766988638921795 Min: -0.03276570973716868\n","ReLU Activation - Max: 1.496173748785187 Min: 0.0\n","ReLU Activation - Max: 0.5621768417614045 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016380219009829403 Min: -0.012299491692775876\n","Layer 1 - Gradient Weights Max: 0.018197747511558327 Min: -0.020174919093933055\n","Layer 0 - Gradient Weights Max: 0.035241524420731474 Min: -0.02826305365341588\n","ReLU Activation - Max: 1.5186424813633175 Min: 0.0\n","ReLU Activation - Max: 0.5872791274979274 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01315332090256082 Min: -0.011746113852601645\n","Layer 1 - Gradient Weights Max: 0.019382926888985283 Min: -0.017356002649867558\n","Layer 0 - Gradient Weights Max: 0.028604875255131744 Min: -0.03095330815544196\n","ReLU Activation - Max: 2.1257925363071006 Min: 0.0\n","ReLU Activation - Max: 0.5668179333117178 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016652072189538215 Min: -0.021756758274483007\n","Layer 1 - Gradient Weights Max: 0.013779939829670844 Min: -0.02370852906246271\n","Layer 0 - Gradient Weights Max: 0.03063651932350366 Min: -0.029310344205786903\n","ReLU Activation - Max: 1.1157909544936093 Min: 0.0\n","ReLU Activation - Max: 0.5874720030715459 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017344120963573382 Min: -0.019510402620679566\n","Layer 1 - Gradient Weights Max: 0.016202858091547635 Min: -0.019619185927194832\n","Layer 0 - Gradient Weights Max: 0.03536645533591393 Min: -0.025885668833923695\n","ReLU Activation - Max: 1.3053473683071666 Min: 0.0\n","ReLU Activation - Max: 0.605542388101083 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021268894346350358 Min: -0.01844453798690896\n","Layer 1 - Gradient Weights Max: 0.021556521718905736 Min: -0.024045037101095814\n","Layer 0 - Gradient Weights Max: 0.04419930009141489 Min: -0.02977255774857772\n","ReLU Activation - Max: 1.439682484083805 Min: 0.0\n","ReLU Activation - Max: 0.528472711061009 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013302470370340795 Min: -0.020690470676775072\n","Layer 1 - Gradient Weights Max: 0.024973636416243953 Min: -0.0182105033799797\n","Layer 0 - Gradient Weights Max: 0.026598864107822785 Min: -0.02894061137979908\n","ReLU Activation - Max: 1.5134740327074527 Min: 0.0\n","ReLU Activation - Max: 0.5634605960492499 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021824131092292497 Min: -0.017136551642853493\n","Layer 1 - Gradient Weights Max: 0.01995407954656283 Min: -0.023883615658032934\n","Layer 0 - Gradient Weights Max: 0.037065292156658304 Min: -0.026421954368805522\n","ReLU Activation - Max: 1.413688364640344 Min: 0.0\n","ReLU Activation - Max: 0.6648058095380998 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021993431079517577 Min: -0.02085620124495458\n","Layer 1 - Gradient Weights Max: 0.02018676330932977 Min: -0.020295141885378565\n","Layer 0 - Gradient Weights Max: 0.025936231593576598 Min: -0.0292376539701561\n","ReLU Activation - Max: 1.234579039525725 Min: 0.0\n","ReLU Activation - Max: 0.5989185341366037 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021490237935466068 Min: -0.013372301303420556\n","Layer 1 - Gradient Weights Max: 0.023547004525785804 Min: -0.023781810506327376\n","Layer 0 - Gradient Weights Max: 0.030386659290922838 Min: -0.030857980653994053\n","ReLU Activation - Max: 1.726255078753719 Min: 0.0\n","ReLU Activation - Max: 0.619198587609272 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01392947432385101 Min: -0.01632253574199165\n","Layer 1 - Gradient Weights Max: 0.02091574737713851 Min: -0.01848801996528846\n","Layer 0 - Gradient Weights Max: 0.022496411908057654 Min: -0.023546016711993304\n","ReLU Activation - Max: 1.8321167539003893 Min: 0.0\n","ReLU Activation - Max: 0.6036973071713965 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01642345610320844 Min: -0.020747491707929118\n","Layer 1 - Gradient Weights Max: 0.02045973643587726 Min: -0.020196098685526943\n","Layer 0 - Gradient Weights Max: 0.03970242407159932 Min: -0.0327746558447314\n","ReLU Activation - Max: 1.4599265790757525 Min: 0.0\n","ReLU Activation - Max: 0.6800989213093737 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01167668824968219 Min: -0.013271063283009046\n","Layer 1 - Gradient Weights Max: 0.016434494813895185 Min: -0.02140974998414663\n","Layer 0 - Gradient Weights Max: 0.024622525420726717 Min: -0.027340747514269362\n","ReLU Activation - Max: 1.6460126267649455 Min: 0.0\n","ReLU Activation - Max: 0.585104350566798 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013254528485237005 Min: -0.01300211094929966\n","Layer 1 - Gradient Weights Max: 0.015898202941080908 Min: -0.02416739222647253\n","Layer 0 - Gradient Weights Max: 0.03338113090874664 Min: -0.025772156571163794\n","ReLU Activation - Max: 1.4332285166081147 Min: 0.0\n","ReLU Activation - Max: 0.681098231819601 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019830593466888705 Min: -0.028242727487931343\n","Layer 1 - Gradient Weights Max: 0.020824289620894398 Min: -0.028233079564822187\n","Layer 0 - Gradient Weights Max: 0.029146340504135147 Min: -0.032324583869474455\n","ReLU Activation - Max: 1.559273721473855 Min: 0.0\n","ReLU Activation - Max: 0.7322629091070136 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019323609477636533 Min: -0.027454325753020127\n","Layer 1 - Gradient Weights Max: 0.022160329140710523 Min: -0.017728230290475756\n","Layer 0 - Gradient Weights Max: 0.027339272884889715 Min: -0.03736055692107506\n","ReLU Activation - Max: 1.5105784963260815 Min: 0.0\n","ReLU Activation - Max: 0.5321336780065292 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014571181597093617 Min: -0.014497616806703827\n","Layer 1 - Gradient Weights Max: 0.02562064189163597 Min: -0.02646138979980513\n","Layer 0 - Gradient Weights Max: 0.033099520723349485 Min: -0.03367638927777032\n","ReLU Activation - Max: 1.8475072003404511 Min: 0.0\n","ReLU Activation - Max: 0.6255817134997049 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.026403463169859396 Min: -0.02503957598936656\n","Layer 1 - Gradient Weights Max: 0.027041458624849322 Min: -0.018962995424390512\n","Layer 0 - Gradient Weights Max: 0.02752918858237057 Min: -0.03550243038167666\n","ReLU Activation - Max: 1.4722587713425497 Min: 0.0\n","ReLU Activation - Max: 0.5880417995601415 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016257156646851286 Min: -0.017797040884289517\n","Layer 1 - Gradient Weights Max: 0.017385043757772344 Min: -0.01864823830380369\n","Layer 0 - Gradient Weights Max: 0.031272493355067404 Min: -0.031164980743590417\n","ReLU Activation - Max: 1.6970610450862929 Min: 0.0\n","ReLU Activation - Max: 0.5705480723273544 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01330866306966917 Min: -0.0119495847369143\n","Layer 1 - Gradient Weights Max: 0.01605049909659182 Min: -0.01698307921471378\n","Layer 0 - Gradient Weights Max: 0.03373690180727454 Min: -0.0327923846228252\n","ReLU Activation - Max: 1.8164097421772707 Min: 0.0\n","ReLU Activation - Max: 0.6301952461609277 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015790934523437498 Min: -0.022014058356221822\n","Layer 1 - Gradient Weights Max: 0.021278239682489673 Min: -0.02375957449206792\n","Layer 0 - Gradient Weights Max: 0.030628583458528674 Min: -0.02869855097242027\n","ReLU Activation - Max: 1.4922305265867166 Min: 0.0\n","ReLU Activation - Max: 0.5810885292883047 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018785978800301143 Min: -0.015377484443599439\n","Layer 1 - Gradient Weights Max: 0.014954742439994369 Min: -0.017704926835869366\n","Layer 0 - Gradient Weights Max: 0.02709688748993849 Min: -0.025709129525342357\n","ReLU Activation - Max: 1.6635275853158291 Min: 0.0\n","ReLU Activation - Max: 0.5927976545905599 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01784253758885766 Min: -0.023872070830829106\n","Layer 1 - Gradient Weights Max: 0.021420031301677456 Min: -0.02790932296968096\n","Layer 0 - Gradient Weights Max: 0.03309022516618763 Min: -0.029899842008268052\n","ReLU Activation - Max: 1.6831968614217252 Min: 0.0\n","ReLU Activation - Max: 0.7036398685012928 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016228502512716986 Min: -0.018923804673085225\n","Layer 1 - Gradient Weights Max: 0.0252346544349067 Min: -0.016993994317674805\n","Layer 0 - Gradient Weights Max: 0.026775794476942557 Min: -0.02914971605426178\n","ReLU Activation - Max: 1.771350362891399 Min: 0.0\n","ReLU Activation - Max: 0.5811929774395224 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017857481927812564 Min: -0.020828856688129834\n","Layer 1 - Gradient Weights Max: 0.017933082965528854 Min: -0.02149848579297899\n","Layer 0 - Gradient Weights Max: 0.03294045997461364 Min: -0.028207944912039394\n","ReLU Activation - Max: 1.3725775551311876 Min: 0.0\n","ReLU Activation - Max: 0.5728031461689164 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014560089818593969 Min: -0.016663178506588626\n","Layer 1 - Gradient Weights Max: 0.015115961186607367 Min: -0.02164955249907205\n","Layer 0 - Gradient Weights Max: 0.028592530484011718 Min: -0.027149631772701795\n","ReLU Activation - Max: 1.4921760812754845 Min: 0.0\n","ReLU Activation - Max: 0.5959892467898574 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013489280800472068 Min: -0.014321294481159543\n","Layer 1 - Gradient Weights Max: 0.014753764921853974 Min: -0.015971585527242108\n","Layer 0 - Gradient Weights Max: 0.0267302284074826 Min: -0.03351259137353753\n","ReLU Activation - Max: 1.5577761521179763 Min: 0.0\n","ReLU Activation - Max: 0.7025288359980372 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015863641111062383 Min: -0.015988343504873492\n","Layer 1 - Gradient Weights Max: 0.017838777709972966 Min: -0.018522125255842584\n","Layer 0 - Gradient Weights Max: 0.024202072697786237 Min: -0.02659619948037077\n","ReLU Activation - Max: 1.4892742418338705 Min: 0.0\n","ReLU Activation - Max: 0.6827412022880973 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01845146099322462 Min: -0.021712303475736695\n","Layer 1 - Gradient Weights Max: 0.028975713580223068 Min: -0.028071604031180242\n","Layer 0 - Gradient Weights Max: 0.026821453192494526 Min: -0.03556113335215676\n","ReLU Activation - Max: 1.6698997078057178 Min: 0.0\n","ReLU Activation - Max: 0.6063272911082005 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019791483259020488 Min: -0.014381246774364108\n","Layer 1 - Gradient Weights Max: 0.020965246296594544 Min: -0.022688828668321382\n","Layer 0 - Gradient Weights Max: 0.02538803044898226 Min: -0.031212293499302796\n","ReLU Activation - Max: 1.2869880801513516 Min: 0.0\n","ReLU Activation - Max: 0.579003168798737 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.021112806992936346 Min: -0.024172009957341017\n","Layer 1 - Gradient Weights Max: 0.018470095744487706 Min: -0.02376838335358255\n","Layer 0 - Gradient Weights Max: 0.026396664444621427 Min: -0.033307872457595075\n","ReLU Activation - Max: 1.3700880413464007 Min: 0.0\n","ReLU Activation - Max: 0.5845379554946954 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0104734091330948 Min: -0.014642503594143335\n","Layer 1 - Gradient Weights Max: 0.014175542395000204 Min: -0.015378234554618151\n","Layer 0 - Gradient Weights Max: 0.029791339267000738 Min: -0.02554898219680304\n","ReLU Activation - Max: 2.1911514014961515 Min: 0.0\n","ReLU Activation - Max: 0.5751902464616592 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023692152515017394 Min: -0.020168876995862218\n","Layer 1 - Gradient Weights Max: 0.024230064744840088 Min: -0.017686157409749333\n","Layer 0 - Gradient Weights Max: 0.03657131136261845 Min: -0.031239490602304448\n","ReLU Activation - Max: 1.408830109645881 Min: 0.0\n","ReLU Activation - Max: 0.6505008782297943 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01609041429737204 Min: -0.02078290159042411\n","Layer 1 - Gradient Weights Max: 0.01952866827839019 Min: -0.02087092267595535\n","Layer 0 - Gradient Weights Max: 0.028100748755271177 Min: -0.031549404358717795\n","ReLU Activation - Max: 1.3032931786360369 Min: 0.0\n","ReLU Activation - Max: 0.5539147221781973 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015221483939149147 Min: -0.02147551308186446\n","Layer 1 - Gradient Weights Max: 0.01670172594878079 Min: -0.02706943876260133\n","Layer 0 - Gradient Weights Max: 0.03221072501623911 Min: -0.02555028672602366\n","ReLU Activation - Max: 1.4321205182893069 Min: 0.0\n","ReLU Activation - Max: 0.514969321492875 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016627387854809373 Min: -0.01911542346675111\n","Layer 1 - Gradient Weights Max: 0.017018470545442907 Min: -0.028558234933603873\n","Layer 0 - Gradient Weights Max: 0.039397758788540675 Min: -0.028231039912876968\n","ReLU Activation - Max: 1.2106687830370109 Min: 0.0\n","ReLU Activation - Max: 0.5746550183946867 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012069458114447311 Min: -0.02606560205398323\n","Layer 1 - Gradient Weights Max: 0.018334523984939812 Min: -0.027609324999779486\n","Layer 0 - Gradient Weights Max: 0.04013559013809462 Min: -0.02676732625219899\n","ReLU Activation - Max: 1.6358074335014838 Min: 0.0\n","ReLU Activation - Max: 0.6145147021589346 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01384541219664548 Min: -0.022406576414578418\n","Layer 1 - Gradient Weights Max: 0.015904473822413875 Min: -0.018187201293735702\n","Layer 0 - Gradient Weights Max: 0.028922999953524972 Min: -0.037557177930497745\n","ReLU Activation - Max: 1.438525532447088 Min: 0.0\n","ReLU Activation - Max: 0.5926726113113333 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01982319115885365 Min: -0.015132717449769809\n","Layer 1 - Gradient Weights Max: 0.014165844619009708 Min: -0.021539263506480597\n","Layer 0 - Gradient Weights Max: 0.02792070412990024 Min: -0.02728438454144355\n","ReLU Activation - Max: 1.5434741319435672 Min: 0.0\n","ReLU Activation - Max: 0.5511931194932882 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011713478920184277 Min: -0.016639306693266992\n","Layer 1 - Gradient Weights Max: 0.021694384952266065 Min: -0.017240185469221285\n","Layer 0 - Gradient Weights Max: 0.02644552455110024 Min: -0.028463127688898058\n","ReLU Activation - Max: 1.4231597234319262 Min: 0.0\n","ReLU Activation - Max: 0.5503994253034665 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012985392340265056 Min: -0.017935039375255153\n","Layer 1 - Gradient Weights Max: 0.019556124590710013 Min: -0.020974928177048064\n","Layer 0 - Gradient Weights Max: 0.034275022719637255 Min: -0.025444511180268376\n","ReLU Activation - Max: 1.4180200861537384 Min: 0.0\n","ReLU Activation - Max: 0.5654416622449192 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01101835495589871 Min: -0.020476497737786436\n","Layer 1 - Gradient Weights Max: 0.018306601019066636 Min: -0.02543380505921834\n","Layer 0 - Gradient Weights Max: 0.028654912302200516 Min: -0.0390942018061893\n","ReLU Activation - Max: 1.4531983688980652 Min: 0.0\n","ReLU Activation - Max: 0.6294325100838224 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013011497465380531 Min: -0.011791487949098862\n","Layer 1 - Gradient Weights Max: 0.019200906329628214 Min: -0.0201496480332739\n","Layer 0 - Gradient Weights Max: 0.028256420919597985 Min: -0.034720715085913095\n","ReLU Activation - Max: 1.5383224419905788 Min: 0.0\n","ReLU Activation - Max: 0.6565891475824212 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015754076172789816 Min: -0.013206182548737937\n","Layer 1 - Gradient Weights Max: 0.015340004506402593 Min: -0.022729641974466596\n","Layer 0 - Gradient Weights Max: 0.027558779611079737 Min: -0.029177485558075426\n","ReLU Activation - Max: 1.5218764916493357 Min: 0.0\n","ReLU Activation - Max: 0.6125937089263965 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01936128238922215 Min: -0.020297164091680354\n","Layer 1 - Gradient Weights Max: 0.024435693180822272 Min: -0.024428029046443853\n","Layer 0 - Gradient Weights Max: 0.026669393591767915 Min: -0.026410507147758253\n","ReLU Activation - Max: 1.4407531962607567 Min: 0.0\n","ReLU Activation - Max: 0.6171644714750949 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015311528681701983 Min: -0.01681132033939297\n","Layer 1 - Gradient Weights Max: 0.01707862110575789 Min: -0.02114243250092799\n","Layer 0 - Gradient Weights Max: 0.042044941067608435 Min: -0.029208483977321315\n","ReLU Activation - Max: 1.5711559379236533 Min: 0.0\n","ReLU Activation - Max: 0.6298881556877348 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015499559022093046 Min: -0.012275245671082007\n","Layer 1 - Gradient Weights Max: 0.016004206048222808 Min: -0.012918571675590807\n","Layer 0 - Gradient Weights Max: 0.024880483397595222 Min: -0.03085012945451905\n","ReLU Activation - Max: 2.0062042082701153 Min: 0.0\n","ReLU Activation - Max: 0.5419523575931323 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01600974863238867 Min: -0.019248361191024438\n","Layer 1 - Gradient Weights Max: 0.017490207257473203 Min: -0.021104794065297475\n","Layer 0 - Gradient Weights Max: 0.041724453856730966 Min: -0.03201664707625615\n","ReLU Activation - Max: 1.3422452233011815 Min: 0.0\n","ReLU Activation - Max: 0.6432609756466925 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01882904830014589 Min: -0.022295185959479585\n","Layer 1 - Gradient Weights Max: 0.026367066540728182 Min: -0.02655571081317411\n","Layer 0 - Gradient Weights Max: 0.03249630551272153 Min: -0.029320297341692392\n","ReLU Activation - Max: 1.3443697343656185 Min: 0.0\n","ReLU Activation - Max: 0.5862310427184335 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01970469062936019 Min: -0.016961003652297267\n","Layer 1 - Gradient Weights Max: 0.021094556955094514 Min: -0.019815727720420076\n","Layer 0 - Gradient Weights Max: 0.029908529530014494 Min: -0.03276444123497523\n","ReLU Activation - Max: 1.2926935708789296 Min: 0.0\n","ReLU Activation - Max: 0.5740203753822619 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.019869209682498532 Min: -0.029418721739436052\n","Layer 1 - Gradient Weights Max: 0.019378436531005322 Min: -0.027155932202084034\n","Layer 0 - Gradient Weights Max: 0.02582661077514012 Min: -0.025714153321183653\n","ReLU Activation - Max: 1.6274254090484592 Min: 0.0\n","ReLU Activation - Max: 0.6201397044299951 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.010199700604389123 Min: -0.017683682769057518\n","Layer 1 - Gradient Weights Max: 0.021683793966184264 Min: -0.019764305838024167\n","Layer 0 - Gradient Weights Max: 0.028715711720292213 Min: -0.030191919171299007\n","ReLU Activation - Max: 1.537718312278917 Min: 0.0\n","ReLU Activation - Max: 0.6469637493114061 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016717233495916355 Min: -0.01156778521333987\n","Layer 1 - Gradient Weights Max: 0.017388335846494934 Min: -0.019150937859892244\n","Layer 0 - Gradient Weights Max: 0.028181141394361252 Min: -0.02680290234371984\n","ReLU Activation - Max: 1.4671468722525771 Min: 0.0\n","ReLU Activation - Max: 0.6007718931886966 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02205168413094712 Min: -0.020581838603320277\n","Layer 1 - Gradient Weights Max: 0.020585773195225636 Min: -0.020857307751179457\n","Layer 0 - Gradient Weights Max: 0.029991422514003503 Min: -0.028531750143088702\n","ReLU Activation - Max: 1.3714083502259675 Min: 0.0\n","ReLU Activation - Max: 0.6406994149959742 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014140606985237386 Min: -0.01865543718680802\n","Layer 1 - Gradient Weights Max: 0.01836930653373406 Min: -0.020552279057340257\n","Layer 0 - Gradient Weights Max: 0.026976546083556038 Min: -0.02344482942793486\n","ReLU Activation - Max: 1.5989502768774473 Min: 0.0\n","ReLU Activation - Max: 0.6027425800109287 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01718001072823816 Min: -0.016797355694058505\n","Layer 1 - Gradient Weights Max: 0.01508209688169208 Min: -0.015758709447876978\n","Layer 0 - Gradient Weights Max: 0.025442471639998167 Min: -0.02621803620205115\n","ReLU Activation - Max: 1.2762040087100854 Min: 0.0\n","ReLU Activation - Max: 0.6101843964471744 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018812805618206866 Min: -0.03369650414389771\n","Layer 1 - Gradient Weights Max: 0.028846529028243285 Min: -0.02496024114017116\n","Layer 0 - Gradient Weights Max: 0.034362896956700396 Min: -0.03952165976697969\n","ReLU Activation - Max: 1.6073472039662873 Min: 0.0\n","ReLU Activation - Max: 0.5301886492201701 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015860949040371945 Min: -0.013749172407848415\n","Layer 1 - Gradient Weights Max: 0.01569369196778656 Min: -0.02241081919992776\n","Layer 0 - Gradient Weights Max: 0.02960577450155329 Min: -0.027178759551817276\n","ReLU Activation - Max: 1.6737270977010543 Min: 0.0\n","ReLU Activation - Max: 0.6154529349742845 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.022872322266819473 Min: -0.025939976164270902\n","Layer 1 - Gradient Weights Max: 0.019115073630127054 Min: -0.02527828751071079\n","Layer 0 - Gradient Weights Max: 0.03760205289791163 Min: -0.031259388189124965\n","ReLU Activation - Max: 1.6055117267236207 Min: 0.0\n","ReLU Activation - Max: 0.6651444936398508 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01435654379501655 Min: -0.02008450421491646\n","Layer 1 - Gradient Weights Max: 0.017524669270805655 Min: -0.023185514448479577\n","Layer 0 - Gradient Weights Max: 0.041508693367188265 Min: -0.027224908680645556\n","ReLU Activation - Max: 1.3918845659634989 Min: 0.0\n","ReLU Activation - Max: 0.6642810054583249 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01203309123671153 Min: -0.019460723062313178\n","Layer 1 - Gradient Weights Max: 0.02626181091120556 Min: -0.025775517361978837\n","Layer 0 - Gradient Weights Max: 0.0370476789593759 Min: -0.030931567432260124\n","ReLU Activation - Max: 1.4023823899774734 Min: 0.0\n","ReLU Activation - Max: 0.6370501110340738 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01146401946760944 Min: -0.012393777957386111\n","Layer 1 - Gradient Weights Max: 0.018177792264784734 Min: -0.01631088250762221\n","Layer 0 - Gradient Weights Max: 0.027987014474349706 Min: -0.027986430208026172\n","ReLU Activation - Max: 1.5247381210694764 Min: 0.0\n","ReLU Activation - Max: 0.5631158801676193 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.014124859228099574 Min: -0.021617264982515354\n","Layer 1 - Gradient Weights Max: 0.01995785619517886 Min: -0.017911151424413824\n","Layer 0 - Gradient Weights Max: 0.03278361526500466 Min: -0.03462074974150293\n","ReLU Activation - Max: 1.9155630346208483 Min: 0.0\n","ReLU Activation - Max: 0.6263805221756624 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.011648399751585955 Min: -0.014125886658360001\n","Layer 1 - Gradient Weights Max: 0.018058677127409044 Min: -0.02168229761665585\n","Layer 0 - Gradient Weights Max: 0.03407070822673677 Min: -0.029080704273531457\n","ReLU Activation - Max: 1.2543979296238414 Min: 0.0\n","ReLU Activation - Max: 0.5439437799387937 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016064601302895113 Min: -0.018167611252465812\n","Layer 1 - Gradient Weights Max: 0.021995905869871536 Min: -0.02505432238266353\n","Layer 0 - Gradient Weights Max: 0.025383644024242053 Min: -0.03326318994104148\n","ReLU Activation - Max: 1.3064663399251069 Min: 0.0\n","ReLU Activation - Max: 0.6190104127731778 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.013411223834660991 Min: -0.011828221899296758\n","Layer 1 - Gradient Weights Max: 0.015626188013260083 Min: -0.017185620233710737\n","Layer 0 - Gradient Weights Max: 0.026862054550195285 Min: -0.025043982185221067\n","ReLU Activation - Max: 1.4342343029993299 Min: 0.0\n","ReLU Activation - Max: 0.5316913332825562 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01698961457240743 Min: -0.024764256140561443\n","Layer 1 - Gradient Weights Max: 0.022059978280270457 Min: -0.024514445088165013\n","Layer 0 - Gradient Weights Max: 0.026975888622800884 Min: -0.026532185565719277\n","ReLU Activation - Max: 1.497555737753515 Min: 0.0\n","ReLU Activation - Max: 0.6132002508647665 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015171565765929254 Min: -0.013641212016042804\n","Layer 1 - Gradient Weights Max: 0.01630207565667516 Min: -0.01431731044321098\n","Layer 0 - Gradient Weights Max: 0.02496545335111136 Min: -0.02749587026727614\n","ReLU Activation - Max: 1.3134935400556222 Min: 0.0\n","ReLU Activation - Max: 0.5361461175277205 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01878353314262177 Min: -0.019245218964088796\n","Layer 1 - Gradient Weights Max: 0.02132266625134857 Min: -0.02318573197417891\n","Layer 0 - Gradient Weights Max: 0.03391770367936827 Min: -0.027345465154134104\n","ReLU Activation - Max: 1.5169534999412262 Min: 0.0\n","ReLU Activation - Max: 0.5969754999385561 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017057843280255666 Min: -0.019400839778677865\n","Layer 1 - Gradient Weights Max: 0.017805526592928403 Min: -0.02491102618989529\n","Layer 0 - Gradient Weights Max: 0.03489747710422659 Min: -0.028430177495409673\n","ReLU Activation - Max: 1.2535556308710019 Min: 0.0\n","ReLU Activation - Max: 0.744118356968823 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.012339103201175403 Min: -0.02772539656576528\n","Layer 1 - Gradient Weights Max: 0.023986961085214375 Min: -0.01664095257649336\n","Layer 0 - Gradient Weights Max: 0.02842008974271917 Min: -0.02671869480162081\n","ReLU Activation - Max: 1.457757396194533 Min: 0.0\n","ReLU Activation - Max: 0.6159289782471048 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015431575831205283 Min: -0.021044521874710717\n","Layer 1 - Gradient Weights Max: 0.02521970887762719 Min: -0.020583218583389146\n","Layer 0 - Gradient Weights Max: 0.026150736189425938 Min: -0.026184934587724164\n","ReLU Activation - Max: 1.798532817381827 Min: 0.0\n","ReLU Activation - Max: 0.701054329349497 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.020846406668848204 Min: -0.02258226902706281\n","Layer 1 - Gradient Weights Max: 0.014426452323974636 Min: -0.03815428578177869\n","Layer 0 - Gradient Weights Max: 0.04051667357184635 Min: -0.030977256769706533\n","ReLU Activation - Max: 1.6888922837552467 Min: 0.0\n","ReLU Activation - Max: 0.648472778639124 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01244786808647893 Min: -0.01690525846937944\n","Layer 1 - Gradient Weights Max: 0.01655879199785161 Min: -0.023304903540821376\n","Layer 0 - Gradient Weights Max: 0.03417572619236259 Min: -0.03564291624981189\n","ReLU Activation - Max: 1.4027165142774833 Min: 0.0\n","ReLU Activation - Max: 0.6042418886137454 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.008787906160032548 Min: -0.012796931225605167\n","Layer 1 - Gradient Weights Max: 0.012786244671300066 Min: -0.02161344321576188\n","Layer 0 - Gradient Weights Max: 0.03119330921630994 Min: -0.027179441110256546\n","ReLU Activation - Max: 1.4314055969741444 Min: 0.0\n","ReLU Activation - Max: 0.7019160637179481 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017671676054050785 Min: -0.02119518988757537\n","Layer 1 - Gradient Weights Max: 0.02109838198985721 Min: -0.020964014057669216\n","Layer 0 - Gradient Weights Max: 0.026825537499671873 Min: -0.028383538940894442\n","ReLU Activation - Max: 1.4080921809326787 Min: 0.0\n","ReLU Activation - Max: 0.6851957866845457 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0271512110203011 Min: -0.015234910340481847\n","Layer 1 - Gradient Weights Max: 0.020714583191304346 Min: -0.023380884057086774\n","Layer 0 - Gradient Weights Max: 0.03611538592065327 Min: -0.024900739781710774\n","ReLU Activation - Max: 1.3381650902570603 Min: 0.0\n","ReLU Activation - Max: 0.5691898655190162 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.023213263442468608 Min: -0.019512699371784865\n","Layer 1 - Gradient Weights Max: 0.020498317819418247 Min: -0.02226533637559073\n","Layer 0 - Gradient Weights Max: 0.03466281550965996 Min: -0.03550671503961929\n","ReLU Activation - Max: 1.117264201395133 Min: 0.0\n","ReLU Activation - Max: 0.6352882775508556 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.016603983284748002 Min: -0.011992122658873818\n","Layer 1 - Gradient Weights Max: 0.017724581633256582 Min: -0.016517715581186636\n","Layer 0 - Gradient Weights Max: 0.02803681078715559 Min: -0.026881020792093084\n","ReLU Activation - Max: 1.7970035847417383 Min: 0.0\n","ReLU Activation - Max: 0.6966909205325669 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.018143767522083437 Min: -0.017594803595165076\n","Layer 1 - Gradient Weights Max: 0.018560771936354522 Min: -0.02010040218420591\n","Layer 0 - Gradient Weights Max: 0.02483890895236845 Min: -0.03373651295762249\n","ReLU Activation - Max: 1.7016987079771717 Min: 0.0\n","ReLU Activation - Max: 0.6526866275671951 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.0116059186238638 Min: -0.013454526540195548\n","Layer 1 - Gradient Weights Max: 0.014267195486950107 Min: -0.018413427355074004\n","Layer 0 - Gradient Weights Max: 0.0323682211153127 Min: -0.028568526687526453\n","ReLU Activation - Max: 1.4866879360851166 Min: 0.0\n","ReLU Activation - Max: 0.6312813421801184 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02643224980307765 Min: -0.01978417795072243\n","Layer 1 - Gradient Weights Max: 0.02609478193084129 Min: -0.02858856451470683\n","Layer 0 - Gradient Weights Max: 0.024441219508299125 Min: -0.026583586896241483\n","ReLU Activation - Max: 1.3385769164667711 Min: 0.0\n","ReLU Activation - Max: 0.5897924821348294 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01869605982438541 Min: -0.025312942431975516\n","Layer 1 - Gradient Weights Max: 0.02050563160099047 Min: -0.026506635575175172\n","Layer 0 - Gradient Weights Max: 0.04138690056077486 Min: -0.026872567415299198\n","ReLU Activation - Max: 1.6184139923975374 Min: 0.0\n","ReLU Activation - Max: 0.5396417193016476 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01688506629833927 Min: -0.021712224444624294\n","Layer 1 - Gradient Weights Max: 0.024014631198629958 Min: -0.019782548204471947\n","Layer 0 - Gradient Weights Max: 0.02986918375225048 Min: -0.032805921877328725\n","ReLU Activation - Max: 1.380617421022883 Min: 0.0\n","ReLU Activation - Max: 0.5801672507191248 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01807733155470304 Min: -0.023433511247393447\n","Layer 1 - Gradient Weights Max: 0.021055082673648466 Min: -0.023524920418419974\n","Layer 0 - Gradient Weights Max: 0.032536501413329144 Min: -0.038407694992135\n","ReLU Activation - Max: 1.6682157206439963 Min: 0.0\n","ReLU Activation - Max: 0.6743944625825427 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.015074544392559014 Min: -0.015372583370000798\n","Layer 1 - Gradient Weights Max: 0.023209294770093203 Min: -0.021303610510781328\n","Layer 0 - Gradient Weights Max: 0.028468622110798647 Min: -0.030970425480609857\n","ReLU Activation - Max: 1.4933081146263238 Min: 0.0\n","ReLU Activation - Max: 0.612013133206984 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.01162501104592335 Min: -0.02760820958066705\n","Layer 1 - Gradient Weights Max: 0.02340217199780843 Min: -0.022665297307511552\n","Layer 0 - Gradient Weights Max: 0.025346739611088628 Min: -0.030693242399752928\n","ReLU Activation - Max: 1.7740779319402364 Min: 0.0\n","ReLU Activation - Max: 0.7040423471223916 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.009967357579230928 Min: -0.01255470597567265\n","Layer 1 - Gradient Weights Max: 0.016861663553457387 Min: -0.01567773473161221\n","Layer 0 - Gradient Weights Max: 0.0269566765588283 Min: -0.023718877634002947\n","ReLU Activation - Max: 1.25409940517698 Min: 0.0\n","ReLU Activation - Max: 0.5910158110302057 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.017624067039623624 Min: -0.013299352357314274\n","Layer 1 - Gradient Weights Max: 0.019449347414060514 Min: -0.02673180300034049\n","Layer 0 - Gradient Weights Max: 0.0356264601591636 Min: -0.024082783538149168\n","ReLU Activation - Max: 1.5641232113124066 Min: 0.0\n","ReLU Activation - Max: 0.615956540449582 Min: 0.0\n","Layer 2 - Gradient Weights Max: 0.02077821522498971 Min: -0.02476696364359008\n","Layer 1 - Gradient Weights Max: 0.028869487005725922 Min: -0.01990483302561224\n","Layer 0 - Gradient Weights Max: 0.03353492603785252 Min: -0.02827406179148496\n","Epoch 100/100 - Loss: 0.7435\n"]}],"source":["# Epochs and batch_size are defined as before\n","training_losses_mse = train_model_mse(nn_model_mse, X_train_normalized, y_train_encoded, X_test_normalized, y_test_encoded, epochs, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":707},"executionInfo":{"elapsed":1211,"status":"ok","timestamp":1711362322563,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"yQSdIWvaJJz5","outputId":"9a65f813-0f93-41df-cacb-149f05d22bff"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1400x700 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, len(training_losses_mse) + 1)\n","\n","# Plot training loss\n","plt.figure(figsize=(14, 7))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, training_losses_mse, label='Training Loss', marker='o')\n","plt.title('Training Loss over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# plt.subplot(1, 2, 2)\n","# plt.plot(epochs, test_accuracies_mse, label='Test Accuracy', color='red', marker='o')  # Assuming test_accuracies_mse exists\n","# plt.title('Test Accuracy over Epochs')\n","# plt.xlabel('Epoch')\n","# plt.ylabel('Accuracy')\n","# plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1711362427908,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"BG7OpDCjJgwJ","outputId":"fe797f1b-2d2e-4959-be1b-9f86e047b844"},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU Activation - Max: 2.2220450000903655 Min: 0.0\n","ReLU Activation - Max: 0.8803252085964569 Min: 0.0\n","Final Model Accuracy (with thresholding): 0.4274\n"]}],"source":["def calculate_accuracy_mse_thresholding(model, X_test, y_test):\n","    \"\"\"\n","    Calculates accuracy for the test set using MSE outputs and thresholding for classification.\n","\n","    Parameters:\n","    - model: The trained neural network model.\n","    - X_test: Test set features (normalized).\n","    - y_test: True labels for the test set, one-hot encoded.\n","\n","    Returns:\n","    - accuracy: The classification accuracy on the test set.\n","    \"\"\"\n","    predictions_continuous, _ = model.forward_pass(X_test)  # Model outputs continuous values\n","    predictions = np.argmax(predictions_continuous, axis=1)  # Classify as the highest output value\n","    true_labels = np.argmax(y_test, axis=1)  # True labels\n","\n","    # Calculate accuracy\n","    correct_predictions = np.sum(predictions == true_labels)\n","    total_predictions = predictions.shape[0]\n","    accuracy = correct_predictions / total_predictions\n","\n","    return accuracy\n","\n","# Assuming nn_model_mse is your trained model with MSE loss\n","final_accuracy = calculate_accuracy_mse_thresholding(nn_model_mse, X_test_normalized, y_test_encoded)\n","print(f\"Final Model Accuracy (with thresholding): {final_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1125,"status":"ok","timestamp":1711362486324,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"d0Tkmw8qJq1a","outputId":"0c8c8d99-d699-4b72-b91a-8c5a72956e9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU Activation - Max: 2.2220450000903655 Min: 0.0\n","ReLU Activation - Max: 0.8803252085964569 Min: 0.0\n","Confusion Matrix:\n","[[559  71 115 102 103 130  52  73 100  73]\n"," [111 555  89  62 132 119  83  64  78 136]\n"," [ 87  84 674  74  62 100  75  98  98  87]\n"," [101  59  88 549  68  86 111  98 129  98]\n"," [ 84  77  91  75 682  71  69 101 101  66]\n"," [ 87  76 100  65  72 582  85 125 120  99]\n"," [ 74 115 106  72 112  79 459  71 124 117]\n"," [ 73  52  82  83 108 118  68 642 101 129]\n"," [ 97  55  86  80  97 103  58  77 620  74]\n"," [ 68  59  81  68  73  95  89 124  89 661]]\n"]}],"source":["def get_predictions_mse_thresholding(model, X):\n","    \"\"\"\n","    Get class predictions for the dataset using the model with MSE loss and thresholding.\n","\n","    Parameters:\n","    - model: The trained model.\n","    - X: The input features (normalized).\n","\n","    Returns:\n","    - predictions: The predicted class labels as integers.\n","    \"\"\"\n","    predictions_continuous, _ = model.forward_pass(X)  # Continuous outputs from the model\n","    predictions = np.argmax(predictions_continuous, axis=1)  # Highest output value as the predicted class\n","    return predictions\n","\n","def compute_confusion_matrix(true_labels, predictions, num_classes):\n","    \"\"\"\n","    Computes the confusion matrix.\n","\n","    Parameters:\n","    - true_labels: The true class labels as integers.\n","    - predictions: The predicted class labels as integers.\n","    - num_classes: The number of classes.\n","\n","    Returns:\n","    - cm: The confusion matrix as a 2D numpy array.\n","    \"\"\"\n","    cm = np.zeros((num_classes, num_classes), dtype=int)\n","    for true, pred in zip(true_labels, predictions):\n","        cm[true, pred] += 1\n","    return cm\n","\n","# nn_model_mse is trained model, and X_test_normalized and y_test_encoded are test datasets\n","predictions = get_predictions_mse_thresholding(nn_model_mse, X_test_normalized)\n","true_labels = np.argmax(y_test_encoded, axis=1)  # Convert one-hot encoded labels back to integers\n","\n","# Compute confusion matrix\n","num_classes = 10\n","confusion_matrix = compute_confusion_matrix(true_labels, predictions, num_classes)\n","\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1549,"status":"ok","timestamp":1711362564728,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"RzwQtahQJ3Am","outputId":"732fa47c-8f31-453f-8ec0-10965bac2976"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class 0: F1 Score = 0.4112\n","Class 1: F1 Score = 0.4217\n","Class 2: F1 Score = 0.4568\n","Class 3: F1 Score = 0.4196\n","Class 4: F1 Score = 0.4662\n","Class 5: F1 Score = 0.4022\n","Class 6: F1 Score = 0.3705\n","Class 7: F1 Score = 0.4384\n","Class 8: F1 Score = 0.4266\n","Class 9: F1 Score = 0.4486\n"]}],"source":["def calculate_precision_recall_f1(confusion_matrix):\n","    \"\"\"\n","    Calculates precision, recall, and F1 score for each class given a confusion matrix.\n","\n","    Parameters:\n","    - confusion_matrix: The confusion matrix as a 2D numpy array.\n","\n","    Returns:\n","    - precision: Precision for each class.\n","    - recall: Recall for each class.\n","    - f1_scores: F1 score for each class.\n","    \"\"\"\n","    precision = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0)\n","    recall = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n","    f1_scores = 2 * (precision * recall) / (precision + recall)\n","\n","    # Handle potential NaNs due to division by zero\n","    f1_scores = np.nan_to_num(f1_scores)\n","\n","    return precision, recall, f1_scores\n","\n","precision, recall, f1_scores = calculate_precision_recall_f1(confusion_matrix)\n","\n","# Print F1 scores for each class\n","for i, f1_score in enumerate(f1_scores):\n","    print(f\"Class {i}: F1 Score = {f1_score:.4f}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNlTrj5TElsa9mdvKFMgxvD","mount_file_id":"1OwxxBeGYYTUGBbEAdrpfXnm7egZ-FwtQ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
